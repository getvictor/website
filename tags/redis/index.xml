<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Redis on Victor on Software</title><link>https://victoronsoftware.com/tags/redis/</link><description>Recent content in Redis on Victor on Software</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 18 Jul 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://victoronsoftware.com/tags/redis/index.xml" rel="self" type="application/rss+xml"/><item><title>Using a distributed lock in production distributed systems</title><link>https://victoronsoftware.com/posts/distributed-lock/</link><pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate><guid>https://victoronsoftware.com/posts/distributed-lock/</guid><description>&lt;img src="https://victoronsoftware.com/posts/distributed-lock/distributed-lock-headline.png" alt="Featured image of post Using a distributed lock in production distributed systems" />&lt;p>This article will present a problem we encountered in our production distributed system and how we solved it using a
distributed lock.&lt;/p>
&lt;h2 id="the-problem----data-inconsistency">&lt;a href="#the-problem----data-inconsistency" class="header-anchor">&lt;/a>The problem &amp;ndash; data inconsistency
&lt;/h2>&lt;p>Recently, we started using the Google Calendar API to monitor calendar changes. However, we noticed that it is possible
to receive a second callback while processing the first one. This second callback can lead to data inconsistency, race
conditions, and deadlocks.&lt;/p>
&lt;figure>&lt;img src="https://victoronsoftware.com/posts/distributed-lock/data-consistency-problem.svg"
alt="Sequence diagram demonstrating data inconsistency issue without a distributed lock">&lt;figcaption>
&lt;h4>Data inconsistency&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>In the above diagram, two servers, A and B, are processing calendar events. Server B receives a callback from Google
Calendar API stating that something has changed in the calendar. Google does not provide information about what event
changed, so server B must fetch the event of interest from the calendar. While server B is fetching the event, server A
also receives a callback. Server A also fetches the event from the calendar. Both servers now have the same event but
are unaware of each other&amp;rsquo;s actions. Server B updates the event with new information. Server A also updates the event
with different details, potentially overwriting or duplicating Server B&amp;rsquo;s changes. The calendar event is now in an
inconsistent state, as is the data in our database.&lt;/p>
&lt;h2 id="what-is-a-distributed-lock">&lt;a href="#what-is-a-distributed-lock" class="header-anchor">&lt;/a>What is a distributed lock?
&lt;/h2>&lt;p>A distributed lock is a mechanism that allows multiple servers to coordinate access to a shared resource. This mechanism
is widely used across the software industry to ensure data consistency in distributed systems.&lt;/p>
&lt;p>In our case, we need to make sure that only one server is processing a calendar event at a time. The distributed lock
will prevent the second server from processing the event until the first server completes.&lt;/p>
&lt;h2 id="implementation-of-distributed-lock">&lt;a href="#implementation-of-distributed-lock" class="header-anchor">&lt;/a>Implementation of distributed lock
&lt;/h2>&lt;p>We implemented a distributed lock using Redis. Redis is an in-memory data structure store that can be used as a
database, cache, and message broker. To acquire the lock, our server sets a key in Redis with a unique value using the
&lt;a class="link" href="https://redis.io/docs/latest/commands/set/" target="_blank" rel="noopener"
>Redis SET command&lt;/a>.&lt;/p>
&lt;pre tabindex="0">&lt;code>SET mykey &amp;#34;myvalue&amp;#34; NX PX 60000
&lt;/code>&lt;/pre>&lt;p>The &lt;code>NX&lt;/code> option only sets the key if it does not exist. The &lt;code>PX 60000&lt;/code> option sets the key&amp;rsquo;s expiration time to 60
seconds. This ensures that the lock is released if the server crashes or does not release it in a timely manner.&lt;/p>
&lt;p>To release the lock, we &lt;code>EVAL&lt;/code> a Lua script that checks if the key&amp;rsquo;s value matches the unique value set by the server.
If the values match, the script deletes the key using the
&lt;a class="link" href="https://redis.io/docs/latest/commands/del/" target="_blank" rel="noopener"
>Redis DEL command&lt;/a>.&lt;/p>
&lt;pre tabindex="0">&lt;code>if redis.call(&amp;#34;get&amp;#34;, KEYS[1]) == ARGV[1] then
return redis.call(&amp;#34;del&amp;#34;, KEYS[1])
else
return 0
end
&lt;/code>&lt;/pre>&lt;p>We only release the lock if the value matches, ensuring that the server that acquired the lock releases it.&lt;/p>
&lt;h2 id="distributed-lock-solution">&lt;a href="#distributed-lock-solution" class="header-anchor">&lt;/a>Distributed lock solution
&lt;/h2>&lt;p>With the distributed lock in place, we can make sure that only one server is processing a calendar event at a time.&lt;/p>
&lt;figure>&lt;img src="https://victoronsoftware.com/posts/distributed-lock/distributed-lock-solution.svg"
alt="Sequence diagram demonstrating using distributed lock and a processing queue to solve data consistency issue">&lt;figcaption>
&lt;h4>Using distributed lock with a processing queue&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>In the above diagram, server B receives a calendar callback and acquires a lock from Redis. Server A also gets a
callback but cannot acquire the lock since it has already been taken by Server B. Instead of waiting, Server A puts the
event in a processing queue. Once Server B finishes processing the event, it releases the lock. Server B then checks the
queue. Finding an event in the queue, the server starts a new worker process to process the events. The worker processes
all outstanding events in the queue and exits on completion.&lt;/p>
&lt;h2 id="waiting-to-acquire-the-lock">&lt;a href="#waiting-to-acquire-the-lock" class="header-anchor">&lt;/a>Waiting to acquire the lock
&lt;/h2>&lt;p>One issue we encountered was that another system process needed to acquire the lock. The process could keep trying to
obtain the lock, but there was no guarantee that it would be successful in a reasonable amount of time because it could
compete with other servers.&lt;/p>
&lt;h2 id="fairness-in-acquiring-the-lock">&lt;a href="#fairness-in-acquiring-the-lock" class="header-anchor">&lt;/a>Fairness in acquiring the lock
&lt;/h2>&lt;p>We implemented a fairness mechanism to ensure a priority process could acquire the lock.&lt;/p>
&lt;figure>&lt;img src="https://victoronsoftware.com/posts/distributed-lock/distributed-fair-lock.svg"
alt="Sequence diagram demonstrating using a fairness mechanism to acquire a distributed lock">&lt;figcaption>
&lt;h4>Cron job is guaranteed to acquire the lock&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>In the above diagram, the worker process has acquired the lock. Another process, a cron job, also needs to acquire the
lock. The cron job is a priority process that needs to run at a specific time. The cron job tries to acquire the lock
but fails because the worker process has it. The cron job sets a key in Redis that indicates that it wants to acquire
the lock next. This action tells the other servers not to acquire the lock. The cron job then retries acquiring the lock
until it is successful.&lt;/p>
&lt;h2 id="distributed-lock-code-on-github">&lt;a href="#distributed-lock-code-on-github" class="header-anchor">&lt;/a>Distributed lock code on GitHub
&lt;/h2>&lt;p>We implemented the distributed lock logic in Go. The crucial part of the code is in the
&lt;a class="link" href="https://github.com/fleetdm/fleet/blob/7ae1fe95272fbbda7efe1e320552539768498839/server/service/redis_lock/redis_lock.go" target="_blank" rel="noopener"
>redis_lock.go&lt;/a>
file.&lt;/p>
&lt;h2 id="distributed-lock-video">&lt;a href="#distributed-lock-video" class="header-anchor">&lt;/a>Distributed lock video
&lt;/h2>&lt;div class="video-wrapper">
&lt;iframe loading="lazy"
src="https://www.youtube.com/embed/833TqEPfF18"
allowfullscreen
title="YouTube Video"
>
&lt;/iframe>
&lt;/div>
&lt;p>&lt;em>Note:&lt;/em> If you want to comment on this article, please do so on the YouTube video.&lt;/p></description></item></channel></rss>