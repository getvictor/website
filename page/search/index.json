[{"content":" Fuzz testing is a software automated testing technique where random inputs are provided to the software under test. My background is in hardware verification, which uses sophisticated methodologies for pseudorandom testing, so I wanted to see what the Go library had to offer out of the box.\nA Go fuzz test can run as:\na normal unit test a test with fuzzing A fuzz test is written similarly to a normal unit test in a *_test.go file, with the following changes. It must have a Fuzz prefix and use the testing.F struct instead of the usual testing.T struct.\nfunc FuzzSample(f *testing.F) { Here is a workflow for using fuzz testing. First, you create a fuzz test. Then, you run it with fuzzing to automatically find failing corner cases and make any fixes. Thirdly, you include the test and the corner cases in your continuous integration testing suite.\nCreate a fuzz test When creating a fuzz test, you should provide a corpus of initial seed inputs. These are the inputs the test will use before applying randomization. Add the seed corpus with the Add method. For example:\nf.Add(tc.Num, tc.Name) f.Add(uint8(0), \u0026#34;\u0026#34;) The inputs to the Add method indicate which types will be fuzzed, and these types must match the subsequent call to the Fuzz method:\nf.Fuzz(func(t *testing.T, num uint8, name string) { The fuzz test can randomize any number of inputs, as long as they are one of the supported types.\nRun the test with fuzzing To run the test with fuzzing, use the -fuzz switch, like:\ngo test -fuzz FuzzSample The test will continuously run on all your CPUs until it fails, or you kill it:\n=== RUN FuzzSample fuzz: elapsed: 0s, gathering baseline coverage: 0/11 completed fuzz: elapsed: 0s, gathering baseline coverage: 11/11 completed, now fuzzing with 12 workers fuzz: elapsed: 3s, execs: 432199 (144036/sec), new interesting: 0 (total: 11) fuzz: elapsed: 6s, execs: 871147 (146328/sec), new interesting: 0 (total: 11) A sample failure:\nfailure while testing seed corpus entry: FuzzSample/49232526a5eabbdc fuzz: elapsed: 1s, gathering baseline coverage: 10/11 completed --- FAIL: FuzzSample (1.03s) --- FAIL: FuzzSample (0.00s) fuzz_test.go:21: Found 0 The failures are automatically added to the seed corpus. The seed corpus includes the initial inputs that were added with the Add method as well as any new fails. These new seed corpus files are automatically created in the testdata/fuzz/Fuzz* directory. Sample contents of one such file:\ngo test fuzz v1 byte(\u0026#39;\\x01\u0026#39;) string(\u0026#34;0a0000\u0026#34;) Adding the failure to the seed corpus means that the failing case will always run when this test is run again as a unit test or with fuzzing.\nNow, you must fix the failing test and continue the loop of fuzzing and fixing.\nInclude the test in continuous integration When checking in the test to your repository, you must either include the testdata/fuzz/Fuzz* files or convert those files into individual Add method calls in your test. Once the test is checked in, all the inputs in the seed corpus will run as part of the standard Go unit test flow.\nInitial impressions Fuzz testing appears to be a good approach to help the development of small functions with limited scope. The library documentation mentions the following about the function under test:\nThis function should be fast and deterministic, and its behavior should not depend on shared state.\nI plan to give fuzzing a try the next time I develop such a function. I will share the results on this blog.\nConcerns and Issues Native fuzzing support was added to Go in 1.18 and seems like a good initial approach. However, it feels limited in features and usability. The types of functions, fast and deterministic, that fuzzing is intended for are generally not very interesting when testing real applications. They are good examples for students learning how to code. However, more interesting testing scenarios include:\nFunctions accessing remote resources in parallel, such as APIs or databases Functions with asynchronous code Secondly, the fuzzing library does not provide a good way to guide the randomization of inputs and does not give feedback about the input state space already covered. It does provide line coverage information, but that doesn\u0026rsquo;t help for unknown corner cases.\nIf one of my inputs is intended to be a percentage, then I want most of the fuzzing to concentrate on the legal range of 0-100, as opposed to all numbers. This lack of constraints becomes a problem when adding additional inputs to the fuzzing function, as the available state space of inputs expands exponentially. If the state space of inputs is huge, there is no guarantee that fuzzing accomplished its goal of finding all corner cases, leaving the developer with a false sense of confidence in their code.\nLastly, the fuzz test is hard to maintain. The seed corpus is stored in files without any context regarding what corner case each seed is hitting. Software engineers unfamiliar with fuzz testing will find this extremely confusing. If the fuzz test needs to be extended in the future with additional inputs or different types, the old seed corpus will become useless. It will be worse than useless \u0026ndash; the test will not run, and the developer unfamiliar with fuzz testing will not have a clear idea why.\nfuzz_test.go:16: wrong number of values in corpus entry: 2, want 3 That said, understanding the fuzz testing limitation, I’m willing to try fuzz testing for more interesting test cases, such as database accesses. I will report my findings in a future post.\nGitHub gist: ","date":"2024-01-04T00:00:00Z","image":"https://victoronsoftware.com/posts/fuzz-testing-with-go/fuzz_hu3b462ca6a7ae1c67a10dfe9835433d85_465404_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/fuzz-testing-with-go/","title":"Fuzz testing in Go"},{"content":" In the ever-evolving landscape of device management and cybersecurity, understanding the mechanics behind tools like Fleet is not just about technical curiosity; it\u0026rsquo;s about empowering IT professionals to safeguard digital assets more effectively. Fleet gathers telemetry from various devices, from laptops to virtual machines, using osquery. At the heart of this system lies a crucial feature: Fleet policies.\nPolicies in Fleet are more than just rules; they are the gatekeepers of your device\u0026rsquo;s security, ensuring stringent adherence to security standards. By dissecting how Fleet policies operate \u0026ldquo;under the hood,\u0026rdquo; IT administrators and security professionals can gain invaluable insights. These insights allow for setting up efficient security protocols and rapid response to potential vulnerabilities, a necessity in a landscape where cyber threats are constantly evolving. This article delves into the inner workings of Fleet policies, providing you with the knowledge to better configure, manage, and leverage these policies for optimal device security and efficiency.\nPolicy creation Policies can be created from the web UI, the command-line interface called fleetctl with config files, or the REST API. The user creates a policy and selects which devices need to be checked using that policy. Policies can be global or team-specific.\nWhen a policy is created, a record for it is stored in the policies table of the MySQL database. A Fleet deployment consists of several servers behind a load balancer, so storing the record in the DB makes all servers aware of the new policy.\nPolicy execution Policies are executed on the devices, which are called hosts in Fleet, according to the FLEET_OSQUERY_POLICY_UPDATE_INTERVAL, which is set to 1 hour by default. This interval can be adjusted with the environment variable or set from the server’s command line.\nPolicies are simply SQL queries that return a true or false result, so the flow they use on the hosts is the same as other queries. Hosts check in with Fleet servers every 10 seconds (the default) and access the /api/v1/osquery/distributed/read API endpoint. The server checks when the policy was last executed to determine whether it should be executed again. If so, the server adds the policy to its response. For example, this policy in the server response checks if the macOS firewall is enabled:\n{ \u0026#34;queries\u0026#34;: { \u0026#34;fleet_policy_query_9\u0026#34;: \u0026#34;SELECT 1 FROM alf WHERE global_state \u0026gt;= 1;\u0026#34; }, \u0026#34;discovery\u0026#34;: { \u0026#34;fleet_policy_query_9\u0026#34;: \u0026#34;SELECT 1\u0026#34; } } Once the host has executed the policy, it writes the result to the server. The server updates the result in the policy_membership table of the MySQL database. At this point, the Host Details page on the web UI is updated with the policy result.\nForce policy execution on a device The user can force the host to execute all of its policies by clicking the Refetch link:\nPolicy results aggregation However, the main Policies page is not updated. This page shows the counts of all passing and failing hosts for each policy. A worker process on one of the Fleet servers updates it once an hour. The worker calculates the counts and stores them in the policy_stats table in the database. This is done for better performance of the UI. For customers with 100,000s of hosts that asynchronously report their policy results, calculating the passing and failing counts in real time was noticeably slow.\nSummary Understanding the intricacies of Fleet policies is essential for IT professionals managing a fleet of devices. This deep dive into the mechanics of Fleet policies — from creation to execution — provides you with the necessary insights to optimize your cybersecurity strategy effectively. By leveraging these policies, you can ensure stringent security standards across your network, enhancing your organization\u0026rsquo;s digital defense. As the cyber landscape evolves, tools like Fleet remain crucial in maintaining robust and responsive security protocols. We encourage you to apply these insights in your Fleet usage, and as always, we welcome your feedback and experiences in the Fleet community Slack channels.\nThis article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2023-12-30T00:00:00Z","image":"https://victoronsoftware.com/posts/understanding-the-intricacies-of-fleet-policies/_hu5aa3a7c771a8aa7077c6e8ce7a0c141c_322631_c7fb8a890b81808a2154c4746d61955b.png","permalink":"https://victoronsoftware.com/posts/understanding-the-intricacies-of-fleet-policies/","title":"Understanding the intricacies of Fleet policies"},{"content":" Fleet is an open-source platform for managing and gathering telemetry from devices such as laptops, desktops, VMs, etc. Osquery agents run on these devices and report to the Fleet server. One of Fleet’s features is the ability to query information from the devices in near real-time, called live queries. This article discusses how live queries work “under the hood.”\nWhy a live query? Live queries enable administrators to ask near real-time questions of all online devices, such as checking the encryption status of SSH keys across endpoints, or obtaining the uptime of each server within their purview. This enables them to promptly identify and address any issues, thereby reducing downtime and maintaining operational efficiency. These tasks, which would be time-consuming and complex if done manually, are streamlined through live queries, offering real-time insights into the status and posture of the entire fleet of devices helping IT and security.\nLive queries under the hood Live queries can be run from the web UI, the command-line interface called fleetctl, or the REST API. The user creates a query and selects which devices will run that query. Here is an example using fleetctl to obtain the operating system name and version for all devices:\nfleetctl query --query \u0026#34;select name, version from os_version;\u0026#34; --labels \u0026#34;All Hosts\u0026#34; When a client initiates a live query, the server first creates a Query Campaign record in the MySQL database. A Fleet deployment consists of several servers behind a load balancer, so storing the record in the DB makes all servers aware of the new query campaign.\nQuery campaign As devices called Hosts in Fleet check in with the servers, they receive instructions to run a query. For example:\n{ \u0026#34;queries\u0026#34;: { \u0026#34;fleet_distributed_query_140\u0026#34;: \u0026#34;SELECT name, version FROM os_version;\u0026#34; }, \u0026#34;discovery\u0026#34;: { \u0026#34;fleet_distributed_query_140\u0026#34;: \u0026#34;SELECT 1\u0026#34; } } Then, the osquery agents run the actual query on their host, and write the result back to a Fleet server. As a server receives the result, it publishes it to the common cache using Redis Pub/Sub.\nOnly the one server communicating with the client subscribes to the results. It processes the data from the cache, keeps track of how many hosts reported back, and communicates results back to the client. The web UI and fleetctl interfaces use a WebSockets API, and results are reported as they come in. The REST API, on the other hand, only sends a response after all online hosts have reported their query results.\nDiscover more Fleet’s live query feature represents a powerful tool in the arsenal of IT and security administrators. By harnessing the capabilities of live queries, tasks that once required extensive manual effort can now be executed swiftly and efficiently. This real-time querying ability enhances operational efficiency and significantly bolsters security and compliance measures across a range of devices.\nThe integration of Fleet with Osquery agents, the flexibility offered by interfaces like the web UI, fleetctl, and the REST API, and the efficient data handling through mechanisms like Redis Pub/Sub and WebSockets API all come together to create a robust, real-time telemetry gathering system. This system is designed to keep you informed about the current state of your device fleet, helping you make informed decisions quickly.\nAs you reflect on the capabilities of live queries with Fleet, consider your network environment\u0026rsquo;s unique challenges and needs. What questions could live queries help you answer about your devices? Whether it\u0026rsquo;s security audits, performance monitoring, or compliance checks, live queries offer a dynamic solution to address these concerns.\nWe encourage you to explore the possibilities and share your thoughts or questions. Perhaps you’re facing a specific query challenge or an innovative use case you’ve discovered. Whatever it may be, the world of live queries is vast and ripe for exploration. Join us in Fleet’s Slack forums to engage with a community of like-minded professionals and deepen your understanding of what live queries can achieve in your environment.\nAPI Documentation:\nRun live query with REST API Run live query with WebSockets This article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2023-12-29T00:00:00Z","image":"https://victoronsoftware.com/posts/get-current-telemetry-from-your-devices-with-live-queries/Live%20Query_hu64e3a4617825ba0468dff8df166d9a56_55465_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/get-current-telemetry-from-your-devices-with-live-queries/","title":"Get current telemetry from your devices with live queries"},{"content":" When starting to code in Go, I encountered the following situation. I needed to create an empty slice, so I did:\nslice := []string{} However, my IDE flagged it as a warning, and pointed me to this Go style guide passage, which recommended using a nil slice instead:\nvar slice []string This recommendation didn\u0026rsquo;t seem right to me. How can a nil variable be better? Won’t I run into issues like null pointer exceptions and other annoyances? Well, as it turns out, that’s not how slices work in Go. When declaring a nil slice, it is not the dreaded null pointer. It is still a slice. This slice includes a slice header, but its value just happens to be nil.\nThe main difference between a nil slice and an empty slice is the following. A nil slice compared to nil will return true. That’s pretty much it.\nif slice == nil { fmt.Println(\u0026#34;Slice is nil.\u0026#34;) } else { fmt.Println(\u0026#34;Slice is NOT nil.\u0026#34;) } When printing a nil slice, it will print like an empty slice:\nfmt.Printf(\u0026#34;Slice is: %v\\n\u0026#34;, slice) Slice is: [] You can append to a nil slice:\nslice = append(slice, \u0026#34;bozo\u0026#34;) You can loop over a nil slice, and the code will not enter the for loop:\nfor range slice { fmt.Println(\u0026#34;We are in a for loop.\u0026#34;) } The length of a nil slice is 0:\nfmt.Printf(\u0026#34;len: %#v\\n\u0026#34;, len(slice)) len: 0 And, of course, you can pass a nil slice by pointer. That’s right \u0026ndash; pass a nil slice by pointer.\nfunc passByPointer(slice *[]string) { fmt.Printf(\u0026#34;passByPointer len: %#v\\n\u0026#34;, len(*slice)) *slice = append(*slice, \u0026#34;bozo\u0026#34;) } You will get the updated slice if the underlying slice is reassigned.\npassByPointer(\u0026amp;slice) fmt.Printf(\u0026#34;len after passByPointer: %#v\\n\u0026#34;, len(slice)) len after passByPointer: 1 The code above demonstrates that a nil slice is not a nil pointer. On the other hand, you cannot dereference a nil pointer like you can a nil slice. This code causes a crash:\nvar nullSlice *[]string fmt.Printf(\u0026#34;Crash: %#v\\n\u0026#34;, len(*nullSlice)) Here\u0026rsquo;s the full gist:\n","date":"2023-12-28T00:00:00Z","image":"https://victoronsoftware.com/posts/nil-slice-versus-empty-slice-in-go/cover_hu8e4888644a780aeecee2f1f52535fba1_61530_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/nil-slice-versus-empty-slice-in-go/","title":"Nil slice versus empty slice in Go"},{"content":" Matter is a recent open-source standard for connecting devices such as light switches, door locks, motion sensors, and many others. The major goals of the standard are compatibility and interoperability. This means that you will no longer need to be an expert hacker when trying to control devices from multiple manufacturers under a single application. Apple, Amazon, and Google are some of the major members driving the standard. This is great news for the majority of adopters who haven’t yet fully embraced home automation and security.\nThe Matter specification is published by the Connectivity Standards Alliance (CSA) and includes a software development kit. Version 1.0 of the specification was released in October of 2022. In 2023, we saw a slew of new devices and software upgrades compatible with Matter. Version 1.2 of the specification was published in October of 2023. However, this latest specification is still missing support for a few important device categories such as cameras and major appliances. Cameras are a top priority for the CSA, and we may see Matter-compatible cameras in 2024.\nMatter is an important step for the management of IoT devices because it finally brings true interoperability where it has been sorely missing for so many years. No longer will device manufacturers need to decide and budget precious software resources to support Amazon Alexa, Google Home, Apple HomeKit, or another connectivity hub. Customers will no longer be locked into using one of the major home automation providers. And home automation solutions from smaller companies will come onto the market.\nAn important feature of Matter is multi-admin, which means that devices can be read and controlled by multiple clients. In Matter terminology, the device, such as a motion sensor, is called a server or node, and the applications controlling it are called clients. For example, a light switch may be simultaneously controlled by the manufacturer’s app, by Alexa, and by the user\u0026rsquo;s hand-written custom API client.\nMulti-admin support means that a home or business may use one application to control their locks, switches, and security sensors, and another application for reading telemetry from those same devices. Businesses will find it easier to integrate physical security with cyber security. For example, suppose a business’s device management server uses Matter to subscribe to the office door lock. It receives an alert that User A has entered their code. Afterwards, via regular scheduled telemetry, it notices a successful login to Computer B. The business SIEM (security information and event management) system should immediately flag this suspicious sequence of events.\nOf course, the example above can be accomplished today by writing some custom code or using a third party integration. What Matter brings is scalability to such security approaches. The code and integration will no longer need to be redone for each new device and version that comes onto the market.\n","date":"2023-12-20T00:00:00Z","image":"https://victoronsoftware.com/posts/physical-security-meets-cybersecurity-with-matter/cover_hu240e6838189f9e304ae612d50248a293_9583_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/physical-security-meets-cybersecurity-with-matter/","title":"Physical security meets cybersecurity with Matter"},{"content":" A prepared statement is a feature of modern databases intended to help execute the same SQL statement multiple times. For example, the following statement is a prepared statement:\nSELECT id, name FROM users WHERE email = ?; The presence of an unspecified parameter, labeled “?”, makes it a prepared statement. When a prepared statement is sent to the database, it is compiled, optimized, and stored in memory on the database server. Subsequently, the client application may execute the same prepared statement multiple times with different parameter values. This results in a speedup.\nPrepared statements are well suited for long and complex queries that require significant compilation and optimization times. They are kept prepared on the DB server, and the application must only pass the parameters to execute them.\nAnother benefit of using prepared statements is the protection they provide against SQL injection. The application does not need to properly escape the parameter values provided to the statement. Because of this protection, many experts recommend always using prepared statements for accessing the database.\nHowever, by always using prepared statements for accessing the database, we force the SQL driver to send the extra prepare command for every ad-hoc statement we execute. The driver sends the following commands:\nPrepare the statement Execute statement with given parameters Close the statement (and deallocate the prepared statement created above) Another issue with prepared statements is the memory requirement. In large application deployments with large numbers of connections, prepared statements can crash your environment. This issue happened to one of our customers.\nA prepared statement is only valid for a single session, which typically maps to a single database connection. If the application runs multiple servers, with many connections, it may end up storing a prepared statement for each one of those sessions.\nFor example, given 100 servers with 100 connections each, we have 10,000 connections to the database. Assuming a memory requirement of 50 KB per prepared statement (derived from the following article), we arrive at the maximum memory requirement of:\n10,000 * 50 KB = 500 MB per single saved prepared statement Some databases also have limits on the number of prepared statements. MySQL’s max_prepared_stmt_count defaults to 16,382 for the entire server. Yes, this is a global limit, and not per session. In the above example, if the application uses prepared statements for every database access, then each database connection will always be using up 1 short-lived prepared statement. A short-lived prepared statement is the prepared statement, as we described above, that will be created for the purposes of executing one statement, and then immediately deallocated afterwards. This means the above application running with a default MySQL config cannot explicitly save any prepared statements \u0026ndash; 10,000 transient prepared statements + 10,000 saved prepared statements is greater than the max_prepared_stmt_count of 16,382.\nThis is extremely inconvenient for application developers, because they must keep track of:\nThe number of saved prepared statements they are using How many application servers are running How many database connections each server has The prepared statement limits of the database This detail can easily be overlooked when scaling applications.\nIn the end, is it really worth using prepared statements, and especially saved prepared statements, in your application? Yes, saved prepared statements can offer performance advantages, especially for complex queries executed frequently. However they must also be kept in check.\nA few ways to mitigate prepared statement issues for large application deployments include:\nLimit the number of database connections per application server Increase the prepared statement limit on the database server(s) Limit the maximum lifespan of connections. When closing a connection, the database will deallocate all prepared statements on that connection. ","date":"2023-12-14T00:00:00Z","image":"https://victoronsoftware.com/posts/sql-prepared-statements-are-broken-when-scaling-applications/cover_hu7756c223ae4ac2243c5cccab03f7b66b_14192_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/sql-prepared-statements-are-broken-when-scaling-applications/","title":"SQL prepared statements are broken when scaling applications"},{"content":" At Fleet, our developer documentation is spread out throughout the codebase, contained in a multitude of README and Markdown files. Much of the documentation is hosted on our webpage, but not all of it.\nAs developers, we need to be able to quickly search project documentation to find answers to specific questions, such as:\nHow to do a database migration How to run integration tests How to deploy a development version of to a specific OS One solution is to use grep or the IDE environment to search for these answers. Unfortunately, such search methods are not optimized for text search \u0026ndash; they frequently generate no relevant results or too many results that we must manually wade through to find the most appropriate. Specialized documentation search tools, on the other hand, prioritize headings and whole words, search for plural versions of the search terms, and offer other conveniences.\nThe lack of good search capability for engineering docs must be solved in order to scale engineering efforts. It is an issue because of the following side effects:\nEngineers are discouraged from writing documentation Documentation may be duplicated Senior developers are frequently interrupted when people can’t find relevant documentation One solution is to use a documentation service, such as a team wiki, Confluence, or GitBook. GitBook integrates with git repositories, and can push documentation changes. GitBook is free for personal use, which makes it easy to use for open source projects such as fleet and osquery. That said, GitBook is a newcomer to the space, and is still reaching maturity.\nTo set up a personal GitBook, make a fork of the open source projects that contain documentation you’d like to search, and integrate them into GitBook spaces. After indexing is complete, you’ll be able to effectively search the documentation.\nTo keep the forks in sync with the parent repositories, we use Github Actions. Github Actions are free for open source projects. Searching GitHub for sync-fork returned several examples. We ended up using the following:\nname: Sync Fork on: schedule: - cron: \u0026#39;55 * * * *\u0026#39; workflow_dispatch: # on button click jobs: sync: runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v4 with: token: ${{ secrets.WORKFLOW_TOKEN }} fetch-depth: 0 - name: Configure Git run: | git config --global user.name \u0026#34;GitHub Actions Bot\u0026#34; git config --global user.email \u0026#34;actions@github.com\u0026#34; - name: Merge upstream run: | git remote add upstream https://github.com/fleetdm/fleet.git git fetch upstream main git checkout main git merge upstream/main git push origin main The WORKFLOW_TOKEN above is a GitHub personal access token (PAT) that allows reading and writing workflows in this repository. This token is not needed for repositories without workflows.\nIn addition to project documentation, GitBook can be used to synchronize personal documentation that’s being held in a private repository. There are several git-based notebook applications on the market. In addition, Markdown notes from the popular note-taking app Obsidian can be kept in GitHub. This turns GitBook into a true personalized developer documentation database \u0026ndash; one place to search through developer docs as well as your own private notes.\n","date":"2023-11-30T00:00:00Z","image":"https://victoronsoftware.com/posts/you-need-a-personal-dev-docs-db-gitbook/cover_hu54e2e2056c8855962719d82a53cb41e3_206961_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/you-need-a-personal-dev-docs-db-gitbook/","title":"You need a personal dev docs DB (GitBook)"},{"content":" Traditionally, network routers used dedicated bare metal machines. However, in the last several years, we’ve seen a rise in software-based routers that can be deployed either on bare metal, on a VM, or even on a container. This means these virtual routers can be used to replace existing router software on an older router. They can run in the cloud. Or they can be installed on do-it-yourself (DIY) hardware. A couple popular open source software-based routers are pfSense and OPNsense.\nWhy use a virtual router? For one, these routers offer enterprise-level features such as build-in VPN support, traffic analysis, and extensive diagnostics, among others. Another reason is that having a virtual router gives you the ability to experiment \u0026ndash; you can install multiple routers on top of your hypervisor, and try all of them out. A third reason is that the virtual router may be only one of many VMs that you run on your hardware. You can use the same piece of hardware to run a router, an ad-blocking service, a media server, and other applications.\nAdvanced virtual router installation and set up When setting up our virtual router, we chose to use PCI Passthrough to allow the virtual router direct access to the NIC hardware. Direct access to hardware improves the latency of our internet traffic. In addition, we wanted our hypervisor to sit behind the router, and not be exposed to the public. This reduces the attack surface for potential bad agents. However, routing hypervisor traffic through the router made our setup a bit tricker. It is like the chicken or the egg dilemma \u0026ndash; how do you put your hypervisor behind the router when the hypervisor is responsible for managing the router? Below is the approach we used when installing pfSense on top of Proxmox Virtual Environment (PVE).\nFor the initial installation, we did not use PCI Passthrough and instead used a virtual network bridge (vmbr0). We configured the router VM to start on boot.\nInitial virtual router configuration This allowed us to continue controlling the virtual router through the PVE web GUI. We set up the router and enabled access to it through the serial interface, which we used in the next step. Then, we put the system into its final configuration.\nFinal virtual router configuration In order to finish configuring, we had to plug in a monitor and keyboard into our hardware. We accessed the virtual router via the serial interface from the PVE command line:\nqm terminal 100 We updated the WAN interface to use eth0. At this point, the LAN interface eth1 had access to the internet.\nIn addition, we added a second LAN interface for the network bridge (vmbr0). We made sure firewall configurations for both LAN interfaces were the same.\nNext, from the PVE command line, we updated the PVE IP and gateway to point at the router by modifying the following files.\n/etc/network/interfaces /etc/hosts After rebooting PVE, we had access to the internet and to the PVE Web GUI from our new LAN.\nUpdating router software Using a virtual router with PCI Passthrough creates a unique challenge when doing software updates. What if the new version doesn’t work? What if you lose all internet access.\nWe can mitigate potential issues. First, we recommend always making a backup of the router VM when upgrading. That way we can easily roll back the change. Switching to a backup, however, requires keyboard and monitor access to your hardware, since it must be done via the PVE command line.\nAnother way to safely upgrade is to spin up a second VM running updated router software. The second VM can be either from a backup or brand new. This VM should use virtual network bridges for its connections. Once it is properly configured, we can stop the first router VM and switch the port connections to the second VM. This flow also requires accessing the router via the serial interface to update the WAN/LAN interfaces.\n","date":"2023-11-22T00:00:00Z","image":"https://victoronsoftware.com/posts/setting-up-a-virtual-router/cover_hu019a91ef521bad7337cb0514ecaaaefc_16004_120x120_fill_q75_box_smart1.jpeg","permalink":"https://victoronsoftware.com/posts/setting-up-a-virtual-router/","title":"Setting up a virtual router"},{"content":" Keychains are the macOS’s method to track and protect secure information such as passwords, private keys, and certificates. Traditionally, the keychain information was stored in files, such as:\n/Library/Keychains/System.keychain /Library/Keychains/apsd.keychain /System/Library/Keychains/SystemRootCertificates.keychain /Users/\u0026lt;username\u0026gt;/Library/Keychains/login.keychain-db In the last several years, Apple also introduced data protection keychains, such as the iCloud Keychain. Although the file-based keychains above are on the road to deprecation in favor of data protection keychains, current macOS systems still heavily rely on them. It is unclear when, if ever, these keychains will be replaced by data protection keychains.\nInspecting file-based keychains has gotten more difficult as Apple deprecated many of the APIs associated with them, such as SecKeychainOpen. In addition, excessive use of these deprecated APIs may result in corruption of the Login Keychain, as mentioned in this osquery issue. By NOT using the deprecated APIs, the user only has access to the following keychains from the above list:\n/Library/Keychains/System.keychain /Users/\u0026lt;username\u0026gt;/Library/Keychains/login.keychain-db Root certificates are missing. And the APSD (Apple Push Service Daemon) keychain is missing, which is used for device management, among other things.\nSo, how can app developers and IT professionals continue to have access to ALL of these keychain files?\nOne way is to continue using deprecated APIs until they stop working. We recommend making a secure copy of the keychain files before accessing them with the APIs.\nAnother option is to use the macOS security command line tool. For example, to list root certificates, do the following:\nsudo security find-certificate -a /System/Library/Keychains/SystemRootCertificates.keychain A third, and hardest, option is to parse the keychain files yourself. Some details on the keychain format are available. Please leave a comment if you or someone else has created a tool to parse Apple keychains.\nThe fourth option is to use an existing tool, such as osquery. Osquery is an open-source tool built for security and IT professionals. Osquery developers are working on fixing any issues to continue providing access to macOS keychain files via the following tables:\ncertificates keychain_acls keychain_items ","date":"2023-11-16T00:00:00Z","permalink":"https://victoronsoftware.com/posts/inspecting-keychain-files-on-macos/","title":"Inspecting keychain files on macOS"},{"content":" Authorization is giving permission to a user to do an action on the server. As developers, we must ensure that users are only allowed to do what they are authorized.\nOne way to ensure that authorization has happened is to loudly flag when it hasn\u0026rsquo;t. This is how we do it at Fleet Device Management.\nIn our code base, we use the go-kit library. Most of the general endpoints are created in the handler.go file. For example:\n// user-authenticated endpoints ue := newUserAuthenticatedEndpointer(svc, opts, r, apiVersions...) ue.POST(\u0026#34;/api/_version_/fleet/trigger\u0026#34;, triggerEndpoint, triggerRequest{}) Every endpoint calls kithttp.NewServer and wraps the endpoint with our AuthzCheck. From handler.go:\ne = authzcheck.NewMiddleware().AuthzCheck()(e) return kithttp.NewServer(e, decodeFn, encodeResponse, opts...) This means that after the business logic is processed, the AuthzCheck is called. This check ensures that authorization was checked. Otherwise, an error is returned. From authzcheck.go:\n// If authorization was not checked, return a response that will // marshal to a generic error and log that the check was missed. if !authzctx.Checked() { // Getting to here means there is an authorization-related bug in our code. return nil, authz.CheckMissingWithResponse(response) } This additional check is useful during our development and QA process, to ensure that authorization always happens in our business logic.\nThis article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2023-11-10T00:00:00Z","permalink":"https://victoronsoftware.com/posts/catch-missed-authorization-checks-during-software-development/","title":"Catch missed authorization checks during software development"}]