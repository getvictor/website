[{"content":"This article is part of a series on building a complete production-ready Chrome extension.\nIn the first article of the series, we introduced the main parts of a Chrome extension \u0026ndash; the service worker (background script), content script, and popup. This article will add a fourth part to our Chrome extension \u0026ndash; an options page. This page will allow users to configure the extension\u0026rsquo;s behavior and settings.\nWhy add an options page? An options page is a user-friendly way for users to customize the extension to their needs. It can be as simple as a few checkboxes or as complex as a full settings page with multiple tabs. Users can access the options page from the Chrome extension\u0026rsquo;s popup or the Chrome extension\u0026rsquo;s context menu.\nAdding an options page To add an options page to our Chrome extension, we need to create a new HTML file and add it to the extension\u0026rsquo;s manifest. Our options page will be a simple HTML file with some JavaScript to handle user interactions.\nOur example options page will allow a user to exclude a web host, like victoronsoftware.com, from the extension\u0026rsquo;s functionality. The extension will store the excluded host in the extension\u0026rsquo;s local storage.\nWe will add several new files, update the extension\u0026rsquo;s configuration, and update the existing files.\nAdd a new file options.html to the static folder:\n\u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Advanced options\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;options.css\u0026#34;/\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h2\u0026gt;Advanced options\u0026lt;/h2\u0026gt; \u0026lt;h3\u0026gt;Web host to exclude\u0026lt;/h3\u0026gt; \u0026lt;input id=\u0026#34;exclude_host\u0026#34; type=\u0026#34;text\u0026#34;/\u0026gt; \u0026lt;script src=\u0026#34;options.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Add a new file options.css to the static folder:\n#exclude_host { margin: 5px; width: 500px; } Add a new file options.ts to the src folder, which will watch for changes on the options page:\nimport {Message, StoredConfig} from \u0026#34;./common\u0026#34; chrome.storage.sync.get(null, (data) =\u0026gt; { const config = data as StoredConfig const excludeHost = config.excludeHost ?? \u0026#34;\u0026#34; const input = document.getElementById( `exclude_host`, ) as HTMLInputElement input.value = excludeHost input.addEventListener(\u0026#34;change\u0026#34;, (event) =\u0026gt; { if (event.target instanceof HTMLInputElement) { const updatedExcludeWebsite = event.target.value const updatedConfig: StoredConfig = {excludeHost: updatedExcludeWebsite} void chrome.storage.sync.set(updatedConfig) // Send message to content script in all tabs void chrome.tabs .query({}) .then((tabs) =\u0026gt; { const message: Message = {excludeHost: updatedExcludeWebsite} for (const tab of tabs) { if (tab.id !== undefined) { chrome.tabs .sendMessage(tab.id, message) .catch(() =\u0026gt; { // We ignore tabs without a proper URL, like chrome://extensions/ // Do nothing }) } } }) .catch((error: unknown) =\u0026gt; { console.error(\u0026#34;Could not query tabs\u0026#34;, error) }) } }) }) The types in src/common.ts need to be updated to include the new excludeHost field:\nexport interface Message { enabled?: boolean excludeHost?: string } export interface StoredConfig { enabled?: boolean item?: string excludeHost?: string } The content script src/content.ts needs to be updated to handle the new excludeHost setting. See the updated file here.\nWe need to update the extension\u0026rsquo;s manifest to include the new options page. Add the following to the manifest.json file:\n{ \u0026#34;options_page\u0026#34;: \u0026#34;static/options.html\u0026#34; } In addition, we need to tell webpack to compile the new options.ts file. Update the webpack.common.ts file to include the new entry point:\nentry: { background: \u0026#34;./src/background.ts\u0026#34;, content: \u0026#34;./src/content.ts\u0026#34;, popup: \u0026#34;./src/popup.ts\u0026#34;, options: \u0026#34;./src/options.ts\u0026#34;, }, Testing the options page To test the options page, load the extension in Chrome and right-click on its icon. You should see a new Options item. Clicking on this item will open the options page.\nNew Options selection Adding a link to the options page We can add a link to the popup to make it easier for users to access the options page. Update the popup.html file to include a link to the options page:\n\u0026lt;button id=\u0026#34;go-to-options\u0026#34; class=\u0026#34;button-link\u0026#34;\u0026gt;Advanced options\u0026lt;/button\u0026gt; Add the CSS for the link:\n.button-link { margin: 10px; background: none !important; border: none; padding: 0 !important; font-family: arial, sans-serif; color: #069; text-decoration: underline; cursor: pointer; } And add the TypeScript in popup.ts to handle the link click:\n// Options page const optionsElement = document.querySelector(\u0026#34;#go-to-options\u0026#34;) if (!optionsElement) { console.error(\u0026#34;Could not find options element\u0026#34;) } else { optionsElement.addEventListener(\u0026#34;click\u0026#34;, function () { // This code is based on Chrome for Developers documentation // eslint-disable-next-line @typescript-eslint/no-unnecessary-condition if (chrome.runtime.openOptionsPage) { chrome.runtime.openOptionsPage().catch((error: unknown) =\u0026gt; { console.error(\u0026#34;Could not open options page\u0026#34;, error) }) } else { window.open(chrome.runtime.getURL(\u0026#34;options.html\u0026#34;)) } }) } We will now see the Advanced options link in the popup. Clicking on the link will take the user to the options page.\nPopup with Advanced options link Embedded options page Instead of a full options page, Chrome extensions can use an embedded options page. However, this approach was confusing and not user-friendly because Chrome takes the user to the extension details page. We recommend using a dedicated options page. To try an embedded options page, add the following to the manifest.json file:\n{ \u0026#34;options_ui\u0026#34;: { \u0026#34;page\u0026#34;: \u0026#34;options.html\u0026#34;, \u0026#34;open_in_tab\u0026#34;: false } } Next steps In the next part of this series, we will focus on the look of our popup and options page. We will add CSS to make it visually appealing and user-friendly.\nOptions page code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/5-options-page\nOptions page video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-07-03T00:00:00Z","image":"https://victoronsoftware.com/posts/add-options-to-chrome-extension/chrome-extension-options-headline_huf92c8489584674a78387b036c7cb3f4c_966643_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/add-options-to-chrome-extension/","title":"Adding options page to Chrome extension (2024)"},{"content":"This article covers how git merge works with GitHub pull requests. We will focus on the use case where developers want to keep their feature branches updated with the main branch. After completing the feature work, developers create a pull request to merge their feature branch into the main branch.\ngit merge Pull request after a merge Updating a protected feature branch with a pull request What is a merge in version control? Git is a distributed version control system that allows multiple developers to work on the same codebase. When developers work on different branches, they must merge their changes into the main branch. A merge is the process of combining changes from one branch into another branch, resulting in a single branch that contains the changes from both branches.\ngit merge The standard git merge command takes each commit from one branch and applies it to another. The final commit has two parent commits: one from the current branch and one from the merged branch.\nIn the following example, we have a branch that we want to merge into the main branch:\ngit merge of two branches before merge The git log of the main branch shows the commit history:\ncommit e493ac8fea4e0efe125a561b9014313bec41a489 (HEAD -\u0026gt; main, origin/main) Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:31:12 2024 -0500 m3 commit b79e810cb86405061dc979ce4fc05fe36a724256 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:29:41 2024 -0500 m2 commit eaccad9476b472dbfb3cdfbd17088425be75b7b1 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:26:55 2024 -0500 m1 commit 4265c0a30b7b6f03d93331bc112261393c97ee1d Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:25:40 2024 -0500 first commit And the git log of the branch shows the commit history:\ncommit 2afb078875a84095327ab2ef7c83711534c5eef8 (HEAD -\u0026gt; branch, origin/branch) Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:30:20 2024 -0500 b2 commit 9fc53c2ca9a58637a5d433de4c6150b832d4d275 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:28:25 2024 -0500 b1 commit eaccad9476b472dbfb3cdfbd17088425be75b7b1 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:26:55 2024 -0500 m1 commit 4265c0a30b7b6f03d93331bc112261393c97ee1d Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:25:40 2024 -0500 first commit We merge the branch into the main branch:\ngit checkout main git merge branch The resulting commit history shows all the commits from both branched as well as the final empty merge commit pointing to the two parent commits: e493ac8 2afb078:\ncommit 09d569bd079162643462dde112246f4167f14889 (HEAD -\u0026gt; main) Merge: e493ac8 2afb078 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 21:03:01 2024 -0500 Merge branch \u0026#39;branch\u0026#39; commit e493ac8fea4e0efe125a561b9014313bec41a489 (origin/main) Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:31:12 2024 -0500 m3 commit 2afb078875a84095327ab2ef7c83711534c5eef8 (origin/branch, branch) Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:30:20 2024 -0500 b2 commit b79e810cb86405061dc979ce4fc05fe36a724256 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:29:41 2024 -0500 m2 commit 9fc53c2ca9a58637a5d433de4c6150b832d4d275 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:28:25 2024 -0500 b1 commit eaccad9476b472dbfb3cdfbd17088425be75b7b1 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:26:55 2024 -0500 m1 commit 4265c0a30b7b6f03d93331bc112261393c97ee1d Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:25:40 2024 -0500 first commit git merge of two branches after merge The result would be the same if instead we merged the main branch into the branch branch:\ngit checkout branch git merge main except the final merge commit would be slightly different:\ncommit 06713a38ed38c12f599c6e810ee50d4cacfe2de7 (HEAD -\u0026gt; branch) Merge: 2afb078 e493ac8 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:32:01 2024 -0500 Merge branch \u0026#39;main\u0026#39; into branch The merged changes on branch can be pushed to the remote repository without issues because the remote branch can be fast-forwarded to the new commit.\ngit push origin branch What is a fast-forward merge? A fast-forward merge is a merge where the base branch (target branch) has no new commits. In this case, git moves the target branch to the commit of the source branch. It is a fast-forward merge because the target branch is moved forward to the new commit.\nA fast-forward merge does not lose any history \u0026ndash; it is always possible to undo a fast-forward merge.\ngit push does not, by default, allow a merge that is not a fast-forward. Use the\u0026rsquo;- force\u0026rsquo; option to enable a merge that is not a fast-forward.\nUndo a git merge The above merge can be undone by resetting the branch to the commit before the merge, which is one of the parent commits of the merge commit.\nThis command resets the branch to the commit before the merge:\ngit reset --hard 2afb078875a84095327ab2ef7c83711534c5eef8 git rebase Another way to combine changes from one branch into another is to use git rebase. This command applies the changes from the source branch to the target branch by reapplying the commits from the source branch to the target branch.\nThe git rebase command will modify the commit history of the source branch. In our pull request examples, we will use git merge instead of git rebase to preserve all the commit histories.\nPull request after a merge When working on a feature branch, developers often want to update their branch with the latest changes from the main to make sure their feature works with the newest code. We start this process with the above-described git merge command, where we merge the main branch into the branch branch.\nAfter the merge, the developer can create a GitHub pull request to merge the branch into the main branch.\nGitHub pull request after merge Note that the commit history only shows the commits from the branch and the merge commit. The main commits are not shown in the pull request.\nGitHub shows a few options for merging the pull request:\nMerge pull request Create a merge commit: This option creates a new merge commit combining the changes from the branch and the main branches. This is the default option. Squash and merge: This option combines all the commits from the branch into a single commit and merges that commit into the main branch. Rebase and merge: This option applies the changes from the branch onto the main branch by rebasing the commits from the branch onto the main branch. Selecting Create a merge commit results in the following commit history:\nCommit history after pull request The last two commits are both merge commits.\ncommit 1eb0af8c0e9ad16a0267d8abd1ce667f125ab7e8 (HEAD -\u0026gt; main, origin/main) Merge: e493ac8 22d2107 Author: Victor Lyuboslavsky \u0026lt;******\u0026gt; Date: Sun Jun 23 07:35:57 2024 -0500 Merge pull request #1 from getvictor/branch My pull request commit 22d2107b49bca56e67b7d4e800d93f93378a0956 (origin/branch, branch) Merge: 2afb078 e493ac8 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 21:25:39 2024 -0500 Merge branch \u0026#39;main\u0026#39; into branch The PR merge commit points to the previous merge commit and the last commit on main.\nDiagram of commit history after pull request Updating a protected feature branch We have two branches in this example: main and feature. Both branches are protected, meaning that changes to them must be made through a pull request. We want to update the feature branch with the latest changes from the main branch.\nWe can do this by merging the main branch into the feature branch, creating a new branch, and creating a pull request.\ngit checkout feature git merge main git checkout -b feature-update git push origin feature-update And create a pull request to merge feature-update into feature.\nCreate a PR to merge into feature branch This pull request shows all the commits from the main branch and the merge commit. This commit history is problematic because the PR may trigger a code review from the code owners of the files that were already reviewed in previous pull requests to the main branch.\nCommits from feature-update branch After the merge, the feature branch commit history looks like:\nCommit history of feature branch after PR Now, we create a pull request to merge the update feature branch into the main branch.\nPR to merge feature branch into main After the merge, the main branch commit history looks like:\nCommit history of main after PR from feature branch The last three commits are merge commits.\ncommit 59caaf1cc5103099f850c32f1729c5ffe3525404 (HEAD -\u0026gt; main, origin/main) Merge: e493ac8 dcbc117 Author: Victor Lyuboslavsky \u0026lt;******\u0026gt; Date: Sun Jun 23 08:56:53 2024 -0500 Merge pull request #3 from getvictor/feature Feature commit dcbc117e5811e683f1074d947bc25da21b5fa5f6 (origin/feature) Merge: 2afb078 373dd82 Author: Victor Lyuboslavsky \u0026lt;******\u0026gt; Date: Sun Jun 23 08:45:44 2024 -0500 Merge pull request #2 from getvictor/feature-update Feature update commit 373dd82672a4879bfcf3b29c4feb97004359adfe (origin/feature-update, feature-update, feature) Merge: 2afb078 e493ac8 Author: Victor on Software \u0026lt;\u0026gt; Date: Sun Jun 23 08:03:22 2024 -0500 Merge branch \u0026#39;main\u0026#39; into feature Diagram of commit history after two pull requests Merging a pull request with Squash and merge If the final pull request is merged with Squash and merge, the commit history will look like:\nCommit history of main after squash and merge The last commit is a single commit that combines all the changes from the feature branch. The merge commits and all other commits are eliminated.\nThe downside of Squash and merge is that the commit history is lost. The commit history is useful for debugging, understanding the changes made, and keeping ownership of the changes when multiple developers work on the same feature branch.\nWatch how git merge works with GitHub pull requests Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-06-26T00:00:00Z","image":"https://victoronsoftware.com/posts/git-merges-and-pull-requests/git-merges-and-pull-requests-feature_hu4fff4fff277b1556912d5db635a4a4b0_1727683_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/git-merges-and-pull-requests/","title":"How git merge works with GitHub pull requests"},{"content":" Setting up linting with ESLint and typescript-eslint Setting up formatting with Prettier Adding linting and formatting to CI This article is part of a series on building a maintainable Chrome extension.\nIn the previous article, we added TypeScript code for communicating between parts of a Chrome extension. This check-in will be our starting point for this article. This article will add linting and formatting to our TypeScript code, which will help us catch errors and enforce a consistent code style for larger teams.\nWhat is linting? Linting is the process of running a static code analysis program to analyze code for potential errors. Linters can catch syntax errors, typos, and other common mistakes that can lead to bugs. They can also enforce coding standards, such as indentation, variable naming, and other style rules.\nWhat is formatting? Formatting automatically changes the code\u0026rsquo;s appearance to match a specific style guide. Formatting tools can automatically add or remove whitespace, change indentation, and reformat code to make it more readable. Formatting tools can enforce a consistent code style across a project.\nWhy use linting and formatting tools? Linters and formatters work together to help developers write better code and accelerate the development process \u0026ndash; linters flag errors, while formatters automatically enforce a consistent code style.\nTogether, they can help prevent bugs, improve code quality, and make it easier for developers to read and understand the code. The result is cleaner, more maintainable code that uses many coding best practices and is easier to work with.\nLinting can also teach developers about best practices and help them avoid common pitfalls. For example, a linter can flag misused promises, such as missing await or uncaught errors.\nSetting up linting with ESLint and typescript-eslint To set up linting for TypeScript code, we will use ESLint with the typescript-eslint plugin. ESLint is a popular linter that can analyze JavaScript and TypeScript code. The typescript-eslint plugin adds TypeScript-specific rules to ESLint.\nTo set up ESLint with typescript-eslint, we need to install the following packages:\nnpm install --save-dev eslint @eslint/js @types/eslint__js typescript-eslint Next, we need to create an ESLint configuration file. We will create an eslint.config.mjs file at the root of our project:\n// @ts-check import eslint from \u0026#39;@eslint/js\u0026#39; import tseslint from \u0026#34;typescript-eslint\u0026#34; const config = tseslint.config( eslint.configs.recommended, ...tseslint.configs.recommendedTypeChecked, ...tseslint.configs.stylisticTypeChecked, { ignores: [\u0026#34;dist/**/*\u0026#34;], }, { languageOptions: { parserOptions: { project: true, tsconfigRootDir: import.meta.dirname, }, }, }, ) export default config This configuration file sets up ESLint with the recommended TypeScript type-checked rules and ignores our dist directory containing the webpack-generated bundles.\nWhy use the .mjs extension instead of .js for the configuration file? We are using .mjs extension for the configuration file to take advantage of ECMAScript modules. Using ES modules allows us to import and export modules using the import and export keywords. There are other ways to enable ECMAScript modules in JavaScript for our project, but this is the simplest way for just one JavaScript file. Our TypeScript files already use ECMAScript modules via these included recommended tsconfig.json settings:\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;module\u0026#34;: \u0026#34;commonjs\u0026#34;, \u0026#34;esModuleInterop\u0026#34;: true, If we used .js extension for the configuration file, we would need to use require and module.exports syntax. Otherwise, we would get an error like this:\n/Users/victor/work/create-chrome-extension/4-linting-and-formatting/eslint.config.js:3 import eslint from \u0026#39;@eslint/js\u0026#39; ^^^^^^ SyntaxError: Cannot use import statement outside a module at internalCompileFunction (node:internal/vm:77:18) at wrapSafe (node:internal/modules/cjs/loader:1288:20) at Module._compile (node:internal/modules/cjs/loader:1340:27) at Module._extensions..js (node:internal/modules/cjs/loader:1435:10) at Module.load (node:internal/modules/cjs/loader:1207:32) at Module._load (node:internal/modules/cjs/loader:1023:12) at cjsLoader (node:internal/modules/esm/translators:356:17) at ModuleWrap.\u0026lt;anonymous\u0026gt; (node:internal/modules/esm/translators:305:7) at ModuleJob.run (node:internal/modules/esm/module_job:218:25) at async ModuleLoader.import (node:internal/modules/esm/loader:329:24) Running ESLint We can run ESLint from the command line using the following command:\n./node_modules/.bin/eslint . Alternatively, we can use npx, which is a package runner tool that comes with npm:\nnpx eslint . This command will run ESLint on all TypeScript files in the current directory and subdirectories. ESLint will output any errors or warnings it finds in the code, such as:\n/Users/victor/work/create-chrome-extension/4-linting-and-formatting/src/background.ts 14:17 error Unsafe member access .enabled on an `any` value @typescript-eslint/no-unsafe-member-access /Users/victor/work/create-chrome-extension/4-linting-and-formatting/src/content.ts 51:9 error Unsafe assignment of an `any` value @typescript-eslint/no-unsafe-assignment 74:17 error Unsafe member access .enabled on an `any` value @typescript-eslint/no-unsafe-member-access 76:9 error Unsafe assignment of an `any` value @typescript-eslint/no-unsafe-assignment 76:27 error Unsafe member access .enabled on an `any` value @typescript-eslint/no-unsafe-member-access /Users/victor/work/create-chrome-extension/4-linting-and-formatting/src/popup.ts 9:23 error Unsafe argument of type `any` assigned to a parameter of type `boolean` @typescript-eslint/no-unsafe-argument 11:37 error Promise returned in function argument where a void return was expected @typescript-eslint/no-misused-promises 23:34 error Unsafe member access .title on an `any` value @typescript-eslint/no-unsafe-member-access 23:50 error Unsafe member access .url on an `any` value @typescript-eslint/no-unsafe-member-access 43:5 error Unsafe assignment of an `any` value @typescript-eslint/no-unsafe-assignment ✖ 10 problems (10 errors, 0 warnings) At this point, we should fix the errors and warnings that ESLint has found in our code.\nWe can also update the scripts section of our package.json file to run ESLint with npm run:\n\u0026#34;scripts\u0026#34;: { \u0026#34;lint\u0026#34;: \u0026#34;eslint .\u0026#34;, Now we can run ESLint with the following command:\nnpm run lint Setting up formatting with Prettier To format TypeScript code, we will use Prettier. Prettier is a popular code formatter that automatically formats code to match a specific style guide.\nTo set up Prettier, we need to install the following package:\nnpm install --save-dev --save-exact prettier Next, create a .prettierignore file in the root of our project to ignore the dist directory:\n/dist By default, Prettier ignores the node_modules directory.\nNext, create a .prettierrc file in the root of our project to configure Prettier:\n{ \u0026#34;semi\u0026#34;: false } We will use the default Prettier settings but turn off the semi rule to remove semicolons from the end of TypeScript lines. Removing semicolons is a common style choice in modern JavaScript and TypeScript code.\nRunning Prettier We can run Prettier from the command line using the following command:\nnpx prettier --write . This command will format all eligible files in the current directory and subdirectories.\nWe can also update the scripts section of our package.json file to run Prettier with the following command:\n\u0026#34;scripts\u0026#34;: { \u0026#34;format\u0026#34;: \u0026#34;prettier --write .\u0026#34;, \u0026#34;format-check\u0026#34;: \u0026#34;prettier --check .\u0026#34;, npm run format will format all eligible files, while npm run format-check will check if the files are formatted.\nAdding linting and formatting to continuous integration (CI) We will use GitHub Actions to automate linting and formatting checks on every pull request and commit to our main branch. This will make sure all code changes are linted and formatted correctly on the main branch.\nThis automatic check means that all contributors can expect that the code they are working on uses a consistent style and meets a quality standard. Consistency is beneficial for open-source projects where contributors may not be familiar with the codebase.\nTo set up GitHub Actions, create a .github/workflows/lint-and-format.yml file in the root of our git repository:\nname: Lint check, format check, and build on: push: branches: - main paths: # We only run the workflow if the code in these files/directories changes - \u0026#39;.github/workflows/lint-and-format.yml\u0026#39; # This file - \u0026#39;4-linting-and-formatting/**\u0026#39; # The working directory for this article pull_request: paths: - \u0026#39;.github/workflows/lint-and-format.yml\u0026#39; - \u0026#39;4-linting-and-formatting/**\u0026#39; # This allows a subsequently queued workflow run to interrupt previous runs concurrency: group: ${{ github.workflow }}-${{ github.head_ref || github.run_id}} cancel-in-progress: true defaults: run: shell: bash working-directory: ./4-linting-and-formatting permissions: contents: read jobs: lint-format-build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v4 - name: Install dependencies run: | npm install --no-save - name: Format check and lint run: | npm run format-check npm run lint - name: Build run: | npm run build Since our git repository is shared by multiple projects (from various articles), we use the paths key to only run the workflow when the code in the 4-linting-and-formatting directory changes.\nAfter pushing our code to GitHub and waiting for the GitHub Actions workflow to run, we can see the results in the Actions tab of our repository. We can see the linting and formatting checks, as well as the build step:\nGitHub Actions workflow results For more details on GitHub Actions workflows, see our article on reusing GitHub Actions workflows and steps.\nAdding stricter linting rules to typescript-eslint The recommended ruleset is a good starting point for linting TypeScript code. However, we can add stricter rules to catch even more potential issues in our code. It is easiest to start with strict rules early in the project when fixing issues is relatively painless. Otherwise, it is a good idea to gradually add stricter rules to avoid overwhelming developers with too many errors and warnings.\nTo switch to a stricter, more opinionated ruleset, replace tseslint.configs.recommendedTypeChecked with tseslint.configs.strictTypeChecked in the eslint.config.mjs file.\nESLint rules can be configured or disabled using configuration comments in the code or the ESLint configuration file. For more details, see the ESLint configure rules.\nNext steps In the next part of this series, we will add an options page to our Chrome extension. This page will allow users to configure the extension\u0026rsquo;s behavior and settings.\nLinting and formatting TypeScript code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/4-linting-and-formatting\nLinting and formatting TypeScript video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-06-19T00:00:00Z","image":"https://victoronsoftware.com/posts/linting-and-formatting-typescript/linting-and-formatting-headline_hu32dff4e139c82c0ce5159261aa2e516e_34503_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/linting-and-formatting-typescript/","title":"Linting and formatting TypeScript in Chrome extension (2024)"},{"content":"This article is part of a series on building a production-ready Chrome extension.\nIn the previous article, we set up our Chrome extension with TypeScript support and the webpack bundler. This article will build on that code, dive into the new APIs, and cover message-passing communication between different parts of our Chrome extension.\nCommunication between parts of a Chrome extension As we covered in the first article, a Chrome extension consists of three main parts:\nservice worker (background script) content script popup These parts need to communicate with each other. For example, a popup needs to send a message to a content script to change the appearance of a webpage. Or a background script needs to send a message to a popup to update the user interface based on the page that\u0026rsquo;s being visited.\nCommunication in a Chrome extension One way to communicate between these parts is to use the local storage via the chrome.storage APIs. We do not recommend this method because it is slow and can cause performance issues. This method is slow because it is not synchronous \u0026ndash; the scripts need to check the storage for changes periodically. A better way to communicate between extension parts is to use message passing.\nWhat is message passing? In computer science, message passing is a method for communicating between different processes or threads. A process or thread sends a message to another process or thread, which receives the message and acts on it. This method is often used in distributed systems, where processes run on different machines and need to communicate with each other. The sender sends a message, and the receiver decodes it and executes the appropriate code.\nMessage passing in a Chrome extension Message passing is a way to communicate between different parts of a Chrome extension. Its main advantage is that it\u0026rsquo;s fast and efficient. When a message is sent, the receiver gets it immediately and can respond to it right away.\nMessage passing is done in Chrome extensions using the chrome.runtime.sendMessage, chrome.tabs.sendMessage and chrome.runtime.onMessage functions. Here\u0026rsquo;s how it works:\nThe sender calls chrome.runtime.sendMessage or chrome.tabs.sendMessage with the message to send. The receiver listens for messages using chrome.runtime.onMessage.addListener The receiver processes the incoming message and, optionally, responds to the message. Message passing from a popup to a content script Let\u0026rsquo;s see how we can use message passing to communicate between a popup and a content script. We will send a message when the user toggles the enable slider in the popup, which will enable or disable the content script\u0026rsquo;s processing. We will use the chrome.tabs.sendMessage function to send a message to a specific tab ID.\nIn the popup script (popup.ts), we send a message to all the tabs when we detect a change in the top slider:\n// Send message to content script in all tabs const tabs = await chrome.tabs.query({}) for (const tab of tabs) { // Note: sensitive tab properties such as tab.title or tab.url can only be accessed for // URLs in the host_permissions section of manifest.json chrome.tabs.sendMessage(tab.id!, {enabled: event.target.checked}) .then((response) =\u0026gt; { console.info(\u0026#34;Popup received response from tab with title \u0026#39;%s\u0026#39; and url %s\u0026#34;, response.title, response.url) }) .catch((error) =\u0026gt; { console.warn(\u0026#34;Popup could not send message to tab %d\u0026#34;, tab.id, error) }) } In the content script (content.ts), we listen for the message and process it:\n// Listen for messages from popup. chrome.runtime.onMessage.addListener((request, sender, sendResponse) =\u0026gt; { if (request.enabled !== undefined) { console.log(\u0026#34;Received message from sender %s\u0026#34;, sender.id, request) enabled = request.enabled if (enabled) { observe() } else { observer.disconnect() } sendResponse({title: document.title, url: window.location.href}) } }) When the user toggles the slider in the popup, the popup sends a message to all tabs. The receiving tab will print this message to the Chrome Developer Tools console.\nContent script received message Then, the popup will receive a response from the content script with the tab\u0026rsquo;s title and URL. This response prints to the Inspect Popup console. System tabs like chrome://extensions/ will not respond to messages.\nPopup received response Message passing from a popup to the service worker (background script) To send a message to the service worker, we must use the chrome.runtime.sendMessage function instead of chrome.tabs.sendMessage. The service worker does not have a tab ID, so we cannot use chrome.tabs.sendMessage.\nchrome.runtime.sendMessage({enabled: event.target.checked}) .then((response) =\u0026gt; { console.info(\u0026#34;Popup received response\u0026#34;, response) }) .catch((error) =\u0026gt; { console.warn(\u0026#34;Popup could not send message\u0026#34;, error) }) In the service worker script (background.ts), we listen for the message and process it:\nchrome.runtime.onMessage.addListener((request, sender, sendResponse) =\u0026gt; { if (request.enabled !== undefined) { console.log(\u0026#34;Service worker received message from sender %s\u0026#34;, sender.id, request) sendResponse({message: \u0026#34;Service worker processed the message\u0026#34;}) } }) Message passing from a content script to the popup and service worker To send a message from the content script, use the chrome.runtime.sendMessage function. The popup and service worker can listen and receive this message.\nMessage passing from the service worker (background script) to a content script and the popup Use the chrome.tabs.sendMessage function to send a message to the content script. Use the chrome.runtime.sendMessage function to send a message to the popup.\nThe code for sending a message from the service worker is the same as the code for sending a message from the popup. The receiving code in the content and popup scripts is also the same.\nNext steps In the next part of this series, we will add linting and formatting tools to our Chrome extension. These tools increase the quality of our code and increase engineering velocity for projects with multiple developers.\nChrome extension with webpack and TypeScript code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/3-message-passing\nMessage passing in a Chrome extension video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-06-12T00:00:00Z","image":"https://victoronsoftware.com/posts/message-passing-in-chrome-extension/message-passing-headline_hu1ed8561cedf8e23a106e917b9f219ce7_1364307_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/message-passing-in-chrome-extension/","title":"Message passing in Chrome extension (2024)"},{"content":"We are creating a series of articles on building a production-ready Chrome extension. In this series, we cover the basics of building a Chrome extension, how to set up industry leading development tooling, and how to test and deploy your extension. Our goal is to show you how to build a Chrome extension that is easy to maintain, test, and deploy for a software development team.\nCreate a Chrome extension from scratch Create a basic Chrome extension without any development tools. We cover the basics such as the major parts of the extension, manifest.json, and manually testing the extension in the Chrome browser.\nAdd webpack and TypeScript to a Chrome extension We add support for TypeScript (which replaces JavaScript) and webpack (which bundles the extension) to our Chrome extension.\nMessage passing in a Chrome extension We cover message passing communication between different parts of a Chrome extension. We dive into the code and show how to communicate between the service worker (background script), content scripts, and the popup.\nLinting and formatting TypeScript in a Chrome extension We set up ESLint and Prettier to lint and format our TypeScript code. This ensures our code is consistent and follows best practices.\nAdding options page to Chrome extension We add an advanced options page to our Chrome extension. This page allows users to configure the extension\u0026rsquo;s behavior and settings.\nBuild a production-ready Chrome extension video playlist ","date":"2024-06-11T00:00:00Z","image":"https://victoronsoftware.com/posts/chrome-extension/chrome-extension-headline_hu7cf724450b4015658905efc628cab81b_42029_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/chrome-extension/","title":"Build a production-ready Chrome extension"},{"content":"What is a webhook? A webhook is a way for one application to send data to another application in real time. It is a simple way to trigger an action based on an event. In other words, a webhook is a custom HTTP callback.\nWhat is Tines? Tines is a no-code automation platform that allows you to automate repetitive tasks. It is a powerful tool that can be used to automate workflows, such as sending emails, creating tickets, and updating databases.\nWhat is Fleet? Fleet is an open-source platform for managing and gathering telemetry from devices such as laptops, desktops, VMs, etc. Osquery agents run on these devices and report to the Fleet server.\nOur example IT workflow In this article, we will build a webhook flow with Tines. When a device has an outdated OS version, Tines will receive a webhook callback from Fleet. Tines will then send an MDM (Mobile Device Management) command to the device to update the device\u0026rsquo;s OS version.\nFleet will send a callback via its calendar integration feature. Fleet can put a \u0026ldquo;System Maintenance\u0026rdquo; event on the device user\u0026rsquo;s calendar. This event warns the device owner that their computer will be restarted to remediate one or more failing policies. During the calendar event time, Fleet sends a webhook. The IT admin must set up a flow to remediate the failing policy. This article is an example of one such flow.\nGetting started \u0026ndash; webhook action First, we create a new Tines story. A story is a sequence of actions that are executed in order. Next, we add a webhook action to the story. The webhook action listens for incoming webhooks. The webhook will contain a JSON body.\nTines webhook action Handling errors Often, webhooks may contain error messages if there is an issue with the configuration, flow, etc. In this example, we add a trigger action that checks whether the webhook body contains an error. Specifically, our action checks whether the webhook body contains a non-empty \u0026ldquo;error\u0026rdquo; field.\nTines trigger action checking for an error We leave this error-handling portion of the story as a stub. In the future, we can expand it by sending an email or triggering other actions.\nChecking whether webhook indicates an outdated OS At the same time, we also check whether the webhook was triggered by a policy indicating an outdated OS. From previous testing, we know that the webhook payload will look like this:\n{ \u0026#34;timestamp\u0026#34;: \u0026#34;2024-03-28T13:57:31.668954-05:00\u0026#34;, \u0026#34;host_id\u0026#34;: 11058, \u0026#34;host_display_name\u0026#34;: \u0026#34;Victor\u0026#39;s Virtual Machine (2)\u0026#34;, \u0026#34;host_serial_number\u0026#34;: \u0026#34;Z5C4L7GKY0\u0026#34;, \u0026#34;failing_policies\u0026#34;: [ { \u0026#34;id\u0026#34;: 479, \u0026#34;name\u0026#34;: \u0026#34;macOS - OS version up to date\u0026#34; } ] } The payload contains:\nThe device\u0026rsquo;s ID (host ID). Display name. Serial number. A list of failing policies. We are interested in the failing policies. When one of the failing policies contains a policy named \u0026ldquo;macOS - OS version up to date,\u0026rdquo; we know that the device\u0026rsquo;s OS is outdated. Hence, we create a trigger that looks for this policy.\nTines trigger action checking for an outdated OS We use the following formula, which loops over all policies and will only allow the workflow to proceed if true:\nIF(FIND(calendar_webhook.body.failing_policies, LAMBDA(item, item.name = \u0026#34;macOS - OS version up to date\u0026#34;)).id \u0026gt; 0, TRUE) Getting device details from Fleet Next, we need to get more details about the device from Fleet. Devices are called hosts in Fleet. We add an \u0026ldquo;HTTP Request\u0026rdquo; action to the story. The action makes a GET request to the Fleet API to get the device details. We use the host ID from the webhook payload. We are looking for the device\u0026rsquo;s UUID, which we need to send the OS update MDM command.\nTines HTTP Request action to get Fleet device details To access Fleet\u0026rsquo;s API, we need to provide an API key. We store the API key as a CREDENTIAL in the current story. The API key should belong to an API-only user in Fleet so that the key does not reset when the user logs out.\nAdd credential to Tines story Creating MDM command payload to update OS version We can create the MDM payload now that we have the device\u0026rsquo;s UUID. The payload contains the command to update the OS version. We use the ScheduleOSUpdate command from Apple\u0026rsquo;s MDM protocol.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;Command\u0026lt;/key\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;RequestType\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;ScheduleOSUpdate\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;Updates\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;InstallAction\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;InstallASAP\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ProductVersion\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;14.4.1\u0026lt;/string\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;key\u0026gt;CommandUUID\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;\u0026lt;\u0026lt;UUID()\u0026gt;\u0026gt;\u0026lt;/string\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt; This command will download macOS 14.4.1, install it, and pop up a 60-second countdown dialog box before restarting the device. Note that the \u0026lt;\u0026lt;UUID()\u0026gt;\u0026gt; Tines function creates a unique UUID for this MDM command.\nTines event to create ScheduleOSUpdate MDM command The Fleet API requires the command to be sent as a base64-encoded string. We add a \u0026ldquo;Base64 Encode\u0026rdquo; action to the story to encode the XML payload. It uses the Tines BASE64_ENCODE function.\nTines Base64 Encode event Run MDM command on device Finally, we send the MDM command to the device. We add another \u0026ldquo;HTTP Request\u0026rdquo; action to the story. The action makes a POST request to the Fleet API to send the MDM command to the device.\nTines HTTP Request action to run MDM command on device The MDM command will run on the device, downloading and installing the OS update.\nmacOS restart notification after OS update Conclusion In this article we built a webhook flow with Tines. We received a webhook callback from Fleet when a device had an outdated OS version. We then sent an MDM command to the device to update the OS version. This example demonstrates how Tines can automate workflows and tasks in IT environments.\nBuilding a webhook flow with Tines video Note: If you want to comment on this article, please do so on the YouTube video.\nThis article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2024-06-05T00:00:00Z","image":"https://victoronsoftware.com/posts/webhook-flow-with-tines/tines-fleet-webhook-workflow_hu50ab5d713a39b45c2b6861f49d4df61d_231112_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/webhook-flow-with-tines/","title":"Building a webhook flow with Tines"},{"content":" Excessive database locks Read-after-write consistency Index limitations When building an application, the database is often an afterthought. The database used in a development environment often contains limited data with little traffic. However, when the application is deployed to production, real-world traffic can expose issues that were not caught in development or testing.\nIn this article, we cover issues we ran into with our customers. We assume the production application is deployed with one master and one or more read replicas. See this article on creating a MySQL slave replica in dev environment.\nExcessive database locks One write query can bring your database to its knees if it locks too many rows.\nConsider this simplified INSERT with a subquery transaction:\nINSERT INTO software_counts (host_id, count) SELECT host_id, COUNT(*) as count FROM host_software GROUP BY host_software.host_id; Simplified INSERT with a subquery The above query scans the entire host_software table index to create a count. While the database is doing the scan and the INSERT, it locks the host_software table, preventing other transactions from writing to that table. If the table and insert are large, the query can hold the lock for a long time. In production, we saw a lock time of over 30 seconds, creating a bottleneck and spiking DB resource usage.\nPay special attention to the following queries, as they can cause performance issues:\nCOUNT(*) Using a non-indexed column, like WHERE non_indexed_column = value Returning a large number of rows, like SELECT * FROM table One way to solve the above performance issue is to separate the SELECT and INSERT queries. First, run the SELECT query on the replica to get the data, then run the INSERT query on the master to insert the data. We completely eliminate the lock since the read is done on the replica. This article goes through a specific example of optimizing an INSERT with subqueries.\nAs general advice, avoid running SELECT queries and subqueries on the master, especially if they scan the entire table.\nRead-after-write consistency When you write to the master and read from the replica, you might not see the data you wrote. The replica is not in sync with the master in real time. In our production, the replica is usually less than 30 milliseconds behind the master.\nRead-after-write database issue These issues are typically not caught in development since dev environments usually have one database instance. Unit or integration tests might not even see these issues if they run on a single database instance. Even in testing or small production environments, you might only see these issues if the replica sync time is high. Customers with large deployments may be experiencing these consistency issues without the development team knowing about it.\nOne way to solve this issue is to read from the master after writing to it. This way, you are guaranteed to see the data you just wrote. In our Go backend, forcing reads from the master can be done by updating the Context:\nctxUsePrimary := ctxdb.RequirePrimary(ctx, true) However, additional master reads increase the load on the master, defeating the purpose of having a replica for read scaling.\nIn addition, what about expensive read queries, like COUNT(*) and calculations, which we don\u0026rsquo;t want to run on the master? In this case, we can wait for the replica to catch up with the master.\nOne generic approach to waiting for the replica is to read the last written data from the replica and retry the read if the data is not found. The app could check the updated_at column to see if the data is recent. If the data is not found, the app can sleep for a few milliseconds and retry the read. This approach is imperfect but a good compromise between read consistency and performance.\nNote: The default precision of MySQL date and time data types is 1 second (0 fractional seconds).\nIndex limitations What are SQL indexes? Indexes are a way to optimize read queries. They are a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure. Indexes are created using one or more database columns and are stored and sorted using a B-tree or a similar data structure. The goal is to reduce the number of data comparisons needed to find the data.\nDatabase index Indexes are generally beneficial. They speed up read queries but slightly slow down write queries. Indexes can also be large and take up a lot of disk space.\nIndex size is limited As the product grows with more features, the number of columns in a specific table can also increase. Sometimes, the new columns need to be part of a unique index. However, the maximum index size in MySQL is 3072 bytes. This limit can be quickly reached if columns are of type VARCHAR or TEXT.\nCREATE TABLE `activities` ( `user_name` VARCHAR(255) NOT NULL, One way to solve the issue of hitting the index size limit is to create a new column that makes the hash of the other relevant column(s), and use that as the unique index. For example, in our backend we use a checksum column in the software table to create a unique index for a software item.\nForeign keys may cause performance issues If a table has a foreign key, any insert, update, or delete with a constraint on the foreign key column will lock the corresponding row in the parent table. This locking can lead to performance issues when\nthe parent table is large the parent has many foreign key constraints the parent table or child tables are frequently updated The performance issue manifests as excessive lock wait times for queries. One way to solve this issue is to remove the foreign key constraint. Instead, the application code can handle the data integrity checks that the foreign key constraint provides. In our application, we run a regular clean-up job to remove orphaned child rows.\nBonus database gotchas Additional database gotchas that we have seen in production include:\nPrepared statements consuming too much memory Deadlocks caused by using an UPDATE/INSERT upsert pattern 3 database gotchas video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-29T00:00:00Z","image":"https://victoronsoftware.com/posts/database-gotchas-when-scaling-apps/database-thumbnail_hu549ea51fd202d1432855e22c14d9252f_231213_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/database-gotchas-when-scaling-apps/","title":"3 database gotchas when building apps for scale"},{"content":"This article is part of a series on creating a production-ready Chrome extension. The previous article covered creating a basic Chrome extension without any tooling. For a list of all articles in the series, see the Chrome extension series overview.\nAdd webpack bundler Add TypeScript Convert webpack configuration from JavaScript to TypeScript Introduction This article will add the webpack module bundler and TypeScript support to the Chrome extension we created in the previous article. This software tooling will allow us to use modern JavaScript features and development tools.\nA module bundler and TypeScript are essential tools for modern web development. They improve the development experience for large or long-running projects.\nPrerequisites - Node.js and npm Before we start, make sure you have Node.js and npm installed. Node.js is a JavaScript runtime. We will use it to run webpack and future development tools. npm is a JavaScript package manager.\nYou can check if you have them installed by running the following commands:\nnode -v npm -v package.json First, we will create a package.json file containing project and dependency info. We can use the npm init command to create the file. Or manually create one containing something like:\n{ \u0026#34;name\u0026#34;: \u0026#34;my-chrome-extension\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/create-chrome-extension\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34; } What is webpack? Webpack is a module bundler for JavaScript applications. It takes modules with dependencies and generates static assets representing those modules. We will use webpack to bundle multiple JavaScript files into a single file.\nA module bundler allows you to write modular code and bundle it into a single file. TypeScript is a superset of JavaScript that adds static typing and other features to the language.\nWe will install webpack with npm:\nnpm install --save-dev webpack webpack-cli webpack-merge copy-webpack-plugin webpack is the core module bundler webpack-cli is the command-line interface for webpack webpack-merge is a utility to merge multiple webpack configurations, which we will use to differentiate development and production configs copy-webpack-plugin is a plugin to copy files and directories in webpack The above npm command will install the packages, create a package-lock.json file, and add them to the devDependencies section of the package.json file. The updated package.json should look like this:\n{ \u0026#34;name\u0026#34;: \u0026#34;my-chrome-extension\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/create-chrome-extension\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;copy-webpack-plugin\u0026#34;: \u0026#34;^12.0.2\u0026#34;, \u0026#34;webpack\u0026#34;: \u0026#34;^5.91.0\u0026#34;, \u0026#34;webpack-cli\u0026#34;: \u0026#34;^5.1.4\u0026#34;, \u0026#34;webpack-merge\u0026#34;: \u0026#34;^5.10.0\u0026#34; } } webpack configuration Next, we will create webpack configuration files. Webpack uses a configuration file to define how to bundle the project. We will create two configurations: one for development and one for production. Initially, we will use JavaScript for the configuration files, but we will convert them to TypeScript later.\nCreate a webpack.common.js file with the shared configuration:\nconst path = require(\u0026#39;path\u0026#39;) const CopyWebpackPlugin = require(\u0026#39;copy-webpack-plugin\u0026#39;); module.exports = { entry: { background: \u0026#39;./src/background.js\u0026#39;, content: \u0026#39;./src/content.js\u0026#39;, popup: \u0026#39;./src/popup.js\u0026#39;, }, output: { filename: \u0026#39;[name].js\u0026#39;, path: path.resolve(__dirname, \u0026#39;dist\u0026#39;), clean: true, // Clean the output directory before emit. }, plugins: [ new CopyWebpackPlugin({ patterns: [{ from: \u0026#39;static\u0026#39; }], }), ] } Create a webpack.dev.js file with the development configuration:\nconst { merge } = require(\u0026#39;webpack-merge\u0026#39;) const common = require(\u0026#39;./webpack.common.js\u0026#39;) module.exports = merge(common, { mode: \u0026#39;development\u0026#39;, devtool: \u0026#39;inline-source-map\u0026#39;, }) Create a webpack.prod.js file with the production configuration:\nconst { merge } = require(\u0026#39;webpack-merge\u0026#39;) const common = require(\u0026#39;./webpack.common.js\u0026#39;) module.exports = merge(common, { mode: \u0026#39;production\u0026#39;, devtool: \u0026#39;source-map\u0026#39;, }) Refactoring directory structure We will refactor the directory structure to separate the source code from the static files. Create a src directory and move the JavaScript files (background.js, content.js, popup.js) into it. Create a static directory and move the manifest.json, popup.html, and popup.css file into it.\nThe directory structure should look like this (running tree . -I node_modules):\n. ├── package-lock.json ├── package.json ├── src │ ├── background.js │ ├── content.js │ └── popup.js ├── static │ ├── manifest.json │ ├── popup.css │ └── popup.html ├── webpack.common.js ├── webpack.dev.js └── webpack.prod.js Running webpack Now, we can run the webpack bundler using the following command:\n./node_modules/.bin/webpack --watch --config webpack.dev.js This command creates a dist directory with the bundled files. The --watch flag tells webpack to continue running, watch for changes, and recompile the files when changes occur. This recompilation is crucial for development, as it allows us to see our code changes in real time.\nWe can run the production build with:\n./node_modules/.bin/webpack --config webpack.prod.js Now, we can add scripts to the package.json file to simplify how we run webpack:\n... \u0026#34;scripts\u0026#34;: { \u0026#34;build\u0026#34;: \u0026#34;webpack --config webpack.prod.js\u0026#34;, \u0026#34;start\u0026#34;: \u0026#34;webpack --watch --config webpack.dev.js\u0026#34; } These scripts allow us to run npm run build to build the production version and npm start (or npm run start) to start the development version.\nAt this point, we can test the browser extension to ensure it is still working as before. Open the Chrome browser, go to chrome://extensions, enable Developer mode, click on Load unpacked, and select the dist directory.\nWhat is TypeScript? TypeScript is a superset of JavaScript that adds static typing and other features to the language. It compiles to plain JavaScript and can be used in any browser or JavaScript engine. Although TypeScript is not required for writing Chrome extensions, it is highly recommended as it can help catch errors early and improve code quality.\nWe install TypeScript with:\nnpm install --save-dev typescript @tsconfig/recommended ts-node ts-loader @types/chrome typescript is the core TypeScript compiler @tsconfig/recommended is a recommended TypeScript configuration, which we will use ts-node is a TypeScript execution environment for Node.js, which is needed for converting the webpack configuration to TypeScript ts-loader is a TypeScript loader for webpack, which is needed for webpack to understand TypeScript source files @types/chrome is the TypeScript type definitions for the Chrome extension API What are TypeScript type definitions? We loaded the @types/chrome packages to provide TypeScript type definitions for the Chrome extension API.\nTypeScript type definitions are files that describe the shape of a JavaScript library. They provide type information for JavaScript libraries that were not written in TypeScript. This information allows TypeScript to understand the library\u0026rsquo;s API and provide type checking. With this information, TypeScript can check our code.\n@types/chrome provides a global chrome object representing the Chrome extension API. No additional code is needed to use it from the command line, as TypeScript automatically loads it. However, IDEs may need to be configured to recognize this global type definition.\ntsconfig.json Next, we will create a tsconfig.json file to configure TypeScript. This file tells the TypeScript compiler how to compile the project. Create a tsconfig.json file with the recommended config:\n{ \u0026#34;extends\u0026#34;: \u0026#34;@tsconfig/recommended/tsconfig.json\u0026#34;, \u0026#34;compilerOptions\u0026#34;: { \u0026#34;sourceMap\u0026#34;: true } } We added the sourceMap option to generate source maps, which help debug TypeScript code in the browser.\nConvert webpack configuration from JavaScript to TypeScript First, rename the webpack configuration files to TypeScript files by changing the extension from .js to .ts. For example, webpack.common.js becomes webpack.common.ts. Then, update the contents of the files to TypeScript syntax.\nwebpack.common.ts:\nimport path from \u0026#39;path\u0026#39; import webpack from \u0026#39;webpack\u0026#39; import CopyWebpackPlugin from \u0026#39;copy-webpack-plugin\u0026#39; const config: webpack.Configuration = { entry: { background: \u0026#39;./src/background.ts\u0026#39;, content: \u0026#39;./src/content.ts\u0026#39;, popup: \u0026#39;./src/popup.ts\u0026#39;, }, output: { filename: \u0026#39;[name].js\u0026#39;, path: path.resolve(__dirname, \u0026#39;dist\u0026#39;), clean: true, // Clean the output directory before emit. }, plugins: [ new CopyWebpackPlugin({ patterns: [{from: \u0026#39;static\u0026#39;}], }), ] } export default config We made the following changes to the shared config:\nWe changed the require statements to import statements We changed the module.exports to export default We added the webpack.Configuration type from the webpack package webpack.dev.ts:\nimport {Configuration} from \u0026#39;webpack\u0026#39; import {merge} from \u0026#39;webpack-merge\u0026#39; import config from \u0026#39;./webpack.common\u0026#39; const merged = merge\u0026lt;Configuration\u0026gt;(config,{ mode: \u0026#39;development\u0026#39;, devtool: \u0026#39;inline-source-map\u0026#39;, }) export default merged webpack.prod.ts:\nimport {Configuration} from \u0026#39;webpack\u0026#39; import {merge} from \u0026#39;webpack-merge\u0026#39; import config from \u0026#39;./webpack.common\u0026#39; const merged = merge\u0026lt;Configuration\u0026gt;(config,{ mode: \u0026#39;production\u0026#39;, devtool: \u0026#39;source-map\u0026#39;, }) export default merged And update the package.json scripts to use the TypeScript configuration files:\n... \u0026#34;scripts\u0026#34;: { \u0026#34;build\u0026#34;: \u0026#34;webpack --config webpack.prod.ts\u0026#34;, \u0026#34;start\u0026#34;: \u0026#34;webpack --watch --config webpack.dev.ts\u0026#34; } We can test npm run start and npm run build to ensure the new webpack Typescript configurations are working correctly.\nConvert JavaScript source files to TypeScript Finally, we will convert the JavaScript source files to TypeScript. Rename the .js files to .ts files. For example, background.js becomes background.ts. Update the contents of the files to TypeScript syntax.\nAlso, we will refactor the common setBadgeText function to a shared common.ts file:\nexport function setBadgeText(enabled: boolean) { const text = enabled ? \u0026#34;ON\u0026#34; : \u0026#34;OFF\u0026#34; void chrome.action.setBadgeText({text: text}) } Updated background.ts:\nimport {setBadgeText} from \u0026#34;./common\u0026#34; function startUp() { chrome.storage.sync.get(\u0026#34;enabled\u0026#34;, (data) =\u0026gt; { setBadgeText(!!data.enabled) }) } // Ensure the background script always runs. chrome.runtime.onStartup.addListener(startUp) chrome.runtime.onInstalled.addListener(startUp) Updated content.ts:\nconst blurFilter = \u0026#34;blur(6px)\u0026#34; let textToBlur = \u0026#34;\u0026#34; // Search this DOM node for text to blur and blur the parent element if found. function processNode(node: Node) { if (node.childNodes.length \u0026gt; 0) { Array.from(node.childNodes).forEach(processNode) } if (node.nodeType === Node.TEXT_NODE \u0026amp;\u0026amp; node.textContent !== null \u0026amp;\u0026amp; node.textContent.trim().length \u0026gt; 0) { const parent = node.parentElement if (parent == null) { return } if (parent.tagName === \u0026#39;SCRIPT\u0026#39; || parent.style.filter === blurFilter) { // Already blurred return } if (node.textContent.includes(textToBlur)) { blurElement(parent) } } } function blurElement(elem: HTMLElement) { elem.style.filter = blurFilter console.debug(\u0026#34;blurred id:\u0026#34; + elem.id + \u0026#34; class:\u0026#34; + elem.className + \u0026#34; tag:\u0026#34; + elem.tagName + \u0026#34; text:\u0026#34; + elem.textContent) } // Create a MutationObserver to watch for changes to the DOM. const observer = new MutationObserver((mutations) =\u0026gt; { mutations.forEach((mutation) =\u0026gt; { if (mutation.addedNodes.length \u0026gt; 0) { mutation.addedNodes.forEach(processNode) } else { processNode(mutation.target) } }) }) // Enable the content script by default. let enabled = true const keys = [\u0026#34;enabled\u0026#34;, \u0026#34;item\u0026#34;] chrome.storage.sync.get(keys, (data) =\u0026gt; { if (data.enabled === false) { enabled = false } if (data.item) { textToBlur = data.item } // Only start observing the DOM if the extension is enabled and there is text to blur. if (enabled \u0026amp;\u0026amp; textToBlur.trim().length \u0026gt; 0) { observer.observe(document, { attributes: false, characterData: true, childList: true, subtree: true, }) // Loop through all elements on the page for initial processing. processNode(document) } }) Updated popup.ts:\nimport {setBadgeText} from \u0026#34;./common\u0026#34; console.log(\u0026#34;Hello, world from popup!\u0026#34;) // Handle the ON/OFF switch const checkbox = document.getElementById(\u0026#34;enabled\u0026#34;) as HTMLInputElement chrome.storage.sync.get(\u0026#34;enabled\u0026#34;, (data) =\u0026gt; { checkbox.checked = !!data.enabled void setBadgeText(data.enabled) }) checkbox.addEventListener(\u0026#34;change\u0026#34;, (event) =\u0026gt; { if (event.target instanceof HTMLInputElement) { void chrome.storage.sync.set({\u0026#34;enabled\u0026#34;: event.target.checked}) void setBadgeText(event.target.checked) } }) // Handle the input field const input = document.getElementById(\u0026#34;item\u0026#34;) as HTMLInputElement chrome.storage.sync.get(\u0026#34;item\u0026#34;, (data) =\u0026gt; { input.value = data.item || \u0026#34;\u0026#34; }); input.addEventListener(\u0026#34;change\u0026#34;, (event) =\u0026gt; { if (event.target instanceof HTMLInputElement) { void chrome.storage.sync.set({\u0026#34;item\u0026#34;: event.target.value}) } }) Update webpack configuration to handle TypeScript source files Update webpack.common.ts to use the new TypeScript source files and add the ts-loader to the webpack configuration:\nimport path from \u0026#39;path\u0026#39; import webpack from \u0026#39;webpack\u0026#39; import CopyWebpackPlugin from \u0026#39;copy-webpack-plugin\u0026#39; const config: webpack.Configuration = { entry: { background: \u0026#39;./src/background.ts\u0026#39;, content: \u0026#39;./src/content.ts\u0026#39;, popup: \u0026#39;./src/popup.ts\u0026#39;, }, resolve: { extensions: [\u0026#34;.ts\u0026#34;], }, module: { rules: [ { test: /\\.ts$/, loader: \u0026#34;ts-loader\u0026#34;, exclude: /node_modules/, }, ], }, output: { filename: \u0026#39;[name].js\u0026#39;, path: path.resolve(__dirname, \u0026#39;dist\u0026#39;), clean: true, // Clean the output directory before emit. }, plugins: [ new CopyWebpackPlugin({ patterns: [{from: \u0026#39;static\u0026#39;}], }), ] } export default config Debug our TypeScript extension in Chrome Build the extension with npm run start and load it in Chrome.\nRight-click the extension icon (M) and select Inspect popup to open the Chrome Developer Tools. By default, you can see the console logs from the popup.ts file.\nGo to the Sources tab in the Chrome Developer Tools and open the top/my-chrome-extension/src/popup.ts file. You can set breakpoints and debug the popup script.\nDebugging Chrome extension popup The popup.ts file should exactly match the TypeScript code we wrote. You can set breakpoints, inspect variables, and step through the code.\nNext steps In the next part of this series, we will add message passing between the content script, the background script, and the popup script. This communication will allow us to make real-time changes across all parts of our Chrome extension.\nChrome extension with webpack and TypeScript code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/2-webpack-typescript\nAdd webpack and TypeScript to a Chrome extension video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-23T00:00:00Z","image":"https://victoronsoftware.com/posts/add-webpack-and-typescript-to-chrome-extension/chrome-typescript-webpack_hu9184b01262dcaf0a34fa52a6019d1dcb_42080_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/add-webpack-and-typescript-to-chrome-extension/","title":"Add webpack and TypeScript to a Chrome extension (2024)"},{"content":"Introduction In this article, we will create a MySQL slave replica. A MySQL slave is a read-only copy of the master database. Using MySQL replication, the slave database is kept in sync with the master database.\nThe steps we will follow are:\nSpin up MySQL master and slave databases Create a user for replication Obtain master binary log coordinates Configure slave and start replication What is database replication? Database replication is a process that allows data from one database server (the master) to be copied to one or more database servers (the slaves or replicas). Replication is asynchronous, meaning that the slave does not need to be connected to the master constantly. The replica can catch up with the master when it is available.\nDatabase replicas are used for:\nScaling read operations High availability Disaster recovery MySQL implements replication using the binary log. The master server writes changes to the binary log, and the slave server reads the binary log and applies the changes to its database.\nCreate MySQL master and slave databases We will use Docker to create the MySQL master and slave databases. We will use the official MySQL Docker image. The master database will run on port 3308, and the slave database will run on port 3309.\nWe run docker compose up using the following docker-compose.yml file:\nCreate a DB user for replication Replication in MySQL requires a user with the REPLICATION SLAVE privilege. We will create a user named replicator with the password rotacilper.\nConnect to the master database using the MySQL client:\nmysql --host 127.0.0.1 --port 3308 -uroot -ptoor Create the replicator user and grant the REPLICATION SLAVE privilege:\nCREATE USER \u0026#39;replicator\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;rotacilper\u0026#39;; GRANT REPLICATION SLAVE ON *.* TO \u0026#39;replicator\u0026#39;@\u0026#39;%\u0026#39;; FLUSH PRIVILEGES; Retrieve master binary log coordinates For the slave to start replication, it needs to know the master\u0026rsquo;s binary log file and position. We can obtain this information using the MySQL client which we opened in the previous step.\nSHOW MASTER STATUS; The output will look like this:\n+------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------+----------+--------------+------------------+-------------------+ | bin.000003 | 861 | | | | +------------+----------+--------------+------------------+-------------------+ 1 row in set (0.01 sec) We must remember the File and Position values for the next step.\nConfigure slave and start replication Now, we will connect to the slave database and configure it to replicate from the master database.\nmysql --host 127.0.0.1 --port 3309 -uroot -ptoor Use the CHANGE MASTER TO command to configure the slave to replicate from the master. Replace MASTER_LOG_FILE and MASTER_LOG_POS with the values obtained in the previous step.\nCHANGE MASTER TO MASTER_HOST=\u0026#39;mysql_master\u0026#39;, MASTER_PORT=3306, MASTER_USER=\u0026#39;replicator\u0026#39;, MASTER_PASSWORD=\u0026#39;rotacilper\u0026#39;, MASTER_LOG_FILE=\u0026#39;bin.000003\u0026#39;, MASTER_LOG_POS=861, GET_MASTER_PUBLIC_KEY=1; MASTER_HOST is the hostname of the master, which matches the docker service name. The GET_MASTER_PUBLIC_KEY option is needed for MySQL 8.0 caching_sha2_password authentication.\nFinally, start the slave:\nSTART SLAVE; The slave will now start replicating data from the master database. You can check the replication status using the SHOW REPLICA STATUS\\G command.\nWe can create a table with data on the master database and check if it is replicated to the slave database:\nUSE test; CREATE TABLE users (id INT PRIMARY KEY, name VARCHAR(255)); INSERT INTO users VALUES (1, \u0026#39;Alice\u0026#39;); Further reading on database scaling Recently, we wrote about database gotchas when scaling applications. One of the issues we summarized was optimizing a MySQL INSERT with subqueries. In the past, we encountered a memory issue with MySQL prepared statements when scaling applications. Follow along with the MySQL master-slave replication on video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-18T00:00:00Z","image":"https://victoronsoftware.com/posts/mysql-master-slave-replication/mysql-master-slave-replication_hube393d6b92f0712a09dcfd6b7cda8c19_40387_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mysql-master-slave-replication/","title":"Create a MySQL slave replica in 4 short steps"},{"content":"In this series, we will be building a production-ready Chrome extension. We will start with a basic extension and then add more features.\nWhat is a Chrome extension? A Chrome extension is a small software program that customizes the browsing experience. It can modify and enhance the functionality of the Chrome browser. Extensions are written using web technologies such as HTML, CSS, and JavaScript.\nWhy build a Chrome extension? Users can utilize Chrome extensions to:\nModify web pages Automate tasks Integrate with third-party services Add new features to the browser And much more Prerequisites For this tutorial, no additional tools are required. We will create the extension using a text editor and the Chrome browser.\nThree parts of a Chrome extension The three main parts of a Chrome extension are the background script, content script(s), and popup. All these parts are optional.\nParts of a Chrome extension background script: Also known as a service worker, this is a long-running script that runs in the background. It can listen for events and perform tasks. content script(s): This script runs in the context of a web page. It can interact with the DOM and modify the page, including adding UI elements. The extension can statically inject this script or dynamically inject it by the background script or the popup. popup: This small HTML page appears when a user clicks the extension icon. It can contain buttons, forms, and other UI elements. This is the extension\u0026rsquo;s user interface. These three parts of the extension run independently but can communicate with each other using message passing, events, and storage.\nOur first extension will have a popup with a turn-on/off switch and an input field. The extension will blur the page elements containing the text in the input field.\nmanifest.json configuration file Create a src directory for the extension. This directory will contain all the extension files.\nThe manifest.json file is the configuration file of a Chrome extension. It contains metadata about the extension, such as its name, version, permissions, and scripts.\nCreating the popup Add a manifest.json file with the following content:\n{ \u0026#34;manifest_version\u0026#34;: 3, \u0026#34;name\u0026#34;: \u0026#34;My Chrome Extension\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;My first Chrome extension.\u0026#34;, \u0026#34;action\u0026#34;: { \u0026#34;default_popup\u0026#34;: \u0026#34;popup.html\u0026#34; }, \u0026#34;permissions\u0026#34;: [ \u0026#34;storage\u0026#34; ] } The permissions specify the permissions required by the extension. In this case, we need the storage permission to store data in the Chrome storage so that the extension can remember the state of its configuration.\nCreate popup.html with the content below.\n\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;My popup\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;popup.css\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;label class=\u0026#34;switch\u0026#34;\u0026gt; \u0026lt;input id=\u0026#34;enabled\u0026#34; type=\u0026#34;checkbox\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;slider round\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;input class=\u0026#34;secret\u0026#34; id=\u0026#34;item\u0026#34; type=\u0026#34;text\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;popup.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Our popup.html includes a CSS file and a script. Create popup.js with the following content:\n\u0026#34;use strict\u0026#34;; console.log(\u0026#34;Hello, world from popup!\u0026#34;) function setBadgeText(enabled) { const text = enabled ? \u0026#34;ON\u0026#34; : \u0026#34;OFF\u0026#34; void chrome.action.setBadgeText({text: text}) } // Handle the ON/OFF switch const checkbox = document.getElementById(\u0026#34;enabled\u0026#34;) chrome.storage.sync.get(\u0026#34;enabled\u0026#34;, (data) =\u0026gt; { checkbox.checked = !!data.enabled void setBadgeText(data.enabled) }) checkbox.addEventListener(\u0026#34;change\u0026#34;, (event) =\u0026gt; { if (event.target instanceof HTMLInputElement) { void chrome.storage.sync.set({\u0026#34;enabled\u0026#34;: event.target.checked}) void setBadgeText(event.target.checked) } }) // Handle the input field const input = document.getElementById(\u0026#34;item\u0026#34;) chrome.storage.sync.get(\u0026#34;item\u0026#34;, (data) =\u0026gt; { input.value = data.item || \u0026#34;\u0026#34; }); input.addEventListener(\u0026#34;change\u0026#34;, (event) =\u0026gt; { if (event.target instanceof HTMLInputElement) { void chrome.storage.sync.set({\u0026#34;item\u0026#34;: event.target.value}) } }) The script listens for changes in the switch and the input field. It saves the switch\u0026rsquo;s state and the input field\u0026rsquo;s value in Chrome storage.\nCreate popup.css with the following content to style the switch and the input field:\n/* The switch - the box around the slider */ .switch { margin-left: 30%; /* Center the switch */ position: relative; display: inline-block; width: 60px; height: 34px; } /* Hide default HTML checkbox */ .switch input { opacity: 0; width: 0; height: 0; } /* The slider */ .slider { position: absolute; cursor: pointer; top: 0; left: 0; right: 0; bottom: 0; background-color: #ccc; } .slider::before { position: absolute; content: \u0026#34;\u0026#34;; height: 26px; width: 26px; left: 4px; bottom: 4px; background-color: white; } input:checked + .slider { background-color: #2196F3; } input:checked + .slider:before { transform: translateX(26px); /* Move the slider to the right when checked */ } /* Rounded sliders */ .slider.round { border-radius: 34px; } .slider.round::before { border-radius: 50%; } .secret { margin: 5px; } Loading and testing the extension in Chrome Even though we have not added the background script and content script, we can load the extension in Chrome.\nOpen the Chrome browser. Go to chrome://extensions/. Enable the Developer mode. Click on Load unpacked. Select the src directory containing the extension files. Click Select Folder. The extension will be loaded. Pin the extension to the toolbar by clicking the pin button in the extension dropdown. This pin will make it easier to test the extension. The popup page will appear when you click the M extension icon. Chrome extension popup We can now do some basic testing:\nTest the switch and the input field. The state of the switch and the value of the input field should be saved in the Chrome storage. The values should persist even after restarting the browser. The badge text of the extension icon should change to \u0026ldquo;ON\u0026rdquo; or \u0026ldquo;OFF\u0026rdquo; based on the state of the switch. To inspect the extension, right-click the extension icon and select Inspect popup. You should see a \u0026ldquo;Hello, world\u0026rdquo; message in the Console tab. Creating the content script Update the manifest.json file to include the content_scripts section. The entire file should look like this:\n{ \u0026#34;manifest_version\u0026#34;: 3, \u0026#34;name\u0026#34;: \u0026#34;My Chrome Extension\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;My first Chrome extension.\u0026#34;, \u0026#34;action\u0026#34;: { \u0026#34;default_popup\u0026#34;: \u0026#34;popup.html\u0026#34; }, \u0026#34;permissions\u0026#34;: [ \u0026#34;storage\u0026#34; ], \u0026#34;content_scripts\u0026#34;: [ { \u0026#34;matches\u0026#34;: [\u0026#34;\u0026lt;all_urls\u0026gt;\u0026#34;], \u0026#34;js\u0026#34;: [\u0026#34;content.js\u0026#34;] } ] } Create the new file content.js with the following content:\n\u0026#34;use strict\u0026#34; const blurFilter = \u0026#34;blur(6px)\u0026#34; let textToBlur = \u0026#34;\u0026#34; // Search this DOM node for text to blur and blur the parent element if found. function processNode(node) { if (node.childNodes.length \u0026gt; 0) { Array.from(node.childNodes).forEach(processNode) } if (node.nodeType === Node.TEXT_NODE \u0026amp;\u0026amp; node.textContent !== null \u0026amp;\u0026amp; node.textContent.trim().length \u0026gt; 0) { const parent = node.parentElement if (parent !== null \u0026amp;\u0026amp; (parent.tagName === \u0026#39;SCRIPT\u0026#39; || parent.style.filter === blurFilter)) { // Already blurred return } if (node.textContent.includes(textToBlur)) { blurElement(parent) } } } function blurElement(elem) { elem.style.filter = blurFilter console.debug(\u0026#34;blurred id:\u0026#34; + elem.id + \u0026#34; class:\u0026#34; + elem.className + \u0026#34; tag:\u0026#34; + elem.tagName + \u0026#34; text:\u0026#34; + elem.textContent) } // Create a MutationObserver to watch for changes to the DOM. const observer = new MutationObserver((mutations) =\u0026gt; { mutations.forEach((mutation) =\u0026gt; { if (mutation.addedNodes.length \u0026gt; 0) { mutation.addedNodes.forEach(processNode) } else { processNode(mutation.target) } }) }) // Enable the content script by default. let enabled = true const keys = [\u0026#34;enabled\u0026#34;, \u0026#34;item\u0026#34;] chrome.storage.sync.get(keys, (data) =\u0026gt; { if (data.enabled === false) { enabled = false } if (data.item) { textToBlur = data.item } // Only start observing the DOM if the extension is enabled and there is text to blur. if (enabled \u0026amp;\u0026amp; textToBlur.trim().length \u0026gt; 0) { observer.observe(document, { attributes: false, characterData: true, childList: true, subtree: true, }) // Loop through all elements on the page for initial processing. processNode(document) } }) The script listens for changes in the DOM and blurs elements that contain the text specified in the input field of the popup.\nAt this point, we can test the extension by entering text in the input field and enabling it. After reloading the page, the extension should blur elements that contain the text.\nCreating the background script Our background script will update the badge text of the extension icon on startup.\nUpdate the manifest.json file to include the background section. The complete file should look like this:\n{ \u0026#34;manifest_version\u0026#34;: 3, \u0026#34;name\u0026#34;: \u0026#34;My Chrome Extension\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;My first Chrome extension.\u0026#34;, \u0026#34;action\u0026#34;: { \u0026#34;default_popup\u0026#34;: \u0026#34;popup.html\u0026#34; }, \u0026#34;permissions\u0026#34;: [ \u0026#34;storage\u0026#34; ], \u0026#34;content_scripts\u0026#34;: [ { \u0026#34;matches\u0026#34;: [\u0026#34;\u0026lt;all_urls\u0026gt;\u0026#34;], \u0026#34;js\u0026#34;: [\u0026#34;content.js\u0026#34;] } ], \u0026#34;background\u0026#34;: { \u0026#34;service_worker\u0026#34;: \u0026#34;background.js\u0026#34; } } Create a new file background.js with the following content:\n\u0026#34;use strict\u0026#34; function setBadgeText(enabled) { const text = enabled ? \u0026#34;ON\u0026#34; : \u0026#34;OFF\u0026#34; void chrome.action.setBadgeText({text: text}) } function startUp() { chrome.storage.sync.get(\u0026#34;enabled\u0026#34;, (data) =\u0026gt; { setBadgeText(!!data.enabled) }) } // Ensure the background script always runs. chrome.runtime.onStartup.addListener(startUp) chrome.runtime.onInstalled.addListener(startUp) The script listens for the startup and installation events and sets the badge text based on the extension\u0026rsquo;s saved state.\nAt this point, our basic extension is complete. We can test the extension.\nNext steps In the next part of this series, we will add development tooling to the Chrome extension, such as TypeScript support, a bundling tool called webpack, and a development mode that will reload the extension automatically when changes are made.\nFor a list of all articles in the series, see the production-ready Chrome extension series overview.\nOther getting started guides Recently, we wrote about creating a React application from scratch while minimizing the amount of tools used. We also have a guide on getting started with CGO in Go. Basic extension code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/1-basic-extension\nCreate a Chrome extension from scratch step-by-step video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-15T00:00:00Z","image":"https://victoronsoftware.com/posts/create-chrome-extension/chrome-extension-headline_hu7cf724450b4015658905efc628cab81b_42029_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/create-chrome-extension/","title":"Create a Chrome extension from scratch step-by-step (2024)"},{"content":"Introduction IPv6 is the latest version of the Internet Protocol. It provides a larger address space than IPv4, which is running out of addresses. IPv6 is essential for the future of the Internet, and many cloud providers support it.\nIn addition, IPv6 is more secure than IPv4. It has built-in security features like IPsec, which is optional in IPv4. IPv6 also has a simplified header, which makes it faster than IPv4.\nMany corporations use IPv6 internally, and some have even disabled IPv4. This tutorial will create a Linux VM using IPv6, with IPv4 disabled.\nThe steps are:\nCreate droplets with IPv6 enabled SSH from IPv4 client to IPv6-only server Disable IPv4 on the Linux server Prerequisites We will use Digital Ocean as our cloud provider. Their IPv6 documentation is available at https://docs.digitalocean.com/products/networking/ipv6/.\nDroplets are Digital Ocean\u0026rsquo;s virtual private servers. They run on virtualized hardware and are available in various sizes. We will create a new droplet with IPv6.\nStep 1: Create droplets with IPv6 enabled We will create two Digital Ocean droplets. The first droplet will have only IPv6 enabled, and the second droplet will have both IPv4 and IPv6 enabled. We only need the second droplet to SSH into the first droplet because our client machine uses IPv4 only.\nBoth droplets will use Ubuntu 24.04 (LTS), although any Linux distribution should work. Both droplets should have IPv6 enabled in Advanced Options.\nThe first droplet will use the Password authentication method.\nThe second droplet can have either Password or SSH authentication.\nStep 2: SSH from IPv4 client to IPv6-only server You can find the Droplet IPv4 and IPv6 addresses in the Droplet details.\nNow, we connect to the second droplet using SSH.\nssh root@143.198.235.211 From there, we can SSH into the first droplet using its IPv6 address.\nssh root@2604:a880:4:1d0::4d3:3000 Install the net-tools package to use the ifconfig command.\nsudo apt update sudo apt install net-tools Step 3: Disable IPv4 on the Linux server To disable IPv4 on the first droplet, edit the /etc/netplan/50-cloud-init.yaml network configuration file by removing all the IPv4 addresses and routes, and adding the IPv6 nameservers, as shown below.\nnetwork: version: 2 ethernets: eth0: accept-ra: false addresses: - 2604:a880:4:1d0::4d3:3000/64 match: macaddress: da:a1:07:89:d9:a1 mtu: 1500 nameservers: addresses: - 2001:4860:4860::8844 - 2001:4860:4860::8888 search: [] routes: - to: ::/0 via: 2604:a880:4:1d0::1 set-name: eth0 Apply the changes.\nsudo netplan apply --debug Now, you can view the network configuration using the ifconfig command. It should look like:\neth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet6 fe80::d8a1:7ff:fe89:d9a1 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; inet6 2604:a880:4:1d0::4d3:3000 prefixlen 64 scopeid 0x0\u0026lt;global\u0026gt; ether da:a1:07:89:d9:a1 txqueuelen 1000 (Ethernet) RX packets 5179 bytes 3832240 (3.8 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 5099 bytes 696019 (696.0 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eth1: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet6 fe80::e826:4cff:feb7:6659 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether ea:26:4c:b7:66:59 txqueuelen 1000 (Ethernet) RX packets 12 bytes 916 (916.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 43 bytes 2266 (2.2 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 233 bytes 22136 (22.1 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 233 bytes 22136 (22.1 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 You can see that the eth0 interface has an IPv6 address but no IPv4 address. The eth1 interface also has an IPv6 address. The lo interface is the loopback interface and still uses the IPv4 127.0.0.1 address. We will not disable IPv4 on the loopback interface at this point since many tools may break.\nTransfer files between IPv4 and IPv6-only servers To transfer files between the IPv4 and IPv6-only servers, you can use the scp command. First, transfer to the droplet that supports both IPv4 and IPv6, like:\nscp fleet-osquery_1.24.0_amd64.deb root@143.198.235.211:~ Then, SSH into that droplet and transfer the file to the IPv6-only droplet:\nscp fleet-osquery_1.24.0_amd64.deb root@\\[2604:a880:4:1d0::4d3:3000\\]:~ Conclusion In this tutorial, we created a Linux VM using IPv6, with IPv4 disabled. We also transferred files between an IPv4 and an IPv6-only server. IPv6 is the future of the Internet, and learning how to use it is essential. You can now create your own IPv6-only servers and experiment with them.\nCreate an IPv6-only Linux server video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-08T00:00:00Z","image":"https://victoronsoftware.com/posts/create-ipv6-only-linux-server/ipv6-only_hu0348352967e0f607820752c439709519_66045_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/create-ipv6-only-linux-server/","title":"Create an IPv6-only Linux server in 3 easy steps"},{"content":"Introduction We recently encountered a performance issue in production. Once an hour, we saw a spike in average DB lock time, along with occasional deadlocks and server errors. We identified the problematic query using Amazon RDS logs. It was an INSERT statement with subqueries.\nINSERT INTO policy_stats (policy_id, inherited_team_id, passing_host_count, failing_host_count) SELECT p.id, t.id AS inherited_team_id, ( SELECT COUNT(*) FROM policy_membership pm INNER JOIN hosts h ON pm.host_id = h.id WHERE pm.policy_id = p.id AND pm.passes = true AND h.team_id = t.id ) AS passing_host_count, ( SELECT COUNT(*) FROM policy_membership pm INNER JOIN hosts h ON pm.host_id = h.id WHERE pm.policy_id = p.id AND pm.passes = false AND h.team_id = t.id ) AS failing_host_count FROM policies p CROSS JOIN teams t WHERE p.team_id IS NULL GROUP BY p.id, t.id ON DUPLICATE KEY UPDATE updated_at = NOW(), passing_host_count = VALUES(passing_host_count), failing_host_count = VALUES(failing_host_count); This statement calculated passing/failing results and inserted them into a policy_stats summary table. Unfortunately, this query took over 30 seconds to execute. During this time, it locked the important policy_membership table, preventing other threads from writing to it.\nReproducing slow SQL queries Since we saw the issue in production, we needed to reproduce it in a test environment. We created a similar schema and loaded it with data. We used a Go script to populate the tables with dummy data: https://github.com/getvictor/mysql/blob/main/insert-with-subqueries-perf/main.go.\nInitially, we used ten policies and ten teams with 10,000 hosts each, resulting in 100 inserted rows with the above query. However, the performance was only three to six seconds. Then, we increased the number of policies to 50, resulting in 500 inserted rows. The performance dropped to 30 to 60 seconds.\nThe above data made it clear that this query needed to be more scalable. As the GROUP BY p.id, t.id clause demonstrates, performance exponentially degrades with the number of policies and teams.\nDebugging slow SQL queries MySQL has powerful tools called EXPLAIN and EXPLAIN ANALYSE. These tools show how MySQL executes a query and help identify performance bottlenecks. We ran EXPLAIN ANALYSE on the problematic query and viewed the results as a tree and a diagram.\nMySQL EXPLAIN result in TREE format MySQL EXPLAIN result as a diagram Although the EXPLAIN output was complex, it was clear that the SELECT subqueries were executing too many times.\nFixing INSERT with subqueries performance The first step was to separate the INSERT from the SELECT. The top SELECT subquery took most of the time. But, more importantly, the SELECT does not block other threads from updating the policy_membership table.\nHowever, the single standalone SELECT subquery was still slow. In addition, memory usage could be high for many teams and policies.\nWe decided to process one policy row at a time. This reduced the time to complete an individual SELECT query to less than two seconds and limited the memory usage. We did not use a transaction to minimize locks. Not utilizing a transaction meant that the INSERT could fail if a parallel process deleted the policy. Also, the INSERT could overwrite a clearing of the policy_stats row. These drawbacks were acceptable, as they were rare cases.\nSELECT p.id as policy_id, t.id AS inherited_team_id, ( SELECT COUNT(*) FROM policy_membership pm INNER JOIN hosts h ON pm.host_id = h.id WHERE pm.policy_id = p.id AND pm.passes = true AND h.team_id = t.id ) AS passing_host_count, ( SELECT COUNT(*) FROM policy_membership pm INNER JOIN hosts h ON pm.host_id = h.id WHERE pm.policy_id = p.id AND pm.passes = false AND h.team_id = t.id ) AS failing_host_count FROM policies p CROSS JOIN teams t WHERE p.team_id IS NULL AND p.id = ? GROUP BY t.id, p.id; After each SELECT, we inserted the results into the policy_stats table.\nINSERT INTO policy_stats (policy_id, inherited_team_id, passing_host_count, failing_host_count) VALUES (?, ?, ?, ?), ... ON DUPLICATE KEY UPDATE updated_at = NOW(), passing_host_count = VALUES(passing_host_count), failing_host_count = VALUES(failing_host_count); Further reading about MySQL MySQL deadlock on UPDATE/INSERT upsert pattern Scaling DB performance using master slave replication Fully supporting Unicode and emojis in your app SQL prepared statements are broken when scaling applications MySQL code to populate DB on GitHub The code to populate our test DB is available on GitHub at: https://github.com/getvictor/mysql/tree/main/insert-with-subqueries-perf\nMySQL query performance: INSERT with subqueries video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-06T00:00:00Z","image":"https://victoronsoftware.com/posts/mysql-query-performance-insert-subqueries/INSERT%20with%20subqueries_hu11abea03c44d899aed8e0fa4c682eba3_53769_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mysql-query-performance-insert-subqueries/","title":"Optimize MySQL query performance: INSERT with subqueries"},{"content":" GitHub reusable workflows GitHub reusable steps (composite action) Introduction GitHub Actions is a way to automate your software development workflows. The approach is similar to CI/CD tools like Jenkins, CircleCI, and TravisCI. However, GitHub Actions are built into GitHub.\nHigh level diagram of GitHub Actions The entry point for GitHub Actions is the .github/workflows directory in your repository. This directory contains one or more YAML files that define your workflows. A workflow is an automated process made up of one or more jobs. Each job runs on a separate runner. A runner is a server that runs the job. A job contains one or more steps. Each step runs a separate command.\nWhy reuse? Code reuse is a fundamental principle of software development. Reusing GitHub Actions code allows you to:\nImprove maintainability by keeping common code in one place and reducing the amount of code Increase consistency since multiple workflows can use the same code Promote best practices Increase productivity Reduce errors Examples of reusable GitHub Actions code include:\nCode signing Uploading artifacts to cloud services Security checks Notifications and reports Data processing and many others Reusable workflows A reusable workflow replaces a job in the main workflow.\nGitHub Actions reusable workflow A reusable workflow may be shared across repositories and run on a different platform than the main workflow.\nFor file sharing, \u0026lsquo;build artifacts\u0026rsquo; must be used to share files with the main workflow. The reusable workflow does not inherit environment variables. However, it accepts inputs and secrets from the calling workflow and may use outputs to pass data back to the main workflow.\nHere is an example of a reusable workflow. It uses the same schema as a regular workflow.\nname: Reusable workflow on: workflow_call: inputs: reusable_input: description: \u0026#39;Input to the reusable workflow\u0026#39; required: true type: string filename: required: true type: string secrets: HELLO_WORLD_SECRET: required: true outputs: # Map the workflow output(s) to job output(s) reusable_output: description: \u0026#39;Output from the reusable workflow\u0026#39; value: ${{ jobs.reusable-workflow-job.outputs.job_output }} defaults: run: shell: bash jobs: reusable-workflow-job: runs-on: ubuntu-20.04 # Map the job output(s) to step output(s) outputs: job_output: ${{ steps.process-step.outputs.step_output }} steps: - name: Process reusable input id: process-step env: HELLO_WORLD_SECRET: ${{ secrets.HELLO_WORLD_SECRET }} run: | echo \u0026#34;reusable_input=${{ inputs.reusable_input }}\u0026#34; echo \u0026#34;HELLO_WORLD_SECRET=${HELLO_WORLD_SECRET}\u0026#34; echo \u0026#34;step_output=${{ inputs.reusable_input }}_processed\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT - uses: actions/download-artifact@v4 with: name: input_file - name: Process file run: | echo \u0026#34;Processing file: ${{ inputs.filename }}\u0026#34; echo \u0026#34;file processed\u0026#34; \u0026gt;\u0026gt; ${{ inputs.filename }} - uses: actions/upload-artifact@v4 with: name: output_file path: ${{ inputs.filename }} The reusable workflow is triggered on: workflow_call. It accepts an input called reusable_input and generates an output called reusable_output. It also downloads an artifact called input_file, processes a file, and uploads an artifact called output_file.\nThe main workflow calls the reusable workflow using the uses keyword.\njob-2: needs: job-1 # We do not need to check out the repository to use the reusable workflow uses: ./.github/workflows/reusable-workflow.yml with: reusable_input: \u0026#34;job-2-input\u0026#34; filename: \u0026#34;input.txt\u0026#34; secrets: # Can also implicitly pass the secrets with: secrets: inherit HELLO_WORLD_SECRET: TERCES_DLROW_OLLEH A successful run of the main workflow looks like this on GitHub:\nGitHub Actions reusable workflow success Reusable steps (composite action) Reusable steps replace a regular step in a job. We will use a composite action for reusable steps in our example.\nGitHub Actions reusable steps (composite action) Like a reusable workflow, a composite action may be shared across repositories, it accepts inputs, and it may use outputs to pass data back to the main workflow.\nUnlike a reusable workflow, a composite action inherits environment variables. However, it does not inherit secrets. Secrets must be passed explicitly as inputs or environment variables. Also, there is no need to use \u0026lsquo;build artifacts\u0026rsquo; to share files since the reusable steps run on the same runner and in the same work area as the main job.\nHere is an example of a composite action. It uses a different schema than a workflow. Also, the file must be named action.yml or similar.\nname: Reusable steps (AKA composite action) description: Demonstrate how to use reusable steps in a workflow # Schema: https://json.schemastore.org/github-action.json inputs: reusable_input: description: \u0026#39;Input to the reusable workflow\u0026#39; required: true filename: required: true outputs: # Map the action output(s) to step output(s) reusable_output: description: \u0026#39;Output from the reusable workflow\u0026#39; value: ${{ steps.process-step.outputs.step_output }} runs: using: \u0026#39;composite\u0026#39; steps: - name: Process reusable input id: process-step # Shell must explicitly specify the shell for each step. https://github.com/orgs/community/discussions/18597 shell: bash run: | echo \u0026#34;reusable_input=${{ inputs.reusable_input }}\u0026#34; echo \u0026#34;HELLO_WORLD_SECRET=${HELLO_WORLD_SECRET}\u0026#34; echo \u0026#34;step_output=${{ inputs.reusable_input }}_processed\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT - name: Process file shell: bash run: | echo \u0026#34;Processing file: ${{ inputs.filename }}\u0026#34; echo \u0026#34;file processed\u0026#34; \u0026gt;\u0026gt; ${{ inputs.filename }} The composite action is called via the uses setting on a step. Our action accepts an input called reusable_input and generates an output called reusable_output. It also processes a file called filename.\nThe following code snippet shows how to use the composite action in a job.\n- name: Use reusable steps id: reusable-steps uses: ./.github/reusable-steps # To use this syntax, we must have the repository checked out with: reusable_input: \u0026#34;job-2-input\u0026#34; filename: \u0026#34;input.txt\u0026#34; env: HELLO_WORLD_SECRET: TERCES_DLROW_OLLEH A successful run of the main workflow with reusable steps looks like this on GitHub:\nGitHub Actions composite action success Conclusion Reusable workflows and steps are powerful tools for improving the maintainability, consistency, and productivity of your GitHub Actions. They allow you to reuse code across repositories and workflows and promote best practices. They are a great way to reduce errors and increase productivity.\nFor larger units of work, a reusable workflow should be used. A composite action should be used for smaller units of work that may run on the same runner and share the same work area.\nExample code on GitHub The example code is available on GitHub at: https://github.com/getvictor/github-reusable-workflows-and-steps\nOther articles related to GitHub git merge and GitHub pull requests explained Use GitHub actions for general-purpose tasks GitHub Actions reusable workflows and steps video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-01T00:00:00Z","image":"https://victoronsoftware.com/posts/github-reusable-workflows-and-steps/GitHub%20Actions%20thumbnail_hu23dcbedfa159fd97fecd36bd64d9d886_118952_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/github-reusable-workflows-and-steps/","title":"How to reuse workflows and steps in GitHub Actions (2024)"},{"content":"What is an SQL deadlock? A deadlock occurs when two or more SQL transactions are waiting for each other to release locks. This can occur when two transactions have locks on separate resources and each is waiting for the other to release its lock.\nWhat is an upsert? An upsert combines the words \u0026ldquo;update\u0026rdquo; and \u0026ldquo;insert.\u0026rdquo; It is a database operation that inserts a new row into a table if the row does not exist or updates the row if it already exists.\nINSERT/UPDATE upsert pattern One common way to implement an upsert operation in MySQL is to use the following pattern:\nUPDATE table_name SET column1 = value1, column2 = value2 WHERE id = ?; -- If the UPDATE statement does not affect any rows, insert a new row: INSERT INTO table_name (id, column1, column2) VALUES (?, value1, value2); UPDATE returns the number of rows that were actually changed.\nThis UPDATE/INSERT pattern is optimized for frequent updates and rare inserts. However, it can lead to deadlocks when multiple transactions try to insert simultaneously.\nMySQL deadlock example We assume the default transaction isolation level of REPEATABLE READ. Given the following table with one row:\nCREATE TABLE my_table ( id int(10) unsigned NOT NULL, amount int(10) unsigned NOT NULL, PRIMARY KEY (id) ); INSERT INTO my_table (id, amount) VALUES (1, 1); One transaction executes the following SQL:\nUPDATE my_table SET amount = 2 WHERE id = 2; Another transaction executes the following SQL:\nUPDATE my_table SET amount = 3 WHERE id = 3; INSERT INTO my_table (id, amount) VALUES (3, 3); At this point, the second transaction is waiting for the first transaction to release the lock.\nThe first transaction then executes the following SQL:\nINSERT INTO my_table (id, amount) VALUES (2, 2); Causing a deadlock:\n[40001][1213] Deadlock found when trying to get lock; try restarting transaction Why does the deadlock occur? The deadlock occurs because both transactions set next-key locks on the rows they attempted to update. Since the rows they attempted to update did not exist, the lock is set on the \u0026ldquo;supremum\u0026rdquo; pseudo-record. This pseudo-record has a value higher than any value actually in the index. This lock prevents the other transaction from inserting the row it needs.\nDebugging MySQL deadlocks To view the last deadlock detected by MySQL, you can use:\nSHOW ENGINE INNODB STATUS; The output will contain a section like this:\n------------------------ LATEST DETECTED DEADLOCK ------------------------ 2024-04-28 12:29:17 281472351068032 *** (1) TRANSACTION: TRANSACTION 1638819, ACTIVE 7 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 3 lock struct(s), heap size 1128, 2 row lock(s) MySQL thread id 97926, OS thread handle 281471580295040, query id 24192112 192.168.65.1 root update /* ApplicationName=DataGrip 2024.1 */ INSERT INTO my_table (id, amount) VALUES (3, 3) *** (1) HOLDS THE LOCK(S): RECORD LOCKS space id 158 page no 4 n bits 72 index PRIMARY of table `test`.`my_table` trx id 1638819 lock_mode X Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 158 page no 4 n bits 72 index PRIMARY of table `test`.`my_table` trx id 1638819 lock_mode X insert intention waiting Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (2) TRANSACTION: TRANSACTION 1638812, ACTIVE 13 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 3 lock struct(s), heap size 1128, 2 row lock(s) MySQL thread id 97875, OS thread handle 281471585578880, query id 24192285 192.168.65.1 root update /* ApplicationName=DataGrip 2024.1 */ INSERT INTO my_table (id, amount) VALUES (2, 2) *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 158 page no 4 n bits 72 index PRIMARY of table `test`.`my_table` trx id 1638812 lock_mode X Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 158 page no 4 n bits 72 index PRIMARY of table `test`.`my_table` trx id 1638812 lock_mode X insert intention waiting Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; We can see the supremum locks held by both transactions: 0: len 8; hex 73757072656d756d; asc supremum;;.\nAnother way to debug MySQL deadlocks is to enable the innodb_print_all_deadlocks option. This option prints all deadlocks to the error log.\nPreventing the UPDATE/INSERT deadlock One way to prevent the deadlock is to use the INSERT \u0026hellip; ON DUPLICATE KEY UPDATE pattern. This syntax allows you to insert a new row or update an existing row in a single statement. However, it is slower than an UPDATE and always increments the auto-increment value if present.\nAnother way is to roll back the transaction once we know that the UPDATE did not affect any rows. This avoids the deadlock by not holding the lock while we insert the new row. After the rollback, we can retry the transaction using the above INSERT ... ON DUPLICATE KEY UPDATE pattern.\nA third way is not to use transactions. In this case, the locks are released immediately after the statement is executed. However, this approach may not be suitable for all use cases.\nConclusion The UPDATE/INSERT upsert pattern can lead to MySQL deadlocks when multiple transactions try to insert simultaneously. To prevent deadlocks, consider using the INSERT ... ON DUPLICATE KEY UPDATE pattern, rolling back the transaction, or not using transactions.\nMySQL deadlock on UPDATE/INSERT upsert pattern video Other articles related to MySQL Optimize MySQL query performance: INSERT with subqueries Master slave replication in MySQL Fully supporting Unicode and emojis in your app SQL prepared statements are broken when scaling applications Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-04-28T00:00:00Z","image":"https://victoronsoftware.com/posts/mysql-upsert-deadlock/MySQL%20deadlock_hu82fd7010b13095095640b2133c3dc2b5_70671_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mysql-upsert-deadlock/","title":"MySQL deadlock on UPDATE/INSERT upsert pattern"},{"content":"Introduction In this article, we will create a simple React app from scratch. We will not use any templates or helper scripts. We aim to reduce tool usage and fully understand each step of the process.\nWhat is React? React is a popular JavaScript library for building user interfaces. It was created by Meta (Facebook) and is maintained by Meta and a community of developers. React is used to build single-page applications (SPAs) and dynamic web applications.\nPrerequisites \u0026ndash; Node.js and npm Node.js and npm are the most popular tools for working with React. Node.js is a JavaScript runtime. npm is a package manager for Node.js. These two tools are essential for modern web development.\npackage.json We will start by creating a package.json file. This file contains metadata about the project and its dependencies. You can use the npm init command to create the package.json file. Or create one yourself containing something like:\n{ \u0026#34;name\u0026#34;: \u0026#34;react-hello-world\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Hello world app using React\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/react\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34; } TypeScript Next, we will add TypeScript to our project. TypeScript is a superset of JavaScript that adds static types to the language. It helps catch errors early in the development process and improves code quality.\nAlthough TypeScript is not required to build a React app, it is strongly recommended. TypeScript is widely used in the React community and provides many benefits. Modern IDEs, such as Visual Studio Code and WebStorm, support TypeScript, making development and learning easier.\nnpm install --save-dev typescript This command updates the package.json file with the TypeScript dependency.\n{ \u0026#34;name\u0026#34;: \u0026#34;react-hello-world\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Hello world app using React\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/react\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;typescript\u0026#34;: \u0026#34;^5.4.5\u0026#34; } } It also creates a node_modules directory. This directory contains the packages installed by npm.\nFinally, the command creates a package-lock.json file. This file locks the dependencies to specific versions, ensuring that the project is built with the same versions of the dependencies across different machines.\nThe --save-dev flag tells npm to save the package as a development dependency. Development dependencies are not required for the production build of the app.\ntsconfig.json We need to create a tsconfig.json file to configure TypeScript. This file specifies the root files and compiler options for the TypeScript compiler. We will extend the recommended base configuration.\nInstall the recommended configuration with the following:\nnpm install --save-dev @tsconfig/recommended Then, create a tsconfig.json file with the following content:\n{ \u0026#34;extends\u0026#34;: \u0026#34;@tsconfig/recommended/tsconfig.json\u0026#34;, \u0026#34;compilerOptions\u0026#34;: { \u0026#34;jsx\u0026#34;: \u0026#34;react-jsx\u0026#34; } } What is JSX? In our tsconfig.json file, we set the jsx option to react-jsx. This option tells TypeScript to treat JSX as React JSX.\nJSX is a syntax extension for JavaScript. It allows you to write HTML-like code in JavaScript. JSX is used in React. It is syntactic sugar that is generally transpiled into JavaScript by the build tool.\nReact and ReactDOM Next, we will add React and ReactDOM to our project. React is the base library. ReactDOM is the package that provides DOM-specific methods for React.\nnpm install react react-dom Since we are using TypeScript, we must also install type definitions for React and ReactDOM. The TypeScript compiler uses these definitions for type checking.\nnpm install --save-dev @types/react @types/react-dom What is Webpack? Webpack is a module bundler for JavaScript. It takes modules with dependencies and generates static assets representing those modules. We will use Webpack as the build tool for our React app.\nWe will install the Webpack packages:\nnpm install --save-dev webpack webpack-cli webpack-dev-server html-webpack-plugin ts-loader webpack is the core package webpack-cli provides the command-line interface, which we will use to run Webpack commands webpack-dev-server is a development server that serves the app html-webpack-plugin will generate the index.html file to serve our app ts-loader is a TypeScript loader for Webpack. It allows Webpack to compile TypeScript files. webpack.config.ts By default, Webpack does not need a configuration file. However, since we use TypeScript, we must create a webpack.config.ts file to configure Webpack.\nNote that we use the .ts extension for the configuration file. The TypeScript compiler will compile this file. Using a .js file is also possible, but we prefer TypeScript for type safety.\nNo additional type definitions are required for our Webpack configuration at this time.\nimport HtmlWebpackPlugin from \u0026#39;html-webpack-plugin\u0026#39;; module.exports = { entry: \u0026#39;./src/index.tsx\u0026#39;, module: { rules: [ { test: /\\.(ts|tsx)$/, loader: \u0026#34;ts-loader\u0026#34;, exclude: /node_modules/, }, ], }, plugins: [new HtmlWebpackPlugin()], } We specify src/index.tsx as our app\u0026rsquo;s top-level file. By default, the build\u0026rsquo;s output will go to the dist directory.\nWe configure the TypeScript loader to compile .ts and .tsx files.\nWe also use the html-webpack-plugin to generate an index.html file. This file will load the Webpack bundle.\nWe need to add a TypeScript execution engine to the Node.js runtime so that it can understand the above TypeScript configuration file. We will use ts-node for this purpose.\nnpm install --save-dev ts-node Final package.json After all the installations, our package.json file should look similar to this:\n{ \u0026#34;name\u0026#34;: \u0026#34;react-hello-world\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Hello world app using React\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/react\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;@tsconfig/recommended\u0026#34;: \u0026#34;^1.0.6\u0026#34;, \u0026#34;@types/react\u0026#34;: \u0026#34;^18.2.79\u0026#34;, \u0026#34;@types/react-dom\u0026#34;: \u0026#34;^18.2.25\u0026#34;, \u0026#34;html-webpack-plugin\u0026#34;: \u0026#34;^5.6.0\u0026#34;, \u0026#34;ts-loader\u0026#34;: \u0026#34;^9.5.1\u0026#34;, \u0026#34;ts-node\u0026#34;: \u0026#34;^10.9.2\u0026#34;, \u0026#34;typescript\u0026#34;: \u0026#34;^5.4.5\u0026#34;, \u0026#34;webpack\u0026#34;: \u0026#34;^5.91.0\u0026#34;, \u0026#34;webpack-cli\u0026#34;: \u0026#34;^5.1.4\u0026#34;, \u0026#34;webpack-dev-server\u0026#34;: \u0026#34;^5.0.4\u0026#34; }, \u0026#34;dependencies\u0026#34;: { \u0026#34;react\u0026#34;: \u0026#34;^18.2.0\u0026#34;, \u0026#34;react-dom\u0026#34;: \u0026#34;^18.2.0\u0026#34; } } src/index.tsx We are finally ready to write some React code. TSX files are TypeScript files that contain JSX.\nWe will create the src/index.tsx file. It will render a simple React component. React components are the reusable building blocks of React apps.\nimport React from \u0026#34;react\u0026#34;; import {createRoot} from \u0026#34;react-dom/client\u0026#34; // A simple Class component class HelloWorld extends React.Component { render() { return \u0026lt;h1\u0026gt;Hello world!\u0026lt;/h1\u0026gt; } } // Use traditional DOM manipulation to create a root element for React document.body.innerHTML = \u0026#39;\u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt;\u0026lt;/div\u0026gt;\u0026#39; // Create a root element for React const app = createRoot(document.getElementById(\u0026#34;app\u0026#34;)!) // Render our HelloWorld component app.render(\u0026lt;HelloWorld/\u0026gt;) Running the app on the Webpack development server Now, we can run the app on the Webpack development server. This server will serve the app and automatically reload the page when the code changes.\nnode_modules/.bin/webpack serve --mode development --open The --mode development flag tells Webpack to build the app in development mode. The --open flag tells Webpack to open the app in the default browser.\nThe browser should show the following:\nReact app served by Webpack dev server package.json scripts Instead of remembering the above webpack command, we can add a script to the package.json file to run the Webpack development server.\n\u0026#34;scripts\u0026#34;: { \u0026#34;start\u0026#34;: \u0026#34;webpack serve --mode development --open\u0026#34; } start is a special script name that maps to the npm start command. Now, we can run the development server with:\nnpm start or\nnpm run start Building the app for production To build the app for production, we can run:\nnode_modules/.bin/webpack --mode production This command will create a dist directory with the app\u0026rsquo;s production build. The directory will contain the index.html file and the main.js JavaScript bundle. The production files are optimized for performance, and they are minified and compressed to reduce their size.\nIt is possible to host these production files on a local HTTP server like Apache or Nginx, or deploy the app to cloud providers such as AWS, Cloudflare Pages, Netlify, Render, or Vercel.\nOther getting started guides Recently, we wrote about creating a Chrome extension from scratch without any additional tooling. As part of that series, we covered adding linting and formatting tooling for TypeScript. We also have a guide on using CGO in Go programming language. Example code on GitHub The example code is available on GitHub at https://github.com/getvictor/react/tree/main/1-hello-world\nReact Hello World video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-04-24T00:00:00Z","image":"https://victoronsoftware.com/posts/react-hello-world/react-hello-world_hu84b0260e55078cc47c5a5b8b4b4901b4_85634_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/react-hello-world/","title":"Build a React app from scratch: getting started (2024)"},{"content":"Why fix security vulnerabilities? Security vulnerabilities are a common issue in software development. They can lead to data breaches, unauthorized access, and other security incidents. It is important to fix security vulnerabilities as soon as possible to protect your data and users.\nFinding vulnerabilities Nowadays, it is possible to integrate various vulnerability scanning tools into your CI/CD pipeline. These tools can help you identify security vulnerabilities in your code and dependencies. One such tool is OpenSSF Scorecard, which combines multiple other tools into a single GitHub action. It uses the OSV service to find vulnerabilities affecting your project\u0026rsquo;s dependencies. OSV (Open Source Vulnerabilities) is a Google-based vulnerability database providing information about open-source projects\u0026rsquo; vulnerabilities.\nIn this article, we will focus on fixing a few recent real-world security vulnerabilities in our yarn.lock dependencies.\nscore is 3: 6 existing vulnerabilities detected: Warn: Project is vulnerable to: GHSA-crh6-fp67-6883 Warn: Project is vulnerable to: GHSA-wf5p-g6vw-rhxx Warn: Project is vulnerable to: GHSA-p6mc-m468-83gw Warn: Project is vulnerable to: GHSA-566m-qj78-rww5 Warn: Project is vulnerable to: GHSA-7fh5-64p2-3v2j Warn: Project is vulnerable to: GHSA-4wf5-vphf-c2xc Click Remediation section below to solve this issue Using local tools to find vulnerabilities In a local environment, we can use OSV-Scanner to find vulnerabilities in our dependencies. Running:\nosv-scanner scan --lockfile yarn.lock It will output the same vulnerabilities mentioned above but with additional details.\n╭─────────────────────────────────────┬──────┬───────────┬────────────────┬─────────┬───────────╮ │ OSV URL │ CVSS │ ECOSYSTEM │ PACKAGE │ VERSION │ SOURCE │ ├─────────────────────────────────────┼──────┼───────────┼────────────────┼─────────┼───────────┤ │ https://osv.dev/GHSA-crh6-fp67-6883 │ 9.8 │ npm │ @xmldom/xmldom │ 0.8.3 │ yarn.lock │ │ https://osv.dev/GHSA-wf5p-g6vw-rhxx │ 6.5 │ npm │ axios │ 0.21.4 │ yarn.lock │ │ https://osv.dev/GHSA-p6mc-m468-83gw │ 7.4 │ npm │ lodash.set │ 4.3.2 │ yarn.lock │ │ https://osv.dev/GHSA-566m-qj78-rww5 │ 5.3 │ npm │ postcss │ 6.0.23 │ yarn.lock │ │ https://osv.dev/GHSA-7fh5-64p2-3v2j │ 5.3 │ npm │ postcss │ 6.0.23 │ yarn.lock │ │ https://osv.dev/GHSA-7fh5-64p2-3v2j │ 5.3 │ npm │ postcss │ 7.0.39 │ yarn.lock │ │ https://osv.dev/GHSA-7fh5-64p2-3v2j │ 5.3 │ npm │ postcss │ 8.4.21 │ yarn.lock │ │ https://osv.dev/GHSA-4wf5-vphf-c2xc │ 7.5 │ npm │ terser │ 5.12.1 │ yarn.lock │ ╰─────────────────────────────────────┴──────┴───────────┴────────────────┴─────────┴───────────╯ Another way to find these vulnerabilities is by using the built-in yarn audit command.\nWaiving vulnerabilities In some cases, you may decide to waive a vulnerability. This approach means that you examine the vulnerability documentation and acknowledge it but decide not to fix it.\nTo waive a vulnerability for the OSV flow, you can create an osv-scanner.toml file in the root of your project. For example, to waive the GHSA-crh6-fp67-6883 vulnerability, you can add the following to the osv-scanner.toml file:\n[[IgnoredVulns]] id = \u0026#34;GHSA-crh6-fp67-6883\u0026#34; reason = \u0026#34;We examined this vulnerability and concluded that it does not affect our project for a very good reason.\u0026#34; In our example, we will not waive any vulnerabilities, but we will fix them by updating the dependencies.\nUpdating an inner dependency In our example, we have a vulnerability in the @xmldom/xmldom package. From the vulnerability URL, we know we must update this package to 0.8.4 or later.\nRunning yarn why @xmldom/xmldom will show that it is an inner dependency of another package:\n=\u0026gt; Found \u0026#34;@xmldom/xmldom@0.8.3\u0026#34; info Reasons this module exists - \u0026#34;msw#@mswjs#interceptors\u0026#34; depends on it - Hoisted from \u0026#34;msw#@mswjs#interceptors#@xmldom#xmldom\u0026#34; Looking at yarn.lock shows:\n\u0026#34;@xmldom/xmldom@^0.8.3\u0026#34;: version \u0026#34;0.8.3\u0026#34; resolved \u0026#34;https://registry.yarnpkg.com/@xmldom/xmldom/-/xmldom-0.8.3.tgz#beaf980612532aa9a3004aff7e428943aeaa0711\u0026#34; integrity sha512-Lv2vySXypg4nfa51LY1nU8yDAGo/5YwF+EY/rUZgIbfvwVARcd67ttCM8SMsTeJy51YhHYavEq+FS6R0hW9PFQ== We see that 0.8.4 will satisfy the dependency requirement of ^0.8.3. We can update the package by deleting the above section from yarn.lock and running yarn install\nWe will then see the update:\n\u0026#34;@xmldom/xmldom@^0.8.3\u0026#34;: version \u0026#34;0.8.10\u0026#34; resolved \u0026#34;https://registry.yarnpkg.com/@xmldom/xmldom/-/xmldom-0.8.10.tgz#a1337ca426aa61cef9fe15b5b28e340a72f6fa99\u0026#34; integrity sha512-2WALfTl4xo2SkGCYRt6rDTFfk9R1czmBvUQy12gK2KuRKIpWEhcbbzy8EZXtz/jkRqHX8bFEc6FC1HjX4TUWYw== Upgrading an inner dependency by overriding the version Our following vulnerability is in the axios package. We need to update it to 0.28.0 or later. By running yarn why axios we see that this package is part of a deep dependency chain:\n=\u0026gt; Found \u0026#34;wait-on#axios@0.21.4\u0026#34; info This module exists because \u0026#34;@storybook#test-runner#jest-playwright-preset#jest-process-manager#wait-on\u0026#34; depends on it. The needed version 0.28.0 does not satisfy the ^0.21.4 requirement. We can override the version by adding the following to the package.json file:\n\u0026#34;resolutions\u0026#34;: { \u0026#34;**/wait-on/axios\u0026#34;: \u0026#34;^0.28.0\u0026#34; }, Upgrading the parent dependency The following vulnerability is in the lodash.set package. The vulnerability URL shows that there is no fix for this vulnerability. We also see at npmjs.com that this package was last updated eight years ago.\nWe need to update the parent package that uses lodash.set. Running yarn why lodash.set shows:\ninfo Reasons this module exists - \u0026#34;nock\u0026#34; depends on it - Hoisted from \u0026#34;nock#lodash.set\u0026#34; We update the parent by running yarn upgrade nock@latest. Luckily, the latest version of nock does not depend on lodash.set, and lodash.set is removed from yarn.lock.\nRemoving a dependency Sometimes the best way to fix a vulnerability is to remove the vulnerable dependency. This can be done with the yarn remove \u0026lt;dependency\u0026gt; command. However, this requires code changes. You must find a different library or implement the removed functionality yourself.\nConclusion We use the above strategies to fix the vulnerabilities in our project.\nUpdating an inner dependency Upgrading an inner dependency by overriding the version Upgrading the parent dependency Removing a dependency We can now rerun the vulnerability scanner to verify that we fixed the vulnerabilities.\nIn addition, we must run our unit test and integration test suite to ensure that the updates do not break our application.\nFix security vulnerabilities in Yarn video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-04-10T00:00:00Z","image":"https://victoronsoftware.com/posts/fix-security-vulnerabilities-yarn/cover_huad6a6c1a561ddeac007cfa2eef03f368_47668_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/fix-security-vulnerabilities-yarn/","title":"Fix security vulnerabilities in Yarn"},{"content":"We recently completed a series of articles on mutual TLS (mTLS). In this series, we covered the basics of mTLS, how to use macOS keychain and Windows certificate store, and how to build an mTLS Go client. Our goal was to show you how to use mTLS in your applications and securely store your mTLS certificates and keys without exposing them to the filesystem.\nHere is a summary of the articles in the series:\nMutual TLS intro and hands-on example An introduction to mTLS and a hands-on example of using an mTLS client to connect to an mTLS server.\nUsing mTLS with the macOS keychain A guide on how to use the macOS system keystore to store your mTLS certificates and keys. We connect to an mTLS server with applications that use the macOS system keychain to find the mTLS certificates.\nCreate an mTLS Go client We create a standard mTLS client in Go using the crypto/tls library. This client loads the client certificate and private key from the filesystem.\nAdd a custom certificate signer to the mTLS Go client We implement a custom crypto.Signer to sign a client certificate during the mTLS handshake. Thus, we are a step closer to removing our client certificate and private key from the filesystem.\nA complete mTLS Go client using the macOS keychain In this article, we continue the previous article by connecting our custom signer to the macOS keychain using CGO and Apple APIs.\nUsing mTLS with the Windows certificate store Switching to Windows, we learn how to use the Windows system keystore to store your mTLS certificates and keys. We connect to an mTLS server with applications that use the Windows certificate store to find the mTLS certificates.\nCreate an mTLS Go client using the Windows certificate store Using the software pattern from the previous articles on the macOS keychain, we build an mTLS client in Go integrated with the Windows certificate store to store the mTLS certificates and keys.\nMutual TLS (mTLS): building a client using the system keystore video playlist ","date":"2024-04-01T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls/mtls-handshake_hu50cf7289da7ab96edbbfa70ffae00656_51351_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mtls/","title":"Mutual TLS (mTLS): building a client using the system keystore"},{"content":"What is code signing? Code signing is the process of digitally signing executables and scripts to confirm the software author and guarantee that the code has not been altered or corrupted since it was signed. The method employs a cryptographic hash to validate the authenticity and integrity of the code.\nThe benefits of code signing Code signing provides several benefits:\nUser trust: Users are likelier to trust signed software because they can verify its origin. Security: Code signing helps prevent tampering and makes sure that bad actors have not altered the software. Malware protection: Code signing helps protect users from malware by verifying the software\u0026rsquo;s authenticity. Software updates: Code signing helps users verify that software updates are legitimate and not malicious. Windows Defender: Code signing helps prevent Windows Defender warnings. Code signing process for Windows The code signing process for Windows involves the following steps:\nObtain a code signing certificate: Purchase a code signing certificate from a trusted certificate authority (CA) or use a self-signed certificate. Sign the code: Use a code signing tool to sign the code with the code signing certificate. Timestamp the signature: Timestamp the signature to make sure that the signature remains valid even after the certificate expires. Distribute the signed code: Distribute the signed code to users. Verify the signature: Users can verify the signature to confirm the software\u0026rsquo;s authenticity. Obtaining a code signing certificate In our example, we will use a self-signed certificate. This approach is suitable for internal business applications. For public applications, you should obtain a code signing certificate from a trusted CA.\nWe will use the OpenSSL command line tool to generate the certificates. OpenSSL is a popular open-source library for TLS and SSL protocols.\nThe following script generates the certificate and key needed for code signing. It also generates a certificate authority (CA) and signs the code signing certificate with the CA.\n#!/usr/bin/env bash # -e: Immediately exit if any command has a non-zero exit status. # -x: Print all executed commands to the terminal. # -u: Exit if an undefined variable is used. # -o pipefail: Exit if any command in a pipeline fails. set -exuo pipefail # This script generates certificates and keys needed for code signing. mkdir -p certs # Certificate authority (CA) openssl genrsa -out certs/ca.key 2048 openssl req -new -x509 -nodes -days 1000 -key certs/ca.key -out certs/ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testCodeSignCA\u0026#34; # Generate a certificate for code signing, signed by the CA openssl req -newkey rsa:2048 -nodes -keyout certs/sign.key -out certs/sign.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testCodeSignCert\u0026#34; openssl x509 -req -in certs/sign.req -days 398 -CA certs/ca.crt -CAkey certs/ca.key -set_serial 01 -out certs/sign.crt # Clean up rm certs/sign.req Building the application We will build a simple \u0026ldquo;Hello World\u0026rdquo; Windows application using the Go programming language for this example. We compile the application with:\nexport GOOS=windows export GOARCH=amd64 go build ./hello-world.go The Go build process generates the hello-world.exe Windows executable.\nSigning and timestamping the code To sign the code, we will use osslsigncode, an open-source code signing tool that uses OpenSSL to sign Windows executables. Unlike Microsoft\u0026rsquo;s signtool, osslsigncode is cross-platform and can be used on Linux and macOS.\nTo sign the code, we use the following script:\n#!/usr/bin/env bash # -e: Immediately exit if any command has a non-zero exit status. # -x: Print all executed commands to the terminal. # -u: Exit if an undefined variable is used. # -o pipefail: Exit if any command in a pipeline fails. set -exuo pipefail input_file=$1 if [ ! -f \u0026#34;$input_file\u0026#34; ] then echo \u0026#39;First argument must be path to binary\u0026#39; exit 1 fi # Check that input file is a windows PE (Portable Executable) if ! ( file \u0026#34;$input_file\u0026#34; | grep -q PE ) then echo \u0026#39;File must be a Portable Executable (PE) file.\u0026#39; exit 0 fi # Check that osslsigncode is installed if ! command -v osslsigncode \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 ; then echo \u0026#34;osslsigncode utility is not present or missing from PATH. Binary cannot be signed.\u0026#34; exit 1 fi orig_file=\u0026#34;${input_file}_unsigned\u0026#34; mv \u0026#34;$input_file\u0026#34; \u0026#34;$orig_file\u0026#34; osslsigncode sign -certs \u0026#34;./certs/sign.crt\u0026#34; -key \u0026#34;./certs/sign.key\u0026#34; -n \u0026#34;Hello Windows code signing\u0026#34; -i \u0026#34;https://victoronsoftware.com/\u0026#34; -t \u0026#34;http://timestamp.comodoca.com/authenticode\u0026#34; -in \u0026#34;$orig_file\u0026#34; -out \u0026#34;$input_file\u0026#34; rm \u0026#34;$orig_file\u0026#34; In addition to signing the code, we timestamp the signature using the Comodo server. Timestamping makes sure the signature remains valid even after the certificate expires or is invalidated.\nWe can use osslsigncode to verify the signature:\n#!/usr/bin/env bash input_file=$1 osslsigncode verify -CAfile ./certs/ca.crt \u0026#34;$input_file\u0026#34; Distributing and manually verifying the signed code After signing the code, we can distribute the signed executable to users. Users can manually verify the signature by right-clicking the executable, selecting \u0026ldquo;Properties,\u0026rdquo; and navigating to the \u0026ldquo;Digital Signatures\u0026rdquo; tab. The user can then view the certificate details and verify that the signature is valid.\nHowever, since we are using the self-signed certificate, users will see a warning that the certificate is not trusted. Our self-signed certificate is not trusted because the certificate authority is not part of the Windows trusted root certificate store.\nCertificate in code signature cannot be verified We can add the certificate authority to the Windows trusted root certificate store with the following Powershell command:\nImport-Certificate -FilePath \u0026#34;certs\\ca.crt\u0026#34; -CertStoreLocation Cert:\\LocalMachine\\Root After adding the certificate authority to the trusted root certificate store, users will see that the certificate is trusted and the signature is valid.\nCertificate in code signature is be verified Code signing using a certificate from a public CA To sign public applications, we must obtain a code signing certificate from a trusted CA. The latest industry standards require private keys for code signing certificates to be stored in hardware security modules (HSMs) to prevent unauthorized access. This security requirement means certificates for code signing in CI/CD pipelines must use a cloud HSM vendor or a private pipeline runner with an HSM.\nIn a future article, we will explore signing a Windows application using a cloud HSM vendor.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/code-sign-windows\nCode signing a Windows application video ","date":"2024-03-27T00:00:00Z","image":"https://victoronsoftware.com/posts/code-signing-windows/digital-signature-ok_hu33b7d29a52e912941cefe9572e7aaf3f_45104_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/code-signing-windows/","title":"Code signing a Windows application"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain mTLS Go client mTLS Go client with custom certificate signer mTLS Go client using macOS keychain mTLS with Windows certificate store Why use the Windows certificate store? Keeping the mTLS client private key on the filesystem is insecure and not recommended. In the mTLS Go client using macOS keychain, we demonstrated achieving greater mTLS security with macOS keychain. In this article, we reach a similar level of protection with the Windows certificate store.\nThe Windows certificate store is a secure location where certificates and keys can be stored. Many applications, such as Edge and Powershell, use it. The Windows certificate store is an excellent place to store mTLS client certificates and keys.\nBuilding a custom tls.Certificate for the Windows certificate store This work builds on the mTLS Go client with custom certificate signer article. We will use the CustomSigner from that article to build a custom tls.Certificate that uses the Windows certificate store.\nHowever, before the application uses the Public and Sign methods of the CustomSigner, we must retrieve the client certificate using Windows APIs.\nRetrieving mTLS client certificate from Windows certificate store using Go We will use the golang.org/x/sys/windows package to access the Windows APIs. We use the windows package to call the CertOpenStore, CertFindCertificateInStore, and CryptAcquireCertificatePrivateKey functions from the crypt32 DLL (dynamic link library).\nFirst, we open the MY store, which is the personal store for the current user. This store contains our client mTLS certificate.\n// Open the certificate store storePtr, err := windows.UTF16PtrFromString(windowsStoreName) if err != nil { return nil, err } store, err := windows.CertOpenStore( windows.CERT_STORE_PROV_SYSTEM, 0, uintptr(0), windows.CERT_SYSTEM_STORE_CURRENT_USER, uintptr(unsafe.Pointer(storePtr)), ) if err != nil { return nil, err } Next, we find the certificate by the common name.\n// Find the certificate var pPrevCertContext *windows.CertContext var certContext *windows.CertContext commonNamePtr, err := windows.UTF16PtrFromString(commonName) for { certContext, err = windows.CertFindCertificateInStore( store, windows.X509_ASN_ENCODING, 0, windows.CERT_FIND_SUBJECT_STR, unsafe.Pointer(commonNamePtr), pPrevCertContext, ) if err != nil { return nil, err } // We can extract the certificate chain and further filter the certificate // we want here. break } Converting the Windows certificate to a Go x509.Certificate After retrieving the certificate from the Windows certificate store, we convert it to a Go x509.Certificate.\n// Copy the certificate data so that we have our own copy outside the windows context encodedCert := unsafe.Slice(certContext.EncodedCert, certContext.Length) buf := bytes.Clone(encodedCert) foundCert, err := x509.ParseCertificate(buf) if err != nil { return nil, err } Building the custom tls.Certificate Finally, we put together the custom tls.Certificate using the x509.Certificate. We hold on to the certContext pointer to get the private key later.\ncustomSigner := \u0026amp;CustomSigner{ store: store, windowsCertContext: certContext, } customSigner.x509Cert = foundCert certificate := tls.Certificate{ Certificate: [][]byte{foundCert.Raw}, PrivateKey: customSigner, SupportedSignatureAlgorithms: []tls.SignatureScheme{supportedAlgorithm}, } Our example only supports the tls.PSSWithSHA256 signature algorithm to keep the code simple.\nSigning the mTLS digest with the Windows certificate store As discussed in the previous mTLS Go client with custom certificate signer article, we must sign the CertificateVerify message during the TLS handshake. We will use the CustomSigner to sign the digest, which implements the crypto.Signer interface as defined in the Go standard library\u0026rsquo;s crypto package.\n// CustomSigner is a crypto.Signer that uses the client certificate and key to sign type CustomSigner struct { store windows.Handle windowsCertContext *windows.CertContext x509Cert *x509.Certificate } func (k *CustomSigner) Public() crypto.PublicKey { fmt.Printf(\u0026#34;crypto.Signer.Public\\n\u0026#34;) return k.x509Cert.PublicKey } func (k *CustomSigner) Sign(_ io.Reader, digest []byte, opts crypto.SignerOpts ) (signature []byte, err error) { ... Retrieve the private key reference from the Windows certificate store We retrieve the private key reference from the Windows certificate store using the CryptAcquireCertificatePrivateKey function.\n// Get private key var ( privateKey windows.Handle pdwKeySpec uint32 pfCallerFreeProvOrNCryptKey bool ) err = windows.CryptAcquireCertificatePrivateKey( k.windowsCertContext, windows.CRYPT_ACQUIRE_CACHE_FLAG|windows.CRYPT_ACQUIRE_SILENT_FLAG| windows.CRYPT_ACQUIRE_ONLY_NCRYPT_KEY_FLAG, nil, \u0026amp;privateKey, \u0026amp;pdwKeySpec, \u0026amp;pfCallerFreeProvOrNCryptKey, ) if err != nil { return nil, err } Signing the mTLS digest We will use the NCryptSignHash function from ncrypt.dll to sign the digest.\nvar ( nCrypt = windows.MustLoadDLL(\u0026#34;ncrypt.dll\u0026#34;) nCryptSignHash = nCrypt.MustFindProc(\u0026#34;NCryptSignHash\u0026#34;) ) But before we do that, we must create a BCRYPT_PSS_PADDING_INFO structure for our supported RSA-PSS algorithm.\nflags := nCryptSilentFlag | bCryptPadPss pPaddingInfo, err := getRsaPssPadding(opts) if err != nil { return nil, err } Where getRsaPssPadding is a helper function:\nfunc getRsaPssPadding(opts crypto.SignerOpts) (unsafe.Pointer, error) { pssOpts, ok := opts.(*rsa.PSSOptions) if !ok || pssOpts.Hash != crypto.SHA256 { return nil, fmt.Errorf(\u0026#34;unsupported hash function %s\u0026#34;, opts.HashFunc().String()) } if pssOpts.SaltLength != rsa.PSSSaltLengthEqualsHash { return nil, fmt.Errorf(\u0026#34;unsupported salt length %d\u0026#34;, pssOpts.SaltLength) } sha256, _ := windows.UTF16PtrFromString(\u0026#34;SHA256\u0026#34;) // Create BCRYPT_PSS_PADDING_INFO structure: // typedef struct _BCRYPT_PSS_PADDING_INFO { // LPCWSTR pszAlgId; // ULONG cbSalt; // } BCRYPT_PSS_PADDING_INFO; return unsafe.Pointer( \u0026amp;struct { pszAlgId *uint16 cbSalt uint32 }{ pszAlgId: sha256, cbSalt: uint32(pssOpts.HashFunc().Size()), }, ), nil } Finally, we sign the digest using the NCryptSignHash function.\n// Sign the digest // The first call to NCryptSignHash retrieves the size of the signature var size uint32 success, _, _ := nCryptSignHash.Call( uintptr(privateKey), uintptr(pPaddingInfo), uintptr(unsafe.Pointer(\u0026amp;digest[0])), uintptr(len(digest)), uintptr(0), uintptr(0), uintptr(unsafe.Pointer(\u0026amp;size)), uintptr(flags), ) if success != 0 { return nil, fmt.Errorf(\u0026#34;NCryptSignHash: failed to get signature length: %#x\u0026#34;, success) } // The second call to NCryptSignHash retrieves the signature signature = make([]byte, size) success, _, _ = nCryptSignHash.Call( uintptr(privateKey), uintptr(pPaddingInfo), uintptr(unsafe.Pointer(\u0026amp;digest[0])), uintptr(len(digest)), uintptr(unsafe.Pointer(\u0026amp;signature[0])), uintptr(size), uintptr(unsafe.Pointer(\u0026amp;size)), uintptr(flags), ) if success != 0 { return nil, fmt.Errorf(\u0026#34;NCryptSignHash: failed to generate signature: %#x\u0026#34;, success) } return signature, nil Putting it all together With the above code, we can create our new Go mTLS client that uses the Windows certificate store.\nfunc main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } client := http.Client{ Transport: \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ GetClientCertificate: signer.GetClientCertificate, MinVersion: tls.VersionTLS13, MaxVersion: tls.VersionTLS13, }, }, } // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } We limit the scope of this example to TLS 1.3\nSetting up the environment The next step is to use the Windows certificate store to store the client certificate and private key. We will use the certificates and keys scripts from the previous mTLS with Windows certificate store article. If you still need to generate the certificates and keys, please follow the instructions in that article.\nFinally, as in the mTLS with Windows certificate store article, we start two nginx servers:\nhttps://\u0026lt;your_host\u0026gt;:8888 for TLS https://\u0026lt;your_host\u0026gt;:8889 for mTLS Running the Go mTLS client using the Windows certificate store We can run our mTLS client without pointing to certificate/key files and retrieving everything from the Windows certificate store. Hitting the ordinary TLS server:\ngo run .\\client-signer.go --url https://myhost:8888/hello-world.txt Returns the expected:\nTLS Hello World! While hitting the mTLS server:\ngo run .\\client-signer.go --url https://myhost:8889/hello-world.txt Returns a more detailed message, including the print statements in our custom code:\nServer requested certificate Found certificate with common name testClientTLS crypto.Signer.Public crypto.Signer.Public crypto.Signer.Sign with key type *rsa.PublicKey, opts type *rsa.PSSOptions, hash SHA-256 mTLS Hello World! Example code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-go-windows\nmTLS Go client using Windows certificate store video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-03-20T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-go-client-windows-certificate-store/mtls-go-windows_hucfa29512f6a856070bdc43f20ddc6a98_80828_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mtls-go-client-windows-certificate-store/","title":"Mutual TLS (mTLS) Go client using Windows certificate store"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain mTLS Go client mTLS Go client with custom certificate signer mTLS Go client using macOS keychain Why use Windows certificate store? In our previous articles, we introduced mTLS and demonstrated how to use mTLS client certificates and keys. Keeping the mTLS client private key on the filesystem is insecure and not recommended. In the mTLS Go client using macOS keychain, we demonstrated achieving greater mTLS security with macOS keychain. In this article, we start exploring how to achieve the same level of protection with Windows certificate store.\nThe Windows certificate store is a secure location to store certificates and keys. Many applications, such as Edge and Powershell use it. The Windows certificate store is an excellent place to store mTLS client certificates and keys.\nThe Windows certificate stores have two types:\nUser certificate store: Certificates and keys are stored for the current user, local to a user account. Local machine certificate store: Certificates and keys are stored for all users on the computer. We will store our client mTLS certificate in the user certificate store and the other certificates in the local machine certificate store.\nGenerating mTLS certificates and keys We will use the following Powershell script to generate the mTLS certificates and keys. OpenSSL must be installed on your computer.\nNew-Item -ItemType Directory -Force certs # Private keys for CAs openssl genrsa -out certs/server-ca.key 2048 openssl genrsa -out certs/client-ca.key 2048 # Generate CA certificates openssl req -new -x509 -nodes -days 1000 -key certs/server-ca.key -out certs/server-ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testServerCA\u0026#34; openssl req -new -x509 -nodes -days 1000 -key certs/client-ca.key -out certs/client-ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testClientCA\u0026#34; # Generate a certificate signing request openssl req -newkey rsa:2048 -nodes -keyout certs/server.key -out certs/server.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testServerTLS\u0026#34; openssl req -newkey rsa:2048 -nodes -keyout certs/client.key -out certs/client.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testClientTLS\u0026#34; # Have the CA sign the certificate requests and output the certificates. openssl x509 -req -in certs/server.req -days 398 -CA certs/server-ca.crt -CAkey certs/server-ca.key -set_serial 01 -out certs/server.crt -extfile localhost.ext openssl x509 -req -in certs/client.req -days 398 -CA certs/client-ca.crt -CAkey certs/client-ca.key -set_serial 01 -out certs/client.crt # Create PFX file for importing to certificate store openssl pkcs12 -export -out certs\\client.pfx -inkey certs\\client.key -in certs\\client.crt -passout pass: # Clean up Remove-Item certs/server.req Remove-Item certs/client.req The maximum validity period for a TLS certificate is 398 days.\nThe localhost.ext file is used to specify the subject alternative name (SAN) for the server certificate. The localhost.ext file contains the following:\n[alt_names] DNS.1 = localhost DNS.2 = myhost We can access the server using either localhost or myhost names.\nThe above script generates the following files:\ncerts/server-ca.crt: Server CA certificate certs/server-ca.key: Server CA private key certs/client-ca.crt: Client CA certificate certs/client-ca.key: Client CA private key certs/server.crt: Server certificate certs/server.key: Server private key certs/client.crt: Client certificate certs/client.key: Client private key certs/client.pfx: Client certificate and private key in PFX format, needed for importing into the Windows certificate store Importing the client certificate and key into the Windows certificate store We will import the client certificate and key into the user certificate store using the following powershell script.\n# Import the server CA Import-Certificate -FilePath \u0026#34;certs\\server-ca.crt\u0026#34; -CertStoreLocation Cert:\\LocalMachine\\Root # Import the client CA so that client TLS certificates can be verified Import-Certificate -FilePath \u0026#34;certs\\client-ca.crt\u0026#34; -CertStoreLocation Cert:\\LocalMachine\\Root # Import the client TLS certificate and key Import-PfxCertificate -FilePath \u0026#34;certs\\client.pfx\u0026#34; -CertStoreLocation Cert:\\CurrentUser\\My The command result should be similar to the following:\nPSParentPath: Microsoft.PowerShell.Security\\Certificate::LocalMachine\\Root Thumbprint Subject ---------- ------- 0A31BF3C48A3D98A91A2F63B5BD286818311A707 CN=testServerCA, OU=Your Unit, O=Your Organization, L=Austin, S=Texas, C=US 7F7E5612F3A90B9EB246762358251F98911A9D1A CN=testClientCA, OU=Your Unit, O=Your Organization, L=Austin, S=Texas, C=US PSParentPath: Microsoft.PowerShell.Security\\Certificate::CurrentUser\\My Thumbprint Subject ---------- ------- E2EBB991E3849E32E934D8465FAE42787D34C9ED CN=testClientTLS, OU=Your Unit, O=Your Organization, L=Austin, S=Texas, C=US By default, the private key is marked as non-exportable. A user or an application cannot export the private key from the certificate store. They can only access the private key via Windows APIs. Using a non-exportable private key is the recommended security approach. You can use the -Exportable parameter if you need to export the private key.\nVerifying imported certificates and keys As an extra step, we can verify that the certificates and keys exist in the Windows certificate store. We can use the certlm Local Machine Certificate Manager GUI, certmgr User Certificate Manager GUI, or the Get-ChildItem powershell command.\nGet-ChildItem -Path Cert:\\LocalMachine\\Root | Where-Object{$_.Subject -match \u0026#39;testServerCA\u0026#39;} | Test-Certificate -Policy SSL Get-ChildItem -Path Cert:\\CurrentUser\\My | Where-Object{$_.Subject -match \u0026#39;testClientTLS\u0026#39;} Running the mTLS server We will use the same docker-compose.yml file from the mTLS Hello World article. The docker-compose.yml file starts two nginx servers:\nhttps://\u0026lt;your_host\u0026gt;:8888 for TLS https://\u0026lt;your_host\u0026gt;:8889 for mTLS We can run Docker on WSL (Windows Subsystem for Linux) or another machine. We will run it on a different machine, so we need to copy the certs directory to the machine running Docker. When running the server on a different machine, we must update the C:\\Windows\\System32\\drivers\\etc\\hosts file to point to the other machine.\n10.0.0.5 myhost Connecting to the TLS and mTLS servers with clients Because we added the server CA to the root certificate store, we can now access the TLS server without any additional flags:\nInvoke-WebRequest -Uri https://myhost:8888/hello-world.txt Result:\nStatusCode : 200 StatusDescription : OK Content : TLS Hello World! RawContent : HTTP/1.1 200 OK Connection: keep-alive Accept-Ranges: bytes Content-Length: 17 Content-Type: text/plain Date: Sun, 03 Mar 2024 17:28:29 GMT ETag: \u0026#34;65b29c19-11\u0026#34; Last-Modified: Thu, 25 Jan 2024 1... Forms : {} Headers : {[Connection, keep-alive], [Accept-Ranges, bytes], [Content-Length, 17], [Content-Type, text/plain]...} Images : {} InputFields : {} Links : {} ParsedHtml : System.__ComObject RawContentLength : 17 However, we cannot access the mTLS server directly.\nInvoke-WebRequest -Uri https://myhost:8889/hello-world.txt The client attempted the TLS handshake, but the server rejected the connection because the client did not provide a certificate. Result:\nInvoke-WebRequest : 400 Bad Request No required SSL certificate was sent nginx/1.25.3 At line:1 char:1 + Invoke-WebRequest -Uri https://myhost:8889/hello-world.txt + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : InvalidOperation: (System.Net.HttpWebRequest:HttpWebRequest) [Invoke-WebRequest], WebException + FullyQualifiedErrorId : WebCmdletWebResponseException,Microsoft.PowerShell.Commands.InvokeWebRequestCommand We can, however, provide the client certificate thumbprint to access the mTLS server. We saw the thumbprint of the client certificate earlier when we imported it into the Windows certificate store.\nInvoke-WebRequest -Uri https://myhost:8889/hello-world.txt -CertificateThumbprint E2EBB991E3849E32E934D8465FAE42787D34C9ED Result:\nStatusCode : 200 StatusDescription : OK Content : mTLS Hello World! RawContent : HTTP/1.1 200 OK Connection: keep-alive Accept-Ranges: bytes Content-Length: 18 Content-Type: text/plain Date: Sun, 03 Mar 2024 17:31:55 GMT ETag: \u0026#34;65b29c19-12\u0026#34; Last-Modified: Thu, 25 Jan 2024 1... Forms : {} Headers : {[Connection, keep-alive], [Accept-Ranges, bytes], [Content-Length, 18], [Content-Type, text/plain]...} Images : {} InputFields : {} Links : {} ParsedHtml : System.__ComObject RawContentLength : 18 Edge browser can access the mTLS server. We can verify this by opening the following URL:\nhttps://myhost:8889/hello-world.txt We see the following popup:\nEdge mTLS popup\nWe can click OK to connect to the mTLS server. Future connections will not show the popup and will automatically use the client certificate.\nNote: Here is a helpful link that may resolve issues trying to use mTLS client certificates on Windows 10: https://superuser.com/questions/1181163/unable-to-use-client-certificates-in-chrome-or-ie-on-windows-10\nExample code on Github The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-with-windows\nCreating our own Windows mTLS client In the following article, we will create a custom Windows mTLS client using the Windows certificate store.\nmTLS with Windows certificate store video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-03-06T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-with-windows/mtls-edge_hu1c5448f514f25a5b4cbc44c26f1cdd9f_40356_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mtls-with-windows/","title":"Mutual TLS (mTLS) with Windows certificate store"},{"content":"Introduction Any app aiming to reach an international audience must support Unicode. Emojis, which are based on Unicode, are everywhere. They are used in text messages, social media, and programming languages. Supporting Unicode and emojis in your app can be tricky. This article will cover common Unicode and emoji support issues and how to fix them.\nWhat is Unicode? Unicode is a standard for encoding, representing, and handling text. It is a character set that assigns a unique number to every character. The most common encoding for Unicode is UTF-8, which stands for Unicode Transformation Format 8-bit. UTF-8 is a variable-width encoding that can represent every character in the Unicode character set.\nUTF-8 format can take one to four bytes to represent a code point. Multiple code points can be combined to form a single character. For example, the emoji \u0026ldquo;👍\u0026rdquo; is represented by the code point U+1F44D. In UTF-8, it is represented by the bytes F0 9F 91 8D. The same emoji with skin tone \u0026ldquo;👍🏽\u0026rdquo; is represented by the code point U+1F44D U+1F3FD. In UTF-8, that emoji is represented by the bytes F0 9F 91 8D F0 9F 8F BD. Generally, emojis take up at least four bytes in UTF-8.\nUnicode equivalence Our first gotcha is unicode equivalence.\nUnicode equivalence is the concept that two different sequences of code points can represent the same character. For example, the character é can be represented by the code point U+00E9 or by the sequence of code points U+0065 U+0301. The first representation is the composed form, and the second is the decomposed form. Unicode equivalence is essential when comparing strings or searching for a string character.\nDatabases typically do not support Unicode equivalence out of the box. For example, given this table using MySQL 5.7:\nCREATE TABLE test ( id INT UNSIGNED NOT NULL AUTO_INCREMENT, name VARCHAR(255) NOT NULL, PRIMARY KEY (id)) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; INSERT INTO test (name) VALUES (\u0026#39;가\u0026#39;), (CONCAT(\u0026#39;ᄀ\u0026#39;, \u0026#39;ᅡ\u0026#39;)); SELECT * from test WHERE name = \u0026#39;가\u0026#39;; The query will return a single row, even though the Korean character 가 and character sequence ᄀ + ᅡ are equivalent. The incorrect result is because the utf8mb4_unicode_ci collation does not support Unicode equivalence. One way to fix this is to use the utf8mb4_0900_ai_ci collation, which supports Unicode equivalence. However, this requires updating the database to MySQL 8.0 or later, which may not be possible in some cases.\nEmoji equivalence Our second gotcha is emoji equivalence.\nSome databases may not support emoji equivalence out of the box. For example, given this table using MySQL 5.7:\nCREATE TABLE test ( id INT UNSIGNED NOT NULL AUTO_INCREMENT, name VARCHAR(255) NOT NULL, PRIMARY KEY (id)) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; INSERT INTO test (name) VALUES (\u0026#39;🔥\u0026#39;), (\u0026#39;🔥🔥\u0026#39;), (\u0026#39;👍\u0026#39;), (\u0026#39;👍🏽\u0026#39;); SELECT * from test WHERE name = \u0026#39;🔥\u0026#39;; The query will return:\n1,🔥 3,👍 And the following query:\nSELECT * from test WHERE name LIKE \u0026#39;%🔥%\u0026#39;; Will return:\n1,🔥 2,🔥🔥 The utf8mb4_unicode_ci collation does not support emoji equivalence, and the behavior of = differs from LIKE.\nOne way to fix the problem of emoji equivalence is to use a different collation during the = comparison. For example:\nSELECT * from test WHERE name COLLATE utf8mb4_bin = \u0026#39;🔥\u0026#39;; Will return the single correct result:\n1,🔥 However, this solution is not ideal because it requires the developer to remember to use the utf8mb4_bin collation for emoji equivalence. There is also a slight performance impact when using a different collation.\nCase-insensitive sorting Our third gotcha is sorting.\nTypically, app users want to see case-insensitive sorting of strings. For example, the strings \u0026ldquo;apple\u0026rdquo;, \u0026ldquo;Banana\u0026rdquo;, and \u0026ldquo;cherry\u0026rdquo; should be sorted as \u0026ldquo;apple\u0026rdquo;, \u0026ldquo;Banana\u0026rdquo;, and \u0026ldquo;cherry\u0026rdquo;. The utf8mb4_unicode_ci collation used above supports case-insensitive sorting. However, switching to another collation, such as utf8mb4_bin, to support emoji equivalence will break case-insensitive sorting. Hence, whatever solution you develop for full Unicode support should also support case-insensitive sorting.\nSolving our gotchas with normalization A partial solution to the above gotchas is to use normalization. Normalization is the process of transforming text into a standard form. Unicode defines four normalization forms: NFC, NFD, NFKC, and NFKD. The most common normalization form is NFC, which is the composed form. NFC is the standard form for most text processing.\nFor example, in the following Go code:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;golang.org/x/text/unicode/norm\u0026#34; \u0026#34;strconv\u0026#34; ) func main() { str1, _ := strconv.Unquote(`\u0026#34;\\uAC00\u0026#34;`) // 가 str2, _ := strconv.Unquote(`\u0026#34;\\u1100\\u1161\u0026#34;`) // ᄀ + ᅡ fmt.Println(str1) fmt.Println(str2) if str1 == str2 { fmt.Println(\u0026#34;raw equal\u0026#34;) } else { fmt.Println(\u0026#34;raw not equal\u0026#34;) } strNorm1 := norm.NFC.String(str1) strNorm2 := norm.NFC.String(str2) if strNorm1 == strNorm2 { fmt.Println(\u0026#34;normalized equal\u0026#34;) } else { fmt.Println(\u0026#34;normalized not equal\u0026#34;) } } The two strings are not equal in their raw form but equal after normalization. Normalizing before inserting, updating, and searching in the database can solve the Unicode equivalence issue while allowing the user to keep the case-insensitive sorting.\nTo solve emoji equivalence, we can use the utf8mb4_bin collation for the = comparison. However, if our column is indexed, we may need to use the utf8mb4_bin collation for the index. We cannot have a different collation for the column and the index, but we could use a second generated column with the utf8mb4_bin collation and index that column.\nConclusion Unicode and emoji support is essential for any app aiming to reach an international audience. Unicode equivalence, emoji equivalence, and case-insensitive sorting are common issues with Unicode and emoji support. Normalization can solve the Unicode equivalence issue while allowing the user to keep the case-insensitive sorting. Using the utf8mb4_bin collation for the = comparison can solve the emoji equivalence issue.\nFully supporting Unicode and emojis in your app video Other articles related to MySQL Optimize MySQL query performance: INSERT with subqueries MySQL deadlock on UPDATE/INSERT upsert pattern SQL prepared statements are broken when scaling applications Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-02-29T00:00:00Z","image":"https://victoronsoftware.com/posts/unicode-and-emoji-gotchas/unicode-emoji_hu444d31683f0bbae4748081daece648d9_456893_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/unicode-and-emoji-gotchas/","title":"Fully supporting Unicode and emojis in your app"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain mTLS Go client mTLS Go client with custom certificate signer Why use macOS keychain? In the mTLS Go client article, we built a simple Go client that uses mTLS. Our client used Go standard library methods and loaded the client certificate and private key from the filesystem. However, keeping the private key on the filesystem is insecure and not recommended. We aim to build an mTLS client fully integrated with the operating system\u0026rsquo;s keystore.\nThe macOS keychain is a secure storage system for passwords and other confidential information. It is used by many Apple applications, such as Safari, Mail, and iCloud, to store the user\u0026rsquo;s passwords and additional sensitive information.\nBuilding a custom tls.Certificate for macOS keychain This work builds on the mTLS Go client with custom certificate signer article. We will use the CustomSigner from that article to build a custom tls.Certificate that uses the macOS keychain.\nHowever, before the application uses the Public and Sign methods of the CustomSigner, we need to retrieve the certificate from the keychain using Apple\u0026rsquo;s API.\nRetrieving certificate from macOS keychain with CGO We will use CGO to call the macOS keychain API to retrieve the client certificate. To set up CGO, we include the following code above our imports:\n/* #cgo LDFLAGS: -framework CoreFoundation -framework Security #include \u0026lt;CoreFoundation/CoreFoundation.h\u0026gt; #include \u0026lt;Security/Security.h\u0026gt; */ import \u0026#34;C\u0026#34; To find the identities from the keychain, we use SecItemCopyMatching. An identity is a certificate and its associated private key.\nidentitySearch := C.CFDictionaryCreateMutable( C.kCFAllocatorDefault, maxCertificatesNum, \u0026amp;C.kCFTypeDictionaryKeyCallBacks, \u0026amp;C.kCFTypeDictionaryValueCallBacks, ) defer C.CFRelease(C.CFTypeRef(unsafe.Pointer(identitySearch))) const commonName = \u0026#34;testClientTLS\u0026#34; var commonNameCFString = stringToCFString(commonName) defer C.CFRelease(C.CFTypeRef(commonNameCFString)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecClass), unsafe.Pointer(C.kSecClassIdentity)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecAttrCanSign), unsafe.Pointer(C.kCFBooleanTrue)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecMatchSubjectWholeString), unsafe.Pointer(commonNameCFString)) // To filter by issuers, we must provide a CFDataRef array of DER-encoded ASN.1 items. // C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecMatchIssuers), unsafe.Pointer(issuerCFArray)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecReturnRef), unsafe.Pointer(C.kCFBooleanTrue)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecMatchLimit), unsafe.Pointer(C.kSecMatchLimitAll)) var identityMatches C.CFTypeRef if status := C.SecItemCopyMatching(C.CFDictionaryRef(identitySearch), \u0026amp;identityMatches); status != C.errSecSuccess { return nil, fmt.Errorf(\u0026#34;failed to find client certificate: %v\u0026#34;, status) } defer C.CFRelease(identityMatches) In our example, we find the identities by a common name, which we hardcode for demonstration purposes. We can filter by the certificate issuer, as shown in the commented-out code. Filtering by issuer requires an array of DER-encoded ASN.1 items, which can be created from the tls.CertificateRequestInfo object. Another approach to finding the proper certificate is to retrieve all the keychain certificates and filter them in Go code.\nConverting the Apple identity to a Go x509.Certificate After we retrieve the array of identities from the keychain, we convert them to Go x509.Certificate objects and pick the first one that is not expired.\nvar foundCert *x509.Certificate var foundIdentity C.SecIdentityRef identityMatchesArrayRef := C.CFArrayRef(identityMatches) numIdentities := int(C.CFArrayGetCount(identityMatchesArrayRef)) fmt.Printf(\u0026#34;Found %d identities\\n\u0026#34;, numIdentities) for i := 0; i \u0026lt; numIdentities; i++ { identityMatch := C.CFArrayGetValueAtIndex(identityMatchesArrayRef, C.CFIndex(i)) x509Cert, err := identityRefToCert(C.SecIdentityRef(identityMatch)) if err != nil { continue } // Make sure certificate is not expired if x509Cert.NotAfter.After(time.Now()) { foundCert = x509Cert foundIdentity = C.SecIdentityRef(identityMatch) fmt.Printf(\u0026#34;Found certificate from issuer %s with public key type %T\\n\u0026#34;, x509Cert.Issuer.String(), x509Cert.PublicKey) break } } The identityRefToCert function converts the SecIdentityRef to a Go x509.Certificate object. It exports the certificate to PEM format using SecItemExport and then parses the PEM to get the x509.Certificate object.\nfunc identityRefToCert(identityRef C.SecIdentityRef) (*x509.Certificate, error) { // Convert the identity to a certificate var certificateRef C.SecCertificateRef if status := C.SecIdentityCopyCertificate(identityRef, \u0026amp;certificateRef); status != 0 { return nil, fmt.Errorf(\u0026#34;failed to get certificate from identity: %v\u0026#34;, status) } defer C.CFRelease(C.CFTypeRef(certificateRef)) // Export the certificate to PEM // SecItemExport: https://developer.apple.com/documentation/security/1394828-secitemexport var pemDataRef C.CFDataRef if status := C.SecItemExport( C.CFTypeRef(certificateRef), C.kSecFormatPEMSequence, C.kSecItemPemArmour, nil, \u0026amp;pemDataRef, ); status != 0 { return nil, fmt.Errorf(\u0026#34;failed to export certificate to PEM: %v\u0026#34;, status) } defer C.CFRelease(C.CFTypeRef(pemDataRef)) certPEM := C.GoBytes(unsafe.Pointer(C.CFDataGetBytePtr(pemDataRef)), C.int(C.CFDataGetLength(pemDataRef))) var x509Cert *x509.Certificate for block, rest := pem.Decode(certPEM); block != nil; block, rest = pem.Decode(rest) { if block.Type == \u0026#34;CERTIFICATE\u0026#34; { var err error x509Cert, err = x509.ParseCertificate(block.Bytes) if err != nil { return nil, fmt.Errorf(\u0026#34;error parsing client certificate: %v\u0026#34;, err) } } } return x509Cert, nil } Retrieve the private key reference from the keychain At this point, we also retrieve the private key reference from the keychain. We will use the private key reference to sign the CertificateVerify message during the TLS handshake. The reference does not contain the private key. When importing private keys to the keychain, they should be marked as non-exportable so that no one can retrieve the private key cleartext from the keychain.\nvar privateKey C.SecKeyRef if status := C.SecIdentityCopyPrivateKey(C.SecIdentityRef(foundIdentity), \u0026amp;privateKey); status != 0 { return nil, fmt.Errorf(\u0026#34;failed to copy private key ref from identity: %v\u0026#34;, status) } Building the custom tls.Certificate Finally, we put together the custom tls.Certificate using the x509.Certificate and the private key reference.\ncustomSigner := \u0026amp;CustomSigner{ x509Cert: foundCert, privateKey: privateKey, } certificate := tls.Certificate{ Certificate: [][]byte{foundCert.Raw}, PrivateKey: customSigner, SupportedSignatureAlgorithms: []tls.SignatureScheme{supportedAlgorithm}, } Our example only supports the tls.PSSWithSHA256 signature algorithm to keep the code simple. Adding additional algorithm support is easy since it only requires passing the right parameter to the SecKeyCreateSignature function, which we will review next.\nSigning the mTLS digest with Apple\u0026rsquo;s keychain As discussed in the previous mTLS Go client with custom certificate signer article, we need to sign the CertificateVerify message during the TLS handshake. We will use the CustomSigner to sign the digest, which implements the crypto.Signer interface as defined in the Go standard library\u0026rsquo;s crypto package.\ntype CustomSigner struct { x509Cert *x509.Certificate privateKey C.SecKeyRef } func (k *CustomSigner) Public() crypto.PublicKey { fmt.Printf(\u0026#34;crypto.Signer.Public\\n\u0026#34;) return k.x509Cert.PublicKey } func (k *CustomSigner) Sign(_ io.Reader, digest []byte, opts crypto.SignerOpts) ( signature []byte, err error) { fmt.Printf(\u0026#34;crypto.Signer.Sign with key type %T, opts type %T, hash %s\\n\u0026#34;, k.Public(), opts, opts.HashFunc().String()) // Convert the digest to a CFDataRef digestCFData := C.CFDataCreate(C.kCFAllocatorDefault, (*C.UInt8)(unsafe.Pointer(\u0026amp;digest[0])), C.CFIndex(len(digest))) defer C.CFRelease(C.CFTypeRef(digestCFData)) // SecKeyAlgorithm: https://developer.apple.com/documentation/security/seckeyalgorithm // SecKeyCreateSignature: https://developer.apple.com/documentation/security/1643916-seckeycreatesignature var cfErrorRef C.CFErrorRef signCFData := C.SecKeyCreateSignature( k.privateKey, C.kSecKeyAlgorithmRSASignatureDigestPSSSHA256, C.CFDataRef(digestCFData), \u0026amp;cfErrorRef, ) if cfErrorRef != 0 { return nil, fmt.Errorf(\u0026#34;failed to sign data: %v\u0026#34;, cfErrorRef) } defer C.CFRelease(C.CFTypeRef(signCFData)) // Convert CFDataRef to Go byte slice return C.GoBytes(unsafe.Pointer(C.CFDataGetBytePtr(signCFData)), C.int(C.CFDataGetLength(signCFData))), nil } We use the SecKeyCreateSignature function to sign the digest. The function takes the private key reference, the algorithm, the digest, and a pointer to a CFErrorRef. The function returns a CFDataRef, which we convert to a Go byte slice. Additional algorithms can be supported by passing the proper parameter to the SecKeyCreateSignature function.\nPutting it all together With the above code, we can create our new Go mTLS client that uses the macOS keychain.\nfunc main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } client := http.Client{ Transport: \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ GetClientCertificate: signer.GetClientCertificate, MinVersion: tls.VersionTLS13, MaxVersion: tls.VersionTLS13, }, }, } // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } We limit the scope of this example to TLS 1.3\nBuild the mTLS client With go build client-signer.go, we generate the client-signer executable.\nSetting up the environment The next step is to use the macOS keychain to store the client certificate and private key. We will use the same certificates and keys script from the mTLS with macOS keychain article. If you still need to generate the certificates and keys, please follow the instructions in that article.\nWe must also import the generated certificates and keys into the macOS keychain.\n# Import the server CA security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain certs/server-ca.crt # Import the client CA so that client TLS certificates can be verified security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain certs/client-ca.crt # Import the client TLS certificate and key security import certs/client.crt -k /Library/Keychains/System.keychain security import certs/client.key -k /Library/Keychains/System.keychain -x -T $PWD/client-signer -T /usr/bin/curl -T /Applications/Safari.app -T \u0026#39;/Applications/Google Chrome.app\u0026#39; We specify our application $PWD/client-signer as one of the trusted applications that can access the private key. If we do not select the trusted application, we will get a security pop-up whenever our app tries to access the private key.\nFinally, as in the mTLS Hello World article, we will use docker compose up to start two nginx servers:\nhttps://localhost:8888 for TLS https://localhost:8889 for mTLS Running the Go mTLS client using the macOS keychain We can now run our mTLS client without pointing to certificate and key files. Hitting the ordinary TLS server:\n./client-signer --url https://localhost:8888/hello-world.txt Returns the expected:\nTLS Hello World! While hitting the mTLS server:\n./client-signer --url https://localhost:8889/hello-world.txt Returns a more detailed message, including the print statements in our custom code:\nServer requested certificate Found 1 identities Found certificate from issuer CN=testClientCA,OU=Your Unit,O=Your Organization,L=Austin,ST=Texas,C=US with public key type *rsa.PublicKey crypto.Signer.Public crypto.Signer.Public crypto.Signer.Sign with key type *rsa.PublicKey, opts type *rsa.PSSOptions, hash SHA-256 mTLS Hello World! Using certificate and key from the Windows certificate store The following article will explore using the Windows certificate store to hold the mTLS client certificate and private key.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-go-apple-keychain\nmTLS Go client using macOS keychain video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-02-22T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-go-client-using-apple-keychain/mtls-go-apple-keychain_hud963623ee8c982f67106bb07901b6549_184805_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mtls-go-client-using-apple-keychain/","title":"Mutual TLS (mTLS) Go client using macOS keychain"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain mTLS Go client Why a custom certificate signer? In the mTLS Go client article, we built a simple Go client that uses mTLS. Our client used Go standard library methods and loaded the client certificate and private key from the filesystem. However, keeping the private key on the filesystem is insecure and not recommended. We aim to build an mTLS client fully integrated with the operating system\u0026rsquo;s keystore.\nThe first step toward that goal is to extract the functionality of the mTLS handshake that requires the private key. Luckily, the client\u0026rsquo;s private key is only needed to sign the CertificateVerify message. The CertificateVerify message is the last in the mTLS handshake. It proves to the server that the client has the private key associated with the client certificate.\nFrom Wikipedia entry on TLS:\nThe client sends a CertificateVerify message, which is a signature over the previous handshake messages using the client\u0026rsquo;s certificate\u0026rsquo;s private key. This signature can be verified by using the client\u0026rsquo;s certificate\u0026rsquo;s public key. This lets the server know that the client has access to the private key of the certificate and thus owns the certificate.\nSetting up the environment We will use the same certificates and keys script from the mTLS with macOS keychain article. If you still need to generate the certificates and keys, please follow the instructions in that article.\nIn addition, we will import the generated certificates and keys into the macOS keychain. (In a future article, we will use the Windows Certificate Store instead.)\nFinally, as in the mTLS Hello World article, we will use docker compose up to start two nginx servers:\nhttps://localhost:8888 for TLS https://localhost:8889 for mTLS Building our crypto.Signer We will build a custom crypto.Signer that signs the CertificateVerify message. The crypto.Signer interface is defined in the Go standard library\u0026rsquo;s crypto package. It is used to sign messages with a private key.\n// CustomSigner is a crypto.Signer that uses the client certificate and key to sign type CustomSigner struct { x509Cert *x509.Certificate clientCertPath string clientKeyPath string } func (k *CustomSigner) Public() crypto.PublicKey { fmt.Printf(\u0026#34;crypto.Signer.Public\\n\u0026#34;) return k.x509Cert.PublicKey } func (k *CustomSigner) Sign(rand io.Reader, digest []byte, opts crypto.SignerOpts) ( signature []byte, err error) { fmt.Printf(\u0026#34;crypto.Signer.Sign\\n\u0026#34;) tlsCert, err := tls.LoadX509KeyPair(k.clientCertPath, k.clientKeyPath) if err != nil { log.Fatalf(\u0026#34;error loading client certificate: %v\u0026#34;, err) } fmt.Printf(\u0026#34;Sign using %T\\n\u0026#34;, tlsCert.PrivateKey) return tlsCert.PrivateKey.(crypto.Signer).Sign(rand, digest, opts) } Although we still use the filesystem to load the client certificate and private key, we now use the crypto.Signer interface to sign the CertificateVerify message. In the future, we will replace this code by calls to the operating system\u0026rsquo;s keystore. The vital thing to note is that we only load the private key when we need to sign the digest and do not load the key during the client configuration.\nGetting the client certificate Besides building a custom crypto.Signer, we will implement a custom GetClientCertificate function. This function will be called during the TLS handshake when the server requests a certificate from the client. The function will load the client certificate and create a CustomSigner instance. It will not load the private key at this time. Once again, the client certificate is only loaded when needed and not during the client\u0026rsquo;s configuration.\nWe set Certificate: [][]byte{cert.Raw}, because the Go implementation of the TLS handshake requires the client certificate here to validate it against the server\u0026rsquo;s CA.\nfunc GetClientCertificate(clientCertPath string, clientKeyPath string) (*tls.Certificate, error) { fmt.Printf(\u0026#34;Server requested certificate\\n\u0026#34;) if clientCertPath == \u0026#34;\u0026#34; || clientKeyPath == \u0026#34;\u0026#34; { return nil, errors.New(\u0026#34;client certificate and key are required\u0026#34;) } clientBytes, err := os.ReadFile(clientCertPath) if err != nil { return nil, fmt.Errorf(\u0026#34;error reading client certificate: %w\u0026#34;, err) } var cert *x509.Certificate for block, rest := pem.Decode(clientBytes); block != nil; block, rest = pem.Decode(rest) { if block.Type == \u0026#34;CERTIFICATE\u0026#34; { cert, err = x509.ParseCertificate(block.Bytes) if err != nil { return nil, fmt.Errorf(\u0026#34;error parsing client certificate: %v\u0026#34;, err) } } } certificate := tls.Certificate{ Certificate: [][]byte{cert.Raw}, PrivateKey: \u0026amp;CustomSigner{ x509Cert: cert, clientCertPath: clientCertPath, clientKeyPath: clientKeyPath, }, } return \u0026amp;certificate, nil } Putting it all together With the above customizations, we create our new Go mTLS client:\npackage main import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/getvictor/mtls/signer\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) clientCert := flag.String(\u0026#34;cert\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Client certificate file\u0026#34;) clientKey := flag.String(\u0026#34;key\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Client key file\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } client := http.Client{ Transport: \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ GetClientCertificate: func(info *tls.CertificateRequestInfo) ( *tls.Certificate, error) { return signer.GetClientCertificate(*clientCert, *clientKey) }, }, }, } // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } Trying to hit the mTLS server with:\ngo run client-signer.go --url https://localhost:8889/hello-world.txt --cert certs/client.crt --key certs/client.key Returns the expected result:\nmTLS Hello World! Using certificate and key from the macOS keychain In the following article, we will use the macOS keychain to load the client certificate and generate the CertificateVerify message without extracting the private key.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-go-custom-signer\nmTLS Go client with custom certificate signer video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-02-14T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-go-custom-signer/signer_hua0a59126f5575ffea6eb49be2c13e061_265380_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mtls-go-custom-signer/","title":"Mutual TLS (mTLS) Go client with custom certificate signer"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain What is Go? Go is a statically typed, compiled programming language designed at Google. It is known for its simplicity, efficiency, and ease of use. Go is often used for building web servers, APIs, and command-line tools. We will use Go to make a client that uses mTLS.\nSetting up the environment We will use the same certificates and keys script from the mTLS with macOS keychain article. If you still need to generate the certificates and keys, please follow the instructions in that article.\nIn addition, we will import the generated certificates and keys into the macOS keychain. (In a future article, we will use the Windows Certificate Store instead.) Keeping private keys on the filesystem is insecure and not recommended. We aim to build an mTLS client fully integrated with the operating system\u0026rsquo;s keystore.\nFinally, as in the mTLS Hello World article, we will use docker compose up to start two nginx servers:\nhttps://localhost:8888 for TLS https://localhost:8889 for mTLS Building the TLS Go client Below is a simple Go HTTP client.\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } client := http.Client{} // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } Trying the ordinary TLS server with:\ngo run client.go --url https://localhost:8888/hello-world.txt Gives the expected result:\nTLS Hello World! The Go client is integrated with the system keystore out of the box.\nHowever, when trying the mTLS server with the following:\ngo run client.go --url https://localhost:8889/hello-world.txt We get the error:\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;400 No required SSL certificate was sent\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;400 Bad Request\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;center\u0026gt;No required SSL certificate was sent\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx/1.25.3\u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; The Go libraries are not integrated with the system keystore for using the mTLS client certificate and key.\nModifying the Go client for mTLS We will use the crypto/tls package to build the mTLS client.\npackage main import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) clientCert := flag.String(\u0026#34;cert\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Client certificate file\u0026#34;) clientKey := flag.String(\u0026#34;key\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Client key file\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } var certificate tls.Certificate if *clientCert != \u0026#34;\u0026#34; \u0026amp;\u0026amp; *clientKey != \u0026#34;\u0026#34; { var err error certificate, err = tls.LoadX509KeyPair(*clientCert, *clientKey) if err != nil { log.Fatalf(\u0026#34;error loading client certificate: %v\u0026#34;, err) } } client := http.Client{ Transport: \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ Certificates: []tls.Certificate{certificate}, }, }, } // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } Now, trying the mTLS server with:\ngo run client-mtls.go --url https://localhost:8889/hello-world.txt --cert certs/client.crt --key certs/client.key Returns the expected result:\nmTLS Hello World! However, we pass the client certificate and key as command-line arguments. In a real-world scenario, we want to use the system keystore to manage the client certificate and key.\nUsing a custom signer for the mTLS client certificate The following article will cover creating a custom Go signer for the mTLS client certificate. This work will pave the way for us to use the system keystore to manage the client certificate and key.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-with-go\nmTLS Go client video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-02-07T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-go-client/go-client_hufc23aaa179917ecd1cfc752f8bc6da43_88506_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mtls-go-client/","title":"Mutual TLS (mTLS) Go client"},{"content":"This article is part of a series on mTLS. Check out the previous article: mTLS Hello World.\nSecuring mTLS certificates and keys In the mTLS Hello World article, we generated mTLS certificates and keys for the client and the server. We also created two certificate authorities (CAs) and signed the client and server certificates with their respective CAs. We ended up with the following files:\nserver CA: certs/server-ca.crt server CA private key: certs/server-ca.key TLS certificate for localhost server: certs/server.crt server TLS certificate private key: certs/server.key client CA: certs/client-ca.crt client CA private key: certs/client-ca.key TLS certificate for client: certs/client.crt client TLS certificate private key: certs/client.key In a real-world scenario, we would need to secure these files. The server CA private key and the client CA private key are the most important files to secure. If an attacker gets access to these files, they can create new certificates and impersonate the server or the client. These two files should be secured in a dedicated secure storage.\nThe server will need access to the client CA, the server TLS certificate, and the server TLS certificate private key. The server TLS certificate private key is the most important to secure out of these three files.\nThe client will need access to the server CA, the client TLS certificate, and the client TLS certificate private key. We can use the macOS keychain to secure these files. In a future article, we will show how to secure these on Windows with certificate stores.\nApple\u0026rsquo;s macOS keychain As I\u0026rsquo;ve written in inspecting keychain files on macOS, keychains are the macOS\u0026rsquo;s method to track and protect secure information such as passwords, private keys, and certificates.\nThe system keychain is located at /Library/Keychains/System.keychain. It contains the root certificates and other certificates. The login keychain is located at /Users/\u0026lt;username\u0026gt;/Library/Keychains/login.keychain-db. It contains the user\u0026rsquo;s certificates and private keys. In this example, we will use the system keychain, which all users on the system can access.\nGenerating mTLS certificates and keys We will use the following script to generate the mTLS certificates and keys. It resembles the script from the mTLS Hello World article.\n#!/bin/bash # This script generates certificates and keys needed for mTLS. mkdir -p certs # Private keys for CAs openssl genrsa -out certs/server-ca.key 2048 openssl genrsa -out certs/client-ca.key 2048 # Generate CA certificates openssl req -new -x509 -nodes -days 1000 -key certs/server-ca.key -out certs/server-ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testServerCA\u0026#34; openssl req -new -x509 -nodes -days 1000 -key certs/client-ca.key -out certs/client-ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testClientCA\u0026#34; # Generate a certificate signing request openssl req -newkey rsa:2048 -nodes -keyout certs/server.key -out certs/server.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testServerTLS\u0026#34; openssl req -newkey rsa:2048 -nodes -keyout certs/client.key -out certs/client.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testClientTLS\u0026#34; # Have the CA sign the certificate requests and output the certificates. openssl x509 -req -in certs/server.req -days 398 -CA certs/server-ca.crt -CAkey certs/server-ca.key -set_serial 01 -out certs/server.crt -extfile localhost.ext openssl x509 -req -in certs/client.req -days 398 -CA certs/client-ca.crt -CAkey certs/client-ca.key -set_serial 01 -out certs/client.crt # Clean up rm certs/server.req rm certs/client.req The maximum validity period for a TLS certificate is 398 days. Apple will reject certificates with a more extended validity period.\nImporting client mTLS certificates and keys into the macOS keychain We will import the client mTLS certificates and keys into the macOS keychain using the following script. The script uses the security command line tool. Accessing the system keychain must be run as root (sudo).\n#!/bin/bash # This script imports mTLS certificates and keys into the Apple Keychain. # Import the server CA security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain certs/server-ca.crt # Import the client CA so that client TLS certificates can be verified security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain certs/client-ca.crt # Import the client TLS certificate and key security import certs/client.crt -k /Library/Keychains/System.keychain security import certs/client.key -k /Library/Keychains/System.keychain -x -T /usr/bin/curl -T /Applications/Safari.app -T \u0026#39;/Applications/Google Chrome.app\u0026#39; The -x option marks the imported key as non-extractable. No application or user can view the private key once it is imported. The private key can only be used indirectly via Apple\u0026rsquo;s APIs.\nThe -T option specifies the applications that can access the key. Additional applications may be added later to the access control list.\nVerifying imported certificates and keys As an extra step, we can verify the client and server certificates before using them in an application.\nWe can verify the server certificate by running the following command:\nsecurity verify-cert -c certs/server.crt -p ssl -s localhost -k /Library/Keychains/System.keychain The output should include:\n...certificate verification successful. The Apple keychain automatically combines the certificate and the private key into an identity. We can verify the client identity by running the following command:\nsecurity find-identity -p ssl-client /Library/Keychains/System.keychain The list of identities should include:\nPolicy: SSL (client) Matching identities 1) B307B90CCD374080E74F1B15AF602B35A75D8401 \u0026#34;testClientTLS\u0026#34; 1 identities found Valid identities only 1) B307B90CCD374080E74F1B15AF602B35A75D8401 \u0026#34;testClientTLS\u0026#34; 1 valid identities found macOS can validate the identity because we also imported the client CA into the system keychain.\nRunning the mTLS server As in the mTLS Hello World article, we will use docker compose up to start two nginx servers:\nhttps://localhost:8888 for TLS https://localhost:8889 for mTLS Connecting to the TLS and mTLS servers with clients Because the server CA was added to the system keychain, curl can now access the TLS server without any additional flags:\ncurl https://localhost:8888/hello-world.txt However, the built-in curl client cannot access the mTLS server. We use the -v option for additional information:\ncurl -v https://localhost:8889/hello-world.txt The output:\n* Trying [::1]:8889... * Connected to localhost (::1) port 8889 * ALPN: curl offers h2,http/1.1 * (304) (OUT), TLS handshake, Client hello (1): * CAfile: /etc/ssl/cert.pem * CApath: none * (304) (IN), TLS handshake, Server hello (2): * (304) (IN), TLS handshake, Unknown (8): * (304) (IN), TLS handshake, Request CERT (13): * (304) (IN), TLS handshake, Certificate (11): * (304) (IN), TLS handshake, CERT verify (15): * (304) (IN), TLS handshake, Finished (20): * (304) (OUT), TLS handshake, Certificate (11): * (304) (OUT), TLS handshake, Finished (20): * SSL connection using TLSv1.3 / AEAD-CHACHA20-POLY1305-SHA256 * ALPN: server accepted http/1.1 * Server certificate: * subject: C=US; ST=Texas; L=Austin; O=Your Organization; OU=Your Unit; CN=testServerTLS * start date: Jan 28 17:08:10 2024 GMT * expire date: Mar 1 17:08:10 2025 GMT * subjectAltName: host \u0026#34;localhost\u0026#34; matched cert\u0026#39;s \u0026#34;localhost\u0026#34; * issuer: C=US; ST=Texas; L=Austin; O=Your Organization; OU=Your Unit; CN=testServerCA * SSL certificate verify ok. * using HTTP/1.1 \u0026gt; GET /hello-world.txt HTTP/1.1 \u0026gt; Host: localhost:8889 \u0026gt; User-Agent: curl/8.4.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 400 Bad Request \u0026lt; Server: nginx/1.25.3 \u0026lt; Date: Sun, 28 Jan 2024 18:28:20 GMT \u0026lt; Content-Type: text/html \u0026lt; Content-Length: 237 \u0026lt; Connection: close \u0026lt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;400 No required SSL certificate was sent\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;400 Bad Request\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;center\u0026gt;No required SSL certificate was sent\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx/1.25.3\u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; * Closing connection The client attempted the TLS handshake, but the server rejected the connection because the client did not provide a certificate. Our built-in curl client does not currently support mTLS using the macOS keychain. The client used for this example is:\ncurl 8.4.0 (x86_64-apple-darwin23.0) libcurl/8.4.0 (SecureTransport) LibreSSL/3.3.6 zlib/1.2.12 nghttp2/1.55.1 Release-Date: 2023-10-11 Protocols: dict file ftp ftps gopher gophers http https imap imaps ldap ldaps mqtt pop3 pop3s rtsp smb smbs smtp smtps telnet tftp Features: alt-svc AsynchDNS GSS-API HSTS HTTP2 HTTPS-proxy IPv6 Kerberos Largefile libz MultiSSL NTLM NTLM_WB SPNEGO SSL threadsafe UnixSockets On the other hand, Safari can access the mTLS server. We can verify this by opening the following URL in Safari:\nhttps://localhost:8889/hello-world.txt We see the following popup:\nSafari mTLS popup\nWe can click Continue to connect to the mTLS server. Future connections will not show the popup and will automatically use the client certificate.\nGoogle Chrome\u0026rsquo;s behavior is similar.\nNote: If we did not add Safari as an application that can access the client key, Safari would ask for a username and password to connect to the system keychain.\nCreating our own mTLS client In the following article, we will create our own mTLS client with the Go programming language. This is the first step toward creating an mTLS client integrated with the macOS keychain.\nLater, we will use mTLS with the Windows certificate store and create an mTLS client integrated with the Windows certificate store.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-with-apple-keychain\nmTLS with macOS keychain video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-01-31T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-with-apple-keychain/mtls-safari_hued6427f760192911581582be7803ed21_87861_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mtls-with-apple-keychain/","title":"Mutual TLS (mTLS) with macOS keychain"},{"content":"What is mTLS (mutual TLS)? TLS stands for Transport Layer Security. It is a cryptographic protocol that provides privacy and data integrity between two communicating applications. It is the successor to SSL (Secure Sockets Layer).\nIn ordinary (non-mutual) TLS, the client authenticates the server, but the server does not authenticate the client. Most websites use regular TLS. The client (web browser) knows it is talking to the correct server (website), but the server knows very little about the client. Instead, web applications use other client authentication methods, such as passwords, cookies, and session tokens.\nMutual TLS (mTLS) is a way to authenticate both the client and the server in a TLS connection. It is also known as client certificate authentication. In addition to the server authenticating itself to the client, the client also authenticates itself to the server.\nmTLS is helpful as an additional layer of security. It is used in many applications, including:\nVPNs Microservices Service mesh IoT (Internet of Things) Mobile apps How does Fleet Device Management use mTLS? Many of Fleet\u0026rsquo;s customers use mTLS as an additional layer of security to authenticate the Fleet server to the Fleet agent. The Fleet agent is a small program that runs on each host device, such as a corporate laptop. It collects information about the host and sends it to the Fleet server.\nHow does mTLS work? TLS is a complex protocol with multiple versions (1.2, 1.3, etc.). We will only go over the basics to understand how mTLS works.\nTLS uses a handshake protocol to establish a secure connection. The handshake protocol is a series of messages between the client and the server.\nThe client sends a \u0026ldquo;Client Hello\u0026rdquo; message to the server. The server responds with a \u0026ldquo;Server Hello\u0026rdquo; message and sends its certificate to the client. As an additional step for mTLS, the server requests a certificate from the client.\nThe client verifies the server\u0026rsquo;s certificate by checking the certificate\u0026rsquo;s signature and verifying that the certificate is valid and has not expired. The client also checks that the server\u0026rsquo;s hostname matches the hostname in the certificate.\nThe client uses the server\u0026rsquo;s public key to encrypt the messages sent to the server, including the session key and its certificate. The server decrypts these messages with its private key.\nThe client also sends a digital signature, encrypted with its private key, to the server. The server verifies the signature by decrypting it with the client\u0026rsquo;s public key.\nAt this point, both the client and the server have verified each other\u0026rsquo;s identity. They complete the TLS handshake and can exchange encrypted messages using a symmetric session key.\nGenerate certificates and keys We will use the OpenSSL command line tool to generate the certificates. OpenSSL is a popular open-source library for TLS and SSL protocols.\nThe following script generates the certificates and keys for the client and the server. It also creates two certificate authorities (CAs) and signs the client and server certificates with their respective CA. The same CA may sign the certificates, but we will use separate CAs for this example.\n#!/bin/bash # This script generates files needed for mTLS. mkdir -p certs # Private keys for CAs openssl genrsa -out certs/server-ca.key 2048 openssl genrsa -out certs/client-ca.key 2048 # Generate CA certificates openssl req -new -x509 -nodes -days 1000 -key certs/server-ca.key -out certs/server-ca.crt openssl req -new -x509 -nodes -days 1000 -key certs/client-ca.key -out certs/client-ca.crt # Generate a certificate signing request openssl req -newkey rsa:2048 -nodes -keyout certs/server.key -out certs/server.req openssl req -newkey rsa:2048 -nodes -keyout certs/client.key -out certs/client.req # Have the CA sign the certificate requests and output the certificates. openssl x509 -req -in certs/server.req -days 1000 -CA certs/server-ca.crt -CAkey certs/server-ca.key -set_serial 01 -out certs/server.crt -extfile localhost.ext openssl x509 -req -in certs/client.req -days 1000 -CA certs/client-ca.crt -CAkey certs/client-ca.key -set_serial 01 -out certs/client.crt # Clean up rm certs/server.req rm certs/client.req The localhost.ext file is used to specify the hostname for the server certificate. In our example, we will use localhost. The file contains the following:\nauthorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment subjectAltName = @alt_names [alt_names] DNS.1 = localhost Run the mTLS server We will use nginx as our mTLS server. nginx is a popular open-source web server.\nUsing docker compose, we can run two nginx servers. One server will use ordinary TLS, and one will use mutual TLS. We will use the following docker-compose.yml file:\n--- version: \u0026#34;2\u0026#34; services: nginx-tls: image: nginx volumes: - ./certs/server.crt:/etc/nginx/certificates/server.crt - ./certs/server.key:/etc/nginx/certificates/server.key - ./nginx-tls/nginx.conf:/etc/nginx/conf.d/default.conf - ./nginx-tls/hello-world.txt:/www/data/hello-world.txt ports: - \u0026#34;8888:8888\u0026#34; nginx-mtls: image: nginx volumes: - ./certs/server.crt:/etc/nginx/certificates/server.crt - ./certs/server.key:/etc/nginx/certificates/server.key - ./certs/client-ca.crt:/etc/nginx/certificates/client-ca.crt - ./nginx-mtls/nginx.conf:/etc/nginx/conf.d/default.conf - ./nginx-mtls/hello-world.txt:/www/data/hello-world.txt ports: - \u0026#34;8889:8889\u0026#34; The nginx-tls service uses the nginx-tls/nginx.conf file, which contains the following:\nserver { listen 8888 ssl; server_name tls-hello-world; # Server TLS certificate (client must have the CA cert to connect) ssl_certificate /etc/nginx/certificates/server.crt; ssl_certificate_key /etc/nginx/certificates/server.key; location / { root /www/data; } } The nginx-mtls service uses the nginx-mtls/nginx.conf file, which contains the following:\nserver { listen 8889 ssl; server_name mtls-hello-world; # Server TLS certificate (client must have the CA cert to connect) ssl_certificate /etc/nginx/certificates/server.crt; ssl_certificate_key /etc/nginx/certificates/server.key; # Enable mTLS ssl_client_certificate /etc/nginx/certificates/client-ca.crt; ssl_verify_client on; location / { root /www/data; } } The hello-world.txt files contain a simple text message.\nConnect to the mTLS server with curl client We can connect to the mTLS server with the curl command line tool. We will use the following command:\ncurl https://localhost:8889/hello-world.txt --cacert ./certs/server-ca.crt --cert ./certs/client.crt --key ./certs/client.key The --cacert option specifies the CA certificate that signed the server certificate. The --cert and --key options select the client certificate and key.\nTo connect to the ordinary TLS server, we do not need to specify the client certificate and key:\ncurl https://localhost:8888/hello-world.txt --cacert ./certs/server-ca.crt Curl can use --insecure to ignore the server certificate:\ncurl --insecure https://localhost:8888/hello-world.txt However, it is impossible to ignore the client certificate for mTLS. The server will reject the connection if the client does not provide a valid certificate.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/hello-world\nSecuring mTLS certificates and keys In the next article, we will secure the mTLS certificates and keys with the macOS keychain.\nIn a later article, we also secure the mTLS certificates and keys with the Windows certificate store.\nThis article is part of a series on mTLS.\nmTLS Hello World video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-01-24T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-hello-world/mtls-handshake_hu50cf7289da7ab96edbbfa70ffae00656_51351_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mtls-hello-world/","title":"Mutual TLS intro and hands-on example"},{"content":"Simple CGO examples CGO is a way to call C code from Go. It helps call existing C libraries or for performance reasons. CGO is enabled by default but can be disabled with the -cgo build flag.\nBelow is a simple example of calling a C function from Go.\npackage main /* double add(double a, double b) { return a + b; } */ import \u0026#34;C\u0026#34; import \u0026#34;fmt\u0026#34; func main() { fmt.Println(C.add(1, 2)) } The C code is embedded in the Go code as a comment above import \u0026quot;C\u0026quot;. The comment must start with /* and end with */. The C code must be valid. The Go compiler compiles the C code and links the resulting object file with the Go code.\nHere is an example of using an existing C library.\npackage main /* #include \u0026#34;math.h\u0026#34; double add(double a, double b) { return a + b; } */ import \u0026#34;C\u0026#34; import \u0026#34;fmt\u0026#34; func main() { fmt.Println(C.floor(C.add(1, 2.1))) } We call the floor function from the math.h library. The math.h library is included with the C compiler, so we don\u0026rsquo;t need to do anything special to use it.\nCGO Hello World fail Here is another example where we print \u0026ldquo;Hello World\u0026rdquo; from C.\npackage main /* #include \u0026#34;stdio.h\u0026#34; */ import \u0026#34;C\u0026#34; func main() { C.printf(C.CString(\u0026#34;Hello World\\n\u0026#34;)) } However, the above seemingly straightforward example will fail to compile with the following enigmatic error:\ncgo: ./exmaple.go:9:2: unexpected type: ... The problem is that printf is a variadic function that can take a variable number of arguments. CGO does not support variadic functions. Even using Go variadic syntax will not work:\nargs := []interface{}{} C.printf(C.CString(\u0026#34;Hello World\\n\u0026#34;), args...) The workaround for this is to use another non-variadic function, such as vprintf, or to wrap the variadic C function in a non-variadic C function.\npackage main /* #include \u0026#34;stdio.h\u0026#34; void wrapPrintf(const char *s) { printf(\u0026#34;%s\u0026#34;, s); } */ import \u0026#34;C\u0026#34; func main() { C.wrapPrintf(C.CString(\u0026#34;Hello, World\\n\u0026#34;)) } C++ Hello World fail Another issue with CGO is only C code can be called from Go. C++ code cannot be called from Go. The following code will fail to compile:\npackage main /* #include \u0026lt;iostream\u0026gt; void helloWorld() { std::cout \u0026lt;\u0026lt; \u0026#34;Hello, World\u0026#34; \u0026lt;\u0026lt; std::endl; } */ import \u0026#34;C\u0026#34; func main() { C.helloWorld() } However, C++ code can be called from C, so we can write a C wrapper for the C++ code.\nCGO real-world example The following is an example of real-world usage of CGO, which uses Apple\u0026rsquo;s APIs to add a secret to the keychain.\npackage keystore /* #cgo LDFLAGS: -framework CoreFoundation -framework Security #include \u0026lt;CoreFoundation/CoreFoundation.h\u0026gt; #include \u0026lt;Security/Security.h\u0026gt; */ import \u0026#34;C\u0026#34; import ( \u0026#34;fmt\u0026#34; \u0026#34;unsafe\u0026#34; ) const service = \u0026#34;com.fleetdm.fleetd.enroll.secret\u0026#34; var serviceStringRef = stringToCFString(service) // AddSecret will add a secret to the keychain. This application can retrieve this // secret without any user authorization. func AddSecret(secret string) error { query := C.CFDictionaryCreateMutable( C.kCFAllocatorDefault, 0, \u0026amp;C.kCFTypeDictionaryKeyCallBacks, \u0026amp;C.kCFTypeDictionaryValueCallBacks, ) defer C.CFRelease(C.CFTypeRef(query)) data := C.CFDataCreate(C.kCFAllocatorDefault, (*C.UInt8)(unsafe.Pointer(C.CString(secret))), C.CFIndex(len(secret))) defer C.CFRelease(C.CFTypeRef(data)) C.CFDictionaryAddValue(query, unsafe.Pointer(C.kSecClass), unsafe.Pointer(C.kSecClassGenericPassword)) C.CFDictionaryAddValue(query, unsafe.Pointer(C.kSecAttrService), unsafe.Pointer(serviceStringRef)) C.CFDictionaryAddValue(query, unsafe.Pointer(C.kSecValueData), unsafe.Pointer(data)) status := C.SecItemAdd(C.CFDictionaryRef(query), nil) if status != C.errSecSuccess { return fmt.Errorf(\u0026#34;failed to add %v to keychain: %v\u0026#34;, service, status) } return nil } // stringToCFString will return a CFStringRef func stringToCFString(s string) C.CFStringRef { bytes := []byte(s) ptr := (*C.UInt8)(\u0026amp;bytes[0]) return C.CFStringCreateWithBytes(C.kCFAllocatorDefault, ptr, C.CFIndex(len(bytes)), C.kCFStringEncodingUTF8, C.false) } The C linker flags are specified with the #cgo LDFLAGS directive.\nThe CGO code uses a lot of casting and data conversion. Let\u0026rsquo;s break down the following segment:\n(*C.UInt8)(unsafe.Pointer(C.CString(secret))) C.CString converts a Go string to a C string. It is one of the CGO special functions to convert between Go and C types. See cgo documentation for more information.\nunsafe.Pointer converts a C pointer to a generic Go pointer. And (*C.UInt8) casts the Go pointer back to a C pointer.\nUnfortunately, CGO cannot cast a C string to a (*C.UInt8) directly. The following will fail to compile:\n(*C.UInt8)(C.CString(secret)) We must go through an intermediate cast to unsafe.Pointer, representing a void C pointer.\nAdditional topics Our custom C and Go code was always in the same file in the above examples. However, the C code can be in a separate file and linked to our Go executable.\nOther getting started guides Recently, we explained how to build a Chrome extension without any additional tools. Also, we wrote a guide to creating a React Hello World app. CGO Hello World fail video ","date":"2024-01-18T00:00:00Z","image":"https://victoronsoftware.com/posts/using-c-and-go-with-cgo-is-tricky/cgo-hello-world-fail_hudd175c840e3984dcc2f0a6c6496dc1b9_172150_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/using-c-and-go-with-cgo-is-tricky/","title":"Using C and Go with CGO is tricky"},{"content":"What are GitHub actions? GitHub actions are a way to automate your software development workflows. They are similar to CI/CD tools like Jenkins, CircleCI, and TravisCI. However, GitHub actions are built into GitHub.\nGitHub actions are not entirely free, but they have very high usage limits for open-source projects. For private repositories, you can run up to 2,000 minutes per month for free. After that, you will be charged.\nGitHub actions for non-CI/CD tasks However, GitHub actions are not just for CI/CD. You can use them for many general-purpose tasks. For example, you can use them as an extension of your application to perform tasks such as:\ngenerating aggregate reports updating a database sending notifications general data processing and many others A GitHub action can run arbitrary code, taking inputs from multiple sources such as API calls, databases, and files.\nYou can use a GitHub action as a worker for your application. For example, you can use it to process data from a database and then send a notification to a user. Or you can use it to generate a report and upload it to a file server.\nAlthough GitHub actions in open-source repositories are public, they can still use secrets that are not accessible to the public. For example, secrets can be API keys and database access credentials.\nA real-world GitHub action doing data processing Below is an example GitHub action that does general data processing. It uses API calls to download data from NVD (National Vulnerability Database), generates files from this data, and then creates a release. Subsequently, the application can download these files and use them directly without making the API calls or processing the data itself.\nGitHub gist: The GitHub action does a checkout of our application code and runs a script cmd/cve/generate.go to generate the files. Then, it publishes the generated files as a new release. As a final step, it deletes any old releases.\nA note of caution. GitHub monitors for cryptocurrency mining and other abusive behavior. So, keep that in mind and be careful with process-intensive actions.\nUse GitHub actions for general-purpose tasks video Other articles related to GitHub How to reuse workflows and steps in GitHub Actions What happens in a GitHub pull request after a git merge Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-01-11T00:00:00Z","image":"https://victoronsoftware.com/posts/use-github-actions-for-general-purpose-tasks/GitHub-action_hu170254bdc12852a8970245ca1a2f169c_44917_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/use-github-actions-for-general-purpose-tasks/","title":"Use GitHub actions for general-purpose tasks"},{"content":" Fuzz testing is a software automated testing technique where random inputs are provided to the software under test. My background is in hardware verification, which uses sophisticated methodologies for pseudorandom testing, so I wanted to see what the Go library had to offer out of the box.\nA Go fuzz test can run as:\na normal unit test a test with fuzzing A fuzz test is written similarly to a normal unit test in a *_test.go file, with the following changes. It must have a Fuzz prefix and use the testing.F struct instead of the usual testing.T struct.\nfunc FuzzSample(f *testing.F) { Here is a workflow for using fuzz testing. First, you create a fuzz test. Then, you run it with fuzzing to automatically find failing corner cases and make any fixes. Thirdly, you include the test and the corner cases in your continuous integration testing suite.\nCreate a fuzz test When creating a fuzz test, you should provide a corpus of initial seed inputs. These are the inputs the test will use before applying randomization. Add the seed corpus with the Add method. For example:\nf.Add(tc.Num, tc.Name) f.Add(uint8(0), \u0026#34;\u0026#34;) The inputs to the Add method indicate which types will be fuzzed, and these types must match the subsequent call to the Fuzz method:\nf.Fuzz(func(t *testing.T, num uint8, name string) { The fuzz test can randomize any number of inputs, as long as they are one of the supported types.\nRun the test with fuzzing To run the test with fuzzing, use the -fuzz switch, like:\ngo test -fuzz FuzzSample The test will continuously run on all your CPUs until it fails, or you kill it:\n=== RUN FuzzSample fuzz: elapsed: 0s, gathering baseline coverage: 0/11 completed fuzz: elapsed: 0s, gathering baseline coverage: 11/11 completed, now fuzzing with 12 workers fuzz: elapsed: 3s, execs: 432199 (144036/sec), new interesting: 0 (total: 11) fuzz: elapsed: 6s, execs: 871147 (146328/sec), new interesting: 0 (total: 11) A sample failure:\nfailure while testing seed corpus entry: FuzzSample/49232526a5eabbdc fuzz: elapsed: 1s, gathering baseline coverage: 10/11 completed --- FAIL: FuzzSample (1.03s) --- FAIL: FuzzSample (0.00s) fuzz_test.go:21: Found 0 The failures are automatically added to the seed corpus. The seed corpus includes the initial inputs that were added with the Add method as well as any new fails. These new seed corpus files are automatically created in the testdata/fuzz/Fuzz* directory. Sample contents of one such file:\ngo test fuzz v1 byte(\u0026#39;\\x01\u0026#39;) string(\u0026#34;0a0000\u0026#34;) Adding the failure to the seed corpus means that the failing case will always run when this test is run again as a unit test or with fuzzing.\nNow, you must fix the failing test and continue the loop of fuzzing and fixing.\nInclude the test in continuous integration When checking in the test to your repository, you must either include the testdata/fuzz/Fuzz* files or convert those files into individual Add method calls in your test. Once the test is checked in, all the inputs in the seed corpus will run as part of the standard Go unit test flow.\nInitial impressions Fuzz testing appears to be a good approach to help the development of small functions with limited scope. The library documentation mentions the following about the function under test:\nThis function should be fast and deterministic, and its behavior should not depend on shared state.\nI plan to give fuzzing a try the next time I develop such a function. I will share the results on this blog.\nConcerns and Issues Native fuzzing support was added to Go in 1.18 and seems like a good initial approach. However, it feels limited in features and usability. The types of functions, fast and deterministic, that fuzzing is intended for are generally not very interesting when testing real applications. They are good examples for students learning how to code. However, more interesting testing scenarios include:\nFunctions accessing remote resources in parallel, such as APIs or databases Functions with asynchronous code Secondly, the fuzzing library does not provide a good way to guide the randomization of inputs and does not give feedback about the input state space already covered. It does provide line coverage information, but that doesn\u0026rsquo;t help for unknown corner cases.\nIf one of my inputs is intended to be a percentage, then I want most of the fuzzing to concentrate on the legal range of 0-100, as opposed to all numbers. This lack of constraints becomes a problem when adding additional inputs to the fuzzing function, as the available state space of inputs expands exponentially. If the state space of inputs is huge, there is no guarantee that fuzzing accomplished its goal of finding all corner cases, leaving the developer with a false sense of confidence in their code.\nLastly, the fuzz test is hard to maintain. The seed corpus is stored in files without any context regarding what corner case each seed is hitting. Software engineers unfamiliar with fuzz testing will find this extremely confusing. If the fuzz test needs to be extended in the future with additional inputs or different types, the old seed corpus will become useless. It will be worse than useless \u0026ndash; the test will not run, and the developer unfamiliar with fuzz testing will not have a clear idea why.\nfuzz_test.go:16: wrong number of values in corpus entry: 2, want 3 That said, understanding the fuzz testing limitation, I’m willing to try fuzz testing for more interesting test cases, such as database accesses. I will report my findings in a future post.\nGitHub gist: ","date":"2024-01-04T00:00:00Z","image":"https://victoronsoftware.com/posts/fuzz-testing-with-go/fuzz_hu3b462ca6a7ae1c67a10dfe9835433d85_465404_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/fuzz-testing-with-go/","title":"Fuzz testing in Go"},{"content":"In the ever-evolving landscape of device management and cybersecurity, understanding the mechanics behind tools like Fleet is not just about technical curiosity; it\u0026rsquo;s about empowering IT professionals to safeguard digital assets more effectively. Fleet gathers telemetry from various devices, from laptops to virtual machines, using osquery. At the heart of this system lies a crucial feature: Fleet policies.\nPolicies in Fleet are more than just rules; they are the gatekeepers of your device\u0026rsquo;s security, ensuring stringent adherence to security standards. By dissecting how Fleet policies operate \u0026ldquo;under the hood,\u0026rdquo; IT administrators and security professionals can gain invaluable insights. These insights allow for setting up efficient security protocols and rapid response to potential vulnerabilities, a necessity in a landscape where cyber threats are constantly evolving. This article delves into the inner workings of Fleet policies, providing you with the knowledge to better configure, manage, and leverage these policies for optimal device security and efficiency.\nPolicy creation Policies can be created from the web UI, the command-line interface called fleetctl with config files, or the REST API. The user creates a policy and selects which devices need to be checked using that policy. Policies can be global or team-specific.\nWhen a policy is created, a record for it is stored in the policies table of the MySQL database. A Fleet deployment consists of several servers behind a load balancer, so storing the record in the DB makes all servers aware of the new policy.\nPolicy execution Policies are executed on the devices, which are called hosts in Fleet, according to the FLEET_OSQUERY_POLICY_UPDATE_INTERVAL, which is set to 1 hour by default. This interval can be adjusted with the environment variable or set from the server’s command line.\nPolicies are simply SQL queries that return a true or false result, so the flow they use on the hosts is the same as other queries. Hosts check in with Fleet servers every 10 seconds (the default) and access the /api/v1/osquery/distributed/read API endpoint. The server checks when the policy was last executed to determine whether it should be executed again. If so, the server adds the policy to its response. For example, this policy in the server response checks if the macOS firewall is enabled:\n{ \u0026#34;queries\u0026#34;: { \u0026#34;fleet_policy_query_9\u0026#34;: \u0026#34;SELECT 1 FROM alf WHERE global_state \u0026gt;= 1;\u0026#34; }, \u0026#34;discovery\u0026#34;: { \u0026#34;fleet_policy_query_9\u0026#34;: \u0026#34;SELECT 1\u0026#34; } } Once the host has executed the policy, it writes the result to the server. The server updates the result in the policy_membership table of the MySQL database. At this point, the Host Details page on the web UI is updated with the policy result.\nForce policy execution on a device The user can force the host to execute all of its policies by clicking the Refetch link:\nPolicy results aggregation However, the main Policies page is not updated. This page shows the counts of all passing and failing hosts for each policy. A worker process on one of the Fleet servers updates it once an hour. The worker calculates the counts and stores them in the policy_stats table in the database. This is done for better performance of the UI. For customers with 100,000s of hosts that asynchronously report their policy results, calculating the passing and failing counts in real time was noticeably slow.\nSummary Understanding the intricacies of Fleet policies is essential for IT professionals managing a fleet of devices. This deep dive into the mechanics of Fleet policies — from creation to execution — provides you with the necessary insights to optimize your cybersecurity strategy effectively. By leveraging these policies, you can ensure stringent security standards across your network, enhancing your organization\u0026rsquo;s digital defense. As the cyber landscape evolves, tools like Fleet remain crucial in maintaining robust and responsive security protocols. We encourage you to apply these insights in your Fleet usage, and as always, we welcome your feedback and experiences in the Fleet community Slack channels.\nUnderstanding the intricacies of Fleet policies video This article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2023-12-30T00:00:00Z","image":"https://victoronsoftware.com/posts/understanding-the-intricacies-of-fleet-policies/_hu5aa3a7c771a8aa7077c6e8ce7a0c141c_322631_c7fb8a890b81808a2154c4746d61955b.png","permalink":"https://victoronsoftware.com/posts/understanding-the-intricacies-of-fleet-policies/","title":"Understanding the intricacies of Fleet policies"},{"content":" Fleet is an open-source platform for managing and gathering telemetry from devices such as laptops, desktops, VMs, etc. Osquery agents run on these devices and report to the Fleet server. One of Fleet’s features is the ability to query information from the devices in near real-time, called live queries. This article discusses how live queries work “under the hood.”\nWhy a live query? Live queries enable administrators to ask near real-time questions of all online devices, such as checking the encryption status of SSH keys across endpoints, or obtaining the uptime of each server within their purview. This enables them to promptly identify and address any issues, thereby reducing downtime and maintaining operational efficiency. These tasks, which would be time-consuming and complex if done manually, are streamlined through live queries, offering real-time insights into the status and posture of the entire fleet of devices helping IT and security.\nLive queries under the hood Live queries can be run from the web UI, the command-line interface called fleetctl, or the REST API. The user creates a query and selects which devices will run that query. Here is an example using fleetctl to obtain the operating system name and version for all devices:\nfleetctl query --query \u0026#34;select name, version from os_version;\u0026#34; --labels \u0026#34;All Hosts\u0026#34; When a client initiates a live query, the server first creates a Query Campaign record in the MySQL database. A Fleet deployment consists of several servers behind a load balancer, so storing the record in the DB makes all servers aware of the new query campaign.\nQuery campaign As devices called Hosts in Fleet check in with the servers, they receive instructions to run a query. For example:\n{ \u0026#34;queries\u0026#34;: { \u0026#34;fleet_distributed_query_140\u0026#34;: \u0026#34;SELECT name, version FROM os_version;\u0026#34; }, \u0026#34;discovery\u0026#34;: { \u0026#34;fleet_distributed_query_140\u0026#34;: \u0026#34;SELECT 1\u0026#34; } } Then, the osquery agents run the actual query on their host, and write the result back to a Fleet server. As a server receives the result, it publishes it to the common cache using Redis Pub/Sub.\nOnly the one server communicating with the client subscribes to the results. It processes the data from the cache, keeps track of how many hosts reported back, and communicates results back to the client. The web UI and fleetctl interfaces use a WebSockets API, and results are reported as they come in. The REST API, on the other hand, only sends a response after all online hosts have reported their query results.\nDiscover more Fleet’s live query feature represents a powerful tool in the arsenal of IT and security administrators. By harnessing the capabilities of live queries, tasks that once required extensive manual effort can now be executed swiftly and efficiently. This real-time querying ability enhances operational efficiency and significantly bolsters security and compliance measures across a range of devices.\nThe integration of Fleet with Osquery agents, the flexibility offered by interfaces like the web UI, fleetctl, and the REST API, and the efficient data handling through mechanisms like Redis Pub/Sub and WebSockets API all come together to create a robust, real-time telemetry gathering system. This system is designed to keep you informed about the current state of your device fleet, helping you make informed decisions quickly.\nAs you reflect on the capabilities of live queries with Fleet, consider your network environment\u0026rsquo;s unique challenges and needs. What questions could live queries help you answer about your devices? Whether it\u0026rsquo;s security audits, performance monitoring, or compliance checks, live queries offer a dynamic solution to address these concerns.\nWe encourage you to explore the possibilities and share your thoughts or questions. Perhaps you’re facing a specific query challenge or an innovative use case you’ve discovered. Whatever it may be, the world of live queries is vast and ripe for exploration. Join us in Fleet’s Slack forums to engage with a community of like-minded professionals and deepen your understanding of what live queries can achieve in your environment.\nAPI Documentation:\nRun live query with REST API Run live query with WebSockets This article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2023-12-29T00:00:00Z","image":"https://victoronsoftware.com/posts/get-current-telemetry-from-your-devices-with-live-queries/Live%20Query_hu64e3a4617825ba0468dff8df166d9a56_55465_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/get-current-telemetry-from-your-devices-with-live-queries/","title":"Get current telemetry from your devices with live queries"},{"content":" When starting to code in Go, I encountered the following situation. I needed to create an empty slice, so I did:\nslice := []string{} However, my IDE flagged it as a warning, and pointed me to this Go style guide passage, which recommended using a nil slice instead:\nvar slice []string This recommendation didn\u0026rsquo;t seem right to me. How can a nil variable be better? Won’t I run into issues like null pointer exceptions and other annoyances? Well, as it turns out, that’s not how slices work in Go. When declaring a nil slice, it is not the dreaded null pointer. It is still a slice. This slice includes a slice header, but its value just happens to be nil.\nThe main difference between a nil slice and an empty slice is the following. A nil slice compared to nil will return true. That’s pretty much it.\nif slice == nil { fmt.Println(\u0026#34;Slice is nil.\u0026#34;) } else { fmt.Println(\u0026#34;Slice is NOT nil.\u0026#34;) } When printing a nil slice, it will print like an empty slice:\nfmt.Printf(\u0026#34;Slice is: %v\\n\u0026#34;, slice) Slice is: [] You can append to a nil slice:\nslice = append(slice, \u0026#34;bozo\u0026#34;) You can loop over a nil slice, and the code will not enter the for loop:\nfor range slice { fmt.Println(\u0026#34;We are in a for loop.\u0026#34;) } The length of a nil slice is 0:\nfmt.Printf(\u0026#34;len: %#v\\n\u0026#34;, len(slice)) len: 0 And, of course, you can pass a nil slice by pointer. That’s right \u0026ndash; pass a nil slice by pointer.\nfunc passByPointer(slice *[]string) { fmt.Printf(\u0026#34;passByPointer len: %#v\\n\u0026#34;, len(*slice)) *slice = append(*slice, \u0026#34;bozo\u0026#34;) } You will get the updated slice if the underlying slice is reassigned.\npassByPointer(\u0026amp;slice) fmt.Printf(\u0026#34;len after passByPointer: %#v\\n\u0026#34;, len(slice)) len after passByPointer: 1 The code above demonstrates that a nil slice is not a nil pointer. On the other hand, you cannot dereference a nil pointer like you can a nil slice. This code causes a crash:\nvar nullSlice *[]string fmt.Printf(\u0026#34;Crash: %#v\\n\u0026#34;, len(*nullSlice)) Here\u0026rsquo;s the full gist:\n","date":"2023-12-28T00:00:00Z","image":"https://victoronsoftware.com/posts/nil-slice-versus-empty-slice-in-go/cover_hu8e4888644a780aeecee2f1f52535fba1_61530_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/nil-slice-versus-empty-slice-in-go/","title":"Nil slice versus empty slice in Go"},{"content":" Matter is a recent open-source standard for connecting devices such as light switches, door locks, motion sensors, and many others. The major goals of the standard are compatibility and interoperability. This means that you will no longer need to be an expert hacker when trying to control devices from multiple manufacturers under a single application. Apple, Amazon, and Google are some of the major members driving the standard. This is great news for the majority of adopters who haven’t yet fully embraced home automation and security.\nThe Matter specification is published by the Connectivity Standards Alliance (CSA) and includes a software development kit. Version 1.0 of the specification was released in October of 2022. In 2023, we saw a slew of new devices and software upgrades compatible with Matter. Version 1.2 of the specification was published in October of 2023. However, this latest specification is still missing support for a few important device categories such as cameras and major appliances. Cameras are a top priority for the CSA, and we may see Matter-compatible cameras in 2024.\nMatter is an important step for the management of IoT devices because it finally brings true interoperability where it has been sorely missing for so many years. No longer will device manufacturers need to decide and budget precious software resources to support Amazon Alexa, Google Home, Apple HomeKit, or another connectivity hub. Customers will no longer be locked into using one of the major home automation providers. And home automation solutions from smaller companies will come onto the market.\nAn important feature of Matter is multi-admin, which means that devices can be read and controlled by multiple clients. In Matter terminology, the device, such as a motion sensor, is called a server or node, and the applications controlling it are called clients. For example, a light switch may be simultaneously controlled by the manufacturer’s app, by Alexa, and by the user\u0026rsquo;s hand-written custom API client.\nMulti-admin support means that a home or business may use one application to control their locks, switches, and security sensors, and another application for reading telemetry from those same devices. Businesses will find it easier to integrate physical security with cyber security. For example, suppose a business’s device management server uses Matter to subscribe to the office door lock. It receives an alert that User A has entered their code. Afterwards, via regular scheduled telemetry, it notices a successful login to Computer B. The business SIEM (security information and event management) system should immediately flag this suspicious sequence of events.\nOf course, the example above can be accomplished today by writing some custom code or using a third party integration. What Matter brings is scalability to such security approaches. The code and integration will no longer need to be redone for each new device and version that comes onto the market.\n","date":"2023-12-20T00:00:00Z","image":"https://victoronsoftware.com/posts/physical-security-meets-cybersecurity-with-matter/cover_hu240e6838189f9e304ae612d50248a293_9583_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/physical-security-meets-cybersecurity-with-matter/","title":"Physical security meets cybersecurity with Matter"},{"content":"A prepared statement is a feature of modern databases intended to help execute the same SQL statement multiple times. For example, the following statement is a prepared statement:\nSELECT id, name FROM users WHERE email = ?; The presence of an unspecified parameter, labeled “?”, makes it a prepared statement. When a prepared statement is sent to the database, it is compiled, optimized, and stored in memory on the database server. Subsequently, the client application may execute the same prepared statement multiple times with different parameter values. This results in a speedup.\nPrepared statements are well suited for long and complex queries that require significant compilation and optimization times. They are kept prepared on the DB server, and the application must only pass the parameters to execute them.\nAnother benefit of using prepared statements is the protection they provide against SQL injection. The application does not need to properly escape the parameter values provided to the statement. Because of this protection, many experts recommend always using prepared statements for accessing the database.\nHowever, by always using prepared statements for accessing the database, we force the SQL driver to send the extra prepare command for every ad-hoc statement we execute. The driver sends the following commands:\nPrepare the statement Execute statement with given parameters Close the statement (and deallocate the prepared statement created above) Another issue with prepared statements is the memory requirement. In large application deployments with large numbers of connections, prepared statements can crash your environment. This issue happened to one of our customers.\nA prepared statement is only valid for a single session, which typically maps to a single database connection. If the application runs multiple servers, with many connections, it may end up storing a prepared statement for each one of those sessions.\nFor example, given 100 servers with 100 connections each, we have 10,000 connections to the database. Assuming a memory requirement of 50 KB per prepared statement (derived from the following article), we arrive at the maximum memory requirement of:\n10,000 * 50 KB = 500 MB per single saved prepared statement Some databases also have limits on the number of prepared statements. MySQL’s max_prepared_stmt_count defaults to 16,382 for the entire server. Yes, this is a global limit, and not per session. In the above example, if the application uses prepared statements for every database access, then each database connection will always be using up 1 short-lived prepared statement. A short-lived prepared statement is the prepared statement, as we described above, that will be created for the purposes of executing one statement, and then immediately deallocated afterwards. This means the above application running with a default MySQL config cannot explicitly save any prepared statements \u0026ndash; 10,000 transient prepared statements + 10,000 saved prepared statements is greater than the max_prepared_stmt_count of 16,382.\nThis is extremely inconvenient for application developers, because they must keep track of:\nThe number of saved prepared statements they are using How many application servers are running How many database connections each server has The prepared statement limits of the database This detail can easily be overlooked when scaling applications.\nIn the end, is it really worth using prepared statements, and especially saved prepared statements, in your application? Yes, saved prepared statements can offer performance advantages, especially for complex queries executed frequently. However they must also be kept in check.\nA few ways to mitigate prepared statement issues for large application deployments include:\nLimit the number of database connections per application server Increase the prepared statement limit on the database server(s) Limit the maximum lifespan of connections. When closing a connection, the database will deallocate all prepared statements on that connection. SQL prepared statements are broken when scaling applications video Other articles related to MySQL Optimize MySQL query performance: INSERT with subqueries MySQL deadlock on UPDATE/INSERT upsert pattern Fully supporting Unicode and emojis in your app Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2023-12-14T00:00:00Z","image":"https://victoronsoftware.com/posts/sql-prepared-statements-are-broken-when-scaling-applications/cover_hu7756c223ae4ac2243c5cccab03f7b66b_14192_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/sql-prepared-statements-are-broken-when-scaling-applications/","title":"SQL prepared statements are broken when scaling applications"},{"content":" At Fleet, our developer documentation is spread out throughout the codebase, contained in a multitude of README and Markdown files. Much of the documentation is hosted on our webpage, but not all of it.\nAs developers, we need to be able to quickly search project documentation to find answers to specific questions, such as:\nHow to do a database migration How to run integration tests How to deploy a development version of to a specific OS One solution is to use grep or the IDE environment to search for these answers. Unfortunately, such search methods are not optimized for text search \u0026ndash; they frequently generate no relevant results or too many results that we must manually wade through to find the most appropriate. Specialized documentation search tools, on the other hand, prioritize headings and whole words, search for plural versions of the search terms, and offer other conveniences.\nThe lack of good search capability for engineering docs must be solved in order to scale engineering efforts. It is an issue because of the following side effects:\nEngineers are discouraged from writing documentation Documentation may be duplicated Senior developers are frequently interrupted when people can’t find relevant documentation One solution is to use a documentation service, such as a team wiki, Confluence, or GitBook. GitBook integrates with git repositories, and can push documentation changes. GitBook is free for personal use, which makes it easy to use for open source projects such as fleet and osquery. That said, GitBook is a newcomer to the space, and is still reaching maturity.\nTo set up a personal GitBook, make a fork of the open source projects that contain documentation you’d like to search, and integrate them into GitBook spaces. After indexing is complete, you’ll be able to effectively search the documentation.\nTo keep the forks in sync with the parent repositories, we use Github Actions. Github Actions are free for open source projects. Searching GitHub for sync-fork returned several examples. We ended up using the following:\nname: Sync Fork on: schedule: - cron: \u0026#39;55 * * * *\u0026#39; workflow_dispatch: # on button click jobs: sync: runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v4 with: token: ${{ secrets.WORKFLOW_TOKEN }} fetch-depth: 0 - name: Configure Git run: | git config --global user.name \u0026#34;GitHub Actions Bot\u0026#34; git config --global user.email \u0026#34;actions@github.com\u0026#34; - name: Merge upstream run: | git remote add upstream https://github.com/fleetdm/fleet.git git fetch upstream main git checkout main git merge upstream/main git push origin main The WORKFLOW_TOKEN above is a GitHub personal access token (PAT) that allows reading and writing workflows in this repository. This token is not needed for repositories without workflows.\nIn addition to project documentation, GitBook can be used to synchronize personal documentation that’s being held in a private repository. There are several git-based notebook applications on the market. In addition, Markdown notes from the popular note-taking app Obsidian can be kept in GitHub. This turns GitBook into a true personalized developer documentation database \u0026ndash; one place to search through developer docs as well as your own private notes.\n","date":"2023-11-30T00:00:00Z","image":"https://victoronsoftware.com/posts/you-need-a-personal-dev-docs-db-gitbook/cover_hu54e2e2056c8855962719d82a53cb41e3_206961_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/you-need-a-personal-dev-docs-db-gitbook/","title":"You need a personal dev docs DB (GitBook)"},{"content":"Traditionally, network routers used dedicated bare metal machines. However, in the last several years, we’ve seen a rise in software-based routers that can be deployed either on bare metal, on a VM, or even on a container. This means these virtual routers can be used to replace existing router software on an older router. They can run in the cloud. Or they can be installed on do-it-yourself (DIY) hardware. A couple popular open source software-based routers are pfSense and OPNsense.\nWhy use a virtual router? For one, these routers offer enterprise-level features such as build-in VPN support, traffic analysis, and extensive diagnostics, among others. Another reason is that having a virtual router gives you the ability to experiment \u0026ndash; you can install multiple routers on top of your hypervisor, and try all of them out. A third reason is that the virtual router may be only one of many VMs that you run on your hardware. You can use the same piece of hardware to run a router, an ad-blocking service, a media server, and other applications.\nAdvanced virtual router installation and set up When setting up our virtual router, we chose to use PCI Passthrough to allow the virtual router direct access to the NIC hardware. Direct access to hardware improves the latency of our internet traffic. In addition, we wanted our hypervisor to sit behind the router, and not be exposed to the public. This reduces the attack surface for potential bad agents. However, routing hypervisor traffic through the router made our setup a bit tricker. It is like the chicken or the egg dilemma \u0026ndash; how do you put your hypervisor behind the router when the hypervisor is responsible for managing the router? Below is the approach we used when installing pfSense on top of Proxmox Virtual Environment (PVE).\nFor the initial installation, we did not use PCI Passthrough and instead used a virtual network bridge (vmbr0). We configured the router VM to start on boot.\nInitial virtual router configuration This allowed us to continue controlling the virtual router through the PVE web GUI. We set up the router and enabled access to it through the serial interface, which we used in the next step. Then, we put the system into its final configuration.\nFinal virtual router configuration In order to finish configuring, we had to plug in a monitor and keyboard into our hardware. We accessed the virtual router via the serial interface from the PVE command line:\nqm terminal 100 We updated the WAN interface to use eth0. At this point, the LAN interface eth1 had access to the internet.\nIn addition, we added a second LAN interface for the network bridge (vmbr0). We made sure firewall configurations for both LAN interfaces were the same.\nNext, from the PVE command line, we updated the PVE IP and gateway to point at the router by modifying the following files.\n/etc/network/interfaces /etc/hosts After rebooting PVE, we had access to the internet and to the PVE Web GUI from our new LAN.\nUpdating router software Using a virtual router with PCI Passthrough creates a unique challenge when doing software updates. What if the new version doesn’t work? What if you lose all internet access.\nWe can mitigate potential issues. First, we recommend always making a backup of the router VM when upgrading. That way we can easily roll back the change. Switching to a backup, however, requires keyboard and monitor access to your hardware, since it must be done via the PVE command line.\nAnother way to safely upgrade is to spin up a second VM running updated router software. The second VM can be either from a backup or brand new. This VM should use virtual network bridges for its connections. Once it is properly configured, we can stop the first router VM and switch the port connections to the second VM. This flow also requires accessing the router via the serial interface to update the WAN/LAN interfaces.\nSetting up a virtual router video ","date":"2023-11-22T00:00:00Z","image":"https://victoronsoftware.com/posts/setting-up-a-virtual-router/cover_hu019a91ef521bad7337cb0514ecaaaefc_16004_120x120_fill_q75_box_smart1.jpeg","permalink":"https://victoronsoftware.com/posts/setting-up-a-virtual-router/","title":"Setting up a virtual router (pfSense on Proxmox)"},{"content":" Keychains are the macOS’s method to track and protect secure information such as passwords, private keys, and certificates. Traditionally, the keychain information was stored in files, such as:\n/Library/Keychains/System.keychain /Library/Keychains/apsd.keychain /System/Library/Keychains/SystemRootCertificates.keychain /Users/\u0026lt;username\u0026gt;/Library/Keychains/login.keychain-db In the last several years, Apple also introduced data protection keychains, such as the iCloud Keychain. Although the file-based keychains above are on the road to deprecation in favor of data protection keychains, current macOS systems still heavily rely on them. It is unclear when, if ever, these keychains will be replaced by data protection keychains.\nInspecting file-based keychains has gotten more difficult as Apple deprecated many of the APIs associated with them, such as SecKeychainOpen. In addition, excessive use of these deprecated APIs may result in corruption of the Login Keychain, as mentioned in this osquery issue. By NOT using the deprecated APIs, the user only has access to the following keychains from the above list:\n/Library/Keychains/System.keychain /Users/\u0026lt;username\u0026gt;/Library/Keychains/login.keychain-db Root certificates are missing. And the APSD (Apple Push Service Daemon) keychain is missing, which is used for device management, among other things.\nSo, how can app developers and IT professionals continue to have access to ALL of these keychain files?\nOne way is to continue using deprecated APIs until they stop working. We recommend making a secure copy of the keychain files before accessing them with the APIs.\nAnother option is to use the macOS security command line tool. For example, to list root certificates, do the following:\nsudo security find-certificate -a /System/Library/Keychains/SystemRootCertificates.keychain A third, and hardest, option is to parse the keychain files yourself. Some details on the keychain format are available. Please leave a comment if you or someone else has created a tool to parse Apple keychains.\nThe fourth option is to use an existing tool, such as osquery. Osquery is an open-source tool built for security and IT professionals. Osquery developers are working on fixing any issues to continue providing access to macOS keychain files via the following tables:\ncertificates keychain_acls keychain_items ","date":"2023-11-16T00:00:00Z","permalink":"https://victoronsoftware.com/posts/inspecting-keychain-files-on-macos/","title":"Inspecting keychain files on macOS"},{"content":" Authorization is giving permission to a user to do an action on the server. As developers, we must ensure that users are only allowed to do what they are authorized.\nOne way to ensure that authorization has happened is to loudly flag when it hasn\u0026rsquo;t. This is how we do it at Fleet Device Management.\nIn our code base, we use the go-kit library. Most of the general endpoints are created in the handler.go file. For example:\n// user-authenticated endpoints ue := newUserAuthenticatedEndpointer(svc, opts, r, apiVersions...) ue.POST(\u0026#34;/api/_version_/fleet/trigger\u0026#34;, triggerEndpoint, triggerRequest{}) Every endpoint calls kithttp.NewServer and wraps the endpoint with our AuthzCheck. From handler.go:\ne = authzcheck.NewMiddleware().AuthzCheck()(e) return kithttp.NewServer(e, decodeFn, encodeResponse, opts...) This means that after the business logic is processed, the AuthzCheck is called. This check ensures that authorization was checked. Otherwise, an error is returned. From authzcheck.go:\n// If authorization was not checked, return a response that will // marshal to a generic error and log that the check was missed. if !authzctx.Checked() { // Getting to here means there is an authorization-related bug in our code. return nil, authz.CheckMissingWithResponse(response) } This additional check is useful during our development and QA process, to ensure that authorization always happens in our business logic.\nThis article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2023-11-10T00:00:00Z","permalink":"https://victoronsoftware.com/posts/catch-missed-authorization-checks-during-software-development/","title":"Catch missed authorization checks during software development"}]