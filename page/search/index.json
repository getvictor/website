[{"content":"Introduction GitHub Actions is a way to automate your software development workflows. The approach is similar to CI/CD tools like Jenkins, CircleCI, and TravisCI. However, GitHub Actions are built into GitHub.\nHigh level diagram of GitHub Actions The entry point for GitHub Actions is the .github/workflows directory in your repository. This directory contains one or more YAML files that define your workflows. A workflow is an automated process made up of one or more jobs. Each job runs on a separate runner. A runner is a server that runs the job. A job contains one or more steps. Each step runs a separate command.\nWhy reuse? Code reuse is a fundamental principle of software development. Reusing GitHub Actions code allows you to:\nImprove maintainability by keeping common code in one place and reducing the amount of code Increase consistency since multiple workflows can use the same code Promote best practices Increase productivity Reduce errors Examples of reusable GitHub Actions code include:\nCode signing Uploading artifacts to cloud services Security checks Notifications and reports Data processing and many others Reusable workflows A reusable workflow replaces a job in the main workflow.\nGitHub Actions reusable workflow A reusable workflow may be shared across repositories and run on a different platform than the main workflow.\nFor file sharing, \u0026lsquo;build artifacts\u0026rsquo; must be used to share files with the main workflow. The reusable workflow does not inherit environment variables. However, it accepts inputs and secrets from the calling workflow and may use outputs to pass data back to the main workflow.\nHere is an example of a reusable workflow. It uses the same schema as a regular workflow.\nname: Reusable workflow on: workflow_call: inputs: reusable_input: description: \u0026#39;Input to the reusable workflow\u0026#39; required: true type: string filename: required: true type: string secrets: HELLO_WORLD_SECRET: required: true outputs: # Map the workflow output(s) to job output(s) reusable_output: description: \u0026#39;Output from the reusable workflow\u0026#39; value: ${{ jobs.reusable-workflow-job.outputs.job_output }} defaults: run: shell: bash jobs: reusable-workflow-job: runs-on: ubuntu-20.04 # Map the job output(s) to step output(s) outputs: job_output: ${{ steps.process-step.outputs.step_output }} steps: - name: Process reusable input id: process-step env: HELLO_WORLD_SECRET: ${{ secrets.HELLO_WORLD_SECRET }} run: | echo \u0026#34;reusable_input=${{ inputs.reusable_input }}\u0026#34; echo \u0026#34;HELLO_WORLD_SECRET=${HELLO_WORLD_SECRET}\u0026#34; echo \u0026#34;step_output=${{ inputs.reusable_input }}_processed\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT - uses: actions/download-artifact@v4 with: name: input_file - name: Process file run: | echo \u0026#34;Processing file: ${{ inputs.filename }}\u0026#34; echo \u0026#34;file processed\u0026#34; \u0026gt;\u0026gt; ${{ inputs.filename }} - uses: actions/upload-artifact@v4 with: name: output_file path: ${{ inputs.filename }} The reusable workflow is triggered on: workflow_call. It accepts an input called reusable_input and generates an output called reusable_output. It also downloads an artifact called input_file, processes a file, and uploads an artifact called output_file.\nThe main workflow calls the reusable workflow using the uses keyword.\njob-2: needs: job-1 # We do not need to check out the repository to use the reusable workflow uses: ./.github/workflows/reusable-workflow.yml with: reusable_input: \u0026#34;job-2-input\u0026#34; filename: \u0026#34;input.txt\u0026#34; secrets: # Can also implicitly pass the secrets with: secrets: inherit HELLO_WORLD_SECRET: TERCES_DLROW_OLLEH A successful run of the main workflow looks like this on GitHub:\nGitHub Actions reusable workflow success Reusable steps (composite action) Reusable steps replace a regular step in a job. We will use a composite action for reusable steps in our example.\nGitHub Actions reusable steps (composite action) Like a reusable workflow, a composite action may be shared across repositories, it accepts inputs, and it may use outputs to pass data back to the main workflow.\nUnlike a reusable workflow, a composite action inherits environment variables. However, it does not inherit secrets. Secrets must be passed explicitly as inputs or environment variables. Also, there is no need to use \u0026lsquo;build artifacts\u0026rsquo; to share files since the reusable steps run on the same runner and in the same work area as the main job.\nHere is an example of a composite action. It uses a different schema than a workflow. Also, the file must be named action.yml or similar.\nname: Reusable steps (AKA composite action) description: Demonstrate how to use reusable steps in a workflow # Schema: https://json.schemastore.org/github-action.json inputs: reusable_input: description: \u0026#39;Input to the reusable workflow\u0026#39; required: true filename: required: true outputs: # Map the action output(s) to step output(s) reusable_output: description: \u0026#39;Output from the reusable workflow\u0026#39; value: ${{ steps.process-step.outputs.step_output }} runs: using: \u0026#39;composite\u0026#39; steps: - name: Process reusable input id: process-step # Shell must explicitly specify the shell for each step. https://github.com/orgs/community/discussions/18597 shell: bash run: | echo \u0026#34;reusable_input=${{ inputs.reusable_input }}\u0026#34; echo \u0026#34;HELLO_WORLD_SECRET=${HELLO_WORLD_SECRET}\u0026#34; echo \u0026#34;step_output=${{ inputs.reusable_input }}_processed\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT - name: Process file shell: bash run: | echo \u0026#34;Processing file: ${{ inputs.filename }}\u0026#34; echo \u0026#34;file processed\u0026#34; \u0026gt;\u0026gt; ${{ inputs.filename }} The composite action is called via the uses setting on a step. Our action accepts an input called reusable_input and generates an output called reusable_output. It also processes a file called filename.\nThe following code snippet shows how to use the composite action in a job.\n- name: Use reusable steps id: reusable-steps uses: ./.github/reusable-steps # To use this syntax, we must have the repository checked out with: reusable_input: \u0026#34;job-2-input\u0026#34; filename: \u0026#34;input.txt\u0026#34; env: HELLO_WORLD_SECRET: TERCES_DLROW_OLLEH A successful run of the main workflow with reusable steps looks like this on GitHub:\nGitHub Actions composite action success Conclusion Reusable workflows and steps are powerful tools for improving the maintainability, consistency, and productivity of your GitHub Actions. They allow you to reuse code across repositories and workflows and promote best practices. They are a great way to reduce errors and increase productivity.\nFor larger units of work, a reusable workflow should be used. A composite action should be used for smaller units of work that may run on the same runner and share the same work area.\nExample code on GitHub The example code is available on GitHub at: https://github.com/getvictor/github-reusable-workflows-and-steps\nGitHub Actions reusable workflows and steps video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-01T00:00:00Z","image":"https://victoronsoftware.com/posts/github-reusable-workflows-and-steps/GitHub%20Actions%20thumbnail_hu23dcbedfa159fd97fecd36bd64d9d886_118952_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/github-reusable-workflows-and-steps/","title":"GitHub Actions reusable workflows and steps"},{"content":"What is an SQL deadlock? A deadlock occurs when two or more SQL transactions are waiting for each other to release locks. This can occur when two transactions have locks on separate resources and each is waiting for the other to release its lock.\nWhat is an upsert? An upsert combines the words \u0026ldquo;update\u0026rdquo; and \u0026ldquo;insert.\u0026rdquo; It is a database operation that inserts a new row into a table if the row does not exist or updates the row if it already exists.\nINSERT/UPDATE upsert pattern One common way to implement an upsert operation in MySQL is to use the following pattern:\nUPDATE table_name SET column1 = value1, column2 = value2 WHERE id = ?; -- If the UPDATE statement does not affect any rows, insert a new row: INSERT INTO table_name (id, column1, column2) VALUES (?, value1, value2); UPDATE returns the number of rows that were actually changed.\nThis UPDATE/INSERT pattern is optimized for frequent updates and rare inserts. However, it can lead to deadlocks when multiple transactions try to insert simultaneously.\nMySQL deadlock example We assume the default transaction isolation level of REPEATABLE READ. Given the following table with one row:\nCREATE TABLE my_table ( id int(10) unsigned NOT NULL, amount int(10) unsigned NOT NULL, PRIMARY KEY (id) ); INSERT INTO my_table (id, amount) VALUES (1, 1); One transaction executes the following SQL:\nUPDATE my_table SET amount = 2 WHERE id = 2; Another transaction executes the following SQL:\nUPDATE my_table SET amount = 3 WHERE id = 3; INSERT INTO my_table (id, amount) VALUES (3, 3); At this point, the second transaction is waiting for the first transaction to release the lock.\nThe first transaction then executes the following SQL:\nINSERT INTO my_table (id, amount) VALUES (2, 2); Causing a deadlock:\n[40001][1213] Deadlock found when trying to get lock; try restarting transaction Why does the deadlock occur? The deadlock occurs because both transactions set next-key locks on the rows they attempted to update. Since the rows they attempted to update did not exist, the lock is set on the \u0026ldquo;supremum\u0026rdquo; pseudo-record. This pseudo-record has a value higher than any value actually in the index. This lock prevents the other transaction from inserting the row it needs.\nDebugging MySQL deadlocks To view the last deadlock detected by MySQL, you can use:\nSHOW ENGINE INNODB STATUS; The output will contain a section like this:\n------------------------ LATEST DETECTED DEADLOCK ------------------------ 2024-04-28 12:29:17 281472351068032 *** (1) TRANSACTION: TRANSACTION 1638819, ACTIVE 7 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 3 lock struct(s), heap size 1128, 2 row lock(s) MySQL thread id 97926, OS thread handle 281471580295040, query id 24192112 192.168.65.1 root update /* ApplicationName=DataGrip 2024.1 */ INSERT INTO my_table (id, amount) VALUES (3, 3) *** (1) HOLDS THE LOCK(S): RECORD LOCKS space id 158 page no 4 n bits 72 index PRIMARY of table `test`.`my_table` trx id 1638819 lock_mode X Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 158 page no 4 n bits 72 index PRIMARY of table `test`.`my_table` trx id 1638819 lock_mode X insert intention waiting Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (2) TRANSACTION: TRANSACTION 1638812, ACTIVE 13 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 3 lock struct(s), heap size 1128, 2 row lock(s) MySQL thread id 97875, OS thread handle 281471585578880, query id 24192285 192.168.65.1 root update /* ApplicationName=DataGrip 2024.1 */ INSERT INTO my_table (id, amount) VALUES (2, 2) *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 158 page no 4 n bits 72 index PRIMARY of table `test`.`my_table` trx id 1638812 lock_mode X Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 158 page no 4 n bits 72 index PRIMARY of table `test`.`my_table` trx id 1638812 lock_mode X insert intention waiting Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; We can see the supremum locks held by both transactions: 0: len 8; hex 73757072656d756d; asc supremum;;.\nAnother way to debug MySQL deadlocks is to enable the innodb_print_all_deadlocks option. This option prints all deadlocks to the error log.\nPreventing the UPDATE/INSERT deadlock One way to prevent the deadlock is to use the INSERT \u0026hellip; ON DUPLICATE KEY UPDATE pattern. This syntax allows you to insert a new row or update an existing row in a single statement. However, it is slower than an UPDATE and always increments the auto-increment value if present.\nAnother way is to roll back the transaction once we know that the UPDATE did not affect any rows. This avoids the deadlock by not holding the lock while we insert the new row. After the rollback, we can retry the transaction using the above INSERT ... ON DUPLICATE KEY UPDATE pattern.\nA third way is not to use transactions. In this case, the locks are released immediately after the statement is executed. However, this approach may not be suitable for all use cases.\nConclusion The UPDATE/INSERT upsert pattern can lead to MySQL deadlocks when multiple transactions try to insert simultaneously. To prevent deadlocks, consider using the INSERT ... ON DUPLICATE KEY UPDATE pattern, rolling back the transaction, or not using transactions.\nMySQL deadlock on UPDATE/INSERT upsert pattern video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-04-28T00:00:00Z","image":"https://victoronsoftware.com/posts/mysql-upsert-deadlock/MySQL%20deadlock_hu82fd7010b13095095640b2133c3dc2b5_70671_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mysql-upsert-deadlock/","title":"MySQL deadlock on UPDATE/INSERT upsert pattern"},{"content":"Introduction In this article, we will create a simple React app from scratch. We will not use any templates or helper scripts. We aim to reduce tool usage and fully understand each step of the process.\nWhat is React? React is a popular JavaScript library for building user interfaces. It was created by Meta (Facebook) and is maintained by Meta and a community of developers. React is used to build single-page applications (SPAs) and dynamic web applications.\nPrerequisites \u0026ndash; Node.js and npm Node.js and npm are the most popular tools for working with React. Node.js is a JavaScript runtime. npm is a package manager for Node.js. These two tools are essential for modern web development.\npackage.json We will start by creating a package.json file. This file contains metadata about the project and its dependencies. You can use the npm init command to create the package.json file. Or create one yourself containing something like:\n{ \u0026#34;name\u0026#34;: \u0026#34;react-hello-world\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Hello world app using React\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/react\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34; } TypeScript Next, we will add TypeScript to our project. TypeScript is a superset of JavaScript that adds static types to the language. It helps catch errors early in the development process and improves code quality.\nAlthough TypeScript is not required to build a React app, it is strongly recommended. TypeScript is widely used in the React community and provides many benefits. Modern IDEs, such as Visual Studio Code and WebStorm, support TypeScript, making development and learning easier.\nnpm install --save-dev typescript This command updates the package.json file with the TypeScript dependency.\n{ \u0026#34;name\u0026#34;: \u0026#34;react-hello-world\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Hello world app using React\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/react\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;typescript\u0026#34;: \u0026#34;^5.4.5\u0026#34; } } It also creates a node_modules directory. This directory contains the packages installed by npm.\nFinally, the command creates a package-lock.json file. This file locks the dependencies to specific versions, ensuring that the project is built with the same versions of the dependencies across different machines.\nThe --save-dev flag tells npm to save the package as a development dependency. Development dependencies are not required for the production build of the app.\ntsconfig.json We need to create a tsconfig.json file to configure TypeScript. This file specifies the root files and compiler options for the TypeScript compiler. We will extend the recommended base configuration.\nInstall the recommended configuration with the following:\nnpm install --save-dev @tsconfig/recommended Then, create a tsconfig.json file with the following content:\n{ \u0026#34;extends\u0026#34;: \u0026#34;@tsconfig/recommended/tsconfig.json\u0026#34;, \u0026#34;compilerOptions\u0026#34;: { \u0026#34;jsx\u0026#34;: \u0026#34;react-jsx\u0026#34; } } What is JSX? In our tsconfig.json file, we set the jsx option to react-jsx. This option tells TypeScript to treat JSX as React JSX.\nJSX is a syntax extension for JavaScript. It allows you to write HTML-like code in JavaScript. JSX is used in React. It is syntactic sugar that is generally transpiled into JavaScript by the build tool.\nReact and ReactDOM Next, we will add React and ReactDOM to our project. React is the base library. ReactDOM is the package that provides DOM-specific methods for React.\nnpm install react react-dom Since we are using TypeScript, we must also install type definitions for React and ReactDOM. The TypeScript compiler uses these definitions for type checking.\nnpm install --save-dev @types/react @types/react-dom What is Webpack? Webpack is a module bundler for JavaScript. It takes modules with dependencies and generates static assets representing those modules. We will use Webpack as the build tool for our React app.\nWe will install the Webpack packages:\nnpm install --save-dev webpack webpack-cli webpack-dev-server html-webpack-plugin ts-loader webpack is the core package webpack-cli provides the command-line interface, which we will use to run Webpack commands webpack-dev-server is a development server that serves the app html-webpack-plugin will generate the index.html file to serve our app ts-loader is a TypeScript loader for Webpack. It allows Webpack to compile TypeScript files. webpack.config.ts By default, Webpack does not need a configuration file. However, since we use TypeScript, we must create a webpack.config.ts file to configure Webpack.\nNote that we use the .ts extension for the configuration file. The TypeScript compiler will compile this file. Using a .js file is also possible, but we prefer TypeScript for type safety.\nNo additional type definitions are required for our Webpack configuration at this time.\nimport HtmlWebpackPlugin from \u0026#39;html-webpack-plugin\u0026#39;; module.exports = { entry: \u0026#39;./src/index.tsx\u0026#39;, module: { rules: [ { test: /\\.(ts|tsx)$/, loader: \u0026#34;ts-loader\u0026#34;, exclude: /node_modules/, }, ], }, plugins: [new HtmlWebpackPlugin()], } We specify src/index.tsx as our app\u0026rsquo;s top-level file. By default, the build\u0026rsquo;s output will go to the dist directory.\nWe configure the TypeScript loader to compile .ts and .tsx files.\nWe also use the html-webpack-plugin to generate an index.html file. This file will load the Webpack bundle.\nWe need to add a TypeScript execution engine to the Node.js runtime so that it can understand the above TypeScript configuration file. We will use ts-node for this purpose.\nnpm install --save-dev ts-node Final package.json After all the installations, our package.json file should look similar to this:\n{ \u0026#34;name\u0026#34;: \u0026#34;react-hello-world\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Hello world app using React\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/react\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;@tsconfig/recommended\u0026#34;: \u0026#34;^1.0.6\u0026#34;, \u0026#34;@types/react\u0026#34;: \u0026#34;^18.2.79\u0026#34;, \u0026#34;@types/react-dom\u0026#34;: \u0026#34;^18.2.25\u0026#34;, \u0026#34;html-webpack-plugin\u0026#34;: \u0026#34;^5.6.0\u0026#34;, \u0026#34;ts-loader\u0026#34;: \u0026#34;^9.5.1\u0026#34;, \u0026#34;ts-node\u0026#34;: \u0026#34;^10.9.2\u0026#34;, \u0026#34;typescript\u0026#34;: \u0026#34;^5.4.5\u0026#34;, \u0026#34;webpack\u0026#34;: \u0026#34;^5.91.0\u0026#34;, \u0026#34;webpack-cli\u0026#34;: \u0026#34;^5.1.4\u0026#34;, \u0026#34;webpack-dev-server\u0026#34;: \u0026#34;^5.0.4\u0026#34; }, \u0026#34;dependencies\u0026#34;: { \u0026#34;react\u0026#34;: \u0026#34;^18.2.0\u0026#34;, \u0026#34;react-dom\u0026#34;: \u0026#34;^18.2.0\u0026#34; } } src/index.tsx We are finally ready to write some React code. TSX files are TypeScript files that contain JSX.\nWe will create the src/index.tsx file. It will render a simple React component. React components are the reusable building blocks of React apps.\nimport React from \u0026#34;react\u0026#34;; import {createRoot} from \u0026#34;react-dom/client\u0026#34; // A simple Class component class HelloWorld extends React.Component { render() { return \u0026lt;h1\u0026gt;Hello world!\u0026lt;/h1\u0026gt; } } // Use traditional DOM manipulation to create a root element for React document.body.innerHTML = \u0026#39;\u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt;\u0026lt;/div\u0026gt;\u0026#39; // Create a root element for React const app = createRoot(document.getElementById(\u0026#34;app\u0026#34;)!) // Render our HelloWorld component app.render(\u0026lt;HelloWorld/\u0026gt;) Running the app on the Webpack development server Now, we can run the app on the Webpack development server. This server will serve the app and automatically reload the page when the code changes.\nnode_modules/.bin/webpack serve --mode development --open The --mode development flag tells Webpack to build the app in development mode. The --open flag tells Webpack to open the app in the default browser.\nThe browser should show the following:\nReact app served by Webpack dev server package.json scripts Instead of remembering the above webpack command, we can add a script to the package.json file to run the Webpack development server.\n\u0026#34;scripts\u0026#34;: { \u0026#34;start\u0026#34;: \u0026#34;webpack serve --mode development --open\u0026#34; } start is a special script name that maps to the npm start command. Now, we can run the development server with:\nnpm start or\nnpm run start Building the app for production To build the app for production, we can run:\nnode_modules/.bin/webpack --mode production This command will create a dist directory with the app\u0026rsquo;s production build. The directory will contain the index.html file and the main.js JavaScript bundle. The production files are optimized for performance, and they are minified and compressed to reduce their size.\nIt is possible to host these production files on a local HTTP server like Apache or Nginx, or deploy the app to cloud providers such as AWS, Cloudflare Pages, Netlify, Render, or Vercel.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/react/tree/main/1-hello-world\nReact Hello World video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-04-24T00:00:00Z","image":"https://victoronsoftware.com/posts/react-hello-world/react-hello-world_hu84b0260e55078cc47c5a5b8b4b4901b4_85634_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/react-hello-world/","title":"Build a React app from scratch: a complete guide (2024)"},{"content":"Why fix security vulnerabilities? Security vulnerabilities are a common issue in software development. They can lead to data breaches, unauthorized access, and other security incidents. It is important to fix security vulnerabilities as soon as possible to protect your data and users.\nFinding vulnerabilities Nowadays, it is possible to integrate various vulnerability scanning tools into your CI/CD pipeline. These tools can help you identify security vulnerabilities in your code and dependencies. One such tool is OpenSSF Scorecard, which combines multiple other tools into a single GitHub action. It uses the OSV service to find vulnerabilities affecting your project\u0026rsquo;s dependencies. OSV (Open Source Vulnerabilities) is a Google-based vulnerability database providing information about open-source projects\u0026rsquo; vulnerabilities.\nIn this article, we will focus on fixing a few recent real-world security vulnerabilities in our yarn.lock dependencies.\nscore is 3: 6 existing vulnerabilities detected: Warn: Project is vulnerable to: GHSA-crh6-fp67-6883 Warn: Project is vulnerable to: GHSA-wf5p-g6vw-rhxx Warn: Project is vulnerable to: GHSA-p6mc-m468-83gw Warn: Project is vulnerable to: GHSA-566m-qj78-rww5 Warn: Project is vulnerable to: GHSA-7fh5-64p2-3v2j Warn: Project is vulnerable to: GHSA-4wf5-vphf-c2xc Click Remediation section below to solve this issue Using local tools to find vulnerabilities In a local environment, we can use OSV-Scanner to find vulnerabilities in our dependencies. Running:\nosv-scanner scan --lockfile yarn.lock It will output the same vulnerabilities mentioned above but with additional details.\n╭─────────────────────────────────────┬──────┬───────────┬────────────────┬─────────┬───────────╮ │ OSV URL │ CVSS │ ECOSYSTEM │ PACKAGE │ VERSION │ SOURCE │ ├─────────────────────────────────────┼──────┼───────────┼────────────────┼─────────┼───────────┤ │ https://osv.dev/GHSA-crh6-fp67-6883 │ 9.8 │ npm │ @xmldom/xmldom │ 0.8.3 │ yarn.lock │ │ https://osv.dev/GHSA-wf5p-g6vw-rhxx │ 6.5 │ npm │ axios │ 0.21.4 │ yarn.lock │ │ https://osv.dev/GHSA-p6mc-m468-83gw │ 7.4 │ npm │ lodash.set │ 4.3.2 │ yarn.lock │ │ https://osv.dev/GHSA-566m-qj78-rww5 │ 5.3 │ npm │ postcss │ 6.0.23 │ yarn.lock │ │ https://osv.dev/GHSA-7fh5-64p2-3v2j │ 5.3 │ npm │ postcss │ 6.0.23 │ yarn.lock │ │ https://osv.dev/GHSA-7fh5-64p2-3v2j │ 5.3 │ npm │ postcss │ 7.0.39 │ yarn.lock │ │ https://osv.dev/GHSA-7fh5-64p2-3v2j │ 5.3 │ npm │ postcss │ 8.4.21 │ yarn.lock │ │ https://osv.dev/GHSA-4wf5-vphf-c2xc │ 7.5 │ npm │ terser │ 5.12.1 │ yarn.lock │ ╰─────────────────────────────────────┴──────┴───────────┴────────────────┴─────────┴───────────╯ Another way to find these vulnerabilities is by using the built-in yarn audit command.\nWaiving vulnerabilities In some cases, you may decide to waive a vulnerability. This approach means that you examine the vulnerability documentation and acknowledge it but decide not to fix it.\nTo waive a vulnerability for the OSV flow, you can create an osv-scanner.toml file in the root of your project. For example, to waive the GHSA-crh6-fp67-6883 vulnerability, you can add the following to the osv-scanner.toml file:\n[[IgnoredVulns]] id = \u0026#34;GHSA-crh6-fp67-6883\u0026#34; reason = \u0026#34;We examined this vulnerability and concluded that it does not affect our project for a very good reason.\u0026#34; In our example, we will not waive any vulnerabilities, but we will fix them by updating the dependencies.\nUpdating an inner dependency In our example, we have a vulnerability in the @xmldom/xmldom package. From the vulnerability URL, we know we must update this package to 0.8.4 or later.\nRunning yarn why @xmldom/xmldom will show that it is an inner dependency of another package:\n=\u0026gt; Found \u0026#34;@xmldom/xmldom@0.8.3\u0026#34; info Reasons this module exists - \u0026#34;msw#@mswjs#interceptors\u0026#34; depends on it - Hoisted from \u0026#34;msw#@mswjs#interceptors#@xmldom#xmldom\u0026#34; Looking at yarn.lock shows:\n\u0026#34;@xmldom/xmldom@^0.8.3\u0026#34;: version \u0026#34;0.8.3\u0026#34; resolved \u0026#34;https://registry.yarnpkg.com/@xmldom/xmldom/-/xmldom-0.8.3.tgz#beaf980612532aa9a3004aff7e428943aeaa0711\u0026#34; integrity sha512-Lv2vySXypg4nfa51LY1nU8yDAGo/5YwF+EY/rUZgIbfvwVARcd67ttCM8SMsTeJy51YhHYavEq+FS6R0hW9PFQ== We see that 0.8.4 will satisfy the dependency requirement of ^0.8.3. We can update the package by deleting the above section from yarn.lock and running yarn install\nWe will then see the update:\n\u0026#34;@xmldom/xmldom@^0.8.3\u0026#34;: version \u0026#34;0.8.10\u0026#34; resolved \u0026#34;https://registry.yarnpkg.com/@xmldom/xmldom/-/xmldom-0.8.10.tgz#a1337ca426aa61cef9fe15b5b28e340a72f6fa99\u0026#34; integrity sha512-2WALfTl4xo2SkGCYRt6rDTFfk9R1czmBvUQy12gK2KuRKIpWEhcbbzy8EZXtz/jkRqHX8bFEc6FC1HjX4TUWYw== Upgrading an inner dependency by overriding the version Our following vulnerability is in the axios package. We need to update it to 0.28.0 or later. By running yarn why axios we see that this package is part of a deep dependency chain:\n=\u0026gt; Found \u0026#34;wait-on#axios@0.21.4\u0026#34; info This module exists because \u0026#34;@storybook#test-runner#jest-playwright-preset#jest-process-manager#wait-on\u0026#34; depends on it. The needed version 0.28.0 does not satisfy the ^0.21.4 requirement. We can override the version by adding the following to the package.json file:\n\u0026#34;resolutions\u0026#34;: { \u0026#34;**/wait-on/axios\u0026#34;: \u0026#34;^0.28.0\u0026#34; }, Upgrading the parent dependency The following vulnerability is in the lodash.set package. The vulnerability URL shows that there is no fix for this vulnerability. We also see at npmjs.com that this package was last updated eight years ago.\nWe need to update the parent package that uses lodash.set. Running yarn why lodash.set shows:\ninfo Reasons this module exists - \u0026#34;nock\u0026#34; depends on it - Hoisted from \u0026#34;nock#lodash.set\u0026#34; We update the parent by running yarn upgrade nock@latest. Luckily, the latest version of nock does not depend on lodash.set, and lodash.set is removed from yarn.lock.\nRemoving a dependency Sometimes the best way to fix a vulnerability is to remove the vulnerable dependency. This can be done with the yarn remove \u0026lt;dependency\u0026gt; command. However, this requires code changes. You must find a different library or implement the removed functionality yourself.\nConclusion We use the above strategies to fix the vulnerabilities in our project.\nUpdating an inner dependency Upgrading an inner dependency by overriding the version Upgrading the parent dependency Removing a dependency We can now rerun the vulnerability scanner to verify that we fixed the vulnerabilities.\nIn addition, we must run our unit test and integration test suite to ensure that the updates do not break our application.\nFix security vulnerabilities in Yarn video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-04-10T00:00:00Z","image":"https://victoronsoftware.com/posts/fix-security-vulnerabilities-yarn/cover_huad6a6c1a561ddeac007cfa2eef03f368_47668_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/fix-security-vulnerabilities-yarn/","title":"Fix security vulnerabilities in Yarn"},{"content":"What is code signing? Code signing is the process of digitally signing executables and scripts to confirm the software author and guarantee that the code has not been altered or corrupted since it was signed. The method employs a cryptographic hash to validate the authenticity and integrity of the code.\nThe benefits of code signing Code signing provides several benefits:\nUser trust: Users are likelier to trust signed software because they can verify its origin. Security: Code signing helps prevent tampering and makes sure that bad actors have not altered the software. Malware protection: Code signing helps protect users from malware by verifying the software\u0026rsquo;s authenticity. Software updates: Code signing helps users verify that software updates are legitimate and not malicious. Windows Defender: Code signing helps prevent Windows Defender warnings. Code signing process for Windows The code signing process for Windows involves the following steps:\nObtain a code signing certificate: Purchase a code signing certificate from a trusted certificate authority (CA) or use a self-signed certificate. Sign the code: Use a code signing tool to sign the code with the code signing certificate. Timestamp the signature: Timestamp the signature to make sure that the signature remains valid even after the certificate expires. Distribute the signed code: Distribute the signed code to users. Verify the signature: Users can verify the signature to confirm the software\u0026rsquo;s authenticity. Obtaining a code signing certificate In our example, we will use a self-signed certificate. This approach is suitable for internal business applications. For public applications, you should obtain a code signing certificate from a trusted CA.\nWe will use the OpenSSL command line tool to generate the certificates. OpenSSL is a popular open-source library for TLS and SSL protocols.\nThe following script generates the certificate and key needed for code signing. It also generates a certificate authority (CA) and signs the code signing certificate with the CA.\n#!/usr/bin/env bash # -e: Immediately exit if any command has a non-zero exit status. # -x: Print all executed commands to the terminal. # -u: Exit if an undefined variable is used. # -o pipefail: Exit if any command in a pipeline fails. set -exuo pipefail # This script generates certificates and keys needed for code signing. mkdir -p certs # Certificate authority (CA) openssl genrsa -out certs/ca.key 2048 openssl req -new -x509 -nodes -days 1000 -key certs/ca.key -out certs/ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testCodeSignCA\u0026#34; # Generate a certificate for code signing, signed by the CA openssl req -newkey rsa:2048 -nodes -keyout certs/sign.key -out certs/sign.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testCodeSignCert\u0026#34; openssl x509 -req -in certs/sign.req -days 398 -CA certs/ca.crt -CAkey certs/ca.key -set_serial 01 -out certs/sign.crt # Clean up rm certs/sign.req Building the application We will build a simple \u0026ldquo;Hello World\u0026rdquo; Windows application using the Go programming language for this example. We compile the application with:\nexport GOOS=windows export GOARCH=amd64 go build ./hello-world.go The Go build process generates the hello-world.exe Windows executable.\nSigning and timestamping the code To sign the code, we will use osslsigncode, an open-source code signing tool that uses OpenSSL to sign Windows executables. Unlike Microsoft\u0026rsquo;s signtool, osslsigncode is cross-platform and can be used on Linux and macOS.\nTo sign the code, we use the following script:\n#!/usr/bin/env bash # -e: Immediately exit if any command has a non-zero exit status. # -x: Print all executed commands to the terminal. # -u: Exit if an undefined variable is used. # -o pipefail: Exit if any command in a pipeline fails. set -exuo pipefail input_file=$1 if [ ! -f \u0026#34;$input_file\u0026#34; ] then echo \u0026#39;First argument must be path to binary\u0026#39; exit 1 fi # Check that input file is a windows PE (Portable Executable) if ! ( file \u0026#34;$input_file\u0026#34; | grep -q PE ) then echo \u0026#39;File must be a Portable Executable (PE) file.\u0026#39; exit 0 fi # Check that osslsigncode is installed if ! command -v osslsigncode \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 ; then echo \u0026#34;osslsigncode utility is not present or missing from PATH. Binary cannot be signed.\u0026#34; exit 1 fi orig_file=\u0026#34;${input_file}_unsigned\u0026#34; mv \u0026#34;$input_file\u0026#34; \u0026#34;$orig_file\u0026#34; osslsigncode sign -certs \u0026#34;./certs/sign.crt\u0026#34; -key \u0026#34;./certs/sign.key\u0026#34; -n \u0026#34;Hello Windows code signing\u0026#34; -i \u0026#34;https://victoronsoftware.com/\u0026#34; -t \u0026#34;http://timestamp.comodoca.com/authenticode\u0026#34; -in \u0026#34;$orig_file\u0026#34; -out \u0026#34;$input_file\u0026#34; rm \u0026#34;$orig_file\u0026#34; In addition to signing the code, we timestamp the signature using the Comodo server. Timestamping makes sure the signature remains valid even after the certificate expires or is invalidated.\nWe can use osslsigncode to verify the signature:\n#!/usr/bin/env bash input_file=$1 osslsigncode verify -CAfile ./certs/ca.crt \u0026#34;$input_file\u0026#34; Distributing and manually verifying the signed code After signing the code, we can distribute the signed executable to users. Users can manually verify the signature by right-clicking the executable, selecting \u0026ldquo;Properties,\u0026rdquo; and navigating to the \u0026ldquo;Digital Signatures\u0026rdquo; tab. The user can then view the certificate details and verify that the signature is valid.\nHowever, since we are using the self-signed certificate, users will see a warning that the certificate is not trusted. Our self-signed certificate is not trusted because the certificate authority is not part of the Windows trusted root certificate store.\nCertificate in code signature cannot be verified We can add the certificate authority to the Windows trusted root certificate store with the following Powershell command:\nImport-Certificate -FilePath \u0026#34;certs\\ca.crt\u0026#34; -CertStoreLocation Cert:\\LocalMachine\\Root After adding the certificate authority to the trusted root certificate store, users will see that the certificate is trusted and the signature is valid.\nCertificate in code signature is be verified Code signing using a certificate from a public CA To sign public applications, we must obtain a code signing certificate from a trusted CA. The latest industry standards require private keys for code signing certificates to be stored in hardware security modules (HSMs) to prevent unauthorized access. This security requirement means certificates for code signing in CI/CD pipelines must use a cloud HSM vendor or a private pipeline runner with an HSM.\nIn a future article, we will explore signing a Windows application using a cloud HSM vendor.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/code-sign-windows\nCode signing a Windows application video ","date":"2024-03-27T00:00:00Z","image":"https://victoronsoftware.com/posts/code-signing-windows/digital-signature-ok_hu33b7d29a52e912941cefe9572e7aaf3f_45104_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/code-signing-windows/","title":"Code signing a Windows application"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain mTLS Go client mTLS Go client with custom certificate signer mTLS Go client using macOS keychain mTLS with Windows certificate store Why use the Windows certificate store? Keeping the mTLS client private key on the filesystem is insecure and not recommended. In the mTLS Go client using macOS keychain, we demonstrated achieving greater mTLS security with macOS keychain. In this article, we reach a similar level of protection with the Windows certificate store.\nThe Windows certificate store is a secure location where certificates and keys can be stored. Many applications, such as Edge and Powershell, use it. The Windows certificate store is an excellent place to store mTLS client certificates and keys.\nBuilding a custom tls.Certificate for the Windows certificate store This work builds on the mTLS Go client with custom certificate signer article. We will use the CustomSigner from that article to build a custom tls.Certificate that uses the Windows certificate store.\nHowever, before the application uses the Public and Sign methods of the CustomSigner, we must retrieve the client certificate using Windows APIs.\nRetrieving mTLS client certificate from Windows certificate store using Go We will use the golang.org/x/sys/windows package to access the Windows APIs. We use the windows package to call the CertOpenStore, CertFindCertificateInStore, and CryptAcquireCertificatePrivateKey functions from the crypt32 DLL (dynamic link library).\nFirst, we open the MY store, which is the personal store for the current user. This store contains our client mTLS certificate.\n// Open the certificate store storePtr, err := windows.UTF16PtrFromString(windowsStoreName) if err != nil { return nil, err } store, err := windows.CertOpenStore( windows.CERT_STORE_PROV_SYSTEM, 0, uintptr(0), windows.CERT_SYSTEM_STORE_CURRENT_USER, uintptr(unsafe.Pointer(storePtr)), ) if err != nil { return nil, err } Next, we find the certificate by the common name.\n// Find the certificate var pPrevCertContext *windows.CertContext var certContext *windows.CertContext commonNamePtr, err := windows.UTF16PtrFromString(commonName) for { certContext, err = windows.CertFindCertificateInStore( store, windows.X509_ASN_ENCODING, 0, windows.CERT_FIND_SUBJECT_STR, unsafe.Pointer(commonNamePtr), pPrevCertContext, ) if err != nil { return nil, err } // We can extract the certificate chain and further filter the certificate // we want here. break } Converting the Windows certificate to a Go x509.Certificate After retrieving the certificate from the Windows certificate store, we convert it to a Go x509.Certificate.\n// Copy the certificate data so that we have our own copy outside the windows context encodedCert := unsafe.Slice(certContext.EncodedCert, certContext.Length) buf := bytes.Clone(encodedCert) foundCert, err := x509.ParseCertificate(buf) if err != nil { return nil, err } Building the custom tls.Certificate Finally, we put together the custom tls.Certificate using the x509.Certificate. We hold on to the certContext pointer to get the private key later.\ncustomSigner := \u0026amp;CustomSigner{ store: store, windowsCertContext: certContext, } customSigner.x509Cert = foundCert certificate := tls.Certificate{ Certificate: [][]byte{foundCert.Raw}, PrivateKey: customSigner, SupportedSignatureAlgorithms: []tls.SignatureScheme{supportedAlgorithm}, } Our example only supports the tls.PSSWithSHA256 signature algorithm to keep the code simple.\nSigning the mTLS digest with the Windows certificate store As discussed in the previous mTLS Go client with custom certificate signer article, we must sign the CertificateVerify message during the TLS handshake. We will use the CustomSigner to sign the digest, which implements the crypto.Signer interface as defined in the Go standard library\u0026rsquo;s crypto package.\n// CustomSigner is a crypto.Signer that uses the client certificate and key to sign type CustomSigner struct { store windows.Handle windowsCertContext *windows.CertContext x509Cert *x509.Certificate } func (k *CustomSigner) Public() crypto.PublicKey { fmt.Printf(\u0026#34;crypto.Signer.Public\\n\u0026#34;) return k.x509Cert.PublicKey } func (k *CustomSigner) Sign(_ io.Reader, digest []byte, opts crypto.SignerOpts ) (signature []byte, err error) { ... Retrieve the private key reference from the Windows certificate store We retrieve the private key reference from the Windows certificate store using the CryptAcquireCertificatePrivateKey function.\n// Get private key var ( privateKey windows.Handle pdwKeySpec uint32 pfCallerFreeProvOrNCryptKey bool ) err = windows.CryptAcquireCertificatePrivateKey( k.windowsCertContext, windows.CRYPT_ACQUIRE_CACHE_FLAG|windows.CRYPT_ACQUIRE_SILENT_FLAG| windows.CRYPT_ACQUIRE_ONLY_NCRYPT_KEY_FLAG, nil, \u0026amp;privateKey, \u0026amp;pdwKeySpec, \u0026amp;pfCallerFreeProvOrNCryptKey, ) if err != nil { return nil, err } Signing the mTLS digest We will use the NCryptSignHash function from ncrypt.dll to sign the digest.\nvar ( nCrypt = windows.MustLoadDLL(\u0026#34;ncrypt.dll\u0026#34;) nCryptSignHash = nCrypt.MustFindProc(\u0026#34;NCryptSignHash\u0026#34;) ) But before we do that, we must create a BCRYPT_PSS_PADDING_INFO structure for our supported RSA-PSS algorithm.\nflags := nCryptSilentFlag | bCryptPadPss pPaddingInfo, err := getRsaPssPadding(opts) if err != nil { return nil, err } Where getRsaPssPadding is a helper function:\nfunc getRsaPssPadding(opts crypto.SignerOpts) (unsafe.Pointer, error) { pssOpts, ok := opts.(*rsa.PSSOptions) if !ok || pssOpts.Hash != crypto.SHA256 { return nil, fmt.Errorf(\u0026#34;unsupported hash function %s\u0026#34;, opts.HashFunc().String()) } if pssOpts.SaltLength != rsa.PSSSaltLengthEqualsHash { return nil, fmt.Errorf(\u0026#34;unsupported salt length %d\u0026#34;, pssOpts.SaltLength) } sha256, _ := windows.UTF16PtrFromString(\u0026#34;SHA256\u0026#34;) // Create BCRYPT_PSS_PADDING_INFO structure: // typedef struct _BCRYPT_PSS_PADDING_INFO { // LPCWSTR pszAlgId; // ULONG cbSalt; // } BCRYPT_PSS_PADDING_INFO; return unsafe.Pointer( \u0026amp;struct { pszAlgId *uint16 cbSalt uint32 }{ pszAlgId: sha256, cbSalt: uint32(pssOpts.HashFunc().Size()), }, ), nil } Finally, we sign the digest using the NCryptSignHash function.\n// Sign the digest // The first call to NCryptSignHash retrieves the size of the signature var size uint32 success, _, _ := nCryptSignHash.Call( uintptr(privateKey), uintptr(pPaddingInfo), uintptr(unsafe.Pointer(\u0026amp;digest[0])), uintptr(len(digest)), uintptr(0), uintptr(0), uintptr(unsafe.Pointer(\u0026amp;size)), uintptr(flags), ) if success != 0 { return nil, fmt.Errorf(\u0026#34;NCryptSignHash: failed to get signature length: %#x\u0026#34;, success) } // The second call to NCryptSignHash retrieves the signature signature = make([]byte, size) success, _, _ = nCryptSignHash.Call( uintptr(privateKey), uintptr(pPaddingInfo), uintptr(unsafe.Pointer(\u0026amp;digest[0])), uintptr(len(digest)), uintptr(unsafe.Pointer(\u0026amp;signature[0])), uintptr(size), uintptr(unsafe.Pointer(\u0026amp;size)), uintptr(flags), ) if success != 0 { return nil, fmt.Errorf(\u0026#34;NCryptSignHash: failed to generate signature: %#x\u0026#34;, success) } return signature, nil Putting it all together With the above code, we can create our new Go mTLS client that uses the Windows certificate store.\nfunc main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } client := http.Client{ Transport: \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ GetClientCertificate: signer.GetClientCertificate, MinVersion: tls.VersionTLS13, MaxVersion: tls.VersionTLS13, }, }, } // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } We limit the scope of this example to TLS 1.3\nSetting up the environment The next step is to use the Windows certificate store to store the client certificate and private key. We will use the certificates and keys scripts from the previous mTLS with Windows certificate store article. If you still need to generate the certificates and keys, please follow the instructions in that article.\nFinally, as in the mTLS with Windows certificate store article, we start two nginx servers:\nhttps://\u0026lt;your_host\u0026gt;:8888 for TLS https://\u0026lt;your_host\u0026gt;:8889 for mTLS Running the Go mTLS client using the Windows certificate store We can run our mTLS client without pointing to certificate/key files and retrieving everything from the Windows certificate store. Hitting the ordinary TLS server:\ngo run .\\client-signer.go --url https://myhost:8888/hello-world.txt Returns the expected:\nTLS Hello World! While hitting the mTLS server:\ngo run .\\client-signer.go --url https://myhost:8889/hello-world.txt Returns a more detailed message, including the print statements in our custom code:\nServer requested certificate Found certificate with common name testClientTLS crypto.Signer.Public crypto.Signer.Public crypto.Signer.Sign with key type *rsa.PublicKey, opts type *rsa.PSSOptions, hash SHA-256 mTLS Hello World! Example code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-go-windows\nmTLS Go client using Windows certificate store video ","date":"2024-03-20T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-go-client-windows-certificate-store/mtls-go-windows_hucfa29512f6a856070bdc43f20ddc6a98_80828_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mtls-go-client-windows-certificate-store/","title":"Mutual TLS (mTLS) Go client using Windows certificate store"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain mTLS Go client mTLS Go client with custom certificate signer mTLS Go client using macOS keychain Why use Windows certificate store? In our previous articles, we introduced mTLS and demonstrated how to use mTLS client certificates and keys. Keeping the mTLS client private key on the filesystem is insecure and not recommended. In the mTLS Go client using macOS keychain, we demonstrated achieving greater mTLS security with macOS keychain. In this article, we start exploring how to achieve the same level of protection with Windows certificate store.\nThe Windows certificate store is a secure location to store certificates and keys. Many applications, such as Edge and Powershell use it. The Windows certificate store is an excellent place to store mTLS client certificates and keys.\nThe Windows certificate stores have two types:\nUser certificate store: Certificates and keys are stored for the current user, local to a user account. Local machine certificate store: Certificates and keys are stored for all users on the computer. We will store our client mTLS certificate in the user certificate store and the other certificates in the local machine certificate store.\nGenerating mTLS certificates and keys We will use the following Powershell script to generate the mTLS certificates and keys. OpenSSL must be installed on your computer.\nNew-Item -ItemType Directory -Force certs # Private keys for CAs openssl genrsa -out certs/server-ca.key 2048 openssl genrsa -out certs/client-ca.key 2048 # Generate CA certificates openssl req -new -x509 -nodes -days 1000 -key certs/server-ca.key -out certs/server-ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testServerCA\u0026#34; openssl req -new -x509 -nodes -days 1000 -key certs/client-ca.key -out certs/client-ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testClientCA\u0026#34; # Generate a certificate signing request openssl req -newkey rsa:2048 -nodes -keyout certs/server.key -out certs/server.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testServerTLS\u0026#34; openssl req -newkey rsa:2048 -nodes -keyout certs/client.key -out certs/client.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testClientTLS\u0026#34; # Have the CA sign the certificate requests and output the certificates. openssl x509 -req -in certs/server.req -days 398 -CA certs/server-ca.crt -CAkey certs/server-ca.key -set_serial 01 -out certs/server.crt -extfile localhost.ext openssl x509 -req -in certs/client.req -days 398 -CA certs/client-ca.crt -CAkey certs/client-ca.key -set_serial 01 -out certs/client.crt # Create PFX file for importing to certificate store openssl pkcs12 -export -out certs\\client.pfx -inkey certs\\client.key -in certs\\client.crt -passout pass: # Clean up Remove-Item certs/server.req Remove-Item certs/client.req The maximum validity period for a TLS certificate is 398 days.\nThe localhost.ext file is used to specify the subject alternative name (SAN) for the server certificate. The localhost.ext file contains the following:\n[alt_names] DNS.1 = localhost DNS.2 = myhost We can access the server using either localhost or myhost names.\nThe above script generates the following files:\ncerts/server-ca.crt: Server CA certificate certs/server-ca.key: Server CA private key certs/client-ca.crt: Client CA certificate certs/client-ca.key: Client CA private key certs/server.crt: Server certificate certs/server.key: Server private key certs/client.crt: Client certificate certs/client.key: Client private key certs/client.pfx: Client certificate and private key in PFX format, needed for importing into the Windows certificate store Importing the client certificate and key into the Windows certificate store We will import the client certificate and key into the user certificate store using the following powershell script.\n# Import the server CA Import-Certificate -FilePath \u0026#34;certs\\server-ca.crt\u0026#34; -CertStoreLocation Cert:\\LocalMachine\\Root # Import the client CA so that client TLS certificates can be verified Import-Certificate -FilePath \u0026#34;certs\\client-ca.crt\u0026#34; -CertStoreLocation Cert:\\LocalMachine\\Root # Import the client TLS certificate and key Import-PfxCertificate -FilePath \u0026#34;certs\\client.pfx\u0026#34; -CertStoreLocation Cert:\\CurrentUser\\My The command result should be similar to the following:\nPSParentPath: Microsoft.PowerShell.Security\\Certificate::LocalMachine\\Root Thumbprint Subject ---------- ------- 0A31BF3C48A3D98A91A2F63B5BD286818311A707 CN=testServerCA, OU=Your Unit, O=Your Organization, L=Austin, S=Texas, C=US 7F7E5612F3A90B9EB246762358251F98911A9D1A CN=testClientCA, OU=Your Unit, O=Your Organization, L=Austin, S=Texas, C=US PSParentPath: Microsoft.PowerShell.Security\\Certificate::CurrentUser\\My Thumbprint Subject ---------- ------- E2EBB991E3849E32E934D8465FAE42787D34C9ED CN=testClientTLS, OU=Your Unit, O=Your Organization, L=Austin, S=Texas, C=US By default, the private key is marked as non-exportable. A user or an application cannot export the private key from the certificate store. They can only access the private key via Windows APIs. Using a non-exportable private key is the recommended security approach. You can use the -Exportable parameter if you need to export the private key.\nVerifying imported certificates and keys As an extra step, we can verify that the certificates and keys exist in the Windows certificate store. We can use the certlm Local Machine Certificate Manager GUI, certmgr User Certificate Manager GUI, or the Get-ChildItem powershell command.\nGet-ChildItem -Path Cert:\\LocalMachine\\Root | Where-Object{$_.Subject -match \u0026#39;testServerCA\u0026#39;} | Test-Certificate -Policy SSL Get-ChildItem -Path Cert:\\CurrentUser\\My | Where-Object{$_.Subject -match \u0026#39;testClientTLS\u0026#39;} Running the mTLS server We will use the same docker-compose.yml file from the mTLS Hello World article. The docker-compose.yml file starts two nginx servers:\nhttps://\u0026lt;your_host\u0026gt;:8888 for TLS https://\u0026lt;your_host\u0026gt;:8889 for mTLS We can run Docker on WSL (Windows Subsystem for Linux) or another machine. We will run it on a different machine, so we need to copy the certs directory to the machine running Docker. When running the server on a different machine, we must update the C:\\Windows\\System32\\drivers\\etc\\hosts file to point to the other machine.\n10.0.0.5 myhost Connecting to the TLS and mTLS servers with clients Because we added the server CA to the root certificate store, we can now access the TLS server without any additional flags:\nInvoke-WebRequest -Uri https://myhost:8888/hello-world.txt Result:\nStatusCode : 200 StatusDescription : OK Content : TLS Hello World! RawContent : HTTP/1.1 200 OK Connection: keep-alive Accept-Ranges: bytes Content-Length: 17 Content-Type: text/plain Date: Sun, 03 Mar 2024 17:28:29 GMT ETag: \u0026#34;65b29c19-11\u0026#34; Last-Modified: Thu, 25 Jan 2024 1... Forms : {} Headers : {[Connection, keep-alive], [Accept-Ranges, bytes], [Content-Length, 17], [Content-Type, text/plain]...} Images : {} InputFields : {} Links : {} ParsedHtml : System.__ComObject RawContentLength : 17 However, we cannot access the mTLS server directly.\nInvoke-WebRequest -Uri https://myhost:8889/hello-world.txt The client attempted the TLS handshake, but the server rejected the connection because the client did not provide a certificate. Result:\nInvoke-WebRequest : 400 Bad Request No required SSL certificate was sent nginx/1.25.3 At line:1 char:1 + Invoke-WebRequest -Uri https://myhost:8889/hello-world.txt + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : InvalidOperation: (System.Net.HttpWebRequest:HttpWebRequest) [Invoke-WebRequest], WebException + FullyQualifiedErrorId : WebCmdletWebResponseException,Microsoft.PowerShell.Commands.InvokeWebRequestCommand We can, however, provide the client certificate thumbprint to access the mTLS server. We saw the thumbprint of the client certificate earlier when we imported it into the Windows certificate store.\nInvoke-WebRequest -Uri https://myhost:8889/hello-world.txt -CertificateThumbprint E2EBB991E3849E32E934D8465FAE42787D34C9ED Result:\nStatusCode : 200 StatusDescription : OK Content : mTLS Hello World! RawContent : HTTP/1.1 200 OK Connection: keep-alive Accept-Ranges: bytes Content-Length: 18 Content-Type: text/plain Date: Sun, 03 Mar 2024 17:31:55 GMT ETag: \u0026#34;65b29c19-12\u0026#34; Last-Modified: Thu, 25 Jan 2024 1... Forms : {} Headers : {[Connection, keep-alive], [Accept-Ranges, bytes], [Content-Length, 18], [Content-Type, text/plain]...} Images : {} InputFields : {} Links : {} ParsedHtml : System.__ComObject RawContentLength : 18 Edge browser can access the mTLS server. We can verify this by opening the following URL:\nhttps://myhost:8889/hello-world.txt We see the following popup:\nEdge mTLS popup\nWe can click OK to connect to the mTLS server. Future connections will not show the popup and will automatically use the client certificate.\nNote: Here is a helpful link that may resolve issues trying to use mTLS client certificates on Windows 10: https://superuser.com/questions/1181163/unable-to-use-client-certificates-in-chrome-or-ie-on-windows-10\nExample code on Github The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-with-windows\nCreating our own Windows mTLS client In the following article, we will create a custom Windows mTLS client using the Windows certificate store.\nmTLS with Windows certificate store video ","date":"2024-03-06T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-with-windows/mtls-edge_hu1c5448f514f25a5b4cbc44c26f1cdd9f_40356_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mtls-with-windows/","title":"Mutual TLS (mTLS) with Windows certificate store"},{"content":"Introduction Any app aiming to reach an international audience must support Unicode. Emojis, which are based on Unicode, are everywhere. They are used in text messages, social media, and programming languages. Supporting Unicode and emojis in your app can be tricky. This article will cover common Unicode and emoji support issues and how to fix them.\nWhat is Unicode? Unicode is a standard for encoding, representing, and handling text. It is a character set that assigns a unique number to every character. The most common encoding for Unicode is UTF-8, which stands for Unicode Transformation Format 8-bit. UTF-8 is a variable-width encoding that can represent every character in the Unicode character set.\nUTF-8 format can take one to four bytes to represent a code point. Multiple code points can be combined to form a single character. For example, the emoji \u0026ldquo;👍\u0026rdquo; is represented by the code point U+1F44D. In UTF-8, it is represented by the bytes F0 9F 91 8D. The same emoji with skin tone \u0026ldquo;👍🏽\u0026rdquo; is represented by the code point U+1F44D U+1F3FD. In UTF-8, that emoji is represented by the bytes F0 9F 91 8D F0 9F 8F BD. Generally, emojis take up at least four bytes in UTF-8.\nUnicode equivalence Our first gotcha is unicode equivalence.\nUnicode equivalence is the concept that two different sequences of code points can represent the same character. For example, the character é can be represented by the code point U+00E9 or by the sequence of code points U+0065 U+0301. The first representation is the composed form, and the second is the decomposed form. Unicode equivalence is essential when comparing strings or searching for a string character.\nDatabases typically do not support Unicode equivalence out of the box. For example, given this table using MySQL 5.7:\nCREATE TABLE test ( id INT UNSIGNED NOT NULL AUTO_INCREMENT, name VARCHAR(255) NOT NULL, PRIMARY KEY (id)) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; INSERT INTO test (name) VALUES (\u0026#39;가\u0026#39;), (CONCAT(\u0026#39;ᄀ\u0026#39;, \u0026#39;ᅡ\u0026#39;)); SELECT * from test WHERE name = \u0026#39;가\u0026#39;; The query will return a single row, even though the Korean character 가 and character sequence ᄀ + ᅡ are equivalent. The incorrect result is because the utf8mb4_unicode_ci collation does not support Unicode equivalence. One way to fix this is to use the utf8mb4_0900_ai_ci collation, which supports Unicode equivalence. However, this requires updating the database to MySQL 8.0 or later, which may not be possible in some cases.\nEmoji equivalence Our second gotcha is emoji equivalence.\nSome databases may not support emoji equivalence out of the box. For example, given this table using MySQL 5.7:\nCREATE TABLE test ( id INT UNSIGNED NOT NULL AUTO_INCREMENT, name VARCHAR(255) NOT NULL, PRIMARY KEY (id)) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; INSERT INTO test (name) VALUES (\u0026#39;🔥\u0026#39;), (\u0026#39;🔥🔥\u0026#39;), (\u0026#39;👍\u0026#39;), (\u0026#39;👍🏽\u0026#39;); SELECT * from test WHERE name = \u0026#39;🔥\u0026#39;; The query will return:\n1,🔥 3,👍 And the following query:\nSELECT * from test WHERE name LIKE \u0026#39;%🔥%\u0026#39;; Will return:\n1,🔥 2,🔥🔥 The utf8mb4_unicode_ci collation does not support emoji equivalence, and the behavior of = differs from LIKE.\nOne way to fix the problem of emoji equivalence is to use a different collation during the = comparison. For example:\nSELECT * from test WHERE name COLLATE utf8mb4_bin = \u0026#39;🔥\u0026#39;; Will return the single correct result:\n1,🔥 However, this solution is not ideal because it requires the developer to remember to use the utf8mb4_bin collation for emoji equivalence. There is also a slight performance impact when using a different collation.\nCase-insensitive sorting Our third gotcha is sorting.\nTypically, app users want to see case-insensitive sorting of strings. For example, the strings \u0026ldquo;apple\u0026rdquo;, \u0026ldquo;Banana\u0026rdquo;, and \u0026ldquo;cherry\u0026rdquo; should be sorted as \u0026ldquo;apple\u0026rdquo;, \u0026ldquo;Banana\u0026rdquo;, and \u0026ldquo;cherry\u0026rdquo;. The utf8mb4_unicode_ci collation used above supports case-insensitive sorting. However, switching to another collation, such as utf8mb4_bin, to support emoji equivalence will break case-insensitive sorting. Hence, whatever solution you develop for full Unicode support should also support case-insensitive sorting.\nSolving our gotchas with normalization A partial solution to the above gotchas is to use normalization. Normalization is the process of transforming text into a standard form. Unicode defines four normalization forms: NFC, NFD, NFKC, and NFKD. The most common normalization form is NFC, which is the composed form. NFC is the standard form for most text processing.\nFor example, in the following Go code:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;golang.org/x/text/unicode/norm\u0026#34; \u0026#34;strconv\u0026#34; ) func main() { str1, _ := strconv.Unquote(`\u0026#34;\\uAC00\u0026#34;`) // 가 str2, _ := strconv.Unquote(`\u0026#34;\\u1100\\u1161\u0026#34;`) // ᄀ + ᅡ fmt.Println(str1) fmt.Println(str2) if str1 == str2 { fmt.Println(\u0026#34;raw equal\u0026#34;) } else { fmt.Println(\u0026#34;raw not equal\u0026#34;) } strNorm1 := norm.NFC.String(str1) strNorm2 := norm.NFC.String(str2) if strNorm1 == strNorm2 { fmt.Println(\u0026#34;normalized equal\u0026#34;) } else { fmt.Println(\u0026#34;normalized not equal\u0026#34;) } } The two strings are not equal in their raw form but equal after normalization. Normalizing before inserting, updating, and searching in the database can solve the Unicode equivalence issue while allowing the user to keep the case-insensitive sorting.\nTo solve emoji equivalence, we can use the utf8mb4_bin collation for the = comparison. However, if our column is indexed, we may need to use the utf8mb4_bin collation for the index. We cannot have a different collation for the column and the index, but we could use a second generated column with the utf8mb4_bin collation and index that column.\nConclusion Unicode and emoji support is essential for any app aiming to reach an international audience. Unicode equivalence, emoji equivalence, and case-insensitive sorting are common issues with Unicode and emoji support. Normalization can solve the Unicode equivalence issue while allowing the user to keep the case-insensitive sorting. Using the utf8mb4_bin collation for the = comparison can solve the emoji equivalence issue.\nFully supporting Unicode and emojis in your app video ","date":"2024-02-29T00:00:00Z","image":"https://victoronsoftware.com/posts/unicode-and-emoji-gotchas/unicode-emoji_hu444d31683f0bbae4748081daece648d9_456893_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/unicode-and-emoji-gotchas/","title":"Fully supporting Unicode and emojis in your app"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain mTLS Go client mTLS Go client with custom certificate signer Why use macOS keychain? In the mTLS Go client article, we built a simple Go client that uses mTLS. Our client used Go standard library methods and loaded the client certificate and private key from the filesystem. However, keeping the private key on the filesystem is insecure and not recommended. We aim to build an mTLS client fully integrated with the operating system\u0026rsquo;s keystore.\nThe macOS keychain is a secure storage system for passwords and other confidential information. It is used by many Apple applications, such as Safari, Mail, and iCloud, to store the user\u0026rsquo;s passwords and additional sensitive information.\nBuilding a custom tls.Certificate for macOS keychain This work builds on the mTLS Go client with custom certificate signer article. We will use the CustomSigner from that article to build a custom tls.Certificate that uses the macOS keychain.\nHowever, before the application uses the Public and Sign methods of the CustomSigner, we need to retrieve the certificate from the keychain using Apple\u0026rsquo;s API.\nRetrieving certificate from macOS keychain with CGO We will use CGO to call the macOS keychain API to retrieve the client certificate. To set up CGO, we include the following code above our imports:\n/* #cgo LDFLAGS: -framework CoreFoundation -framework Security #include \u0026lt;CoreFoundation/CoreFoundation.h\u0026gt; #include \u0026lt;Security/Security.h\u0026gt; */ import \u0026#34;C\u0026#34; To find the identities from the keychain, we use SecItemCopyMatching. An identity is a certificate and its associated private key.\nidentitySearch := C.CFDictionaryCreateMutable( C.kCFAllocatorDefault, maxCertificatesNum, \u0026amp;C.kCFTypeDictionaryKeyCallBacks, \u0026amp;C.kCFTypeDictionaryValueCallBacks, ) defer C.CFRelease(C.CFTypeRef(unsafe.Pointer(identitySearch))) const commonName = \u0026#34;testClientTLS\u0026#34; var commonNameCFString = stringToCFString(commonName) defer C.CFRelease(C.CFTypeRef(commonNameCFString)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecClass), unsafe.Pointer(C.kSecClassIdentity)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecAttrCanSign), unsafe.Pointer(C.kCFBooleanTrue)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecMatchSubjectWholeString), unsafe.Pointer(commonNameCFString)) // To filter by issuers, we must provide a CFDataRef array of DER-encoded ASN.1 items. // C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecMatchIssuers), unsafe.Pointer(issuerCFArray)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecReturnRef), unsafe.Pointer(C.kCFBooleanTrue)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecMatchLimit), unsafe.Pointer(C.kSecMatchLimitAll)) var identityMatches C.CFTypeRef if status := C.SecItemCopyMatching(C.CFDictionaryRef(identitySearch), \u0026amp;identityMatches); status != C.errSecSuccess { return nil, fmt.Errorf(\u0026#34;failed to find client certificate: %v\u0026#34;, status) } defer C.CFRelease(identityMatches) In our example, we find the identities by a common name, which we hardcode for demonstration purposes. We can filter by the certificate issuer, as shown in the commented-out code. Filtering by issuer requires an array of DER-encoded ASN.1 items, which can be created from the tls.CertificateRequestInfo object. Another approach to finding the proper certificate is to retrieve all the keychain certificates and filter them in Go code.\nConverting the Apple identity to a Go x509.Certificate After we retrieve the array of identities from the keychain, we convert them to Go x509.Certificate objects and pick the first one that is not expired.\nvar foundCert *x509.Certificate var foundIdentity C.SecIdentityRef identityMatchesArrayRef := C.CFArrayRef(identityMatches) numIdentities := int(C.CFArrayGetCount(identityMatchesArrayRef)) fmt.Printf(\u0026#34;Found %d identities\\n\u0026#34;, numIdentities) for i := 0; i \u0026lt; numIdentities; i++ { identityMatch := C.CFArrayGetValueAtIndex(identityMatchesArrayRef, C.CFIndex(i)) x509Cert, err := identityRefToCert(C.SecIdentityRef(identityMatch)) if err != nil { continue } // Make sure certificate is not expired if x509Cert.NotAfter.After(time.Now()) { foundCert = x509Cert foundIdentity = C.SecIdentityRef(identityMatch) fmt.Printf(\u0026#34;Found certificate from issuer %s with public key type %T\\n\u0026#34;, x509Cert.Issuer.String(), x509Cert.PublicKey) break } } The identityRefToCert function converts the SecIdentityRef to a Go x509.Certificate object. It exports the certificate to PEM format using SecItemExport and then parses the PEM to get the x509.Certificate object.\nfunc identityRefToCert(identityRef C.SecIdentityRef) (*x509.Certificate, error) { // Convert the identity to a certificate var certificateRef C.SecCertificateRef if status := C.SecIdentityCopyCertificate(identityRef, \u0026amp;certificateRef); status != 0 { return nil, fmt.Errorf(\u0026#34;failed to get certificate from identity: %v\u0026#34;, status) } defer C.CFRelease(C.CFTypeRef(certificateRef)) // Export the certificate to PEM // SecItemExport: https://developer.apple.com/documentation/security/1394828-secitemexport var pemDataRef C.CFDataRef if status := C.SecItemExport( C.CFTypeRef(certificateRef), C.kSecFormatPEMSequence, C.kSecItemPemArmour, nil, \u0026amp;pemDataRef, ); status != 0 { return nil, fmt.Errorf(\u0026#34;failed to export certificate to PEM: %v\u0026#34;, status) } defer C.CFRelease(C.CFTypeRef(pemDataRef)) certPEM := C.GoBytes(unsafe.Pointer(C.CFDataGetBytePtr(pemDataRef)), C.int(C.CFDataGetLength(pemDataRef))) var x509Cert *x509.Certificate for block, rest := pem.Decode(certPEM); block != nil; block, rest = pem.Decode(rest) { if block.Type == \u0026#34;CERTIFICATE\u0026#34; { var err error x509Cert, err = x509.ParseCertificate(block.Bytes) if err != nil { return nil, fmt.Errorf(\u0026#34;error parsing client certificate: %v\u0026#34;, err) } } } return x509Cert, nil } Retrieve the private key reference from the keychain At this point, we also retrieve the private key reference from the keychain. We will use the private key reference to sign the CertificateVerify message during the TLS handshake. The reference does not contain the private key. When importing private keys to the keychain, they should be marked as non-exportable so that no one can retrieve the private key cleartext from the keychain.\nvar privateKey C.SecKeyRef if status := C.SecIdentityCopyPrivateKey(C.SecIdentityRef(foundIdentity), \u0026amp;privateKey); status != 0 { return nil, fmt.Errorf(\u0026#34;failed to copy private key ref from identity: %v\u0026#34;, status) } Building the custom tls.Certificate Finally, we put together the custom tls.Certificate using the x509.Certificate and the private key reference.\ncustomSigner := \u0026amp;CustomSigner{ x509Cert: foundCert, privateKey: privateKey, } certificate := tls.Certificate{ Certificate: [][]byte{foundCert.Raw}, PrivateKey: customSigner, SupportedSignatureAlgorithms: []tls.SignatureScheme{supportedAlgorithm}, } Our example only supports the tls.PSSWithSHA256 signature algorithm to keep the code simple. Adding additional algorithm support is easy since it only requires passing the right parameter to the SecKeyCreateSignature function, which we will review next.\nSigning the mTLS digest with Apple\u0026rsquo;s keychain As discussed in the previous mTLS Go client with custom certificate signer article, we need to sign the CertificateVerify message during the TLS handshake. We will use the CustomSigner to sign the digest, which implements the crypto.Signer interface as defined in the Go standard library\u0026rsquo;s crypto package.\ntype CustomSigner struct { x509Cert *x509.Certificate privateKey C.SecKeyRef } func (k *CustomSigner) Public() crypto.PublicKey { fmt.Printf(\u0026#34;crypto.Signer.Public\\n\u0026#34;) return k.x509Cert.PublicKey } func (k *CustomSigner) Sign(_ io.Reader, digest []byte, opts crypto.SignerOpts) ( signature []byte, err error) { fmt.Printf(\u0026#34;crypto.Signer.Sign with key type %T, opts type %T, hash %s\\n\u0026#34;, k.Public(), opts, opts.HashFunc().String()) // Convert the digest to a CFDataRef digestCFData := C.CFDataCreate(C.kCFAllocatorDefault, (*C.UInt8)(unsafe.Pointer(\u0026amp;digest[0])), C.CFIndex(len(digest))) defer C.CFRelease(C.CFTypeRef(digestCFData)) // SecKeyAlgorithm: https://developer.apple.com/documentation/security/seckeyalgorithm // SecKeyCreateSignature: https://developer.apple.com/documentation/security/1643916-seckeycreatesignature var cfErrorRef C.CFErrorRef signCFData := C.SecKeyCreateSignature( k.privateKey, C.kSecKeyAlgorithmRSASignatureDigestPSSSHA256, C.CFDataRef(digestCFData), \u0026amp;cfErrorRef, ) if cfErrorRef != 0 { return nil, fmt.Errorf(\u0026#34;failed to sign data: %v\u0026#34;, cfErrorRef) } defer C.CFRelease(C.CFTypeRef(signCFData)) // Convert CFDataRef to Go byte slice return C.GoBytes(unsafe.Pointer(C.CFDataGetBytePtr(signCFData)), C.int(C.CFDataGetLength(signCFData))), nil } We use the SecKeyCreateSignature function to sign the digest. The function takes the private key reference, the algorithm, the digest, and a pointer to a CFErrorRef. The function returns a CFDataRef, which we convert to a Go byte slice. Additional algorithms can be supported by passing the proper parameter to the SecKeyCreateSignature function.\nPutting it all together With the above code, we can create our new Go mTLS client that uses the macOS keychain.\nfunc main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } client := http.Client{ Transport: \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ GetClientCertificate: signer.GetClientCertificate, MinVersion: tls.VersionTLS13, MaxVersion: tls.VersionTLS13, }, }, } // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } We limit the scope of this example to TLS 1.3\nBuild the mTLS client With go build client-signer.go, we generate the client-signer executable.\nSetting up the environment The next step is to use the macOS keychain to store the client certificate and private key. We will use the same certificates and keys script from the mTLS with macOS keychain article. If you still need to generate the certificates and keys, please follow the instructions in that article.\nWe must also import the generated certificates and keys into the macOS keychain.\n# Import the server CA security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain certs/server-ca.crt # Import the client CA so that client TLS certificates can be verified security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain certs/client-ca.crt # Import the client TLS certificate and key security import certs/client.crt -k /Library/Keychains/System.keychain security import certs/client.key -k /Library/Keychains/System.keychain -x -T $PWD/client-signer -T /usr/bin/curl -T /Applications/Safari.app -T \u0026#39;/Applications/Google Chrome.app\u0026#39; We specify our application $PWD/client-signer as one of the trusted applications that can access the private key. If we do not select the trusted application, we will get a security pop-up whenever our app tries to access the private key.\nFinally, as in the mTLS Hello World article, we will use docker compose up to start two nginx servers:\nhttps://localhost:8888 for TLS https://localhost:8889 for mTLS Running the Go mTLS client using the macOS keychain We can now run our mTLS client without pointing to certificate and key files. Hitting the ordinary TLS server:\n./client-signer --url https://localhost:8888/hello-world.txt Returns the expected:\nTLS Hello World! While hitting the mTLS server:\n./client-signer --url https://localhost:8889/hello-world.txt Returns a more detailed message, including the print statements in our custom code:\nServer requested certificate Found 1 identities Found certificate from issuer CN=testClientCA,OU=Your Unit,O=Your Organization,L=Austin,ST=Texas,C=US with public key type *rsa.PublicKey crypto.Signer.Public crypto.Signer.Public crypto.Signer.Sign with key type *rsa.PublicKey, opts type *rsa.PSSOptions, hash SHA-256 mTLS Hello World! Using certificate and key from the Windows certificate store The following article will explore using the Windows certificate store to hold the mTLS client certificate and private key.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-go-apple-keychain\nmTLS Go client using macOS keychain video ","date":"2024-02-22T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-go-client-using-apple-keychain/mtls-go-apple-keychain_hud963623ee8c982f67106bb07901b6549_184805_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mtls-go-client-using-apple-keychain/","title":"Mutual TLS (mTLS) Go client using macOS keychain"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain mTLS Go client Why a custom certificate signer? In the mTLS Go client article, we built a simple Go client that uses mTLS. Our client used Go standard library methods and loaded the client certificate and private key from the filesystem. However, keeping the private key on the filesystem is insecure and not recommended. We aim to build an mTLS client fully integrated with the operating system\u0026rsquo;s keystore.\nThe first step toward that goal is to extract the functionality of the mTLS handshake that requires the private key. Luckily, the client\u0026rsquo;s private key is only needed to sign the CertificateVerify message. The CertificateVerify message is the last in the mTLS handshake. It proves to the server that the client has the private key associated with the client certificate.\nFrom Wikipedia entry on TLS:\nThe client sends a CertificateVerify message, which is a signature over the previous handshake messages using the client\u0026rsquo;s certificate\u0026rsquo;s private key. This signature can be verified by using the client\u0026rsquo;s certificate\u0026rsquo;s public key. This lets the server know that the client has access to the private key of the certificate and thus owns the certificate.\nSetting up the environment We will use the same certificates and keys script from the mTLS with macOS keychain article. If you still need to generate the certificates and keys, please follow the instructions in that article.\nIn addition, we will import the generated certificates and keys into the macOS keychain. (In a future article, we will use the Windows Certificate Store instead.)\nFinally, as in the mTLS Hello World article, we will use docker compose up to start two nginx servers:\nhttps://localhost:8888 for TLS https://localhost:8889 for mTLS Building our crypto.Signer We will build a custom crypto.Signer that signs the CertificateVerify message. The crypto.Signer interface is defined in the Go standard library\u0026rsquo;s crypto package. It is used to sign messages with a private key.\n// CustomSigner is a crypto.Signer that uses the client certificate and key to sign type CustomSigner struct { x509Cert *x509.Certificate clientCertPath string clientKeyPath string } func (k *CustomSigner) Public() crypto.PublicKey { fmt.Printf(\u0026#34;crypto.Signer.Public\\n\u0026#34;) return k.x509Cert.PublicKey } func (k *CustomSigner) Sign(rand io.Reader, digest []byte, opts crypto.SignerOpts) ( signature []byte, err error) { fmt.Printf(\u0026#34;crypto.Signer.Sign\\n\u0026#34;) tlsCert, err := tls.LoadX509KeyPair(k.clientCertPath, k.clientKeyPath) if err != nil { log.Fatalf(\u0026#34;error loading client certificate: %v\u0026#34;, err) } fmt.Printf(\u0026#34;Sign using %T\\n\u0026#34;, tlsCert.PrivateKey) return tlsCert.PrivateKey.(crypto.Signer).Sign(rand, digest, opts) } Although we still use the filesystem to load the client certificate and private key, we now use the crypto.Signer interface to sign the CertificateVerify message. In the future, we will replace this code by calls to the operating system\u0026rsquo;s keystore. The vital thing to note is that we only load the private key when we need to sign the digest and do not load the key during the client configuration.\nGetting the client certificate Besides building a custom crypto.Signer, we will implement a custom GetClientCertificate function. This function will be called during the TLS handshake when the server requests a certificate from the client. The function will load the client certificate and create a CustomSigner instance. It will not load the private key at this time. Once again, the client certificate is only loaded when needed and not during the client\u0026rsquo;s configuration.\nWe set Certificate: [][]byte{cert.Raw}, because the Go implementation of the TLS handshake requires the client certificate here to validate it against the server\u0026rsquo;s CA.\nfunc GetClientCertificate(clientCertPath string, clientKeyPath string) (*tls.Certificate, error) { fmt.Printf(\u0026#34;Server requested certificate\\n\u0026#34;) if clientCertPath == \u0026#34;\u0026#34; || clientKeyPath == \u0026#34;\u0026#34; { return nil, errors.New(\u0026#34;client certificate and key are required\u0026#34;) } clientBytes, err := os.ReadFile(clientCertPath) if err != nil { return nil, fmt.Errorf(\u0026#34;error reading client certificate: %w\u0026#34;, err) } var cert *x509.Certificate for block, rest := pem.Decode(clientBytes); block != nil; block, rest = pem.Decode(rest) { if block.Type == \u0026#34;CERTIFICATE\u0026#34; { cert, err = x509.ParseCertificate(block.Bytes) if err != nil { return nil, fmt.Errorf(\u0026#34;error parsing client certificate: %v\u0026#34;, err) } } } certificate := tls.Certificate{ Certificate: [][]byte{cert.Raw}, PrivateKey: \u0026amp;CustomSigner{ x509Cert: cert, clientCertPath: clientCertPath, clientKeyPath: clientKeyPath, }, } return \u0026amp;certificate, nil } Putting it all together With the above customizations, we create our new Go mTLS client:\npackage main import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/getvictor/mtls/signer\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) clientCert := flag.String(\u0026#34;cert\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Client certificate file\u0026#34;) clientKey := flag.String(\u0026#34;key\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Client key file\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } client := http.Client{ Transport: \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ GetClientCertificate: func(info *tls.CertificateRequestInfo) ( *tls.Certificate, error) { return signer.GetClientCertificate(*clientCert, *clientKey) }, }, }, } // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } Trying to hit the mTLS server with:\ngo run client-signer.go --url https://localhost:8889/hello-world.txt --cert certs/client.crt --key certs/client.key Returns the expected result:\nmTLS Hello World! Using certificate and key from the macOS keychain In the following article, we will use the macOS keychain to load the client certificate and generate the CertificateVerify message without extracting the private key.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-go-custom-signer\nmTLS Go client with custom certificate signer video ","date":"2024-02-14T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-go-custom-signer/signer_hua0a59126f5575ffea6eb49be2c13e061_265380_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mtls-go-custom-signer/","title":"Mutual TLS (mTLS) Go client with custom certificate signer"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain What is Go? Go is a statically typed, compiled programming language designed at Google. It is known for its simplicity, efficiency, and ease of use. Go is often used for building web servers, APIs, and command-line tools. We will use Go to make a client that uses mTLS.\nSetting up the environment We will use the same certificates and keys script from the mTLS with macOS keychain article. If you still need to generate the certificates and keys, please follow the instructions in that article.\nIn addition, we will import the generated certificates and keys into the macOS keychain. (In a future article, we will use the Windows Certificate Store instead.) Keeping private keys on the filesystem is insecure and not recommended. We aim to build an mTLS client fully integrated with the operating system\u0026rsquo;s keystore.\nFinally, as in the mTLS Hello World article, we will use docker compose up to start two nginx servers:\nhttps://localhost:8888 for TLS https://localhost:8889 for mTLS Building the TLS Go client Below is a simple Go HTTP client.\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } client := http.Client{} // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } Trying the ordinary TLS server with:\ngo run client.go --url https://localhost:8888/hello-world.txt Gives the expected result:\nTLS Hello World! The Go client is integrated with the system keystore out of the box.\nHowever, when trying the mTLS server with the following:\ngo run client.go --url https://localhost:8889/hello-world.txt We get the error:\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;400 No required SSL certificate was sent\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;400 Bad Request\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;center\u0026gt;No required SSL certificate was sent\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx/1.25.3\u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; The Go libraries are not integrated with the system keystore for using the mTLS client certificate and key.\nModifying the Go client for mTLS We will use the crypto/tls package to build the mTLS client.\npackage main import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) clientCert := flag.String(\u0026#34;cert\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Client certificate file\u0026#34;) clientKey := flag.String(\u0026#34;key\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Client key file\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } var certificate tls.Certificate if *clientCert != \u0026#34;\u0026#34; \u0026amp;\u0026amp; *clientKey != \u0026#34;\u0026#34; { var err error certificate, err = tls.LoadX509KeyPair(*clientCert, *clientKey) if err != nil { log.Fatalf(\u0026#34;error loading client certificate: %v\u0026#34;, err) } } client := http.Client{ Transport: \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ Certificates: []tls.Certificate{certificate}, }, }, } // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } Now, trying the mTLS server with:\ngo run client-mtls.go --url https://localhost:8889/hello-world.txt --cert certs/client.crt --key certs/client.key Returns the expected result:\nmTLS Hello World! However, we pass the client certificate and key as command-line arguments. In a real-world scenario, we want to use the system keystore to manage the client certificate and key.\nUsing a custom signer for the mTLS client certificate The following article will cover creating a custom Go signer for the mTLS client certificate. This work will pave the way for us to use the system keystore to manage the client certificate and key.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-with-go\nmTLS Go client video ","date":"2024-02-07T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-go-client/go-client_hufc23aaa179917ecd1cfc752f8bc6da43_88506_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mtls-go-client/","title":"Mutual TLS (mTLS) Go client"},{"content":"This article is part of a series on mTLS. Check out the previous article: mTLS Hello World.\nSecuring mTLS certificates and keys In the mTLS Hello World article, we generated mTLS certificates and keys for the client and the server. We also created two certificate authorities (CAs) and signed the client and server certificates with their respective CAs. We ended up with the following files:\nserver CA: certs/server-ca.crt server CA private key: certs/server-ca.key TLS certificate for localhost server: certs/server.crt server TLS certificate private key: certs/server.key client CA: certs/client-ca.crt client CA private key: certs/client-ca.key TLS certificate for client: certs/client.crt client TLS certificate private key: certs/client.key In a real-world scenario, we would need to secure these files. The server CA private key and the client CA private key are the most important files to secure. If an attacker gets access to these files, they can create new certificates and impersonate the server or the client. These two files should be secured in a dedicated secure storage.\nThe server will need access to the client CA, the server TLS certificate, and the server TLS certificate private key. The server TLS certificate private key is the most important to secure out of these three files.\nThe client will need access to the server CA, the client TLS certificate, and the client TLS certificate private key. We can use the macOS keychain to secure these files. In a future article, we will show how to secure these on Windows with certificate stores.\nApple\u0026rsquo;s macOS keychain As I\u0026rsquo;ve written in inspecting keychain files on macOS, keychains are the macOS\u0026rsquo;s method to track and protect secure information such as passwords, private keys, and certificates.\nThe system keychain is located at /Library/Keychains/System.keychain. It contains the root certificates and other certificates. The login keychain is located at /Users/\u0026lt;username\u0026gt;/Library/Keychains/login.keychain-db. It contains the user\u0026rsquo;s certificates and private keys. In this example, we will use the system keychain, which all users on the system can access.\nGenerating mTLS certificates and keys We will use the following script to generate the mTLS certificates and keys. It resembles the script from the mTLS Hello World article.\n#!/bin/bash # This script generates certificates and keys needed for mTLS. mkdir -p certs # Private keys for CAs openssl genrsa -out certs/server-ca.key 2048 openssl genrsa -out certs/client-ca.key 2048 # Generate CA certificates openssl req -new -x509 -nodes -days 1000 -key certs/server-ca.key -out certs/server-ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testServerCA\u0026#34; openssl req -new -x509 -nodes -days 1000 -key certs/client-ca.key -out certs/client-ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testClientCA\u0026#34; # Generate a certificate signing request openssl req -newkey rsa:2048 -nodes -keyout certs/server.key -out certs/server.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testServerTLS\u0026#34; openssl req -newkey rsa:2048 -nodes -keyout certs/client.key -out certs/client.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testClientTLS\u0026#34; # Have the CA sign the certificate requests and output the certificates. openssl x509 -req -in certs/server.req -days 398 -CA certs/server-ca.crt -CAkey certs/server-ca.key -set_serial 01 -out certs/server.crt -extfile localhost.ext openssl x509 -req -in certs/client.req -days 398 -CA certs/client-ca.crt -CAkey certs/client-ca.key -set_serial 01 -out certs/client.crt # Clean up rm certs/server.req rm certs/client.req The maximum validity period for a TLS certificate is 398 days. Apple will reject certificates with a more extended validity period.\nImporting client mTLS certificates and keys into the macOS keychain We will import the client mTLS certificates and keys into the macOS keychain using the following script. The script uses the security command line tool. Accessing the system keychain must be run as root (sudo).\n#!/bin/bash # This script imports mTLS certificates and keys into the Apple Keychain. # Import the server CA security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain certs/server-ca.crt # Import the client CA so that client TLS certificates can be verified security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain certs/client-ca.crt # Import the client TLS certificate and key security import certs/client.crt -k /Library/Keychains/System.keychain security import certs/client.key -k /Library/Keychains/System.keychain -x -T /usr/bin/curl -T /Applications/Safari.app -T \u0026#39;/Applications/Google Chrome.app\u0026#39; The -x option marks the imported key as non-extractable. No application or user can view the private key once it is imported. The private key can only be used indirectly via Apple\u0026rsquo;s APIs.\nThe -T option specifies the applications that can access the key. Additional applications may be added later to the access control list.\nVerifying imported certificates and keys As an extra step, we can verify the client and server certificates before using them in an application.\nWe can verify the server certificate by running the following command:\nsecurity verify-cert -c certs/server.crt -p ssl -s localhost -k /Library/Keychains/System.keychain The output should include:\n...certificate verification successful. The Apple keychain automatically combines the certificate and the private key into an identity. We can verify the client identity by running the following command:\nsecurity find-identity -p ssl-client /Library/Keychains/System.keychain The list of identities should include:\nPolicy: SSL (client) Matching identities 1) B307B90CCD374080E74F1B15AF602B35A75D8401 \u0026#34;testClientTLS\u0026#34; 1 identities found Valid identities only 1) B307B90CCD374080E74F1B15AF602B35A75D8401 \u0026#34;testClientTLS\u0026#34; 1 valid identities found macOS can validate the identity because we also imported the client CA into the system keychain.\nRunning the mTLS server As in the mTLS Hello World article, we will use docker compose up to start two nginx servers:\nhttps://localhost:8888 for TLS https://localhost:8889 for mTLS Connecting to the TLS and mTLS servers with clients Because the server CA was added to the system keychain, curl can now access the TLS server without any additional flags:\ncurl https://localhost:8888/hello-world.txt However, the built-in curl client cannot access the mTLS server. We use the -v option for additional information:\ncurl -v https://localhost:8889/hello-world.txt The output:\n* Trying [::1]:8889... * Connected to localhost (::1) port 8889 * ALPN: curl offers h2,http/1.1 * (304) (OUT), TLS handshake, Client hello (1): * CAfile: /etc/ssl/cert.pem * CApath: none * (304) (IN), TLS handshake, Server hello (2): * (304) (IN), TLS handshake, Unknown (8): * (304) (IN), TLS handshake, Request CERT (13): * (304) (IN), TLS handshake, Certificate (11): * (304) (IN), TLS handshake, CERT verify (15): * (304) (IN), TLS handshake, Finished (20): * (304) (OUT), TLS handshake, Certificate (11): * (304) (OUT), TLS handshake, Finished (20): * SSL connection using TLSv1.3 / AEAD-CHACHA20-POLY1305-SHA256 * ALPN: server accepted http/1.1 * Server certificate: * subject: C=US; ST=Texas; L=Austin; O=Your Organization; OU=Your Unit; CN=testServerTLS * start date: Jan 28 17:08:10 2024 GMT * expire date: Mar 1 17:08:10 2025 GMT * subjectAltName: host \u0026#34;localhost\u0026#34; matched cert\u0026#39;s \u0026#34;localhost\u0026#34; * issuer: C=US; ST=Texas; L=Austin; O=Your Organization; OU=Your Unit; CN=testServerCA * SSL certificate verify ok. * using HTTP/1.1 \u0026gt; GET /hello-world.txt HTTP/1.1 \u0026gt; Host: localhost:8889 \u0026gt; User-Agent: curl/8.4.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 400 Bad Request \u0026lt; Server: nginx/1.25.3 \u0026lt; Date: Sun, 28 Jan 2024 18:28:20 GMT \u0026lt; Content-Type: text/html \u0026lt; Content-Length: 237 \u0026lt; Connection: close \u0026lt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;400 No required SSL certificate was sent\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;400 Bad Request\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;center\u0026gt;No required SSL certificate was sent\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx/1.25.3\u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; * Closing connection The client attempted the TLS handshake, but the server rejected the connection because the client did not provide a certificate. Our built-in curl client does not currently support mTLS using the macOS keychain. The client used for this example is:\ncurl 8.4.0 (x86_64-apple-darwin23.0) libcurl/8.4.0 (SecureTransport) LibreSSL/3.3.6 zlib/1.2.12 nghttp2/1.55.1 Release-Date: 2023-10-11 Protocols: dict file ftp ftps gopher gophers http https imap imaps ldap ldaps mqtt pop3 pop3s rtsp smb smbs smtp smtps telnet tftp Features: alt-svc AsynchDNS GSS-API HSTS HTTP2 HTTPS-proxy IPv6 Kerberos Largefile libz MultiSSL NTLM NTLM_WB SPNEGO SSL threadsafe UnixSockets On the other hand, Safari can access the mTLS server. We can verify this by opening the following URL in Safari:\nhttps://localhost:8889/hello-world.txt We see the following popup:\nSafari mTLS popup\nWe can click Continue to connect to the mTLS server. Future connections will not show the popup and will automatically use the client certificate.\nGoogle Chrome\u0026rsquo;s behavior is similar.\nNote: If we did not add Safari as an application that can access the client key, Safari would ask for a username and password to connect to the system keychain.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-with-apple-keychain\nCreating our own mTLS client In the following article, we will create our own mTLS client with the Go programming language.\nmTLS with macOS keychain video ","date":"2024-01-31T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-with-apple-keychain/mtls-safari_hued6427f760192911581582be7803ed21_87861_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mtls-with-apple-keychain/","title":"Mutual TLS (mTLS) with macOS keychain"},{"content":"What is mTLS (mutual TLS)? TLS stands for Transport Layer Security. It is a cryptographic protocol that provides privacy and data integrity between two communicating applications. It is the successor to SSL (Secure Sockets Layer).\nIn ordinary (non-mutual) TLS, the client authenticates the server, but the server does not authenticate the client. Most websites use regular TLS. The client (web browser) knows it is talking to the correct server (website), but the server knows very little about the client. Instead, web applications use other client authentication methods, such as passwords, cookies, and session tokens.\nMutual TLS (mTLS) is a way to authenticate both the client and the server in a TLS connection. It is also known as client certificate authentication. In addition to the server authenticating itself to the client, the client also authenticates itself to the server.\nmTLS is helpful as an additional layer of security. It is used in many applications, including:\nVPNs Microservices Service mesh IoT (Internet of Things) Mobile apps How does Fleet Device Management use mTLS? Many of Fleet\u0026rsquo;s customers use mTLS as an additional layer of security to authenticate the Fleet server to the Fleet agent. The Fleet agent is a small program that runs on each host device, such as a corporate laptop. It collects information about the host and sends it to the Fleet server.\nHow does mTLS work? TLS is a complex protocol with multiple versions (1.2, 1.3, etc.). We will only go over the basics to understand how mTLS works.\nTLS uses a handshake protocol to establish a secure connection. The handshake protocol is a series of messages between the client and the server.\nThe client sends a \u0026ldquo;Client Hello\u0026rdquo; message to the server. The server responds with a \u0026ldquo;Server Hello\u0026rdquo; message and sends its certificate to the client. As an additional step for mTLS, the server requests a certificate from the client.\nThe client verifies the server\u0026rsquo;s certificate by checking the certificate\u0026rsquo;s signature and verifying that the certificate is valid and has not expired. The client also checks that the server\u0026rsquo;s hostname matches the hostname in the certificate.\nThe client uses the server\u0026rsquo;s public key to encrypt the messages sent to the server, including the session key and its certificate. The server decrypts these messages with its private key.\nThe client also sends a digital signature, encrypted with its private key, to the server. The server verifies the signature by decrypting it with the client\u0026rsquo;s public key.\nAt this point, both the client and the server have verified each other\u0026rsquo;s identity. They complete the TLS handshake and can exchange encrypted messages using a symmetric session key.\nGenerate certificates and keys We will use the OpenSSL command line tool to generate the certificates. OpenSSL is a popular open-source library for TLS and SSL protocols.\nThe following script generates the certificates and keys for the client and the server. It also creates two certificate authorities (CAs) and signs the client and server certificates with their respective CA. The same CA may sign the certificates, but we will use separate CAs for this example.\n#!/bin/bash # This script generates files needed for mTLS. mkdir -p certs # Private keys for CAs openssl genrsa -out certs/server-ca.key 2048 openssl genrsa -out certs/client-ca.key 2048 # Generate CA certificates openssl req -new -x509 -nodes -days 1000 -key certs/server-ca.key -out certs/server-ca.crt openssl req -new -x509 -nodes -days 1000 -key certs/client-ca.key -out certs/client-ca.crt # Generate a certificate signing request openssl req -newkey rsa:2048 -nodes -keyout certs/server.key -out certs/server.req openssl req -newkey rsa:2048 -nodes -keyout certs/client.key -out certs/client.req # Have the CA sign the certificate requests and output the certificates. openssl x509 -req -in certs/server.req -days 1000 -CA certs/server-ca.crt -CAkey certs/server-ca.key -set_serial 01 -out certs/server.crt -extfile localhost.ext openssl x509 -req -in certs/client.req -days 1000 -CA certs/client-ca.crt -CAkey certs/client-ca.key -set_serial 01 -out certs/client.crt # Clean up rm certs/server.req rm certs/client.req The localhost.ext file is used to specify the hostname for the server certificate. In our example, we will use localhost. The file contains the following:\nauthorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment subjectAltName = @alt_names [alt_names] DNS.1 = localhost Run the mTLS server We will use nginx as our mTLS server. nginx is a popular open-source web server.\nUsing docker compose, we can run two nginx servers. One server will use ordinary TLS, and one will use mutual TLS. We will use the following docker-compose.yml file:\n--- version: \u0026#34;2\u0026#34; services: nginx-tls: image: nginx volumes: - ./certs/server.crt:/etc/nginx/certificates/server.crt - ./certs/server.key:/etc/nginx/certificates/server.key - ./nginx-tls/nginx.conf:/etc/nginx/conf.d/default.conf - ./nginx-tls/hello-world.txt:/www/data/hello-world.txt ports: - \u0026#34;8888:8888\u0026#34; nginx-mtls: image: nginx volumes: - ./certs/server.crt:/etc/nginx/certificates/server.crt - ./certs/server.key:/etc/nginx/certificates/server.key - ./certs/client-ca.crt:/etc/nginx/certificates/client-ca.crt - ./nginx-mtls/nginx.conf:/etc/nginx/conf.d/default.conf - ./nginx-mtls/hello-world.txt:/www/data/hello-world.txt ports: - \u0026#34;8889:8889\u0026#34; The nginx-tls service uses the nginx-tls/nginx.conf file, which contains the following:\nserver { listen 8888 ssl; server_name tls-hello-world; # Server TLS certificate (client must have the CA cert to connect) ssl_certificate /etc/nginx/certificates/server.crt; ssl_certificate_key /etc/nginx/certificates/server.key; location / { root /www/data; } } The nginx-mtls service uses the nginx-mtls/nginx.conf file, which contains the following:\nserver { listen 8889 ssl; server_name mtls-hello-world; # Server TLS certificate (client must have the CA cert to connect) ssl_certificate /etc/nginx/certificates/server.crt; ssl_certificate_key /etc/nginx/certificates/server.key; # Enable mTLS ssl_client_certificate /etc/nginx/certificates/client-ca.crt; ssl_verify_client on; location / { root /www/data; } } The hello-world.txt files contain a simple text message.\nConnect to the mTLS server with curl client We can connect to the mTLS server with the curl command line tool. We will use the following command:\ncurl https://localhost:8889/hello-world.txt --cacert ./certs/server-ca.crt --cert ./certs/client.crt --key ./certs/client.key The --cacert option specifies the CA certificate that signed the server certificate. The --cert and --key options select the client certificate and key.\nTo connect to the ordinary TLS server, we do not need to specify the client certificate and key:\ncurl https://localhost:8888/hello-world.txt --cacert ./certs/server-ca.crt Curl can use --insecure to ignore the server certificate:\ncurl --insecure https://localhost:8888/hello-world.txt However, it is impossible to ignore the client certificate for mTLS. The server will reject the connection if the client does not provide a valid certificate.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/hello-world\nSecuring mTLS certificates and keys In the next article, we will secure the mTLS certificates and keys with the system keystore.\nmTLS Hello World video ","date":"2024-01-24T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-hello-world/mtls-handshake_hu50cf7289da7ab96edbbfa70ffae00656_51351_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/mtls-hello-world/","title":"Mutual TLS intro and hands-on example"},{"content":"Simple CGO examples CGO is a way to call C code from Go. It helps call existing C libraries or for performance reasons. CGO is enabled by default but can be disabled with the -cgo build flag.\nBelow is a simple example of calling a C function from Go.\npackage main /* double add(double a, double b) { return a + b; } */ import \u0026#34;C\u0026#34; import \u0026#34;fmt\u0026#34; func main() { fmt.Println(C.add(1, 2)) } The C code is embedded in the Go code as a comment above import \u0026quot;C\u0026quot;. The comment must start with /* and end with */. The C code must be valid. The Go compiler compiles the C code and links the resulting object file with the Go code.\nHere is an example of using an existing C library.\npackage main /* #include \u0026#34;math.h\u0026#34; double add(double a, double b) { return a + b; } */ import \u0026#34;C\u0026#34; import \u0026#34;fmt\u0026#34; func main() { fmt.Println(C.floor(C.add(1, 2.1))) } We call the floor function from the math.h library. The math.h library is included with the C compiler, so we don\u0026rsquo;t need to do anything special to use it.\nCGO Hello World fail Here is another example where we print \u0026ldquo;Hello World\u0026rdquo; from C.\npackage main /* #include \u0026#34;stdio.h\u0026#34; */ import \u0026#34;C\u0026#34; func main() { C.printf(C.CString(\u0026#34;Hello World\\n\u0026#34;)) } However, the above seemingly straightforward example will fail to compile with the following enigmatic error:\ncgo: ./exmaple.go:9:2: unexpected type: ... The problem is that printf is a variadic function that can take a variable number of arguments. CGO does not support variadic functions. Even using Go variadic syntax will not work:\nargs := []interface{}{} C.printf(C.CString(\u0026#34;Hello World\\n\u0026#34;), args...) The workaround for this is to use another non-variadic function, such as vprintf, or to wrap the variadic C function in a non-variadic C function.\npackage main /* #include \u0026#34;stdio.h\u0026#34; void wrapPrintf(const char *s) { printf(\u0026#34;%s\u0026#34;, s); } */ import \u0026#34;C\u0026#34; func main() { C.wrapPrintf(C.CString(\u0026#34;Hello, World\\n\u0026#34;)) } C++ Hello World fail Another issue with CGO is only C code can be called from Go. C++ code cannot be called from Go. The following code will fail to compile:\npackage main /* #include \u0026lt;iostream\u0026gt; void helloWorld() { std::cout \u0026lt;\u0026lt; \u0026#34;Hello, World\u0026#34; \u0026lt;\u0026lt; std::endl; } */ import \u0026#34;C\u0026#34; func main() { C.helloWorld() } However, C++ code can be called from C, so we can write a C wrapper for the C++ code.\nCGO real-world example The following is an example of real-world usage of CGO, which uses Apple\u0026rsquo;s APIs to add a secret to the keychain.\npackage keystore /* #cgo LDFLAGS: -framework CoreFoundation -framework Security #include \u0026lt;CoreFoundation/CoreFoundation.h\u0026gt; #include \u0026lt;Security/Security.h\u0026gt; */ import \u0026#34;C\u0026#34; import ( \u0026#34;fmt\u0026#34; \u0026#34;unsafe\u0026#34; ) const service = \u0026#34;com.fleetdm.fleetd.enroll.secret\u0026#34; var serviceStringRef = stringToCFString(service) // AddSecret will add a secret to the keychain. This application can retrieve this // secret without any user authorization. func AddSecret(secret string) error { query := C.CFDictionaryCreateMutable( C.kCFAllocatorDefault, 0, \u0026amp;C.kCFTypeDictionaryKeyCallBacks, \u0026amp;C.kCFTypeDictionaryValueCallBacks, ) defer C.CFRelease(C.CFTypeRef(query)) data := C.CFDataCreate(C.kCFAllocatorDefault, (*C.UInt8)(unsafe.Pointer(C.CString(secret))), C.CFIndex(len(secret))) defer C.CFRelease(C.CFTypeRef(data)) C.CFDictionaryAddValue(query, unsafe.Pointer(C.kSecClass), unsafe.Pointer(C.kSecClassGenericPassword)) C.CFDictionaryAddValue(query, unsafe.Pointer(C.kSecAttrService), unsafe.Pointer(serviceStringRef)) C.CFDictionaryAddValue(query, unsafe.Pointer(C.kSecValueData), unsafe.Pointer(data)) status := C.SecItemAdd(C.CFDictionaryRef(query), nil) if status != C.errSecSuccess { return fmt.Errorf(\u0026#34;failed to add %v to keychain: %v\u0026#34;, service, status) } return nil } // stringToCFString will return a CFStringRef func stringToCFString(s string) C.CFStringRef { bytes := []byte(s) ptr := (*C.UInt8)(\u0026amp;bytes[0]) return C.CFStringCreateWithBytes(C.kCFAllocatorDefault, ptr, C.CFIndex(len(bytes)), C.kCFStringEncodingUTF8, C.false) } The C linker flags are specified with the #cgo LDFLAGS directive.\nThe CGO code uses a lot of casting and data conversion. Let\u0026rsquo;s break down the following segment:\n(*C.UInt8)(unsafe.Pointer(C.CString(secret))) C.CString converts a Go string to a C string. It is one of the CGO special functions to convert between Go and C types. See cgo documentation for more information.\nunsafe.Pointer converts a C pointer to a generic Go pointer. And (*C.UInt8) casts the Go pointer back to a C pointer.\nUnfortunately, CGO cannot cast a C string to a (*C.UInt8) directly. The following will fail to compile:\n(*C.UInt8)(C.CString(secret)) We must go through an intermediate cast to unsafe.Pointer, representing a void C pointer.\nAdditional topics Our custom C and Go code was always in the same file in the above examples. However, the C code can be in a separate file and linked to our Go executable.\nCGO Hello World fail video ","date":"2024-01-18T00:00:00Z","image":"https://victoronsoftware.com/posts/using-c-and-go-with-cgo-is-tricky/cgo-hello-world-fail_hudd175c840e3984dcc2f0a6c6496dc1b9_172150_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/using-c-and-go-with-cgo-is-tricky/","title":"Using C and Go with CGO is tricky"},{"content":" What are GitHub actions? GitHub actions are a way to automate your software development workflows. They are similar to CI/CD tools like Jenkins, CircleCI, and TravisCI. However, GitHub actions are built into GitHub.\nGitHub actions are not entirely free, but they have very high usage limits for open-source projects. For private repositories, you can run up to 2,000 minutes per month for free. After that, you will be charged.\nGitHub actions for non-CI/CD tasks However, GitHub actions are not just for CI/CD. You can use them for many general-purpose tasks. For example, you can use them as an extension of your application to perform tasks such as:\ngenerating aggregate reports updating a database sending notifications general data processing and many others A GitHub action can run arbitrary code, taking inputs from multiple sources such as API calls, databases, and files.\nYou can use a GitHub action as a worker for your application. For example, you can use it to process data from a database and then send a notification to a user. Or you can use it to generate a report and upload it to a file server.\nAlthough GitHub actions in open-source repositories are public, they can still use secrets that are not accessible to the public. For example, secrets can be API keys and database access credentials.\nA real-world GitHub action doing data processing Below is an example GitHub action that does general data processing. It uses API calls to download data from NVD (National Vulnerability Database), generates files from this data, and then creates a release. Subsequently, the application can download these files and use them directly without making the API calls or processing the data itself.\nGitHub gist: The GitHub action does a checkout of our application code and runs a script cmd/cve/generate.go to generate the files. Then, it publishes the generated files as a new release. As a final step, it deletes any old releases.\nA note of caution. GitHub monitors for cryptocurrency mining and other abusive behavior. So, keep that in mind and be careful with process-intensive actions.\n","date":"2024-01-11T00:00:00Z","image":"https://victoronsoftware.com/posts/use-github-actions-for-general-purpose-tasks/GitHub-action_hu170254bdc12852a8970245ca1a2f169c_44917_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/use-github-actions-for-general-purpose-tasks/","title":"Use GitHub actions for general-purpose tasks"},{"content":" Fuzz testing is a software automated testing technique where random inputs are provided to the software under test. My background is in hardware verification, which uses sophisticated methodologies for pseudorandom testing, so I wanted to see what the Go library had to offer out of the box.\nA Go fuzz test can run as:\na normal unit test a test with fuzzing A fuzz test is written similarly to a normal unit test in a *_test.go file, with the following changes. It must have a Fuzz prefix and use the testing.F struct instead of the usual testing.T struct.\nfunc FuzzSample(f *testing.F) { Here is a workflow for using fuzz testing. First, you create a fuzz test. Then, you run it with fuzzing to automatically find failing corner cases and make any fixes. Thirdly, you include the test and the corner cases in your continuous integration testing suite.\nCreate a fuzz test When creating a fuzz test, you should provide a corpus of initial seed inputs. These are the inputs the test will use before applying randomization. Add the seed corpus with the Add method. For example:\nf.Add(tc.Num, tc.Name) f.Add(uint8(0), \u0026#34;\u0026#34;) The inputs to the Add method indicate which types will be fuzzed, and these types must match the subsequent call to the Fuzz method:\nf.Fuzz(func(t *testing.T, num uint8, name string) { The fuzz test can randomize any number of inputs, as long as they are one of the supported types.\nRun the test with fuzzing To run the test with fuzzing, use the -fuzz switch, like:\ngo test -fuzz FuzzSample The test will continuously run on all your CPUs until it fails, or you kill it:\n=== RUN FuzzSample fuzz: elapsed: 0s, gathering baseline coverage: 0/11 completed fuzz: elapsed: 0s, gathering baseline coverage: 11/11 completed, now fuzzing with 12 workers fuzz: elapsed: 3s, execs: 432199 (144036/sec), new interesting: 0 (total: 11) fuzz: elapsed: 6s, execs: 871147 (146328/sec), new interesting: 0 (total: 11) A sample failure:\nfailure while testing seed corpus entry: FuzzSample/49232526a5eabbdc fuzz: elapsed: 1s, gathering baseline coverage: 10/11 completed --- FAIL: FuzzSample (1.03s) --- FAIL: FuzzSample (0.00s) fuzz_test.go:21: Found 0 The failures are automatically added to the seed corpus. The seed corpus includes the initial inputs that were added with the Add method as well as any new fails. These new seed corpus files are automatically created in the testdata/fuzz/Fuzz* directory. Sample contents of one such file:\ngo test fuzz v1 byte(\u0026#39;\\x01\u0026#39;) string(\u0026#34;0a0000\u0026#34;) Adding the failure to the seed corpus means that the failing case will always run when this test is run again as a unit test or with fuzzing.\nNow, you must fix the failing test and continue the loop of fuzzing and fixing.\nInclude the test in continuous integration When checking in the test to your repository, you must either include the testdata/fuzz/Fuzz* files or convert those files into individual Add method calls in your test. Once the test is checked in, all the inputs in the seed corpus will run as part of the standard Go unit test flow.\nInitial impressions Fuzz testing appears to be a good approach to help the development of small functions with limited scope. The library documentation mentions the following about the function under test:\nThis function should be fast and deterministic, and its behavior should not depend on shared state.\nI plan to give fuzzing a try the next time I develop such a function. I will share the results on this blog.\nConcerns and Issues Native fuzzing support was added to Go in 1.18 and seems like a good initial approach. However, it feels limited in features and usability. The types of functions, fast and deterministic, that fuzzing is intended for are generally not very interesting when testing real applications. They are good examples for students learning how to code. However, more interesting testing scenarios include:\nFunctions accessing remote resources in parallel, such as APIs or databases Functions with asynchronous code Secondly, the fuzzing library does not provide a good way to guide the randomization of inputs and does not give feedback about the input state space already covered. It does provide line coverage information, but that doesn\u0026rsquo;t help for unknown corner cases.\nIf one of my inputs is intended to be a percentage, then I want most of the fuzzing to concentrate on the legal range of 0-100, as opposed to all numbers. This lack of constraints becomes a problem when adding additional inputs to the fuzzing function, as the available state space of inputs expands exponentially. If the state space of inputs is huge, there is no guarantee that fuzzing accomplished its goal of finding all corner cases, leaving the developer with a false sense of confidence in their code.\nLastly, the fuzz test is hard to maintain. The seed corpus is stored in files without any context regarding what corner case each seed is hitting. Software engineers unfamiliar with fuzz testing will find this extremely confusing. If the fuzz test needs to be extended in the future with additional inputs or different types, the old seed corpus will become useless. It will be worse than useless \u0026ndash; the test will not run, and the developer unfamiliar with fuzz testing will not have a clear idea why.\nfuzz_test.go:16: wrong number of values in corpus entry: 2, want 3 That said, understanding the fuzz testing limitation, I’m willing to try fuzz testing for more interesting test cases, such as database accesses. I will report my findings in a future post.\nGitHub gist: ","date":"2024-01-04T00:00:00Z","image":"https://victoronsoftware.com/posts/fuzz-testing-with-go/fuzz_hu3b462ca6a7ae1c67a10dfe9835433d85_465404_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/fuzz-testing-with-go/","title":"Fuzz testing in Go"},{"content":" In the ever-evolving landscape of device management and cybersecurity, understanding the mechanics behind tools like Fleet is not just about technical curiosity; it\u0026rsquo;s about empowering IT professionals to safeguard digital assets more effectively. Fleet gathers telemetry from various devices, from laptops to virtual machines, using osquery. At the heart of this system lies a crucial feature: Fleet policies.\nPolicies in Fleet are more than just rules; they are the gatekeepers of your device\u0026rsquo;s security, ensuring stringent adherence to security standards. By dissecting how Fleet policies operate \u0026ldquo;under the hood,\u0026rdquo; IT administrators and security professionals can gain invaluable insights. These insights allow for setting up efficient security protocols and rapid response to potential vulnerabilities, a necessity in a landscape where cyber threats are constantly evolving. This article delves into the inner workings of Fleet policies, providing you with the knowledge to better configure, manage, and leverage these policies for optimal device security and efficiency.\nPolicy creation Policies can be created from the web UI, the command-line interface called fleetctl with config files, or the REST API. The user creates a policy and selects which devices need to be checked using that policy. Policies can be global or team-specific.\nWhen a policy is created, a record for it is stored in the policies table of the MySQL database. A Fleet deployment consists of several servers behind a load balancer, so storing the record in the DB makes all servers aware of the new policy.\nPolicy execution Policies are executed on the devices, which are called hosts in Fleet, according to the FLEET_OSQUERY_POLICY_UPDATE_INTERVAL, which is set to 1 hour by default. This interval can be adjusted with the environment variable or set from the server’s command line.\nPolicies are simply SQL queries that return a true or false result, so the flow they use on the hosts is the same as other queries. Hosts check in with Fleet servers every 10 seconds (the default) and access the /api/v1/osquery/distributed/read API endpoint. The server checks when the policy was last executed to determine whether it should be executed again. If so, the server adds the policy to its response. For example, this policy in the server response checks if the macOS firewall is enabled:\n{ \u0026#34;queries\u0026#34;: { \u0026#34;fleet_policy_query_9\u0026#34;: \u0026#34;SELECT 1 FROM alf WHERE global_state \u0026gt;= 1;\u0026#34; }, \u0026#34;discovery\u0026#34;: { \u0026#34;fleet_policy_query_9\u0026#34;: \u0026#34;SELECT 1\u0026#34; } } Once the host has executed the policy, it writes the result to the server. The server updates the result in the policy_membership table of the MySQL database. At this point, the Host Details page on the web UI is updated with the policy result.\nForce policy execution on a device The user can force the host to execute all of its policies by clicking the Refetch link:\nPolicy results aggregation However, the main Policies page is not updated. This page shows the counts of all passing and failing hosts for each policy. A worker process on one of the Fleet servers updates it once an hour. The worker calculates the counts and stores them in the policy_stats table in the database. This is done for better performance of the UI. For customers with 100,000s of hosts that asynchronously report their policy results, calculating the passing and failing counts in real time was noticeably slow.\nSummary Understanding the intricacies of Fleet policies is essential for IT professionals managing a fleet of devices. This deep dive into the mechanics of Fleet policies — from creation to execution — provides you with the necessary insights to optimize your cybersecurity strategy effectively. By leveraging these policies, you can ensure stringent security standards across your network, enhancing your organization\u0026rsquo;s digital defense. As the cyber landscape evolves, tools like Fleet remain crucial in maintaining robust and responsive security protocols. We encourage you to apply these insights in your Fleet usage, and as always, we welcome your feedback and experiences in the Fleet community Slack channels.\nThis article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2023-12-30T00:00:00Z","image":"https://victoronsoftware.com/posts/understanding-the-intricacies-of-fleet-policies/_hu5aa3a7c771a8aa7077c6e8ce7a0c141c_322631_c7fb8a890b81808a2154c4746d61955b.png","permalink":"https://victoronsoftware.com/posts/understanding-the-intricacies-of-fleet-policies/","title":"Understanding the intricacies of Fleet policies"},{"content":" Fleet is an open-source platform for managing and gathering telemetry from devices such as laptops, desktops, VMs, etc. Osquery agents run on these devices and report to the Fleet server. One of Fleet’s features is the ability to query information from the devices in near real-time, called live queries. This article discusses how live queries work “under the hood.”\nWhy a live query? Live queries enable administrators to ask near real-time questions of all online devices, such as checking the encryption status of SSH keys across endpoints, or obtaining the uptime of each server within their purview. This enables them to promptly identify and address any issues, thereby reducing downtime and maintaining operational efficiency. These tasks, which would be time-consuming and complex if done manually, are streamlined through live queries, offering real-time insights into the status and posture of the entire fleet of devices helping IT and security.\nLive queries under the hood Live queries can be run from the web UI, the command-line interface called fleetctl, or the REST API. The user creates a query and selects which devices will run that query. Here is an example using fleetctl to obtain the operating system name and version for all devices:\nfleetctl query --query \u0026#34;select name, version from os_version;\u0026#34; --labels \u0026#34;All Hosts\u0026#34; When a client initiates a live query, the server first creates a Query Campaign record in the MySQL database. A Fleet deployment consists of several servers behind a load balancer, so storing the record in the DB makes all servers aware of the new query campaign.\nQuery campaign As devices called Hosts in Fleet check in with the servers, they receive instructions to run a query. For example:\n{ \u0026#34;queries\u0026#34;: { \u0026#34;fleet_distributed_query_140\u0026#34;: \u0026#34;SELECT name, version FROM os_version;\u0026#34; }, \u0026#34;discovery\u0026#34;: { \u0026#34;fleet_distributed_query_140\u0026#34;: \u0026#34;SELECT 1\u0026#34; } } Then, the osquery agents run the actual query on their host, and write the result back to a Fleet server. As a server receives the result, it publishes it to the common cache using Redis Pub/Sub.\nOnly the one server communicating with the client subscribes to the results. It processes the data from the cache, keeps track of how many hosts reported back, and communicates results back to the client. The web UI and fleetctl interfaces use a WebSockets API, and results are reported as they come in. The REST API, on the other hand, only sends a response after all online hosts have reported their query results.\nDiscover more Fleet’s live query feature represents a powerful tool in the arsenal of IT and security administrators. By harnessing the capabilities of live queries, tasks that once required extensive manual effort can now be executed swiftly and efficiently. This real-time querying ability enhances operational efficiency and significantly bolsters security and compliance measures across a range of devices.\nThe integration of Fleet with Osquery agents, the flexibility offered by interfaces like the web UI, fleetctl, and the REST API, and the efficient data handling through mechanisms like Redis Pub/Sub and WebSockets API all come together to create a robust, real-time telemetry gathering system. This system is designed to keep you informed about the current state of your device fleet, helping you make informed decisions quickly.\nAs you reflect on the capabilities of live queries with Fleet, consider your network environment\u0026rsquo;s unique challenges and needs. What questions could live queries help you answer about your devices? Whether it\u0026rsquo;s security audits, performance monitoring, or compliance checks, live queries offer a dynamic solution to address these concerns.\nWe encourage you to explore the possibilities and share your thoughts or questions. Perhaps you’re facing a specific query challenge or an innovative use case you’ve discovered. Whatever it may be, the world of live queries is vast and ripe for exploration. Join us in Fleet’s Slack forums to engage with a community of like-minded professionals and deepen your understanding of what live queries can achieve in your environment.\nAPI Documentation:\nRun live query with REST API Run live query with WebSockets This article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2023-12-29T00:00:00Z","image":"https://victoronsoftware.com/posts/get-current-telemetry-from-your-devices-with-live-queries/Live%20Query_hu64e3a4617825ba0468dff8df166d9a56_55465_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/get-current-telemetry-from-your-devices-with-live-queries/","title":"Get current telemetry from your devices with live queries"},{"content":" When starting to code in Go, I encountered the following situation. I needed to create an empty slice, so I did:\nslice := []string{} However, my IDE flagged it as a warning, and pointed me to this Go style guide passage, which recommended using a nil slice instead:\nvar slice []string This recommendation didn\u0026rsquo;t seem right to me. How can a nil variable be better? Won’t I run into issues like null pointer exceptions and other annoyances? Well, as it turns out, that’s not how slices work in Go. When declaring a nil slice, it is not the dreaded null pointer. It is still a slice. This slice includes a slice header, but its value just happens to be nil.\nThe main difference between a nil slice and an empty slice is the following. A nil slice compared to nil will return true. That’s pretty much it.\nif slice == nil { fmt.Println(\u0026#34;Slice is nil.\u0026#34;) } else { fmt.Println(\u0026#34;Slice is NOT nil.\u0026#34;) } When printing a nil slice, it will print like an empty slice:\nfmt.Printf(\u0026#34;Slice is: %v\\n\u0026#34;, slice) Slice is: [] You can append to a nil slice:\nslice = append(slice, \u0026#34;bozo\u0026#34;) You can loop over a nil slice, and the code will not enter the for loop:\nfor range slice { fmt.Println(\u0026#34;We are in a for loop.\u0026#34;) } The length of a nil slice is 0:\nfmt.Printf(\u0026#34;len: %#v\\n\u0026#34;, len(slice)) len: 0 And, of course, you can pass a nil slice by pointer. That’s right \u0026ndash; pass a nil slice by pointer.\nfunc passByPointer(slice *[]string) { fmt.Printf(\u0026#34;passByPointer len: %#v\\n\u0026#34;, len(*slice)) *slice = append(*slice, \u0026#34;bozo\u0026#34;) } You will get the updated slice if the underlying slice is reassigned.\npassByPointer(\u0026amp;slice) fmt.Printf(\u0026#34;len after passByPointer: %#v\\n\u0026#34;, len(slice)) len after passByPointer: 1 The code above demonstrates that a nil slice is not a nil pointer. On the other hand, you cannot dereference a nil pointer like you can a nil slice. This code causes a crash:\nvar nullSlice *[]string fmt.Printf(\u0026#34;Crash: %#v\\n\u0026#34;, len(*nullSlice)) Here\u0026rsquo;s the full gist:\n","date":"2023-12-28T00:00:00Z","image":"https://victoronsoftware.com/posts/nil-slice-versus-empty-slice-in-go/cover_hu8e4888644a780aeecee2f1f52535fba1_61530_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/nil-slice-versus-empty-slice-in-go/","title":"Nil slice versus empty slice in Go"},{"content":" Matter is a recent open-source standard for connecting devices such as light switches, door locks, motion sensors, and many others. The major goals of the standard are compatibility and interoperability. This means that you will no longer need to be an expert hacker when trying to control devices from multiple manufacturers under a single application. Apple, Amazon, and Google are some of the major members driving the standard. This is great news for the majority of adopters who haven’t yet fully embraced home automation and security.\nThe Matter specification is published by the Connectivity Standards Alliance (CSA) and includes a software development kit. Version 1.0 of the specification was released in October of 2022. In 2023, we saw a slew of new devices and software upgrades compatible with Matter. Version 1.2 of the specification was published in October of 2023. However, this latest specification is still missing support for a few important device categories such as cameras and major appliances. Cameras are a top priority for the CSA, and we may see Matter-compatible cameras in 2024.\nMatter is an important step for the management of IoT devices because it finally brings true interoperability where it has been sorely missing for so many years. No longer will device manufacturers need to decide and budget precious software resources to support Amazon Alexa, Google Home, Apple HomeKit, or another connectivity hub. Customers will no longer be locked into using one of the major home automation providers. And home automation solutions from smaller companies will come onto the market.\nAn important feature of Matter is multi-admin, which means that devices can be read and controlled by multiple clients. In Matter terminology, the device, such as a motion sensor, is called a server or node, and the applications controlling it are called clients. For example, a light switch may be simultaneously controlled by the manufacturer’s app, by Alexa, and by the user\u0026rsquo;s hand-written custom API client.\nMulti-admin support means that a home or business may use one application to control their locks, switches, and security sensors, and another application for reading telemetry from those same devices. Businesses will find it easier to integrate physical security with cyber security. For example, suppose a business’s device management server uses Matter to subscribe to the office door lock. It receives an alert that User A has entered their code. Afterwards, via regular scheduled telemetry, it notices a successful login to Computer B. The business SIEM (security information and event management) system should immediately flag this suspicious sequence of events.\nOf course, the example above can be accomplished today by writing some custom code or using a third party integration. What Matter brings is scalability to such security approaches. The code and integration will no longer need to be redone for each new device and version that comes onto the market.\n","date":"2023-12-20T00:00:00Z","image":"https://victoronsoftware.com/posts/physical-security-meets-cybersecurity-with-matter/cover_hu240e6838189f9e304ae612d50248a293_9583_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/physical-security-meets-cybersecurity-with-matter/","title":"Physical security meets cybersecurity with Matter"},{"content":" A prepared statement is a feature of modern databases intended to help execute the same SQL statement multiple times. For example, the following statement is a prepared statement:\nSELECT id, name FROM users WHERE email = ?; The presence of an unspecified parameter, labeled “?”, makes it a prepared statement. When a prepared statement is sent to the database, it is compiled, optimized, and stored in memory on the database server. Subsequently, the client application may execute the same prepared statement multiple times with different parameter values. This results in a speedup.\nPrepared statements are well suited for long and complex queries that require significant compilation and optimization times. They are kept prepared on the DB server, and the application must only pass the parameters to execute them.\nAnother benefit of using prepared statements is the protection they provide against SQL injection. The application does not need to properly escape the parameter values provided to the statement. Because of this protection, many experts recommend always using prepared statements for accessing the database.\nHowever, by always using prepared statements for accessing the database, we force the SQL driver to send the extra prepare command for every ad-hoc statement we execute. The driver sends the following commands:\nPrepare the statement Execute statement with given parameters Close the statement (and deallocate the prepared statement created above) Another issue with prepared statements is the memory requirement. In large application deployments with large numbers of connections, prepared statements can crash your environment. This issue happened to one of our customers.\nA prepared statement is only valid for a single session, which typically maps to a single database connection. If the application runs multiple servers, with many connections, it may end up storing a prepared statement for each one of those sessions.\nFor example, given 100 servers with 100 connections each, we have 10,000 connections to the database. Assuming a memory requirement of 50 KB per prepared statement (derived from the following article), we arrive at the maximum memory requirement of:\n10,000 * 50 KB = 500 MB per single saved prepared statement Some databases also have limits on the number of prepared statements. MySQL’s max_prepared_stmt_count defaults to 16,382 for the entire server. Yes, this is a global limit, and not per session. In the above example, if the application uses prepared statements for every database access, then each database connection will always be using up 1 short-lived prepared statement. A short-lived prepared statement is the prepared statement, as we described above, that will be created for the purposes of executing one statement, and then immediately deallocated afterwards. This means the above application running with a default MySQL config cannot explicitly save any prepared statements \u0026ndash; 10,000 transient prepared statements + 10,000 saved prepared statements is greater than the max_prepared_stmt_count of 16,382.\nThis is extremely inconvenient for application developers, because they must keep track of:\nThe number of saved prepared statements they are using How many application servers are running How many database connections each server has The prepared statement limits of the database This detail can easily be overlooked when scaling applications.\nIn the end, is it really worth using prepared statements, and especially saved prepared statements, in your application? Yes, saved prepared statements can offer performance advantages, especially for complex queries executed frequently. However they must also be kept in check.\nA few ways to mitigate prepared statement issues for large application deployments include:\nLimit the number of database connections per application server Increase the prepared statement limit on the database server(s) Limit the maximum lifespan of connections. When closing a connection, the database will deallocate all prepared statements on that connection. ","date":"2023-12-14T00:00:00Z","image":"https://victoronsoftware.com/posts/sql-prepared-statements-are-broken-when-scaling-applications/cover_hu7756c223ae4ac2243c5cccab03f7b66b_14192_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/sql-prepared-statements-are-broken-when-scaling-applications/","title":"SQL prepared statements are broken when scaling applications"},{"content":" At Fleet, our developer documentation is spread out throughout the codebase, contained in a multitude of README and Markdown files. Much of the documentation is hosted on our webpage, but not all of it.\nAs developers, we need to be able to quickly search project documentation to find answers to specific questions, such as:\nHow to do a database migration How to run integration tests How to deploy a development version of to a specific OS One solution is to use grep or the IDE environment to search for these answers. Unfortunately, such search methods are not optimized for text search \u0026ndash; they frequently generate no relevant results or too many results that we must manually wade through to find the most appropriate. Specialized documentation search tools, on the other hand, prioritize headings and whole words, search for plural versions of the search terms, and offer other conveniences.\nThe lack of good search capability for engineering docs must be solved in order to scale engineering efforts. It is an issue because of the following side effects:\nEngineers are discouraged from writing documentation Documentation may be duplicated Senior developers are frequently interrupted when people can’t find relevant documentation One solution is to use a documentation service, such as a team wiki, Confluence, or GitBook. GitBook integrates with git repositories, and can push documentation changes. GitBook is free for personal use, which makes it easy to use for open source projects such as fleet and osquery. That said, GitBook is a newcomer to the space, and is still reaching maturity.\nTo set up a personal GitBook, make a fork of the open source projects that contain documentation you’d like to search, and integrate them into GitBook spaces. After indexing is complete, you’ll be able to effectively search the documentation.\nTo keep the forks in sync with the parent repositories, we use Github Actions. Github Actions are free for open source projects. Searching GitHub for sync-fork returned several examples. We ended up using the following:\nname: Sync Fork on: schedule: - cron: \u0026#39;55 * * * *\u0026#39; workflow_dispatch: # on button click jobs: sync: runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v4 with: token: ${{ secrets.WORKFLOW_TOKEN }} fetch-depth: 0 - name: Configure Git run: | git config --global user.name \u0026#34;GitHub Actions Bot\u0026#34; git config --global user.email \u0026#34;actions@github.com\u0026#34; - name: Merge upstream run: | git remote add upstream https://github.com/fleetdm/fleet.git git fetch upstream main git checkout main git merge upstream/main git push origin main The WORKFLOW_TOKEN above is a GitHub personal access token (PAT) that allows reading and writing workflows in this repository. This token is not needed for repositories without workflows.\nIn addition to project documentation, GitBook can be used to synchronize personal documentation that’s being held in a private repository. There are several git-based notebook applications on the market. In addition, Markdown notes from the popular note-taking app Obsidian can be kept in GitHub. This turns GitBook into a true personalized developer documentation database \u0026ndash; one place to search through developer docs as well as your own private notes.\n","date":"2023-11-30T00:00:00Z","image":"https://victoronsoftware.com/posts/you-need-a-personal-dev-docs-db-gitbook/cover_hu54e2e2056c8855962719d82a53cb41e3_206961_120x120_fill_box_smart1_3.png","permalink":"https://victoronsoftware.com/posts/you-need-a-personal-dev-docs-db-gitbook/","title":"You need a personal dev docs DB (GitBook)"},{"content":" Traditionally, network routers used dedicated bare metal machines. However, in the last several years, we’ve seen a rise in software-based routers that can be deployed either on bare metal, on a VM, or even on a container. This means these virtual routers can be used to replace existing router software on an older router. They can run in the cloud. Or they can be installed on do-it-yourself (DIY) hardware. A couple popular open source software-based routers are pfSense and OPNsense.\nWhy use a virtual router? For one, these routers offer enterprise-level features such as build-in VPN support, traffic analysis, and extensive diagnostics, among others. Another reason is that having a virtual router gives you the ability to experiment \u0026ndash; you can install multiple routers on top of your hypervisor, and try all of them out. A third reason is that the virtual router may be only one of many VMs that you run on your hardware. You can use the same piece of hardware to run a router, an ad-blocking service, a media server, and other applications.\nAdvanced virtual router installation and set up When setting up our virtual router, we chose to use PCI Passthrough to allow the virtual router direct access to the NIC hardware. Direct access to hardware improves the latency of our internet traffic. In addition, we wanted our hypervisor to sit behind the router, and not be exposed to the public. This reduces the attack surface for potential bad agents. However, routing hypervisor traffic through the router made our setup a bit tricker. It is like the chicken or the egg dilemma \u0026ndash; how do you put your hypervisor behind the router when the hypervisor is responsible for managing the router? Below is the approach we used when installing pfSense on top of Proxmox Virtual Environment (PVE).\nFor the initial installation, we did not use PCI Passthrough and instead used a virtual network bridge (vmbr0). We configured the router VM to start on boot.\nInitial virtual router configuration This allowed us to continue controlling the virtual router through the PVE web GUI. We set up the router and enabled access to it through the serial interface, which we used in the next step. Then, we put the system into its final configuration.\nFinal virtual router configuration In order to finish configuring, we had to plug in a monitor and keyboard into our hardware. We accessed the virtual router via the serial interface from the PVE command line:\nqm terminal 100 We updated the WAN interface to use eth0. At this point, the LAN interface eth1 had access to the internet.\nIn addition, we added a second LAN interface for the network bridge (vmbr0). We made sure firewall configurations for both LAN interfaces were the same.\nNext, from the PVE command line, we updated the PVE IP and gateway to point at the router by modifying the following files.\n/etc/network/interfaces /etc/hosts After rebooting PVE, we had access to the internet and to the PVE Web GUI from our new LAN.\nUpdating router software Using a virtual router with PCI Passthrough creates a unique challenge when doing software updates. What if the new version doesn’t work? What if you lose all internet access.\nWe can mitigate potential issues. First, we recommend always making a backup of the router VM when upgrading. That way we can easily roll back the change. Switching to a backup, however, requires keyboard and monitor access to your hardware, since it must be done via the PVE command line.\nAnother way to safely upgrade is to spin up a second VM running updated router software. The second VM can be either from a backup or brand new. This VM should use virtual network bridges for its connections. Once it is properly configured, we can stop the first router VM and switch the port connections to the second VM. This flow also requires accessing the router via the serial interface to update the WAN/LAN interfaces.\n","date":"2023-11-22T00:00:00Z","image":"https://victoronsoftware.com/posts/setting-up-a-virtual-router/cover_hu019a91ef521bad7337cb0514ecaaaefc_16004_120x120_fill_q75_box_smart1.jpeg","permalink":"https://victoronsoftware.com/posts/setting-up-a-virtual-router/","title":"Setting up a virtual router"},{"content":" Keychains are the macOS’s method to track and protect secure information such as passwords, private keys, and certificates. Traditionally, the keychain information was stored in files, such as:\n/Library/Keychains/System.keychain /Library/Keychains/apsd.keychain /System/Library/Keychains/SystemRootCertificates.keychain /Users/\u0026lt;username\u0026gt;/Library/Keychains/login.keychain-db In the last several years, Apple also introduced data protection keychains, such as the iCloud Keychain. Although the file-based keychains above are on the road to deprecation in favor of data protection keychains, current macOS systems still heavily rely on them. It is unclear when, if ever, these keychains will be replaced by data protection keychains.\nInspecting file-based keychains has gotten more difficult as Apple deprecated many of the APIs associated with them, such as SecKeychainOpen. In addition, excessive use of these deprecated APIs may result in corruption of the Login Keychain, as mentioned in this osquery issue. By NOT using the deprecated APIs, the user only has access to the following keychains from the above list:\n/Library/Keychains/System.keychain /Users/\u0026lt;username\u0026gt;/Library/Keychains/login.keychain-db Root certificates are missing. And the APSD (Apple Push Service Daemon) keychain is missing, which is used for device management, among other things.\nSo, how can app developers and IT professionals continue to have access to ALL of these keychain files?\nOne way is to continue using deprecated APIs until they stop working. We recommend making a secure copy of the keychain files before accessing them with the APIs.\nAnother option is to use the macOS security command line tool. For example, to list root certificates, do the following:\nsudo security find-certificate -a /System/Library/Keychains/SystemRootCertificates.keychain A third, and hardest, option is to parse the keychain files yourself. Some details on the keychain format are available. Please leave a comment if you or someone else has created a tool to parse Apple keychains.\nThe fourth option is to use an existing tool, such as osquery. Osquery is an open-source tool built for security and IT professionals. Osquery developers are working on fixing any issues to continue providing access to macOS keychain files via the following tables:\ncertificates keychain_acls keychain_items ","date":"2023-11-16T00:00:00Z","permalink":"https://victoronsoftware.com/posts/inspecting-keychain-files-on-macos/","title":"Inspecting keychain files on macOS"},{"content":" Authorization is giving permission to a user to do an action on the server. As developers, we must ensure that users are only allowed to do what they are authorized.\nOne way to ensure that authorization has happened is to loudly flag when it hasn\u0026rsquo;t. This is how we do it at Fleet Device Management.\nIn our code base, we use the go-kit library. Most of the general endpoints are created in the handler.go file. For example:\n// user-authenticated endpoints ue := newUserAuthenticatedEndpointer(svc, opts, r, apiVersions...) ue.POST(\u0026#34;/api/_version_/fleet/trigger\u0026#34;, triggerEndpoint, triggerRequest{}) Every endpoint calls kithttp.NewServer and wraps the endpoint with our AuthzCheck. From handler.go:\ne = authzcheck.NewMiddleware().AuthzCheck()(e) return kithttp.NewServer(e, decodeFn, encodeResponse, opts...) This means that after the business logic is processed, the AuthzCheck is called. This check ensures that authorization was checked. Otherwise, an error is returned. From authzcheck.go:\n// If authorization was not checked, return a response that will // marshal to a generic error and log that the check was missed. if !authzctx.Checked() { // Getting to here means there is an authorization-related bug in our code. return nil, authz.CheckMissingWithResponse(response) } This additional check is useful during our development and QA process, to ensure that authorization always happens in our business logic.\nThis article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2023-11-10T00:00:00Z","permalink":"https://victoronsoftware.com/posts/catch-missed-authorization-checks-during-software-development/","title":"Catch missed authorization checks during software development"}]