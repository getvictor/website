[{"content":"AI agents are changing the way we work. They can handle coding, debugging, and research tasks while we focus on other things. But just because an AI can work in parallel does not mean you should. The real question is: when is it worth multitasking, and when will it hurt your results?\nI\u0026rsquo;m a firm believer that humans cannot multitask. Every popular psychology book I have read, and I have read quite a few, says that humans cannot multitask, even though many believe they can. Studies show that when humans do two tasks in series, they finish faster and with higher quality than when switching between them.\nBut that\u0026rsquo;s not the whole story. Humans can multitask in particular situations. For example, when driving a car and talking on the phone (with a Bluetooth headset, of course), we are clearly multitasking. Would our driving and conversational quality be better if we did these tasks separately? Probably. But the difference, depending on the person, might not be that big. Driving is habitual for many people, so we can pair it with another cognitive task without dramatically dropping performance.\nWhat about throwing AI agents in the mix? It is Wednesday morning. You told your AI agent to do a job. You hope it will only take one minute, but it might take up to 15 minutes. What do you do now?\nNote: We will use the term \u0026ldquo;task\u0026rdquo; to refer to work a human does, and \u0026ldquo;job\u0026rdquo; to refer to work an AI agent does.\nTask switching Research from the American Psychological Association suggests that switching between tasks can cost up to 40% of someone\u0026rsquo;s productive time. A University of Michigan study found it takes an average of 23 minutes and 15 seconds to fully return to a task after an interruption. Even brief mental blocks from shifting between tasks can cost significant time.\nSo, if your AI agent is running, can you do something else while waiting to check the results? If you switch to another task, such as looking at another bug or feature, you will spend time ramping up on that second task, and then even more time switching back to the first. Yes, people do it all the time, but you will not be fully engaged in either task. You are more likely to miss something important, like the agent using the wrong method or skipping a critical feature requirement. And yes, that happens in real life.\nWhen not to task switch Avoid switching when working on a significant, important feature or bug. You need your full mental capacity in the context of that task to deliver the best software.\nInstead of switching, consider these options.\nPlan ahead After the agent completes its job, what will you do? Will you check the results and then start the next phase? Is that next phase directly related to the current one? Can you spin up a parallel environment and have another agent start the next step?\nIf waiting for the agent is your bottleneck, parallelize jobs or ensure the agent is always working on something useful. Keeping the agent busy may mean reviewing the agent\u0026rsquo;s work while it jumps ahead and works on the next phase.\nMake sure the agent is efficient Watch the agent\u0026rsquo;s progress to make sure it is not doing unnecessary work. If it is trying to find information you already know, stop it and provide the answer. If you do not know the answer, you may find it faster than the agent.\nFor example, as of August 2025, Claude Code does not create a vector database of the codebase, making it slower for certain queries about the codebase. For example, asking where a specific feature is tested. You might ask Cursor, which does a semantic index of the codebase, to find the answer faster and then feed the answer back to Claude Code.\nAgents can go off track. Many use brute-force approaches, repeatedly trying solutions until something works, even if the \u0026ldquo;working solution\u0026rdquo; is not optimal. By monitoring progress, you can redirect the agent when needed.\nPrepare for manual QA Once the feature is ready for manual testing, is your environment ready? Do you need to populate database data, deploy other services, or sign up for cloud accounts? Prepare these ahead of time.\nFix CI and code review issues Commit regularly, ideally after each agent job run. This lets you run CI checks and even have AI review your code. Fix any lint issues or failing tests, and address review feedback early.\nWhen to task switch Task switching is fine for small, low-risk work, such as:\nFixing a single failing unit test Resolving a simple bug Addressing straightforward code review comments Minor refactoring Adding logging statements These tasks require little mental setup, so switching costs are minimal. Any quality issues are easier to spot.\nWork modes: single-tasking vs multitasking Categorize tasks before starting them to decide whether they need full attention or can run in parallel.\nAspect üéØ Single-Task Mode üîÑ Multitask Mode Focus Level üß† Deep focus required üòå Light mental load Task Types üèóÔ∏è Complex features üîß Minor refactoring üö® Production bug fixes üìù Code review comments üèõÔ∏è Architecture decisions ‚úÖ Simple bug fixes ‚ö° Performance optimization üìä Adding logging Context Switching ‚ùå Avoid at all costs ‚úÖ Switch freely Agent Strategy üé™ One or more agents on same task üé≠ Multiple agents on different tasks Monitoring üëÅÔ∏è Watch progress closely üì± Check periodically Risk Level ‚ö†Ô∏è High - mistakes costly üü¢ Low - easy to spot issues Mental Setup üèîÔ∏è Significant ramp-up time üèÉ Quick to start Quality Impact üíé Quality critical üëç Quality easier to verify Best For üöÄ Mission-critical work üìã Routine repeatable work Single-task mode is for deep-focus work. Run one or more AI agent jobs focused on this task. Monitor progress while planning ahead or preparing test environments. Keep your context intact.\nMultitask mode is for routine, well-defined work. Run multiple AI agent jobs in parallel across different branches. Switch freely between these smaller tasks as agents finish.\nFurther reading AI for software developers\nExplore how AI tools are changing developer workflows and when to embrace AI assistance versus maintaining human control.\nWill AI agents replace software developers?\nUnderstand the evolving role of developers in an AI-driven world and why human judgment remains irreplaceable.\nWatch us discuss multitasking with AI agents Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-08-19T00:00:00Z","image":"https://victoronsoftware.com/posts/multitasking-with-ai-agents/multitasking-with-ai-agents-headline_hu_477466f86f5b393.png","permalink":"https://victoronsoftware.com/posts/multitasking-with-ai-agents/","title":"Multitasking with AI agents: When it works and when it fails"},{"content":"Have you ever felt like you\u0026rsquo;re working in a fog? Where everyone\u0026rsquo;s busy, everyone\u0026rsquo;s trying, but somehow nothing important gets done? I\u0026rsquo;ve been there. The problem isn\u0026rsquo;t the people. It\u0026rsquo;s the system. And specifically, it\u0026rsquo;s the lack of transparency and accountability.\nTaking action: What you can do when you have no visibility Key takeaways The weekly dance of non-delivery Let me take you back a couple of years. I was a tech lead at a large company, working on a project that depended on another upstream engineering team. Every week, we\u0026rsquo;d have a joint meeting with both teams. And every week, I\u0026rsquo;d ask the same question, trying to keep my tone professional:\n\u0026ldquo;Hey Alex, any update on that component we need from your side?\u0026rdquo;\nAnd every week, Alex would say:\n\u0026ldquo;Ah, I meant to, but I got pulled into something else. I\u0026rsquo;ll try to get to it this week.\u0026rdquo;\nAt first, I gave it the benefit of the doubt. Stuff happens. Priorities shift.\nBut then it kept happening. Two weeks, three, four. Same story. No progress. No accountability. No one stepping in.\nI was boiling inside.\nI kept thinking:\n\u0026ldquo;Why is no one telling Alex to work on this? Why don\u0026rsquo;t we have a project manager? Who\u0026rsquo;s making the call on what really matters?\u0026rdquo;\nWe technically had a roadmap. But let\u0026rsquo;s be honest: people didn\u0026rsquo;t actually take it seriously. If something couldn\u0026rsquo;t get done, it quietly disappeared from the plan. Deadlines slipped. Commitments evaporated. Meanwhile, I had to face my boss again with another non-update.\nYears in the fog The frustration started eating at me. I\u0026rsquo;d sit in those meetings, and darker thoughts would creep in:\n\u0026ldquo;How did Alex even get promoted? He can\u0026rsquo;t deliver anything.\u0026rdquo;\n\u0026ldquo;Why am I killing myself here when clearly no one cares? Maybe I should take off for the rest of the day.\u0026rdquo;\n\u0026ldquo;Maybe I should just get a side gig? At least someone would pay me for work that actually ships.\u0026rdquo;\nThis situation wasn\u0026rsquo;t just a few bad weeks. It was years of my life, years of pretending everything was fine when it wasn\u0026rsquo;t, years of wondering if this was just how companies worked.\nI\u0026rsquo;d go home exhausted, not from hard work, but from the sheer weight of organizational dysfunction. My wife would ask how my day was, and I\u0026rsquo;d just shrug. What was there to say? Another day, another meeting, another non-update.\nThe moment of clarity Then I switched companies.\nAnd everything changed.\nIn my new company, everyone has access to clear company priorities: what we are building, why it is important, which customer requests are critical, and who our key customers are. Engineers are empowered to make their own decisions based on this shared context. If someone can\u0026rsquo;t deliver, they openly communicate why and offer alternatives grounded in the company\u0026rsquo;s priorities.\nThere is no hiding or pretending. Just reality, laid bare.\nThat\u0026rsquo;s when it hit me. The problem at my old company wasn\u0026rsquo;t Alex. He wasn\u0026rsquo;t slacking off. He was overwhelmed, like the rest of us. Everyone was reacting to the noise instead of following the signal. No one had the authority or clarity to say what actually mattered.\nThis wasn\u0026rsquo;t a communication problem. It was a trust and transparency problem. And the absence of that created a frustrating, demoralizing fog. Everyone thought they were doing the right thing, yet nothing moved forward.\nThat experience changed my understanding of leadership.\nBecause here\u0026rsquo;s the truth: Without shared visibility and shared accountability, even the best engineers will lose direction. Polite status meetings won\u0026rsquo;t save you from a broken system.\nAs leaders, our job isn\u0026rsquo;t just to set direction. It\u0026rsquo;s to make the invisible visible and to make sure it matters.\n--- title: The hidden work iceberg --- graph TD subgraph Above[\u0026#34;üåä Above water\u0026lt;br/\u0026gt;What management sees\u0026#34;] Spacer1[\u0026#34; \u0026#34;] A1[\u0026#34;Weekly Status Updates\u0026lt;br/\u0026gt;‚úì \u0026#39;Working on it\u0026#39;\u0026#34;] A2[\u0026#34;Sprint Deliverables\u0026lt;br/\u0026gt;‚úì \u0026#39;In progress\u0026#39;\u0026#34;] A3[\u0026#34;Meeting Attendance\u0026lt;br/\u0026gt;‚úì \u0026#39;Present\u0026#39;\u0026#34;] Spacer1 -.-\u0026gt; A1 Spacer1 -.-\u0026gt; A2 Spacer1 -.-\u0026gt; A3 end subgraph Below[\u0026#34;üßä Below water\u0026lt;br/\u0026gt;The reality\u0026#34;] direction TB subgraph Row1[\u0026#34; \u0026#34;] B1[\u0026#34;‚ùå Blocked by\u0026lt;br/\u0026gt;dependencies\u0026#34;] B2[\u0026#34;‚ùå Unclear\u0026lt;br/\u0026gt;priorities\u0026#34;] B3[\u0026#34;‚ùå Context\u0026lt;br/\u0026gt;switching\u0026#34;] end subgraph Row2[\u0026#34; \u0026#34;] B4[\u0026#34;‚ùå Waiting for\u0026lt;br/\u0026gt;decisions\u0026#34;] B5[\u0026#34;‚ùå Duplicate\u0026lt;br/\u0026gt;work\u0026#34;] B6[\u0026#34;‚ùå Technical\u0026lt;br/\u0026gt;debt\u0026#34;] end subgraph Row3[\u0026#34; \u0026#34;] B7[\u0026#34;‚ùå No authority\u0026lt;br/\u0026gt;to say no\u0026#34;] B8[\u0026#34;‚ùå Overwhelmed\u0026lt;br/\u0026gt;with requests\u0026#34;] end end Above -.-\u0026gt;|\u0026#34;Without Transparency\u0026#34;| Below style Above fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000 style Below fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000 style Row1 fill:none,stroke:none style Row2 fill:none,stroke:none style Row3 fill:none,stroke:none style Spacer1 fill:none,stroke:none Taking action: What you can do when you have no visibility At the first level, you need to be aligned with your manager. You need to know your manager\u0026rsquo;s priorities, which means knowing what your manager knows. You should strive to know about 90% of what your manager knows.\nIf your manager attends cross-functional meetings with other managers where priorities and goals are discussed, you need to know what\u0026rsquo;s happening at those meetings. There are a few ways to do that:\nWatch the recordings of those meetings. If they\u0026rsquo;re not recorded, ask if they could be. Read the meeting notes from those meetings. Could your company employ an AI notetaker? Ask your manager for a readout from those meetings in your weekly one-on-one. Once you feel like you\u0026rsquo;re on the same page as your manager, repeat the process with your manager\u0026rsquo;s manager. If you\u0026rsquo;re not meeting regularly with your manager\u0026rsquo;s manager, ask for a skip-level meeting and afterwards extend the ask for a recurring skip-level meeting.\nHow to ask for more visibility Here\u0026rsquo;s a sample script you can adapt:\nHey [Manager], I\u0026rsquo;ve been thinking about how I can be more effective in my role and better support our team\u0026rsquo;s goals. I\u0026rsquo;d love to have more visibility into the broader priorities and context that drive our work.\nSpecifically, I\u0026rsquo;m interested in:\nUnderstanding the key decisions and trade-offs being discussed in cross-functional meetings Getting insight into upcoming priorities that might affect our team\u0026rsquo;s roadmap Learning about dependencies or blockers other teams are facing that might impact us Would it be possible to either:\nGet access to recordings/notes from your planning meetings, or Have a brief weekly sync where you share the key takeaways? I believe having this context would help me make better day-to-day decisions, spot potential issues earlier, and contribute more strategically to our team\u0026rsquo;s success.\nKey takeaways Looking back on my journey from frustration to clarity, here are the lessons that transformed how I work:\nIt\u0026rsquo;s not a communication problem, it\u0026rsquo;s a trust and transparency problem. We had meetings. We had updates. What we didn\u0026rsquo;t have was visibility into what actually mattered.\nThe absence of transparency creates a demoralizing fog. When priorities aren\u0026rsquo;t clear and accountability doesn\u0026rsquo;t exist, everyone thinks they\u0026rsquo;re doing the right thing while nothing moves forward.\nGreat engineers need visibility to thrive. Alex wasn\u0026rsquo;t slacking. He was overwhelmed and reacting to noise instead of signal because no one had the clarity to say what mattered.\nYou need to know what your manager knows. Aim for 90% visibility into your manager\u0026rsquo;s context through meeting recordings, notes, or regular readouts.\nLeaders must make the invisible visible. Their job isn\u0026rsquo;t just to set direction. It is to ensure that priorities, trade-offs, and blockers are explicit and public.\nThe fog I worked in for years wasn\u0026rsquo;t inevitable. It was a broken system where politeness trumped progress. You can build better systems. You just need to start asking for and creating transparency.\nFurther reading Top 3 issues with GitHub code review process\nDiscover how poor visibility in code reviews creates bottlenecks and what you can do to improve team collaboration.\nFull-featured engineering metrics‚Äîfor free\nBuild transparency dashboards that make engineering work visible to everyone using free tools and GitHub data.\nWhat is readable code and why is it important?\nLearn how code clarity impacts team velocity and why transparency starts at the code level.\nWatch us discuss engineering transparency Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-08-12T00:00:00Z","image":"https://victoronsoftware.com/posts/engineering-transparency/engineering-transparency-headline_hu_23d9d2e59093d9c5.png","permalink":"https://victoronsoftware.com/posts/engineering-transparency/","title":"Why transparency beats everything else in engineering"},{"content":"We\u0026rsquo;ve changed our tune on OpenTelemetry.\nIn our previous article on OpenTelemetry with Jaeger, we were skeptical about its value for the average developer. The tooling seemed more suited for production monitoring than day-to-day development work. However, recent production issues have made us reconsider.\nFeature 1: API request and response bodies Feature 2: Database query instrumentation Feature 3: Outgoing HTTP requests Feature 4: Trace-log correlation Feature 5: Comprehensive error handling The wake-up call We discovered a critical gap in our production telemetry: several key API endpoints weren\u0026rsquo;t instrumented. When issues arose, we were flying blind. No traces. No visibility. Just frustrated customers and scrambling engineers.\nThis experience taught us an important lesson: telemetry isn\u0026rsquo;t just an operations concern, it\u0026rsquo;s a development concern. If developers don\u0026rsquo;t use and understand telemetry during development, we end up with half-baked instrumentation in production.\nThe solution? Developers must dogfood their own telemetry.\nSetting up OpenTelemetry for development Using OpenTelemetry during development provides immediate benefits:\nComplete instrumentation coverage before production Faster debugging with familiar tooling (same tooling in dev and production) Better understanding of code behavior Early detection of performance issues We built a TODO application to demonstrate key OpenTelemetry features for developers. The examples use SigNoz as the backend, but any OpenTelemetry-compatible system works (Jaeger, Grafana, Datadog, etc.).\nNote: In development, we often want more detailed telemetry than in production. Production may limit telemetry due to performance or security reasons. Many of the features we\u0026rsquo;ll demonstrate should have environment-specific toggles.\nFeature 1: API request and response bodies Seeing actual request and response bodies is invaluable during development. While you\u0026rsquo;d avoid this in production for security reasons, it\u0026rsquo;s essential for debugging locally.\nKey OpenTelemetry concepts: attributes vs events Attributes: Key-value pairs attached to spans that provide metadata. They\u0026rsquo;re indexed and searchable (e.g., http.method, http.status_code, user.id). Use attributes for the data you need to query or filter.\nEvents: Timestamped records within a span that capture what happened at a specific moment. They\u0026rsquo;re perfect for logging request/response bodies, error messages, or any detailed information that doesn\u0026rsquo;t need to be searchable but provides context when debugging.\nIn our TODO app, we recorded request and response bodies as events:\nspan.AddEvent(\u0026#34;http.request.body\u0026#34;, trace.WithAttributes( attribute.String(\u0026#34;body\u0026#34;, requestBody), )) span.AddEvent(\u0026#34;http.response.body\u0026#34;, trace.WithAttributes( attribute.String(\u0026#34;body\u0026#34;, responseBody), )) This approach provides full API context without bloating searchable attributes.\nFeature 2: Database query instrumentation Database queries often cause performance bottlenecks. OpenTelemetry captures both query timing and the actual SQL executed.\nStandard instrumentation shows parameterized queries:\nSELECT * FROM todos WHERE user_id = ? AND status = ? For debugging, we need the actual values. Our enhanced instrumentation includes both:\nspan.SetAttributes( attribute.String(\u0026#34;db.statement\u0026#34;, \u0026#34;SELECT * FROM todos WHERE user_id = ? AND status = ?\u0026#34;), attribute.String(\u0026#34;db.statement.formatted\u0026#34;, \u0026#34;SELECT * FROM todos WHERE user_id = 123 AND status = \u0026#39;active\u0026#39;\u0026#34;), ) This dual approach maintains security (prepared statements against SQL injection) while providing debugging context. Use populated queries to:\nReproduce issues by running the exact query Understand why a query returned unexpected results Debug complex queries with multiple parameters Alternatively, you could log the populated query statements instead of adding them as span attributes. Putting them in the logs gives you more flexibility in controlling data retention policies, as logs and traces often have different storage durations and access controls in production environments.\nFeature 3: Outgoing HTTP requests Modern applications integrate with external APIs and microservices. Outgoing HTTP calls need the same visibility as incoming requests.\nOur TODO app instruments outgoing HTTP requests, capturing:\n// Before making the request span.AddEvent(\u0026#34;http.request.body\u0026#34;, trace.WithAttributes( attribute.String(\u0026#34;body\u0026#34;, requestBody), attribute.Int(\u0026#34;size\u0026#34;, len(requestBody)), )) // After receiving the response span.AddEvent(\u0026#34;http.response.body\u0026#34;, trace.WithAttributes( attribute.String(\u0026#34;body\u0026#34;, responseBody), attribute.Int(\u0026#34;size\u0026#34;, len(responseBody)), attribute.Int(\u0026#34;status_code\u0026#34;, resp.StatusCode), )) This visibility is crucial when:\nThird-party APIs return unexpected results You need to debug authentication or request formatting issues Performance bottlenecks come from external dependencies You\u0026rsquo;re troubleshooting integration problems External calls receive the same instrumentation as internal operations, eliminating black boxes during development.\nFeature 4: Trace-log correlation OpenTelemetry connects traces with logs, enabling seamless navigation between high-level trace data and detailed log output.\nKey OpenTelemetry concepts: spans vs traces Span: A single unit of work within a trace. It represents an operation like a database query, HTTP request, or function call. Each span has a unique span_id and may have a parent_span_id to form a tree. Spans are connected through these relationships to reflect the call hierarchy.\nTrace: The complete journey of a request through your system, composed of multiple spans that all share the same trace_id. The root span has no parent and marks the start of the trace. Think of a trace as the tree, and spans as the branches and leaves.\nIn our TODO app, we inject trace context into all log entries:\nlogger := log.With( \u0026#34;trace_id\u0026#34;, span.SpanContext().TraceID().String(), \u0026#34;span_id\u0026#34;, span.SpanContext().SpanID().String(), ) logger.Info(\u0026#34;Creating new TODO item\u0026#34;, \u0026#34;user_id\u0026#34;, userID, \u0026#34;title\u0026#34;, todo.Title) This correlation enables:\nQuick navigation from a slow span to its detailed logs Understanding the sequence of operations within a request Debugging complex flows across multiple services Maintaining context when troubleshooting issues Viewing traces and logs together reduces debugging time significantly.\nFeature 5: Comprehensive error handling OpenTelemetry captures complete error context and propagation paths throughout your system.\nError counts may appear inflated because they\u0026rsquo;re counted at each span level:\nDatabase layer throws an error (count: 1) Service layer catches and returns error response (count: 2) This propagation helps identify error origins and their path through application layers.\nExample error instrumentation:\n// Simple error recording span.RecordError(err) span.SetStatus(codes.Error, err.Error()) // With stack trace span.RecordError(err, trace.WithStackTrace(true)) With proper error instrumentation, you get:\nComplete stack traces for debugging Error propagation paths through your system Direct links to the failing trace Associated logs with full context Performance impact of error handling All debugging information exists in one place, reducing troubleshooting from hours to minutes.\nMoving forward Our next step is implementing this approach in our production application. The benefits are clear:\nComplete visibility: No more blind spots in our API endpoints Faster debugging: All context in one place Better habits: Developers who use telemetry build better instrumented code Proactive monitoring: Issues are caught before they impact users Telemetry isn\u0026rsquo;t an afterthought. It\u0026rsquo;s a development tool that we can use from day one.\nTry it yourself See these concepts in action with our instrumented TODO application on GitHub. The AI-generated code has readability issues, but effectively demonstrates all telemetry concepts.\nTo get started with your own setup:\nInstall SigNoz using Docker Clone the TODO app repository Run the app with OpenTelemetry enabled Start exploring your traces! The goal isn\u0026rsquo;t perfect code. It\u0026rsquo;s perfect visibility into code behavior.\nFurther reading Top 5 metrics for software load testing performance\nLearn which telemetry metrics matter most when load testing your applications.\nHow to benchmark performance of Go serializers\nDiscover performance optimization techniques using Go\u0026rsquo;s built-in profiling tools.\nIs OpenTelemetry useful for the average software developer?\nOur initial skeptical take on OpenTelemetry and its challenges for development use.\nNote: What\u0026rsquo;s your experience with OpenTelemetry? Have you found other creative ways to use telemetry during development?\nWatch us demonstrate OpenTelemetry features useful for developers Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-08-05T00:00:00Z","image":"https://victoronsoftware.com/posts/opentelemetry-for-devs/opentelemetry-for-devs-headline_hu_1ae9e906cadcab19.png","permalink":"https://victoronsoftware.com/posts/opentelemetry-for-devs/","title":"Why developers should use OpenTelemetry in dev"},{"content":"TL;DR: AI coding agents are becoming mainstream, but their impact is focused mainly on implementation and automated testing. Productivity gains are real but capped, and engineers must still actively guide and supervise these tools. Complete developer replacement is unlikely anytime soon.\nRequirements gathering Design Implementation Testing Deployment Maintenance AI coding agent impact across the SDLC Will AI coding agents replace software developers? Introduction AI is no longer just autocomplete. With AI coding agents, it is becoming a collaborator. But what happens when that collaborator starts writing and testing entire chunks of your codebase? Will software engineers become managers of fleets of AI agents, or will their jobs evolve in less dramatic ways?\nOur previous article covered the AI trends in the first half of 2025. Using AI for code completion and code generation was becoming mainstream. Today, in the second half of 2025, we can expect most software developers to be using AI, off and on, for these tasks. But what about using AI coding agents to make significant changes to the codebase, including automatically running and fixing tests? Recent surveys show that most software engineers are NOT using AI agents. With AI code completion tools like GitHub Copilot being widely adopted within about one year, we can reasonably assume that AI agents will likewise be widely adopted by the second half of 2026. With that in mind, what will a typical engineer\u0026rsquo;s day look like in 2026?\nAI tools adoption timeline in software development timeline 2022-2023 : GitHub Copilot Launch : Early AI code completion : Developer experimentation 2024 : Code completion mainstream : ChatGPT for development : Basic code generation 2025 : Advanced code generation : AI debugging tools : Early AI agents (experimental) 2026 : AI agents widely adopted : Automated testing with AI : AI code review assistants 2027+ : AI-native development workflows : Advanced agent orchestration : Full SDLC integration Let\u0026rsquo;s walk through the software development lifecycle (SDLC) and understand where these new AI coding agents can be used. We will focus specifically on the new capabilities that AI coding agents bring, and not on the existing and largely mainstream capabilities of AI code completion and chatbots.\n--- title: SDLC --- graph LR Requirements[Requirements gathering] --\u0026gt; Design[Design] Design --\u0026gt; Implementation[Implementation] Implementation --\u0026gt; Testing[Testing] Testing --\u0026gt; Deployment[Deployment] Deployment --\u0026gt; Maintenance[Maintenance] %% Color coding based on AI agent impact level %% Low impact - light blue %% Medium impact - yellow/orange %% High impact - green classDef lowImpact fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000 classDef mediumImpact fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000 classDef highImpact fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000 class Requirements,Deployment lowImpact class Design,Maintenance mediumImpact class Implementation,Testing highImpact Requirements gathering In requirements gathering, we need to figure out why we are adding a feature and what the feature is, at a high level. We have two major sources of product features. The first one is a strategic feature that will open up new sources of revenue for us. The second is a customer request, which we must build to keep our customers happy. A single feature could be both of these.\nTo understand the customer, we actually need to talk to the customer. Barring that, we could do market analysis to see what similar features our competition has. ChatGPT can speed up this process by aggregating and explaining the information. In this case, ChatGPT makes research more efficient, much like Google made research more efficient than going to the library. But this is not a new AI use case. Product managers have been using AI to speed up their work for years now.\nAs far as figuring out what we\u0026rsquo;re building and why, AI coding agents may have little to add. That said, they may have some use cases, such as writing a script to fetch and analyze data from a public API.\nNote: Requirements gathering combines the planning and requirements analysis phases, assuming we use a fast-paced Agile iterative process.\nDesign In the design phase, we get into the details of what we\u0026rsquo;re building. We specify the UI requirements, API changes, integrations with other services, and other technical requirements.\nSpike stories and proof of concepts (POCs) Often, there are enough uncertainties in the feature that software engineers must do a spike story or build a POC. A spike story is an Agile user story to research a technical question. Spike stories uncover things like:\nTechnical feasibility (e.g., can we use TPM to sign HTTP messages?) Implementation approach (e.g., should we use webhooks or polling?) Integration behavior (e.g., how does this 3rd party API handle pagination and errors?) Tool or library evaluation (e.g., does this 3rd party library provide all the features we need?) Unknown complexities or risk (e.g., will this actually work?) In a spike story or POC, we often write quick throwaway code without other architectural considerations, such as maintainability. And this is where AI coding agents can help. Theoretically, an AI coding agent can create a whole POC with only a cursory review from the software developer. But this is an extreme case. In most cases, the work will be a mix of coding, reviewing technical documentation, searching the web, and talking with ChatGPT.\nSo, with the help of an AI coding agent, we could finish our spike story faster. If the work was timeboxed, the end result should be higher quality.\nFinal design After the spike story, we still need to finish the design, providing all the technical specifications required for estimation and implementation. These details include an understanding of how this feature interacts with all the other parts of our application, such as:\nUI configuration authentication monitoring and logging error handling scalability and performance internationalization Although some of these aspects are shared between features, we, as software developers, still need to consider and investigate the new feature\u0026rsquo;s implications. AI coding agents are of little help here, besides providing boilerplate requirements.\nImplementation After the feature has been designed and estimated, it is time to get down to the work of coding. Can an AI agent do all of this? Well, we may not be able to take the design and feed it to our agent. A product manager or another engineer may have done the design, and this design may lack enough details for implementation. So we have to add details, like:\nFunction names (e.g., sendEmailNotification) Constant and enum names (e.g., RoleAdmin) File and package structure (e.g., handlers/user.go) Error handling strategy (e.g., wrap errors and add context) Interface design (e.g., create a new interface to simplify testing) Security considerations (e.g., validate inputs) Yes, an AI coding agent may indeed come up with some of these on its own. However, we must consciously consider these to ensure our codebase is maintainable, testable, scalable, and handles corner cases.\nSo, once we know what we need to code, we can write a prompt to the AI coding agent and let it do its work. Today\u0026rsquo;s AI coding agents are not very fast. In our experience, we ask an AI agent to do a chunk of work, and it completes it in several minutes. Then we do a brief review of what it did and come back to it with corrections. Corrections often include things like:\nUsing the correct coding standards for our codebase Removing unneeded code (e.g., handling cases that we know cannot happen) Removing/merging duplicate code Using better names for functions/variables (e.g., don\u0026rsquo;t use err2) If we let the AI agent do a bigger chunk of work (30+ minutes), there is a greater chance that it will go off the rails, and all the work must be redone. For example, an AI agent could decide to implement a third-party library itself because it couldn\u0026rsquo;t figure out how to use the existing one.\n--- title: AI agent development workflow --- flowchart TD A[Developer writes prompt] --\u0026gt; B[AI agent codes\u0026lt;br/\u0026gt;3-5 minutes] B --\u0026gt; C[Human reviews code] C --\u0026gt; D{Code acceptable?} D --\u0026gt;|No| E[Developer provides\u0026lt;br/\u0026gt;corrections] E --\u0026gt; B D --\u0026gt;|Yes| F[Final code review] F --\u0026gt; G[Ask AI agent to review code] G --\u0026gt; H[Human peer review] H --\u0026gt; I[Ready for QA] %% Styling classDef humanTask fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000 classDef aiTask fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000 classDef reviewTask fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000 class A,C,E,F,H humanTask class B,G aiTask class D,I reviewTask Once we reach a good stopping point, we need to review all the code changes that we and our AI agent have made. Since much of the code was autogenerated, we must take extra care to do a thorough review. The goal is to fully understand the implementation so that we can speak about it as if we coded all of it ourselves.\nOnce we have checked in the code or opened a PR, we can ask another AI agent to review it. With multiple LLMs and coding agents out there, it is good practice to have one AI agent check the work of another AI agent for anything that we may have missed. Unfortunately, this means wading through a few false positives. However, the end result is higher-quality code that is ready for one of our peers to review.\nMultitasking with multiple AI agents Some developers report using multiple AI agents to work on numerous projects simultaneously. In our workflow, this approach is not practical. We know from behavioral science research that it takes a human up to 15 minutes to entirely switch between different tasks because they must reload and recall all of the context associated with the new task into their brain. So, if we switch between tasks every 5-10 minutes, the result is that we\u0026rsquo;re never deeply engaged with any of these tasks, likely leading to lower quality software.\nPerhaps two AI coding agents can work on two tasks for the same feature. However, in this case, the two tasks must be independent, which is more of an exception than the norm.\nEven if we decide to save some time by switching from AI agent coding to a longer task, such as a code review for one of our peers, when we return to the agent, we will need to recall what we told the agent to do and what our expectations were.\nIntegration with the rest of the codebase A lot of software development work involves hooking up the feature into the existing codebase. For example, we need to create the new object and properly do dependency injection. These things often only take a few lines of code. Although AI coding agents can do these integrations, it is often just as fast and more reliable to manually code these smaller snippets.\nFull stack development AI coding agents can help you write in a programming language you\u0026rsquo;re unfamiliar with. The agent will do the brunt work, getting the syntax (mostly) right, and you can review the code to make sure it looks reasonable. This means software developers can easily expand beyond their functional specialty into other languages and parts of the codebase.\nFrom an SDLC perspective, a single feature is now more likely to be assigned to a single developer instead of being partitioned across backend/frontend or server/client boundaries. In this case, a single developer should be able to finish a feature faster, without the handoffs and the issues they entail.\nOverall, AI coding agents can significantly help speed up the implementation phase. However, we must note that AI agent effectiveness varies by task. Specifically, agents struggle with complex/niche designs and unfamiliar contexts.\nTesting The two main testing areas are automated tests, including unit and integration tests, and manual tests.\nAutomated (unit) tests Writing tests has been a primary use case for generative AI over the past few years. So, given detailed instructions regarding the scenarios to test, an AI coding agent should be able to write the test, run the test, and fix any issues. The software developer will still need to review and adjust the tests. Some common problems with agent-generated tests include:\nNot following project standards regarding the test helper methods being used (e.g., wrong HTTP client, wrong assert method) Wordy and hard to maintain tests (e.g., not following a table test approach, not using subtests) Duplicated tests (e.g., testing a case that was covered elsewhere in the test suite) Using numbers in test variables instead of descriptive names (e.g., rsp1, err2) Not actually testing anything (e.g., hard-coding test expectations in the source implementation) The AI agent may come up with some corner cases to test, but it cannot be relied on for full functional correctness. Since the AI agent can see the implementation, it often bases its tests on what has been implemented. It lacks an understanding of the intended requirements and, hence, what should be tested.\nManual tests Before handing over the implementation to the QA team, the software developer should review the test plan themselves and perform all the manual tests. Any issues found can be candidates for additional automated tests.\nThere is no AI agent today that can replace manual testers, but engineers can try to take advantage of AI to help in some areas:\nConvert natural language to test steps Identify visual (UI) regressions Accessibility testing Auto-healing tests: updating tests when UI changes So, for the testing phase, AI coding agents provide much help with creating and fixing automated tests, but not so much with manual testing.\nDeployment Before deploying the app to customers, engineers should do a release readiness check covering items like:\nAll committed features implemented Testing and QA complete Security checks passed Load testing complete After deployment, engineers should conduct smoke tests and health checks to ensure the app is working.\nIn addition, deployment involves communication tasks, such as:\ndemos, guides, FAQs release notes and changelogs Generative AI is being used to generate some communication content. AI coding agents can provide little additional value in the deployment phase.\nMaintenance The maintenance phase of the software development lifecycle includes:\nAnswering customer and internal questions Responding to alerts and incidents (e.g., investigating high CPU usage, resolving a failed background job) Reproducing and fixing customer-reported bugs AI search tools have been helpful with searching the codebase and documentation to answer questions. Some upcoming tools are trying to close the loop between production monitoring and source code fixes, and this is a great area to watch. Reproducing bugs is still very much a manual process.\nFixing bugs is an implementation task, so AI coding agents can help create the fix and a unit test. However, bug fixes tend to be small in nature, with most of the software engineer\u0026rsquo;s time spent figuring out where the bug is and how best to fix it.\nAI coding agent impact across the SDLC SDLC Phase AI Agent Impact üîç Notes üìù Requirements ‚ùå Low Some research support, but little for strategic/product thinking üß† Design ‚ö†Ô∏è Low‚ÄìMedium Help limited to spike stories and boilerplate üíª Implementation ‚úÖ High Most benefit seen here (code gen, agent pair programming) üß™ Automated Testing ‚úÖ High Strong at generating/fixing tests, needs human oversight üßç Manual Testing ‚ùå Low Still mostly a human-driven process üöÄ Deployment ‚ùå Low Some help with writing release notes, limited technical role üîß Maintenance ‚ö†Ô∏è Medium Good at fixes; weak at reproducing or analyzing issues Risk and tradeoffs While AI coding agents offer clear benefits, they also introduce new risks and tradeoffs that teams must actively manage:\nShallow code understanding: Engineers may be tempted to rely on agents without fully understanding the generated code. This erodes accountability and leads to slower debugging when issues arise. Inconsistent quality: Agent-generated code can be verbose, redundant, or subtly incorrect. Without careful review, these issues can slip into production. Developer deskilling: Over-reliance on agents may reduce hands-on practice with fundamentals, particularly for junior engineers who are still developing intuition. Security and compliance: Agents can unknowingly introduce vulnerabilities or use unsafe patterns, especially when integrating with third-party APIs or handling sensitive data. In short, AI agents amplify output, but without discipline and oversight, they can amplify problems too. Teams must treat agent-generated code with greater rigor than human-written code.\nWill AI coding agents replace software developers? Recent studies suggest that current AI tools improve productivity by 10% to 20%.\nAI speeds up Google engineers by 21% JPMorgan claims AI boosts efficiency by up to 20% Atlassian says AI is saving over 10 hours per week AI slows down some developers So, will the productivity improvements jump with AI coding agents becoming mainstream? Will one engineer be able to do the work of a whole team?\nLooking at the whole software development process, we see that the most significant gains from AI coding agents are in the implementation and automated testing phases. The consensus from studies and industry reports is that software engineers spend only about 30% of their time writing code. The rest of the time is spent in the other phases of the SDLC, as well as on other tasks such as:\nattending and preparing for meetings (e.g., planning, retros, 1:1s) mentoring, teaching, and continuous learning context switching and task juggling recruiting, interviewing, and candidate evaluation writing or reviewing internal documentation, ADRs, and RFCs developer advocacy, blogging, and community engagement travel for conferences, offsites, or customer visits improving tooling, automation, and developer environments Even assuming AI agents completely automate coding tasks (30% of work), the maximum productivity gain would be 43%: if 70% of work remains unchanged, then 30% time savings translates to doing 1.43x the work. In other words, 7 people can now do the work of 10 people. This is far from the popular claims that AI will replace all software engineers.\nIn summary, AI in general and AI coding agents in particular should continue to have a noticeable impact on software developer productivity. However, developers\u0026rsquo; work is complex and varied, and AI can only provide efficiency improvements and not wholesale replacement. AI coding agents won\u0026rsquo;t replace developers, but developers who know how to use them will replace those who don\u0026rsquo;t.\nFurther reading How to use AI for software development (2025)\nWhat every software engineer needs to know about AI right now: code completion, generation, and AI agents.\nIntroducing MCP: Lessons from building an AI-driven server\nHow we taught an AI agent to use our product with MCP.\nMultitasking with AI agents: When it works and when it fails\nPractical strategies for managing multiple AI agents and avoiding the productivity traps of task switching.\nWatch us discuss whether AI agents can replace software developers Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-07-16T00:00:00Z","image":"https://victoronsoftware.com/posts/will-ai-agents-replace-developers/ai-agents-replace-developers-headline_hu_bd26199eab2b82c1.png","permalink":"https://victoronsoftware.com/posts/will-ai-agents-replace-developers/","title":"Will AI agents replace software developers?"},{"content":" What is mTLS? mTLS: pros and cons What is an HTTP message signature? HTTP message signature: pros and cons Performance Considerations What about replay attacks? mTLS vs HTTP message signatures: choosing the right tool HTTP is the backbone of modern system communication. It lets your service confirm that it\u0026rsquo;s talking to the device you enrolled, not some impostor. It helps your backend recognize a microservice, not a malicious bot. And when it comes to protecting those channels, you\u0026rsquo;ve got two popular weapons: mTLS and HTTP Message Signatures.\nBoth are powerful and secure, but they take radically different approaches. One is baked into the TLS handshake, tight and deeply integrated. The other is explicit and flexible, wrapping your HTTP messages in cryptographic armor. This post is a no-nonsense breakdown covering what they are, where they shine, and where they cause pain.\nWhat is mTLS? In ordinary TLS (Transport Layer Security), only the server presents a certificate to the client. This way, the client knows it is connecting to a legitimate server. In mutual TLS (mTLS), the client also presents a certificate to the server so that the server can verify the client\u0026rsquo;s identity.\nmTLS is built into the TLS protocol (1.2 and 1.3) and is fast. The OS knows how to do it, and browsers know how to do it. It\u0026rsquo;s also commonly used in internal service meshes, Kubernetes clusters, secure databases, and private APIs where mutual authentication is required.\nWe covered mTLS in greater detail in our series on building an mTLS client using the system keystore.\nmTLS: pros and cons Pros\nBuilt-in support across OSes, browsers, and web servers. Strong identity baked into the connection itself. No need to change application logic: TLS handles it. Integrates more broadly with native certificate stores (e.g., macOS Keychain), with more substantial support across existing OS-level and browser-based applications. However, your specific application may still need to implement its own integration to use these certificate stores. Cons\nHard to scope by endpoint. You either enforce mTLS on the whole server or not. Requires TLS termination at a layer that understands and enforces client authentication. For example, suppose a load balancer terminates the TLS connection (which is standard practice). In that case, it must handle client certificate verification and securely pass the authenticated identity to the downstream server. Not great for public APIs. Many clients (e.g., mobile apps, SDKs) don\u0026rsquo;t handle client certificates well, making mTLS adoption difficult in heterogeneous environments What is an HTTP message signature? HTTP message signatures are precisely what they sound like: you take an HTTP request, select a few headers (or maybe the body), and sign them with a private key. The server uses the corresponding public key to verify that the request came from someone it trusts.\n--- title: HTTP message signature --- sequenceDiagram autonumber participant Client participant Server Client-\u0026gt;\u0026gt;Client: Generate or retrieve private key Client-\u0026gt;\u0026gt;Client: Create HTTP request with required headers Client-\u0026gt;\u0026gt;Client: Sign headers/body using private key Client-\u0026gt;\u0026gt;Server: Send HTTP request with signature in header Server-\u0026gt;\u0026gt;Server: Retrieve client\u0026#39;s public key Server-\u0026gt;\u0026gt;Server: Extract signature and signed headers Server-\u0026gt;\u0026gt;Server: Verify signature with public key alt Signature valid Server-\u0026gt;\u0026gt;Server: Process request else Signature invalid Server-\u0026gt;\u0026gt;Client: Reject request (401 Unauthorized) end HTTP signing works at the application layer. It does not replace TLS, but rides on top of it. That means you don\u0026rsquo;t have to worry about where TLS is terminated, and the HTTP signature can be verified at the ultimate destination server.\nMany legacy systems have used their own approaches to HTTP signing. Apple\u0026rsquo;s MDM protocol relies on a detached CMS signature with a custom header, while GitHub uses HMAC-based payload signing with a shared secret for webhooks. These mechanisms take different paths to the same goal: verifying the integrity and origin of HTTP messages.\nToday, more modern systems are beginning to align with RFC 9421, a proposed IETF standard for HTTP message signatures. The ecosystem is gradually converging on a shared standard for signed HTTP requests. A common standard is beneficial when you don\u0026rsquo;t control the client or want signature-level auditability and control.\nHTTP message signature: pros and cons Pros\nFine-grained control: sign specific API paths, select which headers or body fields to include, and decide when to apply signing based on context. Signature verification works independently of TLS termination, enabling integrity checks even after a load balancer or proxy has terminated the TLS connection. Doesn\u0026rsquo;t require X.509 certificates: keys can be created/exchanged directly, avoiding certificate issuance and renewal workflows. However, certificates can still be used if needed. Easier to debug: you can log and inspect the signature. Can integrate with system keystores (e.g., TPM) when supported by the client application. Signing/verification libraries for RFC 9421 are available. Cons\nYou must integrate/create the signing logic instead of relying on the OS or framework. You must manage replay attacks with nonce and/or timestamp protection Choosing which headers to sign isn\u0026rsquo;t always obvious and can break interoperability. It can also break backward compatibility if header selection changes across client or server updates. Performance Considerations mTLS is fast. The authentication step happens once per connection during the TLS handshake, and it benefits from all the TLS acceleration and session reuse magic that your OS and hardware provide.\nHTTP message signatures happen every time. Every request is signed. Every request is verified. You\u0026rsquo;re layering crypto on top of crypto. That means more CPU cycles, especially if your implementation isn\u0026rsquo;t careful or you aren\u0026rsquo;t using the most efficient algorithms, such as ECDSA P-256. Performance will take a hit if you canonicalize huge headers or sign things unnecessarily.\nüìå Important caveat: For most APIs, performance is dominated by network and I/O. Unless you operate at a very high scale or on constrained devices, the performance difference between mTLS and HTTP signatures might be negligible.\nWhat about replay attacks? A replay attack occurs when a bad actor captures a legitimate request and replays it later to trigger the same action again, such as resubmitting a money transfer or resetting a password.\nmTLS helps mitigate replay attacks by tying authentication to a specific TLS session. An attacker can\u0026rsquo;t simply replay a captured request from another machine or session because they won\u0026rsquo;t have access to the client\u0026rsquo;s private key and can\u0026rsquo;t establish a valid mTLS session. That said, mTLS alone does not prevent replays of application-layer data if an attacker somehow gains access to an active session (extremely unlikely).\nHTTP message signatures require you to build your own defenses. Because there\u0026rsquo;s no TLS session binding to a client key, an attacker who captures a signed request can replay it from any machine. The attacker does not need the private key, just the full request and its signature. That usually means you must include a created timestamp and/or nonce in the signed headers and reject any request that\u0026rsquo;s too old or already used. One way to do that would be:\nServer checks that created is within 10 minutes of current server time (since these fields are included in the signature, we know they have not been tampered with) Server checks that the nonce value has not been used within the last 10 minutes (nonce values can be cached with expiration in ValKey or Redis) mTLS vs HTTP message signatures: choosing the right tool Here\u0026rsquo;s how they compare:\nFeature mTLS HTTP Message Signature Identity verification üü¢ OS-level with client certs üü° App-level with public key Integrity üü¢ Built into TLS üü¢ Signature over headers/body Granular control üî¥ Hard to apply per route üü¢ Easy Deployment complexity üî¥ High (requires certs and mTLS-aware proxies) üü° Moderate (TLS termination agnostic) Public API suitability üî¥ Poor (inconsistent client certs) üü¢ Good (no client certs needed) Certificate handling üî¥ Requires full X.509 cert üü¢ Supports raw public keys Integration support üü¢ Strong OS/browser support üü° App libraries available Replay protection üü° Tied to TLS session üî¥ Requires nonce and/or timestamp Performance üü¢ Fast (once per connection) üü° Moderate (on every request with efficient algo) Debuggability üî¥ Opaque handshake üü¢ Signature is visible So, which one should you use?\nUse mTLS when both ends are under your control. It is fast, OS-integrated, and well-supported for internal services or environments with managed certificates. Use HTTP message signatures when you need per-request control, flexible client support, or operate in ecosystems where certificate management is impractical. If you value fine-grained control and debuggability, HTTP signatures are a better fit üü¢. If you want high performance and strong identity with minimal app changes, mTLS is the clear choice üü¢, as long as you can handle the setup üî¥. In some architectures, combining both may offer the best of both worlds: transport-level trust with application-level verification. Different tools for different jobs. Just don\u0026rsquo;t skip authentication.\nFurther reading Using TPM 2.0 for secure key storage Store private keys in hardware to protect API credentials, signing keys, and more. Watch us compare mTLS and HTTP message signatures Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-07-06T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-vs-http-signature/mtls-vs-http-signature_hu_62a53c942ee11361.png","permalink":"https://victoronsoftware.com/posts/mtls-vs-http-signature/","title":"mTLS vs HTTP signature faceoff: securing your APIs"},{"content":" TPM architecture TPM key storage TPM key hierarchy model TPM signing Trusted Platform Module (TPM) is a secure cryptoprocessor for many security applications requiring strong device identity, data protection, and platform integrity. Some uses include:\nDevice identity and attestation Secure boot Key storage Disk encryption Digital signatures TPM 1.2 was standardized in 2009 as ISO/IEC 11889:2009. TPM 2.0, the most common today, came out in 2015 with the ISO/IEC 11889:2015 standard. The link is for part 1 of the standard (out of 4).\nAlthough many people think of TPM as a hardware chip, it has many possible implementations. They are:\nA dedicated TPM chip Integrated TPM as part of another chip (e.g., an ARM-based SoC) Firmware TPMs (fTPMs) that run in the CPU\u0026rsquo;s trusted execution environment Virtual TPMs (vTPMs) are provided by hypervisors to provide security to virtual environments Software TPMs are emulators of TPMs. They are helpful for development purposes In this article, we will focus on the use case of storing a private key in TPM 2.0.\nTPM architecture graph TD subgraph TPM subgraph Memory VM[\u0026lt;b\u0026gt;Volatile Memory\u0026lt;/b\u0026gt;\u0026lt;br/\u0026gt;Temporary keys, Sessions,\u0026lt;br/\u0026gt;Buffers] NV[\u0026lt;b\u0026gt;Non-Volatile Memory\u0026lt;/b\u0026gt;\u0026lt;br/\u0026gt;EK, SRK, Policies,\u0026lt;br/\u0026gt;NV indexes] end subgraph Crypto engine CC[Cryptographic Coprocessor] RNG[Random Number Generator] KGEN[Key Generation Logic] CC --\u0026gt;|Uses| RNG KGEN --\u0026gt;|Uses| RNG CC --\u0026gt;|Performs| ENC[Encryption/Decryption] CC --\u0026gt;|Performs| SIG[Digital Signature / Hashing] end subgraph Control logic PCR[\u0026#34;Platform Configuration\u0026lt;br/\u0026gt;Registers\u0026lt;br/\u0026gt;(PCRs)\u0026#34;] MM[Command Processing\u0026lt;br/\u0026gt;Engine /\u0026lt;br/\u0026gt;State Machine] end end style TPM fill:#5c2d91,stroke:#fff,stroke-width:2px,color:#fff style CC fill:#1f4e79,stroke:#fff,color:#fff style KGEN fill:#1f4e79,stroke:#fff,color:#fff style RNG fill:#2e7d32,stroke:#fff,color:#fff style NV fill:#5d4037,stroke:#fff,color:#fff style VM fill:#5d4037,stroke:#fff,color:#fff style PCR fill:#7b1fa2,stroke:#fff,color:#fff style MM fill:#37474f,stroke:#fff,color:#fff style ENC fill:#455a64,stroke:#fff,stroke-dasharray: 5 5,color:#fff style SIG fill:#455a64,stroke:#fff,stroke-dasharray: 5 5,color:#fff This TPM architecture diagram illustrates the internal components of a Trusted Platform Module, highlighting its secure cryptographic engine, non-volatile and volatile memory, and control subsystems. Key elements include the cryptographic coprocessor, random number generator, and key generation logic, all operating within a hardware-isolated boundary. The platform configuration registers (PCRs) and command processing engine manage system state and policy enforcement, while non-volatile memory stores persistent keys and metadata. Non-volatile storage includes the Endorsement Key (EK), a unique, factory-installed identity key, and the Storage Root Key (SRK), which anchors the TPM\u0026rsquo;s key hierarchy.\nTPM key storage TPM defines four main authorization hierarchies, each rooted in a different seed and intended for various use cases:\nHierarchy Seed Used Purpose Owner Storage Seed Storage keys, general-purpose keys Endorsement Endorsement Seed Identity, attestation (e.g. EK) Platform Platform Seed Firmware-level trust \u0026amp; control Null (None) Ephemeral keys not tied to any seed Each hierarchy has:\nIts own seed Its own authorization policy (e.g., owner password) Its own logical namespace for creating keys We will focus on the Owner hierarchy.\nWhile TPMs are capable of securely storing cryptographic keys, most applications avoid storing keys directly in the TPM in practice. This is because the amount of available non-volatile storage varies significantly between TPM models and is often limited. Instead, keys are typically generated or loaded temporarily into the TPM or stored externally in encrypted form and only used inside the TPM when needed.\nTPM key hierarchy model In addition to the authorization hierarchies, the TPM organizes keys in a hierarchy. This model helps balance performance, security, and the TPM\u0026rsquo;s limited storage.\nStorage Seed ‚Üí Parent Key ‚Üí Child Key\nStorage seed (Owner) The seed is a non-exportable, hardware-internal value that acts as the TPM\u0026rsquo;s true root key. You can\u0026rsquo;t access it, but you can use it indirectly. The storage seed cannot be modified.\nParent key Parent keys are stored persistently in TPM non-volatile memory or reloaded as needed. A parent key is any key used to encrypt (wrap) one or more child keys. The parent key must be already loaded in the TPM to load/use any of its children.\nWhen creating a parent key, the TPM does not randomly generate the key unless you explicitly ask it to. Instead, if you provide:\nThe same hierarchy (e.g., the Owner ties it to the storage seed) The same key template (same attributes, algorithms, policy) The same authorization (e.g., null password) \u0026hellip; then the TPM will derive the exact same key every time.\nThe TPM uses a deterministic KDF (key derivation function). This determinism means the application does not need to store the parent key explicitly. It can be recreated when needed.\nCreate a parent key example tpm2-tools is the official CLI toolset for interacting with TPM 2.0 via the TPM2 Software Stack (tss). Install it on Ubuntu/Debian like:\nsudo apt update sudo apt install tpm2-tools The tpm2-tss should already be installed or pulled in as part of tpm2-tools installation. You can check for these libraries with: dpkg -l | grep libtss2\nTo interact with the TPM device using tpm2-tools, the user must either be root or a member of the tss group, which has access to /dev/tpmrm0. To add the user to the tss group, you can:\nsudo usermod -aG tss $USER Then log out and log back in for the group change to take effect.\nOur examples use tpm2-tools version 5.6. See the tpm2-tools documentation for details. To create a transient parent key in TPM 2.0 using TSS (tpm2-tools) CLI, use the tpm2_createprimary command:\ntpm2_createprimary \\ --hierarchy=owner \\ --key-algorithm=rsa \\ --hash-algorithm=sha256 \\ --attributes=\u0026#34;fixedtpm|fixedparent|sensitivedataorigin|userwithauth|decrypt|restricted\u0026#34; \\ --key-context=parent.ctx This command creates the parent.ctx context file, which can be used in later commands.\nBelow is an equivalent example of creating the parent key using the go-tpm library:\n// Create a parent key template with the required attributes parentTemplate := tpm2.New2B(tpm2.TPMTPublic{ Type: tpm2.TPMAlgRSA, NameAlg: tpm2.TPMAlgSHA256, ObjectAttributes: tpm2.TPMAObject{ FixedTPM: true, // bound to TPM that created it FixedParent: true, // Required SensitiveDataOrigin: true, // key material generated internally UserWithAuth: true, // Required, even if we use nil password Decrypt: true, // Allows key to be used for decryption/unwrapping Restricted: true, // Limits use to decryption of child keys }, Parameters: tpm2.NewTPMUPublicParms( tpm2.TPMAlgRSA, \u0026amp;tpm2.TPMSRSAParms{ KeyBits: 2048, Symmetric: tpm2.TPMTSymDefObject{ Algorithm: tpm2.TPMAlgAES, KeyBits: tpm2.NewTPMUSymKeyBits( tpm2.TPMAlgAES, tpm2.TPMKeyBits(128), ), Mode: tpm2.NewTPMUSymMode( tpm2.TPMAlgAES, tpm2.TPMAlgCFB, ), }, }, ), }) primaryKey, err := tpm2.CreatePrimary{ PrimaryHandle: tpm2.TPMRHOwner, InPublic: parentTemplate, }.Execute(t.device) if err != nil { return err } Child key A child key in TPM 2.0 is created under a parent key (which must already exist or be loaded). The child key is wrapped (encrypted) by the parent key and is not usable on its own‚Äîit must be unwrapped (loaded) by the TPM using the correct parent key.\nTo create a child ECC key using tpm2-tools, use the tpm2-create command, like:\ntpm2_create \\ --parent-context=parent.ctx \\ --key-algorithm=ecc_nist_p256 \\ --hash-algorithm=sha256 \\ --attributes=\u0026#34;fixedtpm|fixedparent|sensitivedataorigin|userwithauth|sign|decrypt\u0026#34; \\ --public=child.pub \\ --private=child.priv This command creates the child.pub and child.priv files which can be used in later commands.\nTo load the child key into the TPM using tpm2-tools, use the tpm2_load command, like:\ntpm2_load \\ --parent-context=parent.ctx \\ --public=child.pub \\ --private=child.priv \\ --key-context=child.ctx This command creates the child.ctx context file which can be used later.\nBelow is an equivalent example of creating the child key using the go-tpm library, loading the key into the TPM, and saving the context for subsequent reuse:\n// Create an ECC key template for the child key eccTemplate := tpm2.New2B(tpm2.TPMTPublic{ Type: tpm2.TPMAlgECC, NameAlg: tpm2.TPMAlgSHA256, ObjectAttributes: tpm2.TPMAObject{ FixedTPM: true, FixedParent: true, SensitiveDataOrigin: true, UserWithAuth: true, // Required even if the password is nil SignEncrypt: true, Decrypt: true, }, Parameters: tpm2.NewTPMUPublicParms( tpm2.TPMAlgECC, \u0026amp;tpm2.TPMSECCParms{ CurveID: curveID, }, ), }) // Create the key under the transient parent createKey, err := tpm2.Create{ ParentHandle: parentKeyHandle, InPublic: eccTemplate, }.Execute(t.device) if err != nil { return err } // Load the key loadedKey, err := tpm2.Load{ ParentHandle: parentKeyHandle, InPrivate: createKey.OutPrivate, InPublic: createKey.OutPublic, }.Execute(t.device) if err != nil { return err } // Save the key context keyContext, err := tpm2.ContextSave{ SaveHandle: loadedKey.ObjectHandle, }.Execute(t.device) if err != nil { return err } üìÅ Where can a child key be persisted? The most common approach is to persist the child keys outside the TPM.\nYou store the child\u0026rsquo;s public key and private key blob on a disk, in a secure database, or in a file system. When you need to use the key, you load it into the TPM This storage approach is safe because:\nThe private key is always encrypted with the TPM\u0026rsquo;s parent key Only the correct TPM with the correct parent key can decrypt/use it There is also a de facto standard for storing TPM 2.0 key files using ASN.1 structure. OpenConnect VPN and several other tools use this standard.\nTPM signing TPM can sign data using the child key. The output signature is in raw binary.\nTo sign using tpm2-tools, use the tpm2_sign command, like:\n# Create dummy data and a dummy digest echo \u0026#34;hello world\u0026#34; \u0026gt; data.in.raw cat data.in.raw | openssl dgst -sha256 -binary \u0026gt; digest.bin # Sign the digest tpm2_sign \\ --key-context=child.ctx \\ --hash-algorithm=sha256 \\ --digest \\ --format=plain \\ --signature=signature.plain digest.bin # Now we will verify the signature using openssl. # First, we need the child public key in PEM format. tpm2_readpublic \\ --object-context=child.ctx \\ --output=child.pem \\ --format=pem # Now, we can verify the signature. openssl dgst \\ -sha256 \\ -verify=child.pem \\ -keyform=pem \\ -signature=signature.plain \\ data.in.raw Below is an example of signing using the go-tpm library:\nsign := tpm2.Sign{ KeyHandle: childKeyHandle, Digest: tpm2.TPM2BDigest{ Buffer: digest, }, InScheme: tpm2.TPMTSigScheme{ Scheme: tpm2.TPMAlgECDSA, Details: tpm2.NewTPMUSigScheme( tpm2.TPMAlgECDSA, \u0026amp;tpm2.TPMSSchemeHash{ HashAlg: hashAlg, }, ), }, Validation: tpm2.TPMTTKHashCheck{ Tag: tpm2.TPMSTHashCheck, }, } rsp, err := sign.Execute(tpm) if err != nil { return err } // Get the ECDSA signature ecdsaSig, err := rsp.Signature.Signature.ECDSA() if err != nil { return err } Further reading Mutual TLS intro and hands-on example: mTLS Hello World A quick and practical introduction to mutual TLS (mTLS), including how client certificate authentication works and how to try it with a simple example.\nComparing mTLS and HTTP signatures A practical guide to securing HTTP requests with transport-layer or application-layer authentication.\nWhat is a VLAN and why you need it in your home network Learn how VLANs isolate devices to improve home network security and what you need to configure on your router, switch, and access points.\nWatch the explanation of securing private keys with TPM Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-07-02T00:00:00Z","image":"https://victoronsoftware.com/posts/how-to-use-tpm/tpm-key-hierarchy-headline_hu_13986ebdd853f4c7.png","permalink":"https://victoronsoftware.com/posts/how-to-use-tpm/","title":"How to use TPM 2.0 to secure private keys"},{"content":" Monday: Arrival Tuesday: Speaker\u0026rsquo;s dinner Wednesday: Panel discussion Thursday: Showtime Friday: Conclusion Giving the first speech at a conference was an emotional rollercoaster. When I left home, flew overseas, checked into the hotel, and unpacked, there was a constant undercurrent of anxiety that was not going away. And it was going to stay with me for the next several days. Every so often, I remembered what was coming, and my stomach tightened, a wave of vulnerability washing over me. There was a heavy fear of the unknown‚ÄîI\u0026rsquo;d never been to this city, the audience was not Americans, and the conference was held in a movie theater. The anticipation was exhausting, yet I couldn\u0026rsquo;t relax.\nI will never forget the DevDays Europe 2025 conference. When an experience is highly charged‚Äîwhether with fear, excitement, anxiety, or even joy‚Äîthe brain marks it as important, and we remember more details about it. This situation is called emotional arousal, and it\u0026rsquo;s why people remember things like their first kiss, a car accident, or, in this case, a first conference talk. So the anxiety, vulnerability, and fear you feel before your first big speech don\u0026rsquo;t just make it more challenging‚Äîthey also make it unforgettable.\nThe MC introducing the next speaker. Photo courtesy of Data Miner. Monday: Arrival Of course, I knew I was going to be anxious. At the start of my career, I attended Toastmasters over 20 years ago, and I knew all about the fear of public speaking. I knew the best weapon against this fear was preparation. So, I prepared. I started on my slide deck two months before the conference. One month before, I finished the deck and recorded two dry runs of my talk. Fleet\u0026rsquo;s co-founder, Zach Wasserman, helped me with the slides and provided invaluable feedback. I did four more full dry runs of the 1-hour talk, tweaking the talk a little bit each time. And yes, preparing and practicing the talk took way longer than expected.\nOne hour is a long time for a talk. During practice, I noticed my mouth was drying, and I began losing my voice around 40 minutes in. I don\u0026rsquo;t talk much during my typical day, spending most of my time coding. I just needed more practice talking this long in a single stretch.\nI also noticed that I was getting tired halfway through my talk and had difficulty keeping my energy level up. I tried to motivate myself and give myself a pep talk:\nI\u0026rsquo;m an experienced software engineer, and I like to encourage and teach others The audience is my people‚Äîengineers, and I want to spend time with them This is an exciting new experience, and I should be excited I missed my connecting flight in Frankfurt due to slow passport control and arrived Monday evening. The conference started on Wednesday, with a speaker\u0026rsquo;s dinner the night before. I arrived early to be well-rested for the speaker\u0026rsquo;s dinner and the conference. I also didn\u0026rsquo;t eat on the plane to try and help my body adjust to the 8-hour time change.\nPhoto by Augustas Did≈ægalvis The conference organizers set up a WhatsApp chat for the speakers. The chat was very active, with people discussing their arrival times and things to do around town. It was great to have a community on arrival already.\nTuesday: Speaker\u0026rsquo;s dinner I worked, practiced my talk, and visited a local grocery store during the day.\nIn the evening, I walked to La Boheme restaurant from my hotel. At the drinks reception, I spoke to many Europeans. However, I had a hard time understanding their accents at first. The thought running through my mind was, why did I apply to a European conference? I can\u0026rsquo;t even understand these people. This is going to suck.\nYes, it seemed difficult to understand some accents initially, but my brain adjusted to the patterns, and it was fine.\nI had a great time with the other speakers, including a long chat with Paul Conroy about AI and other topics. One of the top benefits of conferences is chatting with other really smart people who are curious and passionate about technology.\nAfter dinner, we took a walking tour of Vilnius Old Town. This outing was also a great decision by the organizers, as I probably wouldn\u0026rsquo;t have done it myself.\nTour guide talks about the Vilnius Presidential Palace When I returned to the hotel after 10 pm, I had no interest in sleeping. I was excited by all the new experiences and anxious about the next day.\nWednesday: Panel discussion I ended up falling asleep around 6 am and woke up with my 8 am alarm.\nIn the speaker\u0026rsquo;s chat, I saw that a couple of speakers could not make it to the conference, and conference organizers were looking for anyone to give another talk. Although I had ideas for other talks, I had nothing ready. Other speakers who had already given talks at different conferences jumped in.\nWhen I arrived at the conference, its scale hit me. All these professionals were mature, intelligent, technical people. Hundreds of people. Most of them were going to watch my talk. It was unbelievable.\nI came to the main hall and sat down for the keynote. It was the biggest auditorium in the movie theater. There were many volunteers, an MC, and an audio-visual technician controlling the excellent sound system, the giant extra-wide screen, and the bright lights for the stage. There were also video cameras and photographers. Wow, talk about intimidating for a first-time speaker. Were they expecting a TED talk from me?\nSuddenly, the lights dimmed, and a male acrobat came out and started dancing on a pole. Then, he was joined by a female acrobat. I thought, where am I? That\u0026rsquo;s not something I expected at a conference. Wait, will there be a show like this before my talk, too?\nPhoto courtesy of Data Miner. The opening keynote speaker, Romano Roth, was great. I noticed he used a hand-held clicker to advance the slides, which also doubled as a laser pointer. Wow, I didn\u0026rsquo;t even think of that. A fellow speaker mentioned that presenters typically bring their own clickers to conferences.\nA few months ago, after my talk proposal was accepted, Ugne Krasauskaite, the organizer, asked who was interested in participating in a panel. At first, I wasn\u0026rsquo;t sure‚Äîwasn\u0026rsquo;t one talk enough, and wouldn\u0026rsquo;t I already be stressed? Apparently not. When I do something, I want to do it right, and I want to do it all. I was going to be at the conference anyway, so I figured I might as well get some experience with my first conference panel.\nSo, my panel was on the first day of the conference, and my talk was on the second. I would get the lay of the land and practice being on stage.\nLike a good boy, I wanted to prepare for the panel‚Äîto know what I would say and the questions the moderator would ask. However, that\u0026rsquo;s not always the case. The moderator may not tell you the questions upfront or rely on the audience for the questions. I figured this was my first time on a panel, so I could always say, \u0026ldquo;Yes, I agree with my fellow panelist.\u0026rdquo;\nThe panelists got hand mics. When Kenneth Rohde Christiansen, the moderator, introduced me on stage, I started speaking and immediately heard my voice boom via the movie theater speakers. I knew my mic was on.\nPhoto courtesy of Data Miner. I was nervous the whole time. It was like a job interview‚ÄîI didn\u0026rsquo;t know the questions, but I better have some good answers. My fellow panelists were extremely knowledgeable and impressive. I was glad to ride on their coattails and make a comment here and there. The other panelists were Romano Roth, Paul Dragoonis, and Alex Olivier.\nPhoto courtesy of Data Miner. After the last talk, I asked the audio-visual technician if I could test the connection. It was pretty straightforward. There was a USB-C hub that you plug into your computer. The hub connects the external monitor and the presentation clicker. I was glad I would have a hand clicker. A monitor in front of the stage showed what was presented on the massive screen behind the speaker.\nI do not recommend being jet lagged for your first conference talk. Since I didn\u0026rsquo;t sleep the night before, my brain felt like goo. I wanted to meet up with fellow speakers after the day but also needed sleep. Back at the hotel, I took a sleeping pill and slept for 11 hours that night.\nThursday: Showtime I\u0026rsquo;ve been in tech for over 25 years, but why didn\u0026rsquo;t I speak at a conference before? The simple answer is that my career and personal goals never really aligned with conference speaking. I was always looking to do something different in my career than what I was doing at the time. Whether it was to get a master\u0026rsquo;s, switch my technical area from hardware to software, become a manager, try to get my startup off the ground, etc. Speaking at conferences seemed like a huge time-sink that wouldn\u0026rsquo;t help me with my shorter-term goals. Also, I didn\u0026rsquo;t think conferences were valuable due to travel time, preparation, and time away from potentially more useful activities.\nSo, what changed? I\u0026rsquo;ve gotten calmer and more mature. I decided to try out conference speaking and see if I liked it. Also, as a senior developer at Fleet, getting external exposure seemed like a good idea. Plus, I had written enough blog content to repackage some of it as conference talks.\nOn the day of my talk, I put on the nice sweater my wife packed for me. \u0026ldquo;You\u0026rsquo;re going to Europe,\u0026rdquo; she told me, \u0026ldquo;so you need to dress properly.\u0026rdquo; The day was humid and warm, so I started sweating when I got in the Uber to go to the venue. The sun was shining at me through the window of the moving car, which made things even worse. I moved to the other side of the vehicle, and the driver looked at me suspiciously. I ended up sweating most of the day.\nPhoto courtesy of Data Miner. My talk was the closing keynote of the day. As I watched other people\u0026rsquo;s talks, I saw that they all used a dark theme‚Äîa dark background for their slides‚Äîand it looked great on the large movie screens. My talk used the light Fleet template. Well, I thought, I can\u0026rsquo;t do anything about that now.\nI spent the hour before my talk in the speaker\u0026rsquo;s lounge. I reviewed my slides and then sat there, paying attention to my emotions. I wished my talk was over already. I reminded myself that this was an exciting experience, and I should savor it. Then, I walked down into the main hall where I would be presenting.\n\u0026ldquo;You look pretty calm,\u0026rdquo; Mattias, a conference volunteer, said.\n\u0026ldquo;Well, I prepared as much as I could. Can\u0026rsquo;t do much else at this point,\u0026rdquo; I replied.\nI left my phone and my conference badge in my bag. The audio-visual technician put on the Britney Spears mic and attached it to my clothes. I was ready to go.\nI remembered another piece of advice‚Äîask someone to take pictures of you so you can share them on LinkedIn. Although the conference had professional photographers, those pictures would not be available for a few days. I asked Ricky, the MC, to take some photos of me.\nThere were no acrobats before my talk, but there was a group game where people could win prizes. People were pouring in‚ÄîI\u0026rsquo;m guessing around 300 people in the seats.\nThe game consisted of 10 multiple-choice questions, and people voted with their phones. As I sat there watching, I wondered how weird it would be if I were also playing this game and then I won.\nAnd then it was time‚Äîtime for action, time to let my practice take over. There was no time to think or worry. Stick to the script, try to make eye contact with the audience, and move around once in a while.\nPhoto courtesy of Data Miner. The clicker may have been a mistake. To use the laser, I always looked down to make sure I was pressing the right button. Miro Horacek later told me he saw me having trouble with the thing. I didn\u0026rsquo;t think it was so obvious at the time. The lesson here is to practice with the clicker. You want to be pressing the right buttons without having to look down.\nAbout halfway through the talk, my nerves calmed down. I felt so calm that it was weird. I felt like I was simply talking to another engineer next to me. It\u0026rsquo;s unbelievable how the brain works and how quickly you can get used to the patterns around you. I was in front of a giant screen, in the bright lights, with 100s of people looking at me (plus some unknown number watching the live stream). This calm experience is often called settling in or stage comfort.\nI finished my presentation five minutes faster than any of my practice runs. Afterward, I answered a few questions. I didn\u0026rsquo;t understand a couple of them, so I asked the person to come to talk to me afterward.\nPhoto courtesy of Data Miner. Ricky forgot to take pictures during my presentation, so I got back up on stage, and he took one as if I were still presenting.\nI was high with excitement and relief as I headed to the after-party. Several people made positive comments about my talk. It felt like a great evening. I was riding the post-performance high (also known as speaker\u0026rsquo;s high). This high was so nice that I could see how it could be addicting‚Äîyou purposefully put yourself into high-stakes public events to get it.\nJon Bennalick, Alfonso Sandoval Rosas, Victor Lyuboslavsky, and Eleni Grosdouli After the party, I had a great dinner with three other speakers, Jon Bennalick, Kalle Sirkesalo, and Miro Horacek.\nFriday: Conclusion I attended a couple of panels on Friday, and I noticed that when a panelist was speaking, they were on camera and taking up the entire giant screen behind them. Now that\u0026rsquo;s a close-up. It\u0026rsquo;s a good thing I wasn\u0026rsquo;t aware of that during my panel.\nBesides the great talks, a considerable value of the conference comes from talking with other smart people, also known as the hallway track. As a speaker, it felt easy to start a conversation with other speakers by asking about their talk, etc. A few people approached me to discuss my talk, and these discussions were also educational.\nPhoto courtesy of Data Miner. People tend to be friendlier and more open in any new situation, such as a conference. So, I left my comfort zone and talked to other engineers. It\u0026rsquo;s not often that you get so much technical talent in one place. I had some great conversations with Dmytro Sukhariev.\nSpeakers\u0026#39; photo. Courtesy of Data Miner. Further reading The original article the talk was based on: Readable code\nKeynote | Readable Code: The Secret to Clean, Productive Software Panel Discussion | Building the Future: Trends in Modern Application Architecture Watch me discuss my first conference talk experience Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-05-28T00:00:00Z","image":"https://victoronsoftware.com/posts/first-conference-talk/devdays-talk-headline_hu_9bdb507f047f0452.jpg","permalink":"https://victoronsoftware.com/posts/first-conference-talk/","title":"My first conference talk experience"},{"content":"Tracking engineering metrics doesn\u0026rsquo;t have to mean signing up for another expensive SaaS tool. This guide will show you how to build a flexible, powerful, and completely free metrics dashboard using open-source tools like Grafana and a fast analytical database. Whether you\u0026rsquo;re monitoring cycle time, bug fix velocity, or pull request activity, this setup gives you deep visibility that your team will actually want to look at.\nIn our previous engineering metrics guide, we showed a basic way to track metrics using GitHub Actions and the Google Sheets API. The data in Google Sheets can be visualized using a tool like Google\u0026rsquo;s Looker Studio. However, we wanted more powerful visualizations that could be dynamically sliced, diced, and drilled down into. Also, we wanted the tools to be free so we wouldn\u0026rsquo;t need to justify the costs to our management.\nThe free engineering benchmarks stack As in our previous example, we used GitHub Actions to schedule our metrics collection and compute the data we wanted to track.\nFor visualizations, we decided on Grafana. Grafana is a leader in visualizing data, is open source, and can be self-hosted. We used Grafana\u0026rsquo;s free cloud tier for this example since we didn\u0026rsquo;t want to spin up any infrastructure ourselves.\nHowever, the problem with Grafana Cloud\u0026rsquo;s free tier is that data is retained for only 14 days. We wanted to keep and compare our engineering metrics year over year. Due to its generous free tier, we decided to use Google\u0026rsquo;s BigQuery analytics database.\nStorage: The first 10 GiB per month is free. Queries: The first 1 TiB of query data processed per month is free. We created a project in Google Cloud Console, created a dataset (github_metrics), made a service account with BigQuery roles, and created a JWT key for the service account to use with our app.\nNote: BigQuery has a sandbox mode that does not require a credit card. However, it has some API limitations. We recommend enabling billing on the account and monitoring your usage to ensure you stay within the free limits.\nPicking the metric to track: pickup time There are many engineering benchmark metrics we could have started with, but we decided to start with one that we\u0026rsquo;ve been keenly aware of at many points in our development life. When developers finish implementing and testing their feature and put it up for review, sometimes the PR sits there for days at a time. This waiting time is a frustrating experience for developers, especially if they or others depend on those changes to make progress in their work. This time to review has a direct impact on engineering velocity.\nWe define pickup time as the time from when the PR is marked as ready for review until the code review is done. The reviewer does not need to approve the change; they can also reject or simply comment on it. We are measuring the time from when the developer finishes their work until they get feedback on their work.\nImplementing metrics gathering Since we knew our code would run in a GitHub Actions workflow, we implemented it as a reusable GitHub action. We decided to use JavaScript (not TypeScript) to simplify the action.\nWe used the octokit.js GitHub API client to fetch the pull requests from our repo, extract the timeline events and review events from the PRs, and calculate the pickup time.\nIn our initial review of the data, we noticed occasional spikes in pickup time on Mondays. That\u0026rsquo;s because pull requests created on Friday were sitting around over the weekend until being reviewed on Monday. Since we don\u0026rsquo;t expect our software developers to work weekends, we removed weekends from our calculations. Removing weekends was one of the most time-consuming implementation details. We created a unit test to make sure we got the details right.\nWe added a --print-only option to our program to manually examine the data before we uploaded it to the analytical database.\nTo interface with BigQuery, we used Google\u0026rsquo;s BigQuery Node.js client. Our table schema was:\nfields: [ { name: \u0026#39;review_date\u0026#39;, type: \u0026#39;DATE\u0026#39;, mode: \u0026#39;REQUIRED\u0026#39; }, { name: \u0026#39;pr_creator\u0026#39;, type: \u0026#39;STRING\u0026#39;, mode: \u0026#39;REQUIRED\u0026#39; }, { name: \u0026#39;pr_url\u0026#39;, type: \u0026#39;STRING\u0026#39;, mode: \u0026#39;REQUIRED\u0026#39; }, { name: \u0026#39;pickup_time_seconds\u0026#39;, type: \u0026#39;INTEGER\u0026#39;, mode: \u0026#39;REQUIRED\u0026#39; }, { name: \u0026#39;repository\u0026#39;, type: \u0026#39;STRING\u0026#39;, mode: \u0026#39;REQUIRED\u0026#39; }, { name: \u0026#39;pr_number\u0026#39;, type: \u0026#39;INTEGER\u0026#39;, mode: \u0026#39;REQUIRED\u0026#39; }, { name: \u0026#39;target_branch\u0026#39;, type: \u0026#39;STRING\u0026#39;, mode: \u0026#39;REQUIRED\u0026#39; }, { name: \u0026#39;ready_time\u0026#39;, type: \u0026#39;TIMESTAMP\u0026#39;, mode: \u0026#39;REQUIRED\u0026#39; }, { name: \u0026#39;first_review_time\u0026#39;, type: \u0026#39;TIMESTAMP\u0026#39;, mode: \u0026#39;REQUIRED\u0026#39; } ] The pr_number was the primary key. Once we calculate and save the pickup time for a PR, we do not update it on future runs.\nCreating Grafana chart In Grafana, we created a data source connected to our BigQuery database. Then, we created a new dashboard with a new pickup time chart. We had to decide how to visualize the data. We decided on:\n7-day moving average filter by time range filter by GitHub username filter by user group (e.g., engineering) include the PR numbers for each data point for drill down We created Grafana variables to filter by username/group.\nCreating the correct SQL query that gathered all the data and integrated with Grafana\u0026rsquo;s features took considerable time. Below is the query we came up with:\nWITH daily AS ( SELECT DATE(first_review_time, \u0026#34;America/Chicago\u0026#34;) AS day, COUNT(*) AS pr_count, SUM(pickup_time_seconds) AS total_seconds FROM `engineering-metrics-123456.github_metrics.pr_pickup_time` WHERE first_review_time BETWEEN TIMESTAMP_SUB(TIMESTAMP_MILLIS($__from), INTERVAL 6 DAY) AND TIMESTAMP_MILLIS($__to) AND ( -- No filters selected: show all ( (\u0026#39;${user:csv}\u0026#39; = \u0026#39;\u0026#39; OR \u0026#39;${user:csv}\u0026#39; = \u0026#39;__all\u0026#39;) AND (\u0026#39;${user_group:csv}\u0026#39; = \u0026#39;\u0026#39; OR \u0026#39;${user_group:csv}\u0026#39; = \u0026#39;__all\u0026#39;) ) OR ( -- Only user filter applied (\u0026#39;${user:csv}\u0026#39; != \u0026#39;__all\u0026#39; AND \u0026#39;${user:csv}\u0026#39; != \u0026#39;\u0026#39;) AND (\u0026#39;${user_group:csv}\u0026#39; = \u0026#39;\u0026#39; OR \u0026#39;${user_group:csv}\u0026#39; = \u0026#39;__all\u0026#39;) AND pr_creator IN UNNEST(SPLIT(\u0026#39;${user:csv}\u0026#39;, \u0026#39;,\u0026#39;)) ) OR ( -- Only user_group filter applied (\u0026#39;${user_group:csv}\u0026#39; != \u0026#39;__all\u0026#39; AND \u0026#39;${user_group:csv}\u0026#39; != \u0026#39;\u0026#39;) AND (\u0026#39;${user:csv}\u0026#39; = \u0026#39;\u0026#39; OR \u0026#39;${user:csv}\u0026#39; = \u0026#39;__all\u0026#39;) AND pr_creator IN UNNEST(SPLIT(\u0026#39;${user_group:csv}\u0026#39;, \u0026#39;;\u0026#39;)) ) OR ( -- Both filters applied ‚Üí take intersection (\u0026#39;${user:csv}\u0026#39; != \u0026#39;__all\u0026#39; AND \u0026#39;${user:csv}\u0026#39; != \u0026#39;\u0026#39;) AND (\u0026#39;${user_group:csv}\u0026#39; != \u0026#39;__all\u0026#39; AND \u0026#39;${user_group:csv}\u0026#39; != \u0026#39;\u0026#39;) AND pr_creator IN ( SELECT val FROM UNNEST(SPLIT(\u0026#39;${user:csv}\u0026#39;, \u0026#39;,\u0026#39;)) val INTERSECT DISTINCT SELECT val FROM UNNEST(SPLIT(\u0026#39;${user_group:csv}\u0026#39;, \u0026#39;;\u0026#39;)) val ) ) ) GROUP BY day ), calendar AS ( -- Build list of days in the visible Grafana range SELECT day FROM UNNEST( GENERATE_DATE_ARRAY( DATE(TIMESTAMP_MILLIS($__from)), DATE(TIMESTAMP_MILLIS($__to)) ) ) AS day ), rolling_avg AS ( SELECT c.day, TIMESTAMP(CONCAT(CAST(c.day AS STRING), \u0026#39; 12:00:00\u0026#39;)) AS time, -- True 7-day weighted average ( SELECT SUM(d.total_seconds) FROM daily d WHERE d.day BETWEEN DATE_SUB(c.day, INTERVAL 6 DAY) AND c.day ) / ( SELECT SUM(d.pr_count) FROM daily d WHERE d.day BETWEEN DATE_SUB(c.day, INTERVAL 6 DAY) AND c.day ) / 3600 AS moving_avg_hours FROM calendar c ) SELECT r.time, r.moving_avg_hours, ( -- Optional: attach PR numbers per window SELECT ARRAY_AGG(DISTINCT pr_number) FROM `engineering-metrics-459517.github_metrics.pr_pickup_time` p WHERE DATE(p.first_review_time, \u0026#34;America/Chicago\u0026#34;) BETWEEN DATE(DATE_SUB(r.time, INTERVAL 6 DAY)) AND DATE(r.time) AND ( -- No filters selected: show all ( (\u0026#39;${user:csv}\u0026#39; = \u0026#39;\u0026#39; OR \u0026#39;${user:csv}\u0026#39; = \u0026#39;__all\u0026#39;) AND (\u0026#39;${user_group:csv}\u0026#39; = \u0026#39;\u0026#39; OR \u0026#39;${user_group:csv}\u0026#39; = \u0026#39;__all\u0026#39;) ) OR ( -- Only user filter applied (\u0026#39;${user:csv}\u0026#39; != \u0026#39;__all\u0026#39; AND \u0026#39;${user:csv}\u0026#39; != \u0026#39;\u0026#39;) AND (\u0026#39;${user_group:csv}\u0026#39; = \u0026#39;\u0026#39; OR \u0026#39;${user_group:csv}\u0026#39; = \u0026#39;__all\u0026#39;) AND pr_creator IN UNNEST(SPLIT(\u0026#39;${user:csv}\u0026#39;, \u0026#39;,\u0026#39;)) ) OR ( -- Only user_group filter applied (\u0026#39;${user_group:csv}\u0026#39; != \u0026#39;__all\u0026#39; AND \u0026#39;${user_group:csv}\u0026#39; != \u0026#39;\u0026#39;) AND (\u0026#39;${user:csv}\u0026#39; = \u0026#39;\u0026#39; OR \u0026#39;${user:csv}\u0026#39; = \u0026#39;__all\u0026#39;) AND pr_creator IN UNNEST(SPLIT(\u0026#39;${user_group:csv}\u0026#39;, \u0026#39;;\u0026#39;)) ) OR ( -- Both filters applied ‚Üí take intersection (\u0026#39;${user:csv}\u0026#39; != \u0026#39;__all\u0026#39; AND \u0026#39;${user:csv}\u0026#39; != \u0026#39;\u0026#39;) AND (\u0026#39;${user_group:csv}\u0026#39; != \u0026#39;__all\u0026#39; AND \u0026#39;${user_group:csv}\u0026#39; != \u0026#39;\u0026#39;) AND pr_creator IN ( SELECT val FROM UNNEST(SPLIT(\u0026#39;${user:csv}\u0026#39;, \u0026#39;,\u0026#39;)) val INTERSECT DISTINCT SELECT val FROM UNNEST(SPLIT(\u0026#39;${user_group:csv}\u0026#39;, \u0026#39;;\u0026#39;)) val ) ) ) ) AS pr_numbers_window FROM rolling_avg r ORDER BY r.time In the above query, we used Grafana-specific variables such as $__from, $__to, ${user:csv}, etc.\nWe also had to deal with the timestamp shift problem, where the chart data was off by 1 day, since we were stripping the time from first_review_time but displaying the data in local time (the day before UTC midnight). This statement attempts to fix the problem by changing the time to noon: TIMESTAMP(CONCAT(CAST(c.day AS STRING), '12:00:00')) AS time\nThe final chart looked like this:\nWe can inspect the data to drill down into the PR numbers for each data point:\nWith the PR numbers, we can explore our data using the following query:\n-- Sample query to dig into the details. Replace the PR numbers list. SELECT * FROM `engineering-metrics-123456.github_metrics.pr_pickup_time` WHERE pr_number IN (28625,28570,28417,28658,28382,28538,28608) ORDER BY pickup_time_seconds DESC Creating the GitHub Actions workflow Once everything worked, we set up a GitHub workflow to run daily and automatically update the metrics.\nname: Collect PR Pickup Time Metrics on: schedule: - cron: \u0026#39;0 9 * * *\u0026#39; # Run at 4am CDT (9am UTC) workflow_dispatch: # Allow manual triggering jobs: collect-metrics: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v4 - name: Set up Node.js uses: actions/setup-node@v4 with: node-version: \u0026#39;16\u0026#39; cache: \u0026#39;npm\u0026#39; - name: Install dependencies run: npm ci - name: Create service account key file run: | echo \u0026#39;${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}\u0026#39; \u0026gt; service-account-key.json # Verify the file is valid JSON cat service-account-key.json | jq . \u0026gt; /dev/null - name: Collect and upload metrics uses: ./ with: github-token: ${{ secrets.GITHUB_TOKEN }} config-path: \u0026#39;./config.json\u0026#39; bigquery-project: ${{ secrets.BIGQUERY_PROJECT_ID }} bigquery-dataset: \u0026#39;github_metrics\u0026#39; bigquery-table: \u0026#39;pr_pickup_time\u0026#39; target-branch: \u0026#39;main\u0026#39; lookback-days: \u0026#39;30\u0026#39; print-only: \u0026#39;false\u0026#39; env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} BIGQUERY_PROJECT_ID: ${{ secrets.BIGQUERY_PROJECT_ID }} SERVICE_ACCOUNT_KEY_PATH: \u0026#39;./service-account-key.json\u0026#39; Code on GitHub Our code is available on GitHub at: https://github.com/getvictor/pickup-time\nFurther reading Why transparency beats everything else in engineering\nHow visibility into priorities and work creates accountability and transforms engineering culture.\nHow to spot and reduce code complexity\nA practical guide to the most useful metrics for understanding and improving code structure and maintainability.\nTurning messy code into clean, readable systems\nWhy code readability matters‚Äîand how to measure and improve it for long-term productivity.\nWatch how to set up free engineering metrics Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-05-16T00:00:00Z","image":"https://victoronsoftware.com/posts/engineering-metrics-no-cost/engineering-metrics-headline_hu_be2248040eff2c08.png","permalink":"https://victoronsoftware.com/posts/engineering-metrics-no-cost/","title":"Full-featured engineering metrics‚Äîfor free"},{"content":"Recently, we developed a proof of concept using an MCP(Model Context Protocol) server to control Fleet\u0026rsquo;s device management server. This server made a successful demo video. The takeaways from the video were:\nüß† Natural language understanding ‚Äî no commands to memorize ü§ñ Actionable context awareness ‚Äî the AI remembers what you\u0026rsquo;re asking about üîó Tool selection and orchestration ‚Äî chooses the proper action automatically In this article, we\u0026rsquo;ll talk about our process for integrating our API with agentic AI, mistakes made, and lessons learned.\nWhat is MCP, and why do I need one? MCP (Model Context Protocol) is a client-server framework that enables AI agents to interact with your tools and APIs through natural language. MCP was introduced by Anthropic in November 2024. Its primary use case was to use local tools available on your local machine, such as databases.\nBut, some may ask why we need another framework‚Äîaren\u0026rsquo;t AI agents good enough to use those tools directly? Yes, it is true that by providing AI agents with instructions on how to use an arbitrary tool, the AI agent should be able to use it. However, MCP standardizes this approach. Thus, an MCP server created for one tool can be reused by many developers and by many AI agents.\nInstead of telling your AI agent about every tool, imagine that the AI agent has access to a library of MCP servers that it can install whenever you want to \u0026ldquo;teach\u0026rdquo; it about a tool. Think of MCP as an intelligent middleware that allows AI agents to understand and use your system\u0026rsquo;s capabilities appropriately. With MCP, the AI can dynamically learn and adapt to your system\u0026rsquo;s interface.\nThe MCP protocol The MCP protocol consists of several key components:\nTools: These are functions or actions that can be performed. Tools are defined by name, description, and parameters they accept. They represent the capabilities your system provides to AI agents.\nResources: These are data objects that tools can operate on. Resources have properties and relationships with other resources. For example, a User resource might have properties like name and email.\nPrompts: These are reusable prompt templates and workflows intended to standardize common LLM interactions.\nSchema Definition: The protocol uses a schema to describe available tools and resources, making it easy for AI agents to understand possible actions.\nThe main distinction between tools and resources is that tools are actions (verbs), while resources are objects (nouns) on which these actions may operate. For example, a createUser tool might operate on a User resource. In practice, however, you can create a User tool that returns a user and not worry about associating it with a resource. The semantics of tools are simpler than those of resources, so it might be faster only to use tools if your goal is to get something working quickly.\nBuilding an MCP server with agentic AI Our biggest mistake was asking our AI agent (Roo Code) to build an MCP server without fully understanding the MCP details ourselves. To summarize, this process was not successful.\nMCP has SDKs available for multiple languages. We decided to build our MCP server with TypeScript. Our AI agent pulled in the right TypeScript MCP SDK. However, it couldn\u0026rsquo;t get the compile to work, so it removed the SDK and implemented the MCP protocol from scratch. We allowed our agent to proceed since we didn\u0026rsquo;t know any better.\nAfter implementing a basic tool, we couldn\u0026rsquo;t get the MCP server to work reliably. It worked using curl, but the MCP inspector debug tool couldn\u0026rsquo;t connect to our MCP server. Another mistake we made was not telling the AI agent to use liberal debug messages up front, which would have sped up the debug effort.\nAfter letting our AI agent waste our API credits for too long, we told it to rewrite the code using the TypeScript SDK. We copied and pasted the examples that our agent could reference. After that, things went smoothly.\nAnother engineer suggested that using the Python SDK would have been more effective and required less handholding.\nOur proof of concept MCP server for Fleet API is on GitHub. Here\u0026rsquo;s a code example for the install_software tool:\n// Register the install_software tool this.mcpServer.tool( \u0026#39;install_software\u0026#39;, \u0026#39;Install software on a host managed by Fleet\u0026#39;, { host_id: z.string().describe(\u0026#39;Required. The host ID\u0026#39;), software_id: z.string().describe(\u0026#39;Required. The software title ID\u0026#39;) }, async (params: { host_id: string; software_id: string }) =\u0026gt; { try { console.log(`Installing software ID ${params.software_id} on host ID ${params.host_id}`); const url = `/api/v1/fleet/hosts/${params.host_id}/software/${params.software_id}/install`; const response = await this.axiosInstance.post(url); console.log(\u0026#39;Fleet API install request successful\u0026#39;); return { content: [ { type: \u0026#39;text\u0026#39;, text: JSON.stringify(response.data, null, 2), }, ], }; } catch (error) { console.error(\u0026#39;Fleet API error:\u0026#39;, error); throw { code: \u0026#39;internal_error\u0026#39;, message: `Fleet API error: ${error instanceof Error ? error.message : String(error)}`, }; } } ); As you can see above, the code is pretty straightforward. We\u0026rsquo;re simply wrapping an existing API call with an MCP tool.\nOverall impressions AI agent interactions with our MCP server are complex to test and challenging to debug. First, as an MCP server developer, you must be familiar with AI agents, MCP protocol, and the tool interface you\u0026rsquo;re trying to use. For example, in one case, the AI agent didn\u0026rsquo;t use the proper parameter to call the tool and came up with wrong conclusions. Since we are dealing with AI, it is hard to tell how often such mistakes will happen and also hard to fix or prevent them. Also, we don\u0026rsquo;t know exactly how customers may use the MCP server, so the QA test space is quite large.\nOur product already has an API, a CLI, and a UI interface. Adding MCP to the mix feels hard to justify‚Äîare customers going to use all these ways to interact with our product? For a small product team, investing in MCP support might not make sense until a customer is ready to pay for it.\nFurther reading We recently covered the broader implications of AI agents in software development. Previously, we discussed what every software engineer needs to know about AI right now. Watch how we taught an AI agent to use our product with MCP Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-05-08T00:00:00Z","image":"https://victoronsoftware.com/posts/introducing-mcp/introducing-mcp-headline_hu_5e8cf177119eb22c.png","permalink":"https://victoronsoftware.com/posts/introducing-mcp/","title":"Introducing MCP: Lessons from building an AI-driven server"},{"content":"Fast Continuous Integration (CI) test results are crucial for maintaining a good developer velocity. Quick test results give developers immediate feedback on their changes, resulting in a more enjoyable development process. For critical changes, slow tests can become a bottleneck, delaying deployments.\nPreviously, we covered how to accurately measure the execution time of Go tests. This article will demonstrate one approach to breaking apart a large Go test suite and running each part in parallel. This approach should reduce the CI cycle time, benefitting developers and the organization.\nSplit up the Go test suite Create parallel Go test jobs in CI Understanding your Go test suite The standard way to run all the tests in your Go project is with the following command:\ngo test ./... This command will compile all the Go packages and their tests, each compiling to a separate binary. The Go toolchain will then run each binary in parallel. The -p flag controls this parallel test behavior, which defaults to the number of CPUs on your machine.\nSplitting up the test suite makes sense if you have a lot of packages to test. If you have few packages or if you have 1 or 2 packages that dominate your project\u0026rsquo;s test run time, then a simple split may not help much. You may need to refactor your code or split the tests in a single Go package across multiple CI jobs. Splitting a single package is generally inefficient since each CI job must compile the same package separately, and we will not cover this approach.\nTo find all the packages in your project, you can list them with the following command:\ngo list ./... To identify time-consuming packages, you can run your test suite with the -json switch and save the results. Then, find the elapsed time of each package and sort the times. This operation can be done with jq:\ncat test-result.json | jq -s \u0026#39;map(select(has(\u0026#34;Test\u0026#34;) | not)) | group_by(.Package) | map({package: .[0].Package, elapsed: (map(.Elapsed) | add)}) | sort_by(.elapsed) | reverse\u0026#39; Split up the Go test suite You can specify the packages at the end of the go test command to run a subset of packages in a CI job. For example:\ngo test ./cmd/fleetctl/... ./server/service Next, manually create groups of packages and identify them with a name.\nTo create a catchall group for packages that were not explicitly assigned to a group, you can use the Linux comm command to generate the remaining packages. For example:\ncomm -23 \u0026lt;(go list ./... | sort) \u0026lt;({ go list ./cmd/fleetctl/... \u0026amp;\u0026amp; go list ./server/service/... ;} | sort) The above command returns the packages unique to the first list, which includes all the packages (./...).\nThe following is a real-world example from Fleet\u0026rsquo;s Makefile that creates test suite groups with identifiers:\n# Set up packages for CI testing. DEFAULT_PKGS_TO_TEST := ./cmd/... ./ee/... ./orbit/pkg/... ./orbit/cmd/orbit ./pkg/... ./server/... ./tools/... # fast tests are quick and do not require out-of-process dependencies (such as MySQL, etc.) FAST_PKGS_TO_TEST := \\ ./ee/tools/mdm \\ ./orbit/pkg/cryptoinfo \\ ./orbit/pkg/dataflatten \\ ./orbit/pkg/keystore \\ ./server/goose \\ ./server/mdm/apple/appmanifest \\ ./server/mdm/lifecycle \\ ./server/mdm/scep/challenge \\ ./server/mdm/scep/x509util \\ ./server/policies FLEETCTL_PKGS_TO_TEST := ./cmd/fleetctl/... MYSQL_PKGS_TO_TEST := ./server/datastore/mysql/... ./server/mdm/android/mysql SCRIPTS_PKGS_TO_TEST := ./orbit/pkg/scripts SERVICE_PKGS_TO_TEST := ./server/service VULN_PKGS_TO_TEST := ./server/vulnerabilities/... ifeq ($(CI_TEST_PKG), main) # This is the bucket of all the tests that are not in a specific group. We take a diff between DEFAULT_PKG_TO_TEST and all the specific *_PKGS_TO_TEST. CI_PKG_TO_TEST=$(shell /bin/bash -c \u0026#34;comm -23 \u0026lt;(go list ${DEFAULT_PKGS_TO_TEST} | sort) \u0026lt;({ \\ go list $(FAST_PKGS_TO_TEST) \u0026amp;\u0026amp; \\ go list $(FLEETCTL_PKGS_TO_TEST) \u0026amp;\u0026amp; \\ go list $(MYSQL_PKGS_TO_TEST) \u0026amp;\u0026amp; \\ go list $(SCRIPTS_PKGS_TO_TEST) \u0026amp;\u0026amp; \\ go list $(SERVICE_PKGS_TO_TEST) \u0026amp;\u0026amp; \\ go list $(VULN_PKGS_TO_TEST) \\ ;} | sort)\u0026#34;) else ifeq ($(CI_TEST_PKG), fast) CI_PKG_TO_TEST=$(FAST_PKGS_TO_TEST) else ifeq ($(CI_TEST_PKG), fleetctl) CI_PKG_TO_TEST=$(FLEETCTL_PKGS_TO_TEST) else ifeq ($(CI_TEST_PKG), mysql) CI_PKG_TO_TEST=$(MYSQL_PKGS_TO_TEST) else ifeq ($(CI_TEST_PKG), scripts) CI_PKG_TO_TEST=$(SCRIPTS_PKGS_TO_TEST) else ifeq ($(CI_TEST_PKG), service) CI_PKG_TO_TEST=$(SERVICE_PKGS_TO_TEST) else ifeq ($(CI_TEST_PKG), vuln) CI_PKG_TO_TEST=$(VULN_PKGS_TO_TEST) else CI_PKG_TO_TEST=$(DEFAULT_PKGS_TO_TEST) endif Create parallel Go test jobs in CI The major CI tools provide a way to start multiple jobs in parallel. In GitHub, this is done with a matrix strategy.\nIn the previous step, we gave an example of named test suites. Now, we feed those names into the GitHub matrix job:\njobs: test-go: strategy: matrix: suite: [\u0026#34;fast\u0026#34;, \u0026#34;fleetctl\u0026#34;, \u0026#34;main\u0026#34;, \u0026#34;mysql\u0026#34;, \u0026#34;scripts\u0026#34;, \u0026#34;service\u0026#34;, \u0026#34;vuln\u0026#34;] os: [ubuntu-latest] runs-on: ${{ matrix.os }} steps: - name: Checkout Code uses: actions/checkout@c85c95e3d7251135ab7dc9ce3241c5835cc595a9 # v3.5.3 - name: Install Go uses: actions/setup-go@0a12ed9d6a96ab950c8f026ed9f722fe0da7ef32 # v5.0.2 with: go-version-file: \u0026#39;go.mod\u0026#39; - name: Run Go Tests run: CI_TEST_PKG=${{ matrix.suite }} make test-go The above workflow runs our test suites in parallel, speeding up our overall CI cycle time.\nFurther reading Recently, we covered analyzing Go build times.\nIn the past, we reviewed the state of fuzz testing in Go.\nWatch how to break apart a large Go test suite Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-04-09T00:00:00Z","image":"https://victoronsoftware.com/posts/large-go-test-suite/large-go-test-suite-headline_hu_b61c66f5351b67a4.png","permalink":"https://victoronsoftware.com/posts/large-go-test-suite/","title":"How to speed up a large Go test suite"},{"content":"Importance of technical diagrams Diagrams are an effective way to communicate technical concepts to other engineers or business stakeholders. They even help us better understand technical concepts while creating the diagrams. Effective senior engineers frequently use diagrams in their documentation and presentations. The top Mermaid diagrams are:\nSequence diagrams Flowcharts Class diagrams Entity relationship diagrams (ERDs) What is Mermaid Mermaid is a popular JavaScript-based diagramming tool that allows anyone to create diagrams using a simple text-based syntax. Some key reasons for its popularity include:\nText-based syntax: Mermaid is easy to write and version control, fitting seamlessly into code reviews and collaborative workflows. Integration with Markdown: Mermaid works well with Markdown, which is widely used in documentation and static site generators. Integration with GitHub and GitLab: These top Git hosting platforms allow you to put Mermaid diagrams almost anywhere on their site. Versatility: Mermaid supports various diagram types, from flowcharts and sequence diagrams to ERDs. Automation-friendly: Diagrams can be generated programmatically, enabling dynamic updates and integration with CI/CD pipelines. Accessibility: Mermaid does not need specialized software; diagrams can be created and rendered within code editors or browsers. In our experience, Mermaid\u0026rsquo;s text-based syntax is simple enough to be used for ad hoc diagramming, as opposed to the proverbial \u0026ldquo;back of the napkin\u0026rdquo; drawing.\nThe above features make Mermaid an invaluable tool for creating maintainable technical diagrams. We recommend creating your first diagram on Mermaid Playground to get started.\nSequence diagrams Sequence diagrams depict how users and entities interact with each other over time. They are handy for modeling the flow of messages, events, or data and are great for visualizing customer use cases, API interactions, or the behavior of distributed systems.\nWe love sequence diagrams because they are clear and consistent. Mermaid displays sequence diagrams consistently, which greatly speeds up understanding.\nBelow is a basic sequence diagram, which starts with the keyword sequenceDiagram.\nsequenceDiagram UI-\u0026gt;\u0026gt;MDM Server: Upload profile MDM Server-\u0026gt;\u0026gt;Apple: Request push notification Apple-\u0026gt;\u0026gt;macOS: Push notification macOS-\u0026gt;\u0026gt;MDM Server: Request command MDM Server-\u0026gt;\u0026gt;macOS: InstallProfile command The above Mermaid diagram is rendered as follows:\nsequenceDiagram UI-\u0026gt;\u0026gt;MDM server: Upload profile MDM server-\u0026gt;\u0026gt;Apple: Request push notification Apple-\u0026gt;\u0026gt;macOS: Push notification macOS-\u0026gt;\u0026gt;MDM server: Request command MDM server-\u0026gt;\u0026gt;macOS: InstallProfile command The above syntax consists of \u0026lt;entity\u0026gt;-\u0026gt;\u0026gt;\u0026lt;entity\u0026gt;: \u0026lt;description\u0026gt;. In many cases, that\u0026rsquo;s all that\u0026rsquo;s necessary. We can quickly create a simple sequence diagram to explain a flow during a meeting.\nTo add a title to your diagram, add the following at the top:\n--- title: My diagram --- To go beyond the basics, we recommend learning some additional syntax from sequence diagram docs:\nactor: Actors (aka people) Aliases --\u0026gt;\u0026gt;: Dotted line with arrow for responses --): Dotted line with empty arrow for async messages activate \u0026lt;entity\u0026gt; and deactivate \u0026lt;entity\u0026gt;: Activations of actors Loops(loop), alternative paths(alt and else), and parallel actions(par) autonumber: Sequence numbers Notes Here is the updated diagram with additional syntax:\n--- title: Upload macOS configuration profile flow --- sequenceDiagram autonumber actor Admin participant mdm as MDM server Admin-\u0026gt;\u0026gt;mdm: Upload profile activate mdm mdm--\u0026gt;\u0026gt;Admin: OK deactivate mdm Note right of mdm: After 30 seconds max mdm-\u0026gt;\u0026gt;Apple: Request push notification activate mdm activate Apple Apple--\u0026gt;\u0026gt;mdm: OK deactivate mdm Apple--)macOS: Push notification deactivate Apple activate macOS macOS-\u0026gt;\u0026gt;mdm: Request command (Idle) activate mdm mdm--\u0026gt;\u0026gt;macOS: InstallProfile command deactivate mdm macOS-\u0026gt;\u0026gt;macOS: Install profile macOS-\u0026gt;\u0026gt;mdm: Acknowledge activate mdm mdm--\u0026gt;\u0026gt;macOS: No more commands deactivate mdm deactivate macOS --- title: Upload macOS configuration profile flow --- sequenceDiagram autonumber actor Admin participant mdm as MDM server Admin-\u0026gt;\u0026gt;mdm: Upload profile activate mdm mdm--\u0026gt;\u0026gt;Admin: OK deactivate mdm Note right of mdm: After 30 seconds max mdm-\u0026gt;\u0026gt;Apple: Request push notification activate mdm activate Apple Apple--\u0026gt;\u0026gt;mdm: OK deactivate mdm Apple--)macOS: Push notification deactivate Apple activate macOS macOS-\u0026gt;\u0026gt;mdm: Request command (Idle) activate mdm mdm--\u0026gt;\u0026gt;macOS: InstallProfile command deactivate mdm macOS-\u0026gt;\u0026gt;macOS: Install profile macOS-\u0026gt;\u0026gt;mdm: Acknowledge activate mdm mdm--\u0026gt;\u0026gt;macOS: No more commands deactivate mdm deactivate macOS Flowcharts Flowcharts are graphical representations of processes, workflows, or decision logic. They use standardized symbols, such as rectangles, diamonds, and arrows, to illustrate the flow of steps or tasks visually. Flowcharts are generic diagrams that can be adapted to any purpose. They are often used to explain concepts to non-technical stakeholders.\nBelow is a basic flowchart diagram:\nflowchart Admin-- \u0026#34;Uses\u0026#34; ---\u0026gt;Server agent-- \u0026#34;Checks in\u0026#34; ---\u0026gt;Server Server-- \u0026#34;Accesses\u0026#34; ---\u0026gt;MySQL Server-- \u0026#34;Accesses\u0026#34; ---\u0026gt;Redis Server-- \u0026#34;Upload analytics\u0026#34; ---\u0026gt;fleetdm.com Which renders as follows:\nflowchart Admin-- \u0026#34;Uses\u0026#34; ---\u0026gt;Server agent-- \u0026#34;Checks in\u0026#34; ---\u0026gt;Server Server-- \u0026#34;Accesses\u0026#34; ---\u0026gt;MySQL Server-- \u0026#34;Accesses\u0026#34; ---\u0026gt;Redis Server-- \u0026#34;Upload analytics\u0026#34; ---\u0026gt;fleetdm.com The above example is a simple block diagram that can quickly demonstrate the basic parts of your system/container/component. Explore additional syntax at flowchart docs.\nHere is another flowchart example:\nflowchart LR S([Start]) PD[Process data] E([End]) S --\u0026gt; PD PD --\u0026gt; V{Valid data?} V -- \u0026#34;No\u0026#34; ---\u0026gt; E V -- \u0026#34;Yes\u0026#34; ---\u0026gt; W[Send webhook] W --\u0026gt; E Which renders as follows:\nflowchart LR S([Start]) PD[Process data] E([End]) S --\u0026gt; PD PD --\u0026gt; V{Valid data?} V -- \u0026#34;No\u0026#34; ---\u0026gt; E V -- \u0026#34;Yes\u0026#34; ---\u0026gt; W[Send webhook] W --\u0026gt; E We defined some of the elements up front. The Process data rectangle contains multi-line text.\nNote the direction of the flowchart is left-to-right, as specified by LR after flowchart. The direction options are:\nTB: Top-to-bottom BT: Bottom-to-top RL: Right-to-left LR: Left-to-right Standard symbols used in flowcharts Flowcharts use a variety of standardized symbols to represent different types of actions, processes, and decisions. Below are some of the most commonly used symbols:\nTerminator (Oval) Represents the start or end of a process. Example: ([Start]) or ([End]). Process (Rectangle) Represents a step or task in the process. Example: [Calculate sum]. Decision (Diamond) Represents a decision point with two or more possible outcomes. Example: {Is user authenticated?}. Arrow (Line) Represents the flow or direction of the process. Example: Connects steps in a process. Input/Output (Parallelogram) Represents input to or output from a process. Example: [/User enters credentials/]. Database (Cylinder) Represents a data store Example: [(MySQL)]. Additional symbols may be used for specialized diagrams or contexts, depending on the needs of the specific workflow.\nClass diagrams A class diagram is one of the core components of the Unified Modeling Language (UML) and is used to visually represent the static structure of a system. Traditionally, it models classes, their attributes, operations (methods), and the relationships among the classes in the system.\nHowever, we found that the most helpful use of class diagrams is to model relationships between higher-level entities. We can also model higher-level entities with a flowchart, but the benefit of using a class diagram is the ability to use standard relationships.\nBelow is an example diagram modeling some domain concepts:\n--- config: class: hideEmptyMembersBox: true --- classDiagram Fleet o-- Host: \u0026#34;manages\u0026#34; Fleet o-- Policy: \u0026#34;checks (may enforce)\u0026#34; Host o-- IdPUser: \u0026#34;assigned to\u0026#34; Host \u0026lt;|-- MacOS Host \u0026lt;|-- Windows Host \u0026lt;|-- Linux Fleet --\u0026gt; Apple: communicates with It renders as follows:\n--- config: class: hideEmptyMembersBox: true --- classDiagram Fleet o-- Host: \u0026#34;manages\u0026#34; Fleet o-- Policy: \u0026#34;checks (may enforce)\u0026#34; Host o-- IdPUser: \u0026#34;assigned to\u0026#34; Host \u0026lt;|-- MacOS Host \u0026lt;|-- Windows Host \u0026lt;|-- Linux Fleet --\u0026gt; Apple: communicates with The syntax is straightforward, as in the basic examples of previous diagrams, and we can quickly throw together this type of diagram during a meeting.\nSix standard types of relationships can be modeled in a class diagram:\nAssociation: A \u0026ldquo;uses-a\u0026rdquo; relationship. Example: User --\u0026gt; Order : \u0026quot;places\u0026quot; Inheritance (Generalization): An \u0026ldquo;is-a\u0026rdquo; relationship. Example: Vehicle \u0026lt;|-- Car Aggregation: A \u0026ldquo;has-a\u0026rdquo; relationship (whole-part with shared ownership). Example: School o-- Student : \u0026quot;contains\u0026quot; Composition: A stronger form of aggregation (whole-part with exclusive ownership). Example: Car *-- Engine: \u0026quot;includes\u0026quot; Dependency: Shows that a class depends on another class. Example: Payment \u0026lt;.. Invoice : \u0026quot;depends on\u0026quot; Realization: Represents an \u0026ldquo;implements\u0026rdquo; relationship. Example: Interface \u0026lt;|.. Class Class diagrams can also include data members and methods. This amount of detail is generally too much to maintain in a diagram. We recommend only adding this detail when presenting a snapshot or a one-time proposal.\nHere is an example of a class diagram which includes data members and methods:\nclassDiagram class Person { - name: String - age: Integer + getName(): String + setName(name: String): void } class Student { - studentId: Integer + enroll(course: Course): void } class Course { - courseName: String - courseCode: String + getDetails(): String } Person \u0026lt;|-- Student Student *-- Course The diagram is rendered as follows:\nclassDiagram class Person { - name: String - age: Integer + getName(): String + setName(name: String): void } class Student { - studentId: Integer + enroll(course: Course): void } class Course { - courseName: String - courseCode: String + getDetails(): String } Person \u0026lt;|-- Student Student *-- Course Entity relationship diagrams (ERDs) An Entity-Relationship Diagram (ERD) is a type of diagram used for visualizing the structure of a database. There are tools for automatically creating Mermaid ERD diagrams from database schemas. In our experience, ER diagrams are helpful in proposing changes or examining a snapshot in time. You should not actively maintain ERDs to always match the current database schema.\nBelow is an example schema change proposal:\nerDiagram HOST_SCIM { host_id uint PK scim_user_id uint FK } SCIM_USERS { id string PK user_name string first_name string middle_name string family_name string active bool } SCIM_USER_EMAILS { scim_user_id string PK email string PK primary bool type string } SCIM_USER_GROUPS { scim_user_id string PK \u0026#34;FK\u0026#34; group_id uint PK \u0026#34;FK\u0026#34; } SCIM_GROUPS { id uint PK name string \u0026#34;Unique\u0026#34; } HOST_SCIM }|--|| SCIM_USERS : \u0026#34;multiple hosts can have the same SCIM user\u0026#34; SCIM_USERS ||--o{ SCIM_USER_GROUPS: \u0026#34;zero-to-many\u0026#34; SCIM_USER_GROUPS }|--|| SCIM_GROUPS: \u0026#34;one-to-many\u0026#34; SCIM_USERS ||--o{ SCIM_USER_EMAILS: \u0026#34;zero-to-many\u0026#34; It renders as follows:\nerDiagram HOST_SCIM { host_id uint PK scim_user_id uint FK } SCIM_USERS { id string PK user_name string first_name string middle_name string family_name string active bool } SCIM_USER_EMAILS { scim_user_id string PK email string PK primary bool type string } SCIM_USER_GROUPS { scim_user_id string PK \u0026#34;FK\u0026#34; group_id uint PK \u0026#34;FK\u0026#34; } SCIM_GROUPS { id uint PK name string \u0026#34;Unique\u0026#34; } HOST_SCIM }|--|| SCIM_USERS : \u0026#34;multiple hosts can have the same SCIM user\u0026#34; SCIM_USERS ||--o{ SCIM_USER_GROUPS: \u0026#34;zero-to-many\u0026#34; SCIM_USER_GROUPS }|--|| SCIM_GROUPS: \u0026#34;one-to-many\u0026#34; SCIM_USERS ||--o{ SCIM_USER_EMAILS: \u0026#34;zero-to-many\u0026#34; In Mermaid diagrams, cardinality defines the numerical relationship between entities. It specifies how many instances of one entity can be related to instances of another. Commonly used cardinality syntax in ER diagrams is:\n||--|| means \u0026ldquo;one-to-one\u0026rdquo; ||--o{ means \u0026ldquo;zero-to-many\u0026rdquo; ||--|{ means \u0026ldquo;one-to-many\u0026rdquo; }|--|{ means \u0026ldquo;many-to-many\u0026rdquo; The left and right parts of the above cardinality examples can be flipped as needed.\nFurther reading Recently, we explained the business benefits of software modularity and cohesion. We also discussed how to scale your codebase with evolutionary architecture. Watch the intro to the key Mermaid diagrams Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-03-26T00:00:00Z","image":"https://victoronsoftware.com/posts/mermaid-intro/mermaid-intro-headline_hu_865f95be3d28b388.png","permalink":"https://victoronsoftware.com/posts/mermaid-intro/","title":"4 key Mermaid diagrams for software developers"},{"content":"This article is part of a series on technical debt. Check out the previous articles:\nWhy readable code is essential Incrementally scaling a codebase with evolutionary architecture Top software code complexity metrics Introduction Modularity and cohesion are key software engineering concepts that software engineers frequently misunderstand. Frequently, software developers know that modularity is good in some vague general sense, but they can\u0026rsquo;t quantify the benefits. This lack of understanding leads to poor decisions when adding new functionality or fixing system bugs, often leading to the proverbial big ball of mud codebase. Sometimes, engineers feel like it is their manager or software architect\u0026rsquo;s job to define the modules of a system, and they are simply responsible for implementing the details in the quickest way they know how.\nThis article provides specific reasons why using or adding modularity to your existing growing codebase is a good business decision. Sometimes, engineers intuitively know how to do a good job, but they can\u0026rsquo;t explain it to business stakeholders. We try to explain.\nIt\u0026rsquo;s all about complexity One of the challenging problems in software engineering is managing complexity. Modularity is a tool for managing complexity. When we speak of complexity, we are referring to a mature, growing codebase with 10 or more software developers. A young codebase with a couple of developers will benefit from modularity but not as much as a bigger, more complex codebase.\nBigger organizations must manage the complexity of both people and systems. People must be able to work independently at maximum velocity without being slowed down by others. Systems must be simple enough to think about without being overwhelmed. Complexity increases the cost of ownership of software because:\nengineers cannot move at maximum velocity due to coupling to other teams or parts of the codebase engineers cannot understand the code, leading to slow development and more bugs The \u0026ldquo;hero engineer\u0026rdquo; anti-pattern Complex codebases often have \u0026ldquo;hero developers\u0026rdquo; who know the codebase and become the go-to people for solving critical issues or implementing complex features. The prevalence of such heroes may be a sign that your codebase is too complex and that you must change things to scale your business.\nThe \u0026ldquo;hero engineer\u0026rdquo; may be contributing to the complexity issue by:\nfocusing on quick fixes rather than long-term solutions insisting that the current codebase is just fine because \u0026ldquo;that\u0026rsquo;s how we\u0026rsquo;ve always done it\u0026rdquo; What is modularity and cohesion Modularity is the process of breaking up a complex system into smaller, independent, and interchangeable modules. Small means small enough to easily understand. Independent means that we can compile and test the module independently of all the other modules. Interchangeable means that we can substitute other implementations of modules in our system, which often happens during testing.\nCohesion refers to the degree to which the functionality inside a module belongs together. It is the metric we use for creating modules. A good module has high cohesion. Logical changes in that module should generally not leak to other modules. This metric means that a module comprising random functions is not a good one.\nBusiness benefits of a modular codebase 1. Faster development and easier maintenance Modularity increases engineering velocity, which in turn lowers labor costs and reduces time to market. Since modules are cohesive and easy to understand, engineers focus their changes on a limited set of modules without wading through the whole codebase.\nSince modules are independent and interchangeable, they are easier to test. Writing tests becomes faster and easier.\n2. Risk reduction A frequent occurrence in a complex codebase is that a change in one place introduces a bug in another seemingly unrelated functionality. Because modules are cohesive and independent, a bug in one module is less likely to bring down the entire system. Also, since modules are more straightforward to test, they are less likely to have bugs in the first place.\n3. Organizational scalability Management expects the software output to scale proportionally as the engineering organization scales. However, this is not always the case. Adding more people to a codebase causes merge conflicts, ownership confusion, duplicated effort, and communication overheads.\nModularity allows engineers to work in parallel. Each person or team can work in parallel on their modules, minimizing organizational coupling.\nThe main reason microservices have become so popular is that engineers can work on them in parallel. The organizational scalability benefits outweigh the added complexity of microservices.\n4. Faster onboarding When new developers join the team, the engineering velocity often dips as senior developers help with onboarding.\nSince modules are small, they can \u0026ldquo;fit in your head\u0026rdquo; without having to understand all other parts of the system. New developers can contribute more quickly by focusing their initial contributions on a limited set of modules. This \u0026ldquo;simplicity\u0026rdquo; of the codebase means fewer distractions and less hand-holding for senior staff.\n5. Flexibility Modularity gives the business more options for the product\u0026rsquo;s future direction since small modules are more straightforward to modify or replace with new functionality.\nModularity also allows engineers to experiment with newer and potentially better approaches. For example, an engineer can try a different JSON library on one module. Engineering management would not consider such a library change in a monolithic codebase since it would pose too much risk to existing functionality.\n6. Professional growth for software engineers What about the engineers who have been with the company for years and are comfortable with (or used to) the current monolithic approach?\nModular software architectures are becoming the norm in the software industry. Building and maintaining a genuinely modular codebase provides a valuable experience that engineers can carry to future projects within and beyond the current company. If we interviewed a candidate whose preferred working style was to minimize the number of modules in the codebase, that would be a serious red flag.\nDownsides of a modular codebase 1. Initial module creation overhead Creating a new module requires defining its interface and directory structure and writing a new test harness. These steps require more up-front work than simply dumping the code into an existing package.\n2. CI complexity We can compile and test modules independently. To maximize the development speed of modules, each one can have its own CI run. However, as the number of modules grows, this process can become complicated over time.\n3. Cross-cutting features can become trickier Some features, such as security and auditing, affect multiple modules. Ensuring consistency while keeping modules independent can require extra thought and coordination.\nFurther reading Check out our other articles in the technical debt series. Links are at the top of this article. Recently, examined the 4 key Mermaid diagrams for software developers. Previously, we explained the difference between Go modules and Go packages. Watch us explain the business benefits of software modularity Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-03-19T00:00:00Z","image":"https://victoronsoftware.com/posts/software-modularity/software-modularity-headline_hu_845b3d887f689955.png","permalink":"https://victoronsoftware.com/posts/software-modularity/","title":"6 business benefits of software modularity and cohesion"},{"content":"This article examines the literate programming paradigm introduced in 1984 by Donald Knuth. We go through a \u0026ldquo;Hello World\u0026rdquo; example and extract the key lessons relevant to making today\u0026rsquo;s software more readable and maintainable.\nLiterate programming example Key takeaways from literate programming What is literate programming Literate programming is a paradigm in which a computer program is written in a natural language, such as English. The programming language source code is embedded into the program\u0026rsquo;s description. The aim was to create an artifact that a human can easily read without jumping back and forth between different sections of the code file. The writer completely controls the flow of the document, which can be reorganized in any fashion.\nKnuth called his implementation of literate programming WEB to emphasize that a computer program is built from many different pieces. He picked the name before the World Wide Web was prominent. To produce source code, the user runs the tangle command. To create documentation, the user runs the weave command.\nLiterate programming \u0026ldquo;Hello World\u0026rdquo; example Writing a literate program To demonstrate literate programming, we will use the noweb literate programming tool to write a simple program in Go.\nWe create a hello.nw file and start it with:\nThis program teaches us how to print to the screen using: \u0026lt;\u0026lt;print\u0026gt;\u0026gt;= fmt.Println(message) @ To print \u0026#34;Hello World\u0026#34;, pass a literal string to the function: \u0026lt;\u0026lt;message\u0026gt;\u0026gt;= \u0026#34;Hello World\u0026#34; @ We wrote the program in text with embedded code starting with \u0026lt;\u0026lt;name\u0026gt;\u0026gt;= and ending with @. The \u0026lt;\u0026lt;name\u0026gt;\u0026gt; sections are macros that we can reuse in other sections of the document, such as:\nNow, we can create a function that prints a message: \u0026lt;\u0026lt;mypackage_print\u0026gt;\u0026gt;= func Print(message string) { \u0026lt;\u0026lt;print\u0026gt;\u0026gt; } @ Finally, we can call this function from the main function: \u0026lt;\u0026lt;main_call\u0026gt;\u0026gt;= mypackage.Print(\u0026lt;\u0026lt;message\u0026gt;\u0026gt;) @ See the complete literate program on GitHub.\nGenerating code and documentation from the literate program To install the noweb tool with brew on macOS, run the following:\nbrew install noweb To generate the Go source code (tangle):\nnotangle -Rgo.mod hello.nw \u0026gt; go.mod mkdir -p mypackage notangle -R\u0026#39;mypackage/mypackage.go\u0026#39; hello.nw \u0026gt; mypackage/mypackage.go notangle -Rmain.go hello.nw \u0026gt; main.go Now we can run the program: go run main.go\nTo generate the HTML documentation (weave):\nnoweave -html hello.nw \u0026gt; hello.html We can open the hello.html documentation in our web browser.\nKey takeaways from literate programming 1. The developer orders the code for maximum readability Most of today\u0026rsquo;s programming languages were not designed with readability as their top guiding principle. They often require the developer to put code in specific file sections, distracting the reader trying to understand the code. Some examples include:\nimports function and variable declarations, including nested functions error handling Today\u0026rsquo;s IDEs (Integrated Development Environments) have tried to help with the situation by automatically collapsing boilerplate sections. However, we have not seen them take the next step of entirely hiding or virtually relocating distracting code. This area is where today\u0026rsquo;s programming languages and IDEs need to improve.\n2. Comments are first-class citizens In literate programming, comments (natural language) are the main body of the program. Source code, on the other hand, is delegated to macros. Comments are easy to write and can be enhanced with additional processing, such as Markdown, Mermaid diagrams, etc.\nMany of today\u0026rsquo;s language toolchains also have processors that generate HTML documentation from the comments. However, none can mix arbitrary pieces of code with their documentation.\nLinting requirements to include comments often lead to meaningless comments that make the code less readable:\n// This is a class. Today, the closest mainstream approaches to literate programming are computational notebooks such as Jupyter and various online tutorials. These are great for sharing examples and small programs with others but not sufficient for larger software projects.\nSome IDEs support rendering comments in a different style than the rest of the code, including rendering diagrams. However, no standard works across IDEs and version control hosting systems like GitHub.\n3. Code from multiple source files can be present in one place Literate programming allows us to include arbitrary source files in one program file. This behavior is helpful when you want to keep related code in one place, such as an interface (abstract class) and its implementation.\nModern IDEs can find all the implementations of an interface and often have a quick shortcut, allowing the developer to jump between the two.\nDevelopers may also write their own preprocessors that split a single file into several modules or compile units.\nAlthough having more code in one file is sometimes useful, today\u0026rsquo;s developers typically have issues with splitting and decoupling code files that have become too large and are no longer scalable.\n4. Writing code is more difficult The main issue with literate programming is that it makes writing code much more difficult for the developer. It introduces another level of abstraction and another set of tools and concepts that the software developer must be familiar with.\nThis general lesson applies to any system that tries to enhance the coding experience by adding another layer between the user and the code. The new system must provide overwhelming benefits for software developers to switch to it. TypeScript is an example of a successful layer over JavaScript.\n5. Macros make reading code more difficult The literate program contains macros with their own names, adding to the namespace of functions and variables already present in the computer program. These additional names increase the cognitive load of both reading and creating literate programs.\nToday\u0026rsquo;s standard guidance is to make your variable and function names descriptive so the reader knows what they do without additional comments. We can effectively replicate much of literate programming by replacing the literate programming macros with our own well-named functions and ordering these functions in a file for maximum comprehension.\n6. No tooling support Since literate programming is not widely used, it has little to no tooling support, syntax highlighting, or IDE support. There is also no standard build system. Instead, the literate programming user must maintain their own custom build system for the \u0026ldquo;tangle\u0026rdquo; and \u0026ldquo;weave\u0026rdquo; flows.\nSee literate programming example code on GitHub Literate programming example using noweb Further reading Previously, we explained what readable code is and why it is important. We also reviewed the top code complexity metrics. Watch the literate programming example and takeaways Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-03-12T00:00:00Z","image":"https://victoronsoftware.com/posts/literate-programming-lessons/literate-programming-headline_hu_e2df29ad18bfdd82.png","permalink":"https://victoronsoftware.com/posts/literate-programming-lessons/","title":"6 lessons from literate programming"},{"content":"This article is part of a series on technical debt. Check out the previous articles:\nWays to improve your code for readability How to scale a codebase with evolutionary architecture Intro to code complexity metrics Code style Code size Cyclomatic complexity Cognitive complexity In the previous article on readable code, we discussed a few metrics for measuring unreadable code. In this article, we will expand on some of those ideas and specifically focus on code complexity.\nCode complexity primarily refers to the difficulty of understanding a piece of code or a piece of the codebase, such as a module. Complex code is difficult to modify because engineers must spend considerable mental energy to understand it. Frequently, engineers will not understand the code well enough, so they\u0026rsquo;ll make a change to fix a bug, and the change will introduce a new bug somewhere else. Lack of understanding also leads to a fear of refactoring, because engineers don\u0026rsquo;t want to break the codebase.\nCode complexity metrics Many code complexity measures overlap since they all try to measure the same thing.\nCode style A standard code style is helpful for readability. For example, if I opened a file and saw that it had no indentation, the max line length was 20, and somebody named all the variables with a leading iwuzhere, I would be confused. I would have to stop and carefully process the file. I would not have to slow down if the code style were consistent.\nThe metric to track is the number of code style violations or the number of files violating the code style. Most companies enforce a code style with their CI pipeline. Modern tooling can automatically reformat code to match the agreed-upon code style, so code style should no longer be a complexity or readability issue.\nCode size How much code is there? The more code there is, the longer it takes to read and understand it. The common metrics are:\nprogram size or lines of code (LOC) in a function in a file number of functions/classes/modules/files The motivation for tracking these metrics is to help engineers split their functions/files/projects into smaller, more manageable pieces. James Lewis from ThoughtWorks said that \u0026ldquo;a microservice should be as big as my head.\u0026rdquo; His idea is that one person should be able to understand the entire codebase. The smaller the piece of code, the easier it is to understand.\nHalstead introduced a set of software complexity measures in 1977, and one of his metrics was the Halstead volume, which is directly related to code size. We can approximate the Halstead volume by ignoring all comments and whitespace, then multiplying the average code line length by the number of lines of code. This approximation is a good enough metric for our purposes.\nCyclomatic complexity Cyclomatic complexity measures the number of linearly independent paths through a program\u0026rsquo;s source code. It is often used as the master metric for code complexity, uncovering maintainability and hard-to-test parts of the codebase.\nA typical calculation of cyclomatic complexity is as follows:\n1 is the base complexity for a function for each if, case, while, for, or other branching statement, add 1 A good cyclomatic complexity is 10 or less. A score of 20 or more is generally considered difficult to understand. This metric encourages us to write smaller functions.\nCognitive complexity An alternative to cyclomatic complexity is cognitive complexity. This metric tries to adjust the cyclomatic complexity metric to focus on the human reader\u0026rsquo;s mental load \u0026ndash; on the maintainability, and not on the testability, of the code.\nThe key differences in the calculation are:\nfor nested structures, extra incremental penalties are added recursion is penalized jumps to labels, such as goto LABEL, are penalized switch is preferred over nested if groups of similar logical operators are NOT penalized for example, a \u0026amp;\u0026amp; b \u0026amp;\u0026amp; c \u0026amp;\u0026amp; d is easier to understand than a \u0026amp;\u0026amp; b || c \u0026amp;\u0026amp; d This metric is more difficult to calculate than cyclomatic complexity, but it is generally considered a better approximation of code complexity. Many companies are adopting this metric.\nTool and language-specific considerations Modern tools can help with code maintainability issues. For example, AI tools that index the codebase can help explain how a piece of code (or a feature) works. IDEs can also help by collapsing boilerplate code or improving readability in other ways.\nIn the Go programming language, the idiomatic way to check for errors is:\nif err != nil { return err } The above code is repeated everywhere and is typically collapsed by modern IDEs. However, cyclomatic complexity and cognitive complexity metrics penalize it.\nWe need a complexity tool where the user can adjust the penalties. This way, an engineering team can agree on what is considered complex code based on their experience, language, and code style.\nGo complexity metrics For measuring cyclomatic complexity, Go has gocyclo. For measuring cognitive complexity, there is gocognit.\nWhat is modularity and why is it important In the next article of this technical debt series, we explain what is modularity and why it is important for modern scalable software systems.\nFurther reading Track your team‚Äôs engineering performance with open-source tools\nA step-by-step guide to setting up meaningful metrics using free-tier infrastructure‚Äîno SaaS subscriptions required.\nRecently, we showed an example and key takeaways from literate programming.\nWe also wrote an overview of using AI in software development.\nWatch us discuss and show examples of code complexity metrics Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-03-05T00:00:00Z","image":"https://victoronsoftware.com/posts/code-complexity-metrics/code-complexity-headline_hu_334bed8d66ed1e6d.png","permalink":"https://victoronsoftware.com/posts/code-complexity-metrics/","title":"Top code complexity metrics every software dev should know"},{"content":"The AI landscape is changing rapidly. A new tool seems to come out weekly, or we hear about a breakthrough. As a software engineer, it is hard to keep up with all the latest developments and even harder to figure out what is relevant to your day-to-day work. Many tech leaders claim that AI is helping them achieve greater engineering velocity, improving the quality of their code, and enhancing the engineering experience, to name a few. But how can you leverage AI to improve your work as a software engineer? And, most importantly, what is the most effective way to use AI right now?\nWe have followed AI since the early 2000s, when Jeff Hawkins published On Intelligence. AI has come a long way since then. Today, most software engineers can benefit from some AI.\nThe top AI use cases for software engineers are:\nCode completion Code generation Summarizing or explaining code Agentic coding Some software developers have other use cases for AI, such as image generation, but this article will focus on the above ones.\nCode completion The original GitHub Copilot popularized AI code completion. It is a tool that can be integrated into your IDE and can perform multi-line code completions. GitHub Copilot and similar tools have been extremely popular with software developers. We found AI code completion extremely helpful, even though it occasionally generates function signatures and other code that is not quite right.\nFor example, after creating the PubSubMessage struct in our code, Copilot recommended the PubSubMessageAttributes struct. Opening the completions window shows other multi-line suggestions.\nGitHub Copilot code completion example The code completion is not only for software code. We used it for documentation, such as writing a user guide for a new feature. AI is most useful for boilerplate introductory content. However, beware that some search engines may be able to detect the use of AI in your documentation and reduce your search ranking.\nCode generation Code generation is similar to code completion, but the desired result is a complete function or code block. Typically, engineers use a chat interface such as ChatGPT and provide it with a description of the desired function. Providing a detailed description along with inputs and outputs is key to getting the best results.\nFor example, given the following prompt:\nWrite a Golang function that takes path.URL, makes a GET request to that URL, parses the JSON response, and returns the \u0026ldquo;attributes\u0026rdquo; and \u0026ldquo;data\u0026rdquo; fields of the response.\nChatGPT o1 model generated the following code:\n// FetchAttributesAndData makes a GET request to the provided URL, parses // the JSON response, and returns the \u0026#34;attributes\u0026#34; and \u0026#34;data\u0026#34; fields. func FetchAttributesAndData(u *url.URL) (map[string]interface{}, interface{}, error) { resp, err := http.Get(u.String()) if err != nil { return nil, nil, err } defer resp.Body.Close() // You may also check resp.StatusCode for error handling if desired if resp.StatusCode != http.StatusOK { return nil, nil, fmt.Errorf(\u0026#34;unexpected status code: %d\u0026#34;, resp.StatusCode) } // Define a struct to capture the JSON structure var result struct { Attributes map[string]interface{} `json:\u0026#34;attributes\u0026#34;` Data interface{} `json:\u0026#34;data\u0026#34;` } if err := json.NewDecoder(resp.Body).Decode(\u0026amp;result); err != nil { return nil, nil, err } return result.Attributes, result.Data, nil } ChatGPT also provided some documentation and a usage example.\nPreviously, software engineers would accomplish this task by searching Google or StackOverflow for similar code snippets. Now, they can use AI chatbots to generate this code. This approach is most useful when you need to generate code that\u0026rsquo;s not your area of expertise.\nSome programming languages have dedicated AI code-generation tools. For example, you can create Go functions and packages online with sketch.dev.\nSummarizing or explaining code When software engineers work on a large codebase, they often need to understand code written by other engineers, some of whom may have left the company, and they may also need to find a specific piece of code in the codebase.\nAI can help by reading the codebase and adding it to its context. This is an example of Retrieval-Augmented Generation (RAG), where AI combines its general knowledge with the specific context of the codebase.\nTo understand the codebase, the AI tool needs to add the code to its context or to index the codebase. Then, the tool can use a combination of AI and deterministic search to find the relevant code. Below is an example of a question we asked Cursor IDE about our codebase:\nIn which Go files is macOS disk encryption code located?\nCursor provided a list of files and additional information on the top hits.\nCursor codebase search example Codebase-wide queries are a great way to find information yourself before asking the broader team. They are also a decent way to learn the codebase.\nAgentic coding Agentic coding refers to using an AI agent to write code and perform tasks on your behalf. Using agents is a more advanced use case, requiring you to know the AI tools, processes, and LLMs well. A good AI agent can:\nWrite code, including creating and moving files Write and run tests, including Browser UI tests Write, read, and follow documentation Do terminal operations such as installing applications Do Git operations such as pushing Connect to other servers with SSH Currently, the top agentic coding tools are:\nRoo Code (VSCode plugin) Cline (VSCode plugin) Cursor (IDE built on top of VSCode) There are many other tools and platforms available. GitHub Copilot also announced Agent mode, which is available in preview as of this writing. JetBrains has announced Junie, which is only available via the Early Access Program.\nAs the agentic coding tools are still in their early stages, changing rapidly, and require a lot of handholding, it is reasonable to wait 6 to 12 months before revisiting them.\nAI coding agent workflows The following are some workflow suggestions for using an AI coding agent to create a small application.\nFirst, start with some context regarding what you want to build. Create a README or a plan outlining how you want to structure the application and the steps to implement it. You can use another general-purse AI, such as ChatGPT, to help you create the high-level plan.\nFor example, we asked ChatGPT to create a high-level plan with the following prompt:\nWe want to create a mock Google Android Management API server using an AI agent. The server is written in Golang and will interact with our MDM solution during testing. It should hold the state for enterprises, profiles, and fake devices enrolled in it. The server should have a mock PubSub webhook that will push notifications regarding ENROLLMENT and STATUS_REPORTs. Please create a plan that another AI agent can implement in several steps.\nNext, ask the AI agent to read the plan, update it, and create a more detailed plan. It may make sense to break the plan into smaller parts and treat each part as a separate project. In effect, you act as the AI agent\u0026rsquo;s project manager.\nMake sure to have documentation and have the AI agent update it regularly. In addition to the README, you can have API specs, secrets, and other documentation files.\nTell the AI agent to initialize a git repo, create the project structure, and start implementing the plan. For each step, ask the AI agent to create tests. After each step, ask the AI agent to update the documentation and commit the changes. This way, you can easily rollback if the AI agent gets stuck or goes off the rails.\nTry to be as precise as possible in your prompts.\nWhen adding a new feature, you can start a new session with the AI agent and ask it to read all the documentation. This will \u0026ldquo;initialize\u0026rdquo; the AI agent with the project context.\nWork in small development iterations with your AI agent.\nAI agent workflow Learn about your AI agent\u0026rsquo;s specific features to level up your skills. Often, there are ways to provide context to the agent or give special meaning to certain words or files.\nAt some point, you may want to take over the maintenance of the code from the AI agent. For example, check the code into your main repository and maintain it as any other human-written code.\nAI coding agent issues The main issue with AI coding agents is that they make mistakes. If you spot their mistake, you can tell them about it, and they will generally correct it. However, if you can\u0026rsquo;t spot their mistake, the agent may end up in a loop where it keeps trying to fix the issue, but your application still doesn\u0026rsquo;t work. That is why it is essential to work in small iterations where you can roll back and start over.\nThe other issue is that AI agents are slow. Often, they need to take several steps to make progress, and the human is left waiting‚Äîbeing there just in case they need help or go off track. Theoretically, a single human could manage multiple AI agents, but in practice, it is hard for people to frequently switch between multiple cognitively demanding tasks.\nFurther reading Will AI agents replace software developers?\nA realistic look at how AI coding agents are reshaping software development and what engineers need to do to stay ahead.\nIntroducing MCP: Lessons from building an AI-driven server\nOur lessons learned from building an AI-driven MCP server and how we taught an AI agent to use our product.\nMultitasking with AI agents: When it works and when it fails\nPractical tips on managing AI agent workflows and balancing focus with efficiency.\nHow to scale your codebase with incremental design\nLearn evolutionary architecture patterns that help your codebase grow without becoming unmaintainable.\nWhat is readable code and why is it important?\nUnderstand how code clarity impacts team velocity and why readability should be a priority.\nWatch how to use AI for software development Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-02-26T00:00:00Z","image":"https://victoronsoftware.com/posts/ai-for-software-developers/ai-building-software_hu_2cd598135c787927.png","permalink":"https://victoronsoftware.com/posts/ai-for-software-developers/","title":"How to use AI for software development (2025)"},{"content":"Go is designed for fast compilation. However, sometimes, you notice that your builds have gotten slower or that certain code changes cause an unexpectedly long recompile time. In this article, we show how to analyze your compilation times and take steps to improve them.\nTurn on the Go build cache First, you must know that Go is very good at caching build artifacts. If you make a small change and rerun the build, the rerun will be significantly faster because Go will reuse the cached artifacts from the previous build. However, if you update the Go version, change the build flags, or pull in new or different dependencies, Go may rebuild everything from scratch.\nThe first step in improving your build time is to make sure you are using a build cache. The cache is enabled by default on your development machine, but that may not be true on your CI/CD system. Ensure you use a build cache across multiple CI/CD runs. For example, the GitHub Actions setup-go action has caching turned on by default.\nAnalyze a Go build We can clear the build cache with the following:\ngo clean -cache Now, we can run a clean build with the -debug-trace flag:\ntime go build -debug-trace=debug-trace.json ./cmd/fleet We use the time command to measure the time the build takes. It is good practice to always use the time command when measuring performance. The time command is built into our Z shell (zsh), but a similar command is available in other shells and OSes.\nIn the time output, we see how long our build took:\n79.25s user 20.13s system 518% cpu 19.183 total The total time (19.183s) is the wall clock time we waited for the build to finish. The user and system times are spent executing user and system code. They are larger than the wall clock time because we use a multi-core machine.\nThe debug trace is in Trace Event Format and looks like this:\n[ {\u0026#34;name\u0026#34;:\u0026#34;Running build command\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;B\u0026#34;,\u0026#34;ts\u0026#34;:1739801608027038,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ,{\u0026#34;name\u0026#34;:\u0026#34;load.PackagesAndErrors\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;B\u0026#34;,\u0026#34;ts\u0026#34;:1739801608027222,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ,{\u0026#34;name\u0026#34;:\u0026#34;modfetch.download github.com/WatchBeam/clock@v0.0.0-20170901150240-b08e6b4da7ea\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;B\u0026#34;,\u0026#34;ts\u0026#34;:1739801608038996,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ,{\u0026#34;name\u0026#34;:\u0026#34;modfetch.download github.com/WatchBeam/clock@v0.0.0-20170901150240-b08e6b4da7ea\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;E\u0026#34;,\u0026#34;ts\u0026#34;:1739801608039035,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ,{\u0026#34;name\u0026#34;:\u0026#34;modfetch.download github.com/briandowns/spinner@v1.23.1\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;B\u0026#34;,\u0026#34;ts\u0026#34;:1739801608039382,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ,{\u0026#34;name\u0026#34;:\u0026#34;modfetch.download github.com/briandowns/spinner@v1.23.1\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;E\u0026#34;,\u0026#34;ts\u0026#34;:1739801608039410,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ,{\u0026#34;name\u0026#34;:\u0026#34;modfetch.download github.com/e-dard/netbug@v0.0.0-20151029172837-e64d308a0b20\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;B\u0026#34;,\u0026#34;ts\u0026#34;:1739801608039643,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ,{\u0026#34;name\u0026#34;:\u0026#34;modfetch.download github.com/e-dard/netbug@v0.0.0-20151029172837-e64d308a0b20\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;E\u0026#34;,\u0026#34;ts\u0026#34;:1739801608039808,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ,{\u0026#34;name\u0026#34;:\u0026#34;modfetch.download github.com/getsentry/sentry-go@v0.18.0\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;B\u0026#34;,\u0026#34;ts\u0026#34;:1739801608053496,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ... and so on A widespread tool for visualizing Trace Event Format is Perfetto. Click Open trace file and upload your trace. Use Ctrl + Scroll to zoom in and out and Shift + Scroll to move right or left. The WASD keyboard keys also work.\nThe Perfetto tool showing a Go build debug trace The trace shows that our https://github.com/mattn/go-sqlite3 dependency is taking most of the build time. The fact that we have 16 cores doesn\u0026rsquo;t help because Go is not parallelizing the build for this dependency. This dependency uses CGO, so the build takes time to compile C files.\nWe attempted to speed up the build by adding the go-sqlite3 dependency to our top ./cmd/fleet package, assuming the build tool would start compiling it first. However, the total build took longer because the subsequent link step became much slower.\nAs we mentioned above, the initial compile time is usually not a big concern if you are using a build cache. So, let\u0026rsquo;s try making a small change and analyzing the recompile time. We make a change to a frequently modified package.\necho \u0026#39;var _ = \u0026#34;bozo\u0026#34;\u0026#39; \u0026gt;\u0026gt; ./server/datastore/mysql/mysql.go time go build -debug-trace=debug-trace-recompile.json ./cmd/fleet The total recompile time is 1.229s, and the trace looks like this:\nThe Perfetto tool showing a Go recompile debug trace We see that the mysql package we modified is taking about half the recompile time. The load.PackagesAndErrors step takes ~300ms and is not parallelized. This step is part of the Go toolchain. Modifying a smaller package would reduce the recompile time. If you have a large package that is frequently modified, you can improve the build time by splitting it into smaller packages.\nFind why dependencies are included in the build In a previous article, we described how to find Go package dependencies. A way to analyze the build and see why a dependency is being pulled in is to use the -debug-actiongraph flag:\ngo clean -cache time go build -debug-actiongraph=actiongraph.json ./cmd/fleet The resulting actiongraph.json is a JSON file containing an array of entries such as:\n{ \u0026#34;ID\u0026#34;: 27, \u0026#34;Mode\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/server/datastore/filesystem\u0026#34;, \u0026#34;Deps\u0026#34;: [ 4, 11, 23, 33, 93, 97, 180, 103 ], \u0026#34;Objdir\u0026#34;: \u0026#34;/var/folders/r6/br06kz3s6lxb_75zz6dkjvvc0000gn/T/go-build3105381437/b845/\u0026#34;, \u0026#34;Priority\u0026#34;: 843, \u0026#34;NeedBuild\u0026#34;: true, \u0026#34;ActionID\u0026#34;: \u0026#34;JGOAJdypDJJbwlHvaUPE\u0026#34;, \u0026#34;BuildID\u0026#34;: \u0026#34;JGOAJdypDJJbwlHvaUPE/B47ZHL3FCDKdll6TubU2\u0026#34;, \u0026#34;TimeReady\u0026#34;: \u0026#34;2025-02-18T09:02:54.257806-06:00\u0026#34;, \u0026#34;TimeStart\u0026#34;: \u0026#34;2025-02-18T09:02:54.272756-06:00\u0026#34;, \u0026#34;TimeDone\u0026#34;: \u0026#34;2025-02-18T09:02:54.293356-06:00\u0026#34;, \u0026#34;Cmd\u0026#34;: [ \u0026#34;/opt/homebrew/Cellar/go/1.23.4/libexec/pkg/tool/darwin_arm64/compile -o /var/folders/r6/br06kz3s6lxb_75zz6dkjvvc0000gn/T/go-build3105381437/b845/_pkg_.a -trimpath \\\u0026#34;/var/folders/r6/br06kz3s6lxb_75zz6dkjvvc0000gn/T/go-build3105381437/b845=\\u003e\\\u0026#34; -p github.com/fleetdm/fleet/v4/server/datastore/filesystem -lang=go1.23 -complete -buildid JGOAJdypDJJbwlHvaUPE/JGOAJdypDJJbwlHvaUPE -goversion go1.23.4 -c=4 -shared -nolocalimports -importcfg /var/folders/r6/br06kz3s6lxb_75zz6dkjvvc0000gn/T/go-build3105381437/b845/importcfg -pack /Users/victor/work/fleet/server/datastore/filesystem/software_installer.go\u0026#34; ], \u0026#34;CmdReal\u0026#34;: 17479792, \u0026#34;CmdUser\u0026#34;: 17327000, \u0026#34;CmdSys\u0026#34;: 5692000 }, The CmdReal, CmdUser, and CmdSys fields show the real, user, and system time spent executing the command. The Deps field shows the package\u0026rsquo;s dependencies.\nAlthough we can write our own tool to analyze the actiongraph.json file, we can also use the https://github.com/icio/actiongraph tool. Install the tool with:\ngo install github.com/icio/actiongraph@latest We can find the longest compile steps with:\nactiongraph -f actiongraph.json top 13.786s 16.14% build github.com/mattn/go-sqlite3 1.396s 17.78% build runtime/cgo 1.327s 19.33% build github.com/aws/aws-sdk-go/service/s3 1.295s 20.85% build github.com/aws/aws-sdk-go/aws/endpoints 1.095s 22.13% build github.com/google/go-github/v37/github 1.078s 23.39% build github.com/elastic/go-sysinfo/providers/darwin 0.983s 24.55% build github.com/open-policy-agent/opa/ast 0.975s 25.69% build github.com/klauspost/compress/zstd 0.916s 26.76% build github.com/shoenig/go-m1cpu 0.755s 27.64% build crypto/tls 0.742s 28.51% build github.com/fleetdm/fleet/v4/server/fleet 0.722s 29.36% build github.com/shirou/gopsutil/v3/process 0.664s 30.14% build net 0.626s 30.87% build github.com/open-policy-agent/opa/topdown 0.625s 31.60% build runtime 0.622s 32.33% build google.golang.org/protobuf/internal/impl 0.609s 33.04% build github.com/fleetdm/fleet/v4/server/datastore/mysql 0.605s 33.75% build golang.org/x/net/http2 0.577s 34.43% build github.com/aws/aws-sdk-go/service/lambda 0.576s 35.10% build github.com/spf13/pflag The tool also has a graph subcommand to highlight all import paths from the build target to the package indicated by --why. We can convert the .dot file to an SVG file with the Graphviz dot command.\nactiongraph -f actiongraph.json graph --why github.com/mattn/go-sqlite3 \u0026gt; actiongraph-sqlite3.dot dot -Tsvg \u0026lt; actiongraph-sqlite3.dot \u0026gt; actiongraph-sqlite3.svg Why the go-sqlite3 package is included in the build We can use this knowledge to refactor the codebase or, perhaps, hide the problematic dependency behind a build flag.\nThere is no official documentation for the above debug flags. However, they can be found in the Go source code:\n// Undocumented, unstable debugging flags. cmd.Flag.StringVar(\u0026amp;cfg.DebugActiongraph, \u0026#34;debug-actiongraph\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;) cmd.Flag.StringVar(\u0026amp;cfg.DebugRuntimeTrace, \u0026#34;debug-runtime-trace\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;) cmd.Flag.StringVar(\u0026amp;cfg.DebugTrace, \u0026#34;debug-trace\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;) Further reading Previously, we explained how to accurately measure the execution time of Go tests and how to break apart a large Go test suite. We also demonstrated some common code refactorings that can be done with your IDE. Watch how to analyze Go builds Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-02-19T00:00:00Z","image":"https://victoronsoftware.com/posts/analyze-go-build/analyze-go-build-headline_hu_c5924c4c3f29b335.png","permalink":"https://victoronsoftware.com/posts/analyze-go-build/","title":"How to analyze Go build times"},{"content":" Extract method (aka extract function) Inline variable Extract variable Inline method (aka inline function) What is code refactoring? Code refactoring involves modifying existing software code without changing its observable behavior. Refactoring improves the code base\u0026rsquo;s readability and maintainability. See our previous article on why readable code is important.\nWhy are engineers afraid of refactoring? Refactoring is essential to software development and should be done regularly as part of day-to-day work. Unfortunately, many engineers are afraid of refactoring, don\u0026rsquo;t know how to do it, or don\u0026rsquo;t consider it part of their job responsibilities.\nSome engineers fear that refactoring will introduce bugs or break existing functionality. The root cause of this fear is the lack of automated tests. Without automated tests, ensuring that the refactored code behaves as expected is difficult. A code base without automated tests is a ticking time bomb and cannot be maintained by any sane engineer. Before refactoring code in such a code base, you should add automated tests for the targeted code.\nOther engineers fear that refactoring will take too much time. This fear is often unfounded, as refactoring can be done incrementally and in small steps. For example, after refactoring the code for an hour or less, merge your changes to your main branch and, if needed, continue doing the next small refactoring steps. Your organization should never allocate weeks of development for \u0026ldquo;large refactorings.\u0026rdquo;\nEngineers may also fear refactoring because they don\u0026rsquo;t want to make too many changes to the code, making it difficult for reviewers to review the changes. The issue is that many current code review systems don\u0026rsquo;t understand the code changes\u0026rsquo; semantics (i.e., the meaning). These systems only understand line changes and are frequently confused by relocated code. In this case, the coder should explain the changes to the reviewer. Alternatively, the organization can adopt a better code review tool. For a further discussion of issues with GitHub code reviews, see our previous article.\nThis article will show some common refactorings you can safely do with automation from your IDE (Integrated Development Environment).\nExtract method (aka extract function) The extract method refactoring takes a piece of code and moves it into a new method. There are several reasons to do this, all of which improve the readability and maintainability of the code:\nThe code is too long and must be broken into smaller, more manageable pieces. The code is duplicated in multiple places and needs to be consolidated into a single method. We want to separate the code implementation from the code intention. The code implementation is what the code does, and the code intention is why it does it. Move the code implementation into its own method and name the new method based on the code intention. For example, consider the following code:\nfunc (pt PackageTest) expandPackages(pkgs []string) []string { if !slices.ContainsFunc(pkgs, func(p string) bool { return strings.Contains(p, \u0026#34;...\u0026#34;) }) { return pkgs } // lots more code ... When entering the expandPackages method, the reader is immediately confronted with a complex expression. They must stop and think about what the code does. Even though the amount of code is small, it still hampers readability. The code implementation is mixed with the code intention. One way to improve the situation is to add a comment. A better way is to extract the code into its own method and name the new method based on the code intention.\nfunc (pt PackageTest) expandPackages(pkgs []string) []string { if !needExpansion(pkgs) { return pkgs } // lots more code ... } func needExpansion(packages []string) bool { return slices.ContainsFunc(packages, func(p string) bool { return strings.Contains(p, \u0026#34;...\u0026#34;) }) } Most IDEs automatically perform this refactoring. Highlight the code you want to extract, open the refactoring menu, and select the Extract Method option.\nInline variable Every variable should have a purpose and a good explanatory name that describes its intent. As the number of variables grows in a method, it becomes increasingly difficult to understand the code. One way to improve the readability of the code is to inline variables. Inlining a variable is replacing the variable with the right-hand side of the assignment.\nFor example, consider the following code:\nfunc (pd *packageDependency) chain() (string, int) { name := pd.name if pd.parent == nil { return name + \u0026#34;\\n\u0026#34;, 1 } // lots more code ... The variable name does not add any value to the code. It is simply a copy of the pd.name field. We can inline the variable to improve the readability of the code:\nfunc (pd *packageDependency) chain() (string, int) { if pd.parent == nil { return pd.name + \u0026#34;\\n\u0026#34;, 1 } // lots more code ... Many IDEs automatically perform this refactoring. Highlight the variable you want to inline, open the refactoring menu, and select the Inline option.\nExtract variable The extract variable refactoring takes a complex expression and moves it into a new variable. Mechanically, it is the opposite of the above inline variable refactoring. There are several reasons to do this:\nThe expression is complex and must be broken into smaller, more manageable pieces. The meaning of the expression is unclear and needs to be clarified with a descriptive variable name. Sometimes, you have a choice between extracting a method or a variable. In general, you should extract a method to make the code more readable. However, if the method is only used once and the parent function is not complex, it may be better to extract a variable.\nFor example, consider the same code from our extract method example above:\nfunc (pt PackageTest) expandPackages(pkgs []string) []string { if !slices.ContainsFunc(pkgs, func(p string) bool { return strings.Contains(p, \u0026#34;...\u0026#34;) }) { return pkgs } // lots more code ... We can extract the complex expression into a variable to improve the readability of the code:\nfunc (pt PackageTest) expandPackages(pkgs []string) []string { needExpansion := slices.ContainsFunc(pkgs, func(p string) bool { return strings.Contains(p, \u0026#34;...\u0026#34;) }) if !needExpansion { return pkgs } // lots more code ... Many IDEs automatically perform this refactoring. Highlight the expression you want to extract, open the refactoring menu, and select the Extract Variable or Introduce Variable option.\nInline method (aka inline function) The inline method refactoring takes a method and moves its code into the caller. Mechanically, it is the opposite of the extract method refactoring. There are several reasons to do this:\nThe method is too simple, and its body is as clear as its name. We want to simplify code and remove a level of indirection. We must regroup code into a single method before proceeding with a better refactoring. For example, consider the following code:\n// method code ... if dep.has(expandedPackages) { // more coe ... func (pd *packageDependency) has(pkgs []string) bool { return slices.Contains(pkgs, pd.name) } We can inline the has method:\n// method code ... if slices.Contains(expandedPackages, pd.name) { // more coe ... Many IDEs automatically perform this refactoring. Highlight the method call you want to inline, open the refactoring menu, and select the Inline Function/Method option.\nFurther reading The code examples above are from our article on finding package dependencies of a Go package. We also discussed how to scale your codebase with evolutionary architecture. And how to analyze Go build times. Watch examples of top code refactorings Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-02-12T00:00:00Z","image":"https://victoronsoftware.com/posts/common-refactorings/common-refactorings-headline_hu_63da035087f5b0b3.png","permalink":"https://victoronsoftware.com/posts/common-refactorings/","title":"Top code refactorings every software engineer should know"},{"content":" Find package dependencies using go list Find package dependencies using Go code What are package dependencies and module dependencies? A package dependency is another package that your Go package imports. When you import a package in Go, you create a dependency on that package. The Go compiler will not compile your package if it cannot find and compile the package you depend on.\nOn the other hand, a module dependency is a dependency on a module. A module is a collection of related Go packages that are versioned together. You declare your module dependencies in your go.mod file. Your code may use one or more packages from your module dependencies.\nWhy are package dependencies important? Understanding your package dependencies is essential because they:\nindicate the amount of internal coupling in your codebase help you understand the structure of your codebase help you avoid too many dependencies help you avoid circular dependencies help you optimize your build times As your codebase grows, keeping track of package dependencies is vital to ensure that the codebase remains maintainable. Many developers import dependencies without considering the consequences. In modern IDE tools, they quickly click Import in a pop-up to make the squiggly lines go away. In some cases, IDEs add imports without even asking the developer. However, code with many dependencies becomes coupled to other potentially unrelated code. This entanglement makes the codebase harder to understand, test, and maintain. For additional details, see the list of problems with a coupled architecture from our previous article.\nWhat is an architectural test? An architectural test is a test that makes sure your code follows the architectural rules that you have defined. Codebases tend to devolve into a Big Ball of Mud as time passes. Architectural tests are one way to keep your codebase clean.\nIn our example below, we will check to ensure that our Go package is NOT dependent on another package in our codebase. This is a common scenario when you want to refactor your codebase and remove a dependency or add a new package and want to ensure that it is not dependent on other parts of the codebase.\nFind package dependencies using go list go list is a powerful tool that you can use to list information about Go packages. You can use the -deps flag with go list to find package dependencies. Here is an example:\ngo list -deps ./server/android... The result is a list of all the direct and indirect package dependencies of the ./server/android and its subpackages. To filter out standard library packages and sort the list, you can use the following command on macOS:\ngo list -deps ./server/android... | grep -E \u0026#39;^[^\\/]*\\.[^\\/]*\\/\u0026#39; | sort The above regular expression looks for packages with a . before the first / in the package path. This regex filters out standard library packages. The sort command sorts the list alphabetically.\nTo check if a package is dependent on another package, you can use the following command:\n! (go list -deps ./server/android... | grep -q \u0026#39;github.com/fleetdm/fleet/v4/server/mdm/nanomdm/mdm\u0026#39;) The leading ! inverts the command\u0026rsquo;s exit status. If the package is dependent on the specified package, the command will return 1; if it is not, the command will return 0. You can use this command in your CI/CD pipelines to ensure that your package is not dependent on a specific package.\nFind package dependencies using Go code packages is a Go package that allows one to load, parse, type-check, and import Go packages. We will use the Load function to get a list of Package values. In addition, we will use Context.Import method from build package to recursively find dependencies.\nBelow is an example architecture test you can add to your test suite.\nThe above example is based on https://github.com/matthewmcnew/archtest. You can jump to the code example section of the video below for a full explanation.\nA failing run of our architecture test will look like this:\n=== RUN TestPackageDependencies arch_test.go:41: Error: package dependency not allowed. Dependency chain: github.com/fleetdm/fleet/v4/server/android/service github.com/fleetdm/fleet/v4/server/fleet github.com/fleetdm/fleet/v4/server/mdm/nanomdm/mdm --- FAIL: TestPackageDependencies (14.66s) Find how a dependency is included in the build In our article on analyzing Go build times, we show how to use the -debug-actiongraph flag to find why a dependency is included in the build.\nFurther reading In the previous article, we discussed how to scale your codebase with evolutionary architecture. Before that, we explained the difference between Go modules and Go packages. We also covered common code refactorings in Go for readability and maintainability. Watch how to find package dependencies of a Go package Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-02-05T00:00:00Z","image":"https://victoronsoftware.com/posts/go-package-dependencies/go-dependencies-headline_hu_a579fbf09f721b4c.png","permalink":"https://victoronsoftware.com/posts/go-package-dependencies/","title":"How to find package dependencies of a Go package"},{"content":"This article is part of a series on technical debt. Check out the previous article:\nWhy readable code is important and how to refactor code for readability Intro to evolutionary architecture Current architecture Problems with the current architecture Good architecture Evolutionary architecture Evolutionary architecture refers to a software design approach that embraces change as a fundamental aspect of system development. Instead of aiming to create a fixed and perfect architecture upfront, it allows the system to evolve in response to new requirements, technologies, and insights. Evolutionary architecture is a critical tool for reducing technical debt.\nEvolutionary design, or incremental design, is another term for this approach. Generally, evolutionary design refers to changes on a smaller scale, such as refactoring code or adding new features. On the other hand, evolutionary architecture refers to changes on a larger scale, such as reorganizing the codebase or splitting a monolithic application into microservices. That said, there is no strict boundary between the two terms. We will use the term evolutionary architecture.\nIn this article, we provide an example of scaling your codebase to accommodate a growing number of features and developers.\nCurrent architecture We base our example on a theoretical codebase, but real-world experiences inspire it. The problems and solutions we discuss are common in software development, especially in startups and small companies.\nThe initial state of our example codebase is a web application developed in a mono-repository. The application was built from the ground up with a simple architecture, focusing on adding new features and finding product-market fit.\nCurrent design with a few large modules. The current design divides the codebase into a few large modules. We use the term module to mean a logical grouping of code in the same files and directories.\nHowever, after a couple of years, the application has grown significantly in features, complexity, and team size. The organization now has three product teams working on different functional areas of the application. No one has updated the initial architecture, which is insufficient to support the growing codebase and development team.\nProblems with the current architecture A significant problem that the engineering team has been facing is an increase in bugs and a longer time to fix them. The code for each feature is sprinkled throughout the codebase and tightly coupled to other seemingly unrelated features. This complexity makes it difficult to understand, test, and keep existing features working as new ones are added.\nSpeaking of new features, the team has been struggling to add them on time. The codebase has become a tangled web of dependencies, and any change in one part of the codebase can have unintended consequences in other parts. Adding a feature requires modifying many parts of the codebase, which requires understanding the entire codebase, which many developers lack. The lack of knowledge and the changes to many parts of the codebase have led to features taking significantly longer to implement than initially estimated.\nMaintaining feature branches for over a few days and making patch fixes to existing releases has become impossible. The codebase is so intertwined that any changes may cause merge conflicts. The increased likelihood of merge conflicts has discouraged developers from refactoring and cleaning up the code base. This tendency to leave the code as-is has perpetuated the slide in code quality.\nTests have also become a problem. The test suite has been in a frequent state of disrepair. There is no clear ownership of tests, so engineers have been reluctant to fix them. Some engineers have stopped paying attention to failing CI alerts, figuring that the problems are caused by one of the other two teams.\nTests have also become slower and slower, especially the integration tests that test the API and include the service layer, the datastore layer, and an actual database. These tests do not run in parallel; every additional feature slows down the compile and increases test time. Test files have become bloated with tests for multiple features, making them slow to load in the editor, difficult to navigate, and impossible to diff for PR reviews.\nFinally, the onboarding time for new developers has been growing. It takes weeks for new developers to understand the codebase and start contributing.\nGood architecture At this point in the company\u0026rsquo;s life, an exemplary architecture would be separate groups of modules corresponding to the three product teams.\nGood design with dedicated modules for each product team. Each team would be responsible for its own set of modules, which aligns with Agile principles. The modules would be loosely coupled, and the teams would be able to work independently on their features without affecting other teams. The amount of code that each engineer has to understand and change would be drastically reduced.\nThis architecture would have eliminated or significantly reduced the problems that the engineering team has been facing.\nThe reduced complexity and increased understanding of the codebase would lead to fewer and faster to fix bugs Faster feature development due to cleaner code and fewer dependencies Reduced merge conflicts for PRs, especially for database migrations and schema changes Rarely failing test suite due to clear ownership of tests Faster tests due to each team focusing on testing their slice of the product. Limited complete product integration tests would still be present. Faster onboarding time for new developers However, the company does not have this architecture. Building this architecture upfront would have been foolish since it would have consumed critical engineering time. Yes, there was value in creating this structure upfront because it would have saved time in the long run, but this value was insufficient for a young company that may not be around in a few months.\nEvolutionary architecture Many companies and engineers find themselves in this situation. They have a codebase with poor architecture for today\u0026rsquo;s reality, blame the organization for not thinking about these problems earlier, and feel like they can\u0026rsquo;t improve the situation.\nEvolutionary architecture is a way to incrementally improve the architecture of a codebase without having to do a big rewrite. It is a way to make the codebase better today than it was yesterday and better tomorrow than it is today.\nThis situation is not unique to this company. It is the norm. Most companies start with a simple architecture and codebase that is good enough for the first few features. As the company grows, the architecture becomes a bottleneck. Instead of worrying about not making the right decisions in the past, consider where the architecture needs to be a year or two from now and start moving towards that.\nFor example, when adding a new prominent feature to the product, decouple it from the rest of the codebase.\nEvolutionary design with big features going into dedicated modules. Our example shows all the modules decoupled, but it may be OK to decouple one or two.\nDecoupling a feature from the rest of the codebase has many benefits similar to those we listed above for \u0026ldquo;good architecture.\u0026rdquo; Additional benefits include:\nMost of the feature can be tested by itself, reducing test time. The business gets the option to create a new team dedicated to the feature quickly \u0026ndash; the code is already separate/independent Engineering can scale the feature separately from the rest of the product. For example, assign a dedicated database or split the feature into a microservice. Code example of splitting the database schema It is nice to read about a theoretical example, but seeing an actual code example is even better. In this code example, we begin with a monolithic application that has a single database schema. We then split the schema into two separate schemas. It is the starting point and a reference for decoupling a new feature from the rest of the codebase. Since this code example is a bit long and requires some context regarding the current implementation, we will not cover it in this article. Instead, jump to the code example section of the video below.\nLink to the source code example decoupling a new backend feature from the rest of the codebase.\nTrack code complexity metrics In the next article of this technical debt series, we go over the top code complexity metrics every software engineer should know.\nFurther reading Recently, we covered how to easily track engineering metrics. Previously, we demonstrated the most significant issues with GitHub\u0026rsquo;s code review process. We also showed how to create an architectural test that finds Go package dependencies. We also published an article on the common code refactorings to improve code readability. In addition, we summarized what every software engineer should know about AI. Lastly, we introduced the top Mermaid diagrams for software developers. Watch how to scale your codebase with evolutionary architecture Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-01-29T00:00:00Z","image":"https://victoronsoftware.com/posts/scaling-codebase-evolutionary-architecture/evolutionary-architecture-headline_hu_c8a750d616187c20.png","permalink":"https://victoronsoftware.com/posts/scaling-codebase-evolutionary-architecture/","title":"How to scale your codebase with evolutionary architecture"},{"content":" Metrics for unreadable code How to make your codebase more readable Fix poor software structure Refactor local code for understanding Use unit and integration tests Useful comments Readable code is software code that is easy to understand and easy to change.\nUnreadable code is a common complaint among software developers and one of the main contributors to technical debt. Abandoning unreadable code is one of the reasons engineers love to work on greenfield projects‚Äîthere is no legacy code to deal with.\nYou\u0026rsquo;ve probably heard comments like, \u0026ldquo;It would take me longer to understand this code than to rewrite it from scratch.\u0026rdquo; This sentiment illustrates the problem with unreadable code: it slows down the development process.\nSome engineers refer to readable code as \u0026ldquo;clean code.\u0026rdquo; In our opinion, \u0026ldquo;readable code\u0026rdquo; and \u0026ldquo;clean code\u0026rdquo; are synonymous, and the term \u0026ldquo;readable code\u0026rdquo; is easier to understand and, therefore, more \u0026ldquo;readable.\u0026rdquo;\nWhy is unreadable code a problem? Although the \u0026ldquo;unreadable code\u0026rdquo; claim feels like a subjective opinion, it has a concrete business impact on software projects.\nWe want our code to be readable to speed up adding new features and bug fixes.\nFor example, we recently spent three hours figuring out how a feature worked, only to realize there was a documentation bug. Unfortunately, we made no improvements to make the code more readable, and the next developer will likely have the same problem. This next developer may be ourselves one year from now when we will have forgotten everything we learned about the feature.\nMetrics for unreadable code You can use several metrics to measure your code\u0026rsquo;s readability. The ideal metric would be the time it takes to understand the code, but this isn\u0026rsquo;t easy to measure. Instead, you can use the following proxies:\nTime to fix a bug‚ÄîMeasure the time from when a developer starts working on a bug until the bug fix is ready for code review. Alternatively, measure the time from when a developer starts working on a bug until their first commit. A first commit is a good proxy for understanding the bug and starting to fix it.\nTime to add a new feature‚ÄîMeasure the time from when a developer starts working on a new feature until it is ready for code review.\nTime to onboard a new team member‚ÄîMeasure the time it takes for a new team member to make their first commit.\nCode style violations‚ÄîMeasure the codebase\u0026rsquo;s number of code style violations. Code style violations can be measured using linters or static analysis tools. Some examples of code style violations relevant to readability are:\nLong functions Long files Deeply nested control structures Poorly named variables, such as 1-character variable names Instead of measuring these style code violations, you can also enforce them in your CI pipeline. Most languages have linters that update your code to match a standard style. For example, Go has gofmt.\nHow to make your codebase more readable Readability is not a goal but a process. You can\u0026rsquo;t make your codebase readable overnight, but you can start making incremental improvements. Whenever you touch a piece of code, try to make it more readable.\nFix poor software structure One pattern we see frequently is that the functionality of a core feature is spread across multiple software modules. The first problem this creates is that the software developer trying to understand the feature has to discover all the modules that implement the feature. Often, this requires grepping the codebase for key names \u0026ndash; a tedious and error-prone process. The second problem is that the developer has to jump between files and directories to understand how the feature works, files that often have tons of other unrelated and distracting code.\nHard to understand feature due to poor software design Poor software structure often arises when we hurry to implement a feature and don\u0026rsquo;t consider future developers needing to make changes. This behavior is reactive software design‚Äîone developer reacts to the immediate need to implement a feature. Later, when implementing new features becomes almost impossible due to unreadable code, they react again by restructuring the code or rewriting old functionality from scratch. This process makes sense for prototypes or early products looking for product-market fit, but it is not sustainable for mature long-term software projects.\nOften, developers may not be able to create a good software design when they start working on a new feature because they don\u0026rsquo;t understand all its ramifications. However, they should restructure their work before moving on to the next task‚Äîthe best time to improve code is when you have all the context in your head.\nWe can restructure the above code example to move all the feature\u0026rsquo;s functionality into one or two modules. This reorganization makes it easier to understand the feature because we have to look at a much smaller number of files and are not distracted by unrelated code.\nEasier to understand feature encapsulated in separate modules Refactor local code for understanding When entering a function, you should quickly understand what it does. The function code should be readable. If an engineer who first sees the function can\u0026rsquo;t understand it, it is too complex and should be refactored.\nLong functions are difficult to understand because they require the developer to simultaneously keep a lot of information in their head. Oftentimes, the function presents implementation details to the developer before they can grasp the big picture. This process is cognitively demanding and error-prone.\nInstead, we can refactor extended functions into smaller functions that each do one thing. This refactoring makes the code easier to understand because we can understand each small function in isolation. Hide complex logic in functions with descriptive names.\nIn addition, use descriptive names for variables. Good names make the code self-documenting and reduce the need for comments.\nAs an example of a function before and after refactoring, see this refactoring example gist. For a full explanation, you can jump to the refactoring section of the video below.\nFor more examples of common refactorings, see our article on top refactorings every software developer should know.\nUse unit and integration tests From a readability perspective, tests are a form of documentation. They show how the code is supposed to work. When reading a test, you can see how the code is supposed to behave in different scenarios.\nTests should also be readable. The same restructuring and refactoring principles apply to tests.\nAnother essential benefit of tests is that they allow developers to refactor code with confidence. When you refactor code, you can run the tests to ensure that the code still works as expected. Unfortunately, this means that when you want to make a change in legacy code without tests, you either have to write tests first or do a lot of manual testing to ensure that the code still works.\nUseful comments Comments should explain why the code is written the way it is, not what the code does. The code should be self-explanatory with descriptive variable and function names and encapsulated implementation details.\nSometimes, it is hard to tell the difference between \u0026ldquo;why\u0026rdquo; and \u0026ldquo;what,\u0026rdquo; so feel free to err on the side of commenting.\nYou can remove the comment if you renamed a variable or a function, and now the comment duplicates the code. One problem with comments is that they can get out of date, which is worse than no comments.\nFor example, before refactoring, you had this code:\n// figure out which declarations we should not delete, and put those into keepNames list keepNames := make([]string, 0, len(existingDecls)+len(fleetmdm.ListFleetReservedMacOSDeclarationNames())) for _, p := range existingDecls { if newP := incomingDecls[p.Name]; newP != nil { keepNames = append(keepNames, p.Name) } } keepNames = append(keepNames, fleetmdm.ListFleetReservedMacOSDeclarationNames()...) After refactoring, the comment is a duplicate and no longer needed. It is even worse in this case because we renamed the variable, but the comment still refers to the old name. The comment is not only a duplicate but also misleading:\n// figure out which declarations we should not delete, and put those into keepNames list namesToKeep := namesOfDeclarationsToNotDelete(existingDecls, enrichedDeclarations) Language features that make the code less readable Some language features can make the code less readable. We will give an example from Go because we are familiar with Go, but the same principles apply to other languages.\nGo nested functions Go supports nested functions like this:\nfunc outer(foo Foo, bar Bar) { inner := func(item Item) { // many lines // ... // of implementation details } // many lines // ... // of additional code for _, i := range something { inner(i) } // more code // ... return } Upon entering the function as a reader, the first thing you see is the inner function. The reader is presented with specific implementation details before understanding the big picture. Instead, the reader should know where the nested function is used before reading these implementation details.\nOne way to solve this issue is to forbid nested functions in your style guide. Always extract nested functions to the struct level or file level. However, this approach loses the benefits of closures and increases the number of functions at the struct/file level.\nWe hope that the Go team will improve the readability of nested functions in the future. For example, they could allow nested functions to be defined at the end of the function after the primary implementation:\nfunc outer(foo Foo, bar Bar) { // many lines // ... // of additional code for _, i := range something { inner(i) } // more code // ... return // nested functions func inner(item Item) { // many lines // ... // of implementation details } } Alternatively, IDE vendors can improve readability by entirely hiding nested functions by default.\nAdditional benefits of readable code As you improve the readability of your code, you will notice several side effects:\nMany bugs will be easier to spot Other developers will be less likely to interrupt you with questions about your code If your code is open source, you may get more contributions Make bigger improvements to your codebase with evolutionary architecture In the following article, we discuss how to make bigger improvements to your codebase with evolutionary architecture.\nFurther reading Why transparency beats everything else in engineering\nHow making work visible transforms teams from frustrated to high-performing through organizational transparency.\nMy first conference talk experience\nLessons learned from presenting about readable code at a major software engineering conference.\nBuild beautiful engineering dashboards‚Äîwithout paying a dime\nLearn how to use GitHub, Grafana, and SQL to create powerful, drill-down metrics using only free tools.\nKey takeaways from literate programming\nDiscover how Donald Knuth\u0026rsquo;s literate programming principles can improve your code documentation practices.\nHow to easily track engineering metrics with GitHub Actions and Google APIs\nBuild automated systems to measure and visualize your team\u0026rsquo;s engineering performance over time.\nMeasuring and improving the execution time of Go tests\nOptimize your test suite performance and reduce CI/CD pipeline duration with practical techniques.\nTop 3 issues with GitHub\u0026rsquo;s code review process\nUnderstand the scalability and developer experience problems with GitHub\u0026rsquo;s default review workflow.\nCommon use cases of AI for today\u0026rsquo;s software developers\nExplore how AI tools can enhance productivity without replacing the need for readable, maintainable code.\nWatch us discuss why readable code is important Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-01-22T00:00:00Z","image":"https://victoronsoftware.com/posts/readable-code/readable-code-headline_hu_4105272c635fab02.png","permalink":"https://victoronsoftware.com/posts/readable-code/","title":"What is readable code and why is it important?"},{"content":" Server CPU and memory utilization Server errors Server API latency (response time) Database slow queries Database performance metrics What is software load testing? Software load testing is a type of performance testing that simulates real-world user load on a software application. Load tests usually run in a test environment identical to the production environment.\nThe goals of load testing may include:\nEnsure the application meets the required performance criteria Ensure the application performance did not degrade after changes Test a new feature\u0026rsquo;s performance before releasing it to production Identify bottlenecks in the application to reduce compute costs and/or risks Run chaos engineering performance experiments Load testing can be done manually or automatically. Many open-source and commercial tools are available to help you run load tests. Some features of load testing tools include:\nRecord and replay user interactions, including simulating unique users Simulate different user loads Monitor the application\u0026rsquo;s performance during the test Generate reports with performance metrics This article lists the key metrics you should gather during a software load test of your web application.\nServer CPU and memory utilization CPU utilization is the percentage of time the CPU is busy processing instructions, and memory utilization is the percentage of memory used by the server. Companies deploy multiple instances of the same application web server, and the load balancer distributes the user requests among them. These metrics are averages across all instances.\nHigh CPU or memory utilization can indicate a bottleneck in the application or server. It may also signal that the application needs to be scaled horizontally (add more instances) or vertically (increase the server\u0026rsquo;s resources).\nLow CPU or memory utilization may indicate that the application is over-provisioned, and infrastructure engineers could reduce resources to save costs.\nTypical expectations for CPU and memory utilization are:\nCPU utilization should be below 80% on average Memory utilization should be below 80% on average High CPU utilization during load test Server errors Server errors are error messages in the application logs or 5XX HTTP status codes. They can indicate that the application is not handling the load well, has a bug, or is misconfigured.\nError logs are a key debugging tool for developers. They can help identify the root cause of a functional or performance error and fix it. As such, developers must use error logs to report actual server errors and not just informational messages. For example, a 404 error is typically not a server error but a client error. A website user requesting a resource that does not exist is a common scenario. Client errors should be logged as informational messages or tagged appropriately to be excluded from the server error metric.\nAWS Logs Insights JSON error filter and sample error patterns The ideal number of server errors is zero. However, in practice, some errors are expected. For example, some startup or shutdown-related errors may occur if application servers are scaling up or down due to load. Note the expected errors in the test plan and adjust the error filter accordingly.\nServer API latency (response time) API latency is the time it takes for the server to respond to a request, measured in milliseconds. Typically, the business cares about user-facing API endpoints, such as the login, checkout, or search endpoints.\nAPI latency is a critical metric for user experience. High latency can lead to user frustration and abandonment.\nOne standard metric is the 95th percentile latency. This metric indicates the latency that 95% of the requests are faster than. It is a good indicator of the user experience because it filters out outliers.\nExample spike in latency during a load test experiment Telemetry tools such as OpenTelemetry can help you gather API latency metrics and correlate them with other metrics, such as server errors or CPU utilization.\nDatabase slow queries Query response time is the time it takes for the database to respond to a query. Slow queries can indicate that the query is not optimized or that the table needs an index.\nSlow queries can lead to high API latency and server errors. They can also lead to high CPU and memory utilization on the database server.\nTypically, we want to look at the average query response time multiplied by the number of queries per second for each query signature. This will identify the queries that have the most impact on database performance.\nThe list of slow queries should remain stable during a load test. If it changes, it may indicate a new unoptimized query or a new bug in the application.\nAWS RDS Performance Insights uses Average Active Sessions (AAS) as its slow query metric Database performance metrics Along with slow queries, we always gather the following database performance metrics:\nDatabase CPU utilization Just like the server, we monitor the database\u0026rsquo;s CPU utilization. The typical expectation is that CPU utilization should be below 80% on average.\nMemory utilization may not be as critical for the database as for the server. We expect the database to use as much memory as possible to cache data and speed up queries.\nDatabase threads running (sessions) Database threads running is the number of database connections actively processing queries. High thread counts can indicate that the database is under heavy load.\nThe number of threads should be at or below the number of CPUs on the database server.\nDatabase IO operations per second (IOPS) Database IOPS is the number of disk read and write operations the database performs per second. High IOPS can indicate that the database is not effectively caching data or that too many writes are occurring.\nIOPS should be in line with the database\u0026rsquo;s provisioned IOPS. If IOPS are consistently higher than provisioned, the database may need to be scaled up.\nAdditional metrics The following metrics may also be necessary. However, these additional metrics may be more situational than the above top 5 metrics.\nNetwork traffic Network traffic includes the number of bytes sent and received by the server. Typically, the data received by the server is the user\u0026rsquo;s request, and the data sent by the server is the response.\nHowever, in microservices architectures and servers with 3rd party integrations, our server may also make requests to other web servers.\nUser traffic is typically consistent from load test to load test. Traffic to other servers may change as engineers add new features. If the network traffic changes significantly, it may indicate a new bug in the application, such as application servers making too many requests to a 3rd party service.\nPerformance profile Many performance tools and modern programming languages can generate a performance profile. A performance profile is a breakdown of the time spent in each function of the application. It can help identify bottlenecks in the application code.\nIf code performance is a significant concern, take a performance profile during the load test and compare it to a baseline or the previous load test profile. If the profile changes significantly, it may indicate a new bug in the application or a new performance bottleneck.\nExample performance profile from Go pprof Database replication lag If the database is replicated, the replication lag is the time it takes for changes to be sent from the primary database and applied to the replica database. High replication lag can indicate that the replica is not keeping up with the primary database.\nHigh replication lag can lead to a bad user experience \u0026ndash; for example, if the user saves data, then immediately retrieves it and receives stale data.\nFurther reading OpenTelemetry: A developer\u0026rsquo;s best friend for production-ready code\nSee how developers can leverage OpenTelemetry during development to build better instrumented applications.\nIs OpenTelemetry useful for the average software developer?\nOur initial exploration of OpenTelemetry\u0026rsquo;s practical value for everyday development tasks.\nHow to benchmark performance of Go serializers\nMeasure and optimize your Go application\u0026rsquo;s performance with effective benchmarking techniques.\nWatch us discuss the software load testing performance metrics Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-01-15T00:00:00Z","image":"https://victoronsoftware.com/posts/software-load-testing/loadtest-fail_hu_aa30435f7ee9485d.png","permalink":"https://victoronsoftware.com/posts/software-load-testing/","title":"Top 5 metrics for software load testing performance"},{"content":" Create a CloudFront distribution Create a CloudFront key pair and add it to a key group Associate the key group with the CloudFront distribution Generate a signed URL using AWS SDKs What is CloudFront CDN? Amazon CloudFront is a content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. CloudFront is a popular choice for serving users worldwide with static assets, such as images, videos, and software package files.\nCloudFront uses S3 buckets, EC2 instances, and other AWS resources as origins to cache and serve content. When a user requests a file from a CloudFront distribution, CloudFront checks its cache for the file. If the file is not in the cache, CloudFront retrieves it from the origin and caches it for future requests.\nUsers around the world requesting data from their local Cloudfront CDN cache What are CloudFront signed URLs? CloudFront signed URLs grant access to private content served by CloudFront. By default, CloudFront distributions are public and serve content to anyone who requests it. However, your signed URLs can restrict access according to some of the following rules:\nsource IP address begin access time and/or expiration time Signed URLs are helpful when you want to serve private content to specific users or for a limited time. For example:\nServe paid content to customers who have purchased a subscription Share private documents with a specific group of users Provide temporary access to a file for a limited time Serve content to users without requiring them to log in A signed URL looks like a regular CloudFront URL but contains additional query parameters that specify the access restrictions. Depending on the limits you apply, a signed URL may be quite lengthy.\nhttps://d1nsa5964r3p4i.cloudfront.net/hello-world.txt?Expires=1736178766\u0026Signature=HpcpyniNSBkS695mZhkZRjXo6UQ5JtXQ2sk0poLEMDMeF063IjsBj2O56rruzk3lomYFjqoxc3BdnFqEjrEXQSieSALiCufZ2LjTfWffs7f7qnNVZwlkg-upZd5KBfrCHSIyzMYSPhgWFPOpNRVqOc4NFXx8fxRLagK7NBKFAEfCAwo0~KMCSJiof0zWOdY0a8p0NNAbBn0uLqK7vZLwSttVpoK6ytWRaJlnemofWNvLaa~Et3p5wJJRfYGv73AK-pe4FMb8dc9vqGNSZaDAqw2SOdXrLhrpvSMjNmMO3OvTcGS9hVHMtJvBmgqvCMAWmHBK6v5C9BobSh4TCNLIuA__\u0026Key-Pair-Id=K1HFGXOMBB6TFF\nHow to create CloudFront signed URLs You must have an AWS account and an S3 bucket with private content as a prerequisite.\nCreate a CloudFront distribution Open the CloudFront console. Choose Create Distribution. In the Origin domain section, choose your S3 bucket as the origin. In the Origin access section, select Origin access control settings (recommended) and click Create new OAC. In the Create new OAC modal, click Create. Choose one option in the WebApplication Firewall (WAF) section. Click Create Distribution to create the CloudFront distribution. In the yellow The S3 bucket policy needs to be updated banner, click Copy policy and then click Go to S3 bucket permissions to update policy. Under bucket Permissions \u0026gt; Bucket policy, click Edit and paste the copied policy. Click Save changes. Back in the CloudFront console, wait for the distribution to deploy. When the distribution is done deploying, the Last modified column will change from Deploying to a date and time. At this point, the CloudFront distribution will serve content from the S3 bucket to anyone who requests it. Signed URLs do NOT protect it until we set them up in the following steps. Test the distribution by accessing a file using the CloudFront URL.\nCreate a CloudFront key pair and add it to a key group The recommended method for signing URLs is using trusted key groups. A key group is a collection of public keys that CloudFront uses to verify signed URLs.\nUse OpenSSL to generate a private key and a public key: openssl genrsa -out private_key.pem 2048 openssl rsa -pubout -in private_key.pem -out public_key.pem Open the CloudFront console. In the side menu, choose Key management \u0026gt; Public keys. Click Create public key. Enter a name for the key, paste the contents of the public_key.pem file, and click Create public key. Remember the key ID for a later step. In the CloudFront side menu, choose Key management \u0026gt; Key groups. Click Create key group. Enter a name for the key group, select the public key you created, and click Create key group. Associate the key group with the CloudFront distribution Open the CloudFront console. Click on the CloudFront distribution you created. In the Behaviors tab, select a behavior and click Edit. In the Restrict viewer access section, select Yes, choose the key group you created, and Save changes. Now, the CloudFront URL will only serve content to users using a signed URL with the private key. Accessing content without a signed URL will result in an access denied 403 error.\n\u0026lt;Error\u0026gt; \u0026lt;Code\u0026gt;MissingKey\u0026lt;/Code\u0026gt; \u0026lt;Message\u0026gt;Missing Key-Pair-Id query parameter or cookie value\u0026lt;/Message\u0026gt; \u0026lt;/Error\u0026gt; Generate a signed URL using AWS SDKs You can generate signed URLs using the AWS SDKs for various programming languages. Amazon provides examples for several languages. We will show an example using the Go SDK.\nIn a new directory, create a Go project and add the AWS SDK as a dependency:\ngo mod init cloudfront-signed-urls go get github.com/aws/aws-sdk-go-v2/feature/cloudfront/sign@v1.8.3 Copy the private_key.pem file to the project directory and create a new Go file with the following code:\nRun the Go program to generate a signed URL:\n2025/01/06 08:52:46 Signed URL: https://d1nsa5964r3p4i.cloudfront.net/hello-world.txt?Expires=1736178766\u0026amp;Signature=HpcpyniNSBkS695mZhkZRjXo6UQ5JtXQ2sk0poLEMDMeF063IjsBj2O56rruzk3lomYFjqoxc3BdnFqEjrEXQSieSALiCufZ2LjTfWffs7f7qnNVZwlkg-upZd5KBfrCHSIyzMYSPhgWFPOpNRVqOc4NFXx8fxRLagK7NBKFAEfCAwo0~KMCSJiof0zWOdY0a8p0NNAbBn0uLqK7vZLwSttVpoK6ytWRaJlnemofWNvLaa~Et3p5wJJRfYGv73AK-pe4FMb8dc9vqGNSZaDAqw2SOdXrLhrpvSMjNmMO3OvTcGS9hVHMtJvBmgqvCMAWmHBK6v5C9BobSh4TCNLIuA__\u0026amp;Key-Pair-Id=K1HFGXOMBB6TFF The signed URL will expire in 1 hour.\nPotential issues Server side encryption (SSE) may be an issue. AWS-managed KMS keys are not supported by CloudFront. One solution is to switch to a customer-managed KMS key. Further reading Recently, we explained launchd agents and daemons on macOS. Previously, we set up a remote development environment for our web app. Watch how to start using CloudFront signed URLs Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-01-08T00:00:00Z","image":"https://victoronsoftware.com/posts/cloudfront-signed-urls/signed-url-headline_hu_f998f4ff0e3724f9.png","permalink":"https://victoronsoftware.com/posts/cloudfront-signed-urls/","title":"Secure private CDN content with CloudFront signed URLs"},{"content":"Introduction In this article, we will create a MySQL replica database. A MySQL replica is a read-only copy of the primary database, which is kept in sync with the main database using MySQL replication threads.\nThe steps we will follow are:\nSpin up MySQL source and replica databases Create a user for replication Obtain source binary log coordinates Configure replica DB and start replication To add a replica to an existing MySQL database, see the copy database from source and start replication manually section.\nTerminology: master-slave vs source-replica In database replication, the terms master-slave and source-replica are used interchangeably. In recent MySQL versions, the term source-replica is preferred over master-slave due to its more neutral connotation. Many keywords and variables in MySQL have recently been renamed to use neutral terms. We will use the terms source and replica in this article.\nWhat is database replication? Database replication is a process that allows data from one database server (the source) to be copied to one or more database servers (replicas). Replication is asynchronous, meaning the replica instance does not need to be connected to the source constantly. The replica can catch up with the source when either becomes available.\nDatabase replicas are used for:\nScaling read operations High availability Disaster recovery MySQL implements replication using the binary log. The source server writes changes to the binary log, and the replica server reads it and applies the changes to its database.\nCreate MySQL source and replica databases We will use Docker to create the MySQL source and replica databases. We will use the official MySQL Docker image. The source database will run on port 3308, and the replica database will run on port 3309. Both servers will have the database named test. We tested these instructions on MySQL 8.0.36, MySQL 8.4.3, and MySQL 9.1.0.\nWe run docker compose up using the following docker-compose.yml file:\nCreate a DB user for replication Replication in MySQL requires a user with the REPLICATION SLAVE privilege. We will create a user named replicator with the password rotacilper.\nConnect to the source database using the MySQL client:\nmysql --host 127.0.0.1 --port 3308 -uroot -ptoor Create the replicator user and grant the REPLICATION SLAVE privilege:\nCREATE USER \u0026#39;replicator\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;rotacilper\u0026#39;; GRANT REPLICATION SLAVE ON *.* TO \u0026#39;replicator\u0026#39;@\u0026#39;%\u0026#39;; FLUSH PRIVILEGES; Retrieve source binary log coordinates For the replica server to start replication, it needs to know the source\u0026rsquo;s binary log file and position. We can obtain this information using the MySQL client we opened in the previous step.\nSHOW BINARY LOG STATUS; In MySQL 8.0, use the SHOW MASTER STATUS command instead of SHOW BINARY LOG STATUS.\nThe output will look like this:\n+------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------+----------+--------------+------------------+-------------------+ | bin.000003 | 862 | | | | +------------+----------+--------------+------------------+-------------------+ 1 row in set (0.01 sec) We must remember the File and Position values for the next step.\nConfigure replica DB and start replication Now, we will connect to the replica database and configure it to replicate from the source database.\nmysql --host 127.0.0.1 --port 3309 -uroot -ptoor Use the CHANGE REPLICATION SOURCE TO command to configure the replica to replicate from the source. Replace SOURCE_LOG_FILE and SOURCE_LOG_POS with the values obtained in the previous step.\nCHANGE REPLICATION SOURCE TO SOURCE_HOST=\u0026#39;mysql_source\u0026#39;, SOURCE_PORT=3306, SOURCE_USER=\u0026#39;replicator\u0026#39;, SOURCE_PASSWORD=\u0026#39;rotacilper\u0026#39;, SOURCE_LOG_FILE=\u0026#39;bin.000003\u0026#39;, SOURCE_LOG_POS=862, GET_SOURCE_PUBLIC_KEY=1; SOURCE_HOST is the primary source\u0026rsquo;s hostname, which matches the docker service name. The GET_SOURCE_PUBLIC_KEY option is needed for caching_sha2_password authentication.\nFinally, start the replica:\nSTART REPLICA; The replica will now start cloning data from the source database. You can check the replication status using the SHOW REPLICA STATUS\\G command. Use this command to check for errors if you suspect something is wrong.\nWe can create a table with data on the source database and check if it is replicated to the replica database:\nUSE test; CREATE TABLE users (id INT PRIMARY KEY, name VARCHAR(255)); INSERT INTO users VALUES (1, \u0026#39;Alice\u0026#39;); Restore replication after an issue If the replica crashes and comes back up, it may be unable to resume replication from where it left off. If the replica stops replicating due to an error, first try to restart replication on the replica:\nSTOP REPLICA; START REPLICA; Check the replication status for errors using the SHOW REPLICA STATUS\\G command.\nIf the replica still does not replicate, we need to copy the database from the source and restart replication manually.\nCopy database from source and start replication manually Reset the replica:\nSTOP REPLICA; RESET REPLICA ALL; Optionally, drop and recreate the database on the replica:\nDROP DATABASE test; CREATE DATABASE test; If the source database still has the binary log files around from the first time we set up replication, we can redo the original steps using the same source log file and position. If not, we need to back up the source database and restore it on the replica.\nBackup the source database (on port 3308 with database name test):\nbash -c \u0026#39;mysqldump --host 127.0.0.1 --port 3308 -uroot -ptoor test | gzip -\u0026#39; \u0026gt; backup.sql.gz Restore the backup on the replica database (on port 3309):\nbash -c \u0026#39;gzip -dc - | mysql --host 127.0.0.1 --port 3308 -uroot -ptoor test\u0026#39; \u0026lt; backup.sql.gz Now, redo the following steps from above:\nObtain source binary log coordinates Configure replica DB and start replication As you can see, restarting replication after an issue can be more involved than just restarting the replica. Regular backups can help in such situations. When backing up the source database, make sure to include the binary log files, along with the corresponding binary log position. You can then use these files to restore or spin up a new replica while the source database is actively running.\nFurther reading on database scaling Recently, we wrote about database gotchas when scaling applications. One of the issues we summarized was optimizing a MySQL INSERT with subqueries. In the past, we encountered a memory issue with MySQL prepared statements when scaling applications. We also wrote about securing your MySQL Docker container for Zero Trust. Follow along with the MySQL source-replica video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-01-06T00:00:00Z","image":"https://victoronsoftware.com/posts/mysql-master-slave-replication/mysql-master-slave-replication_hu_b96bd16c41a6e112.png","permalink":"https://victoronsoftware.com/posts/mysql-master-slave-replication/","title":"Create a MySQL replica database in 4 short steps (2025)"},{"content":" Example of method overriding with embedded structs What is method overriding? Method overriding is a feature of object-oriented programming languages that allows a subclass to provide a specific implementation of a method already defined in its superclass. When a subclass overrides a method, the subclass\u0026rsquo;s method is called instead of the superclass\u0026rsquo;s method when the object is of the subclass type. Method overriding is an example of polymorphism, where the same method name can have different implementations depending on the object\u0026rsquo;s type.\nGo does not have classes, but it has structs and interfaces that can be used to achieve similar functionality.\nEmbedded structs Go favors composition over inheritance. Multiple smaller types can be combined to create a larger type. A clean way to do this is through embedded structs. When a struct embeds another struct, it inherits the embedded struct\u0026rsquo;s fields and methods. For example, consider the following code:\npackage main import \u0026#34;fmt\u0026#34; type MyPrint struct { i int } func (m* MyPrint) Do() { fmt.Printf(\u0026#34;Hello, World! %d\\n\u0026#34;, m.i) m.i++ } type MyComposedPrint struct { MyPrint } func main() { fmt.Println(\u0026#34;Base:\u0026#34;) var m MyPrint m.Do() m.Do() fmt.Println(\u0026#34;Composed:\u0026#34;) var s MyComposedPrint s.Do() s.Do() } This code outputs the following when run:\nBase: Hello, World! 0 Hello, World! 1 Composed: Hello, World! 0 Hello, World! 1 The composed struct forwards the method call to the embedded struct.\nMethod overriding with embedded structs To override a method in Go, you can define a method with the same name and parameters in the composed struct. For example, consider the following code:\npackage main import \u0026#34;fmt\u0026#34; type MyPrint struct { i int } func (m* MyPrint) Do() { fmt.Printf(\u0026#34;Hello, World! %d\\n\u0026#34;, m.i) m.i++ } type MyComposedPrint struct { MyPrint } func (m*MyComposedPrint) Do() { // call the parent method, similar to super.Do() in other languages m.MyPrint.Do() fmt.Printf(\u0026#34;Hello, Composed World! %d\\n\u0026#34;, m.i) m.i++ } func main() { fmt.Println(\u0026#34;Base:\u0026#34;) var m MyPrint m.Do() m.Do() fmt.Println(\u0026#34;Composed:\u0026#34;) var s MyComposedPrint s.Do() s.Do() } This code outputs the following when run:\nBase: Hello, World! 0 Hello, World! 1 Composed: Hello, World! 0 Hello, Composed World! 1 Hello, World! 2 Hello, Composed World! 3 MyComposedPrint.Do overrides MyPrint.Do. When deciding which method to call, Go first looks for the method in the type itself. If the method is not found, Go looks for the method in the embedded types.\nMethod overriding and interfaces Method overriding is often used in conjunction with interfaces. An interface defines a set of methods a type must implement to satisfy the interface. When a type implements an interface, it can be used wherever the interface is expected. An interface is another example of polymorphism in Go.\nOne typical pattern is overriding a method in a 3rd party library. For example, you can embed a type from a library and override a method to add additional functionality. However, you will still pass the new type in your code using the library\u0026rsquo;s original interface.\nFurther reading We recently described the differences between Go modules and packages. Previously, we explained how to properly unmarshal JSON null, set, and missing fields in Go. We also explained the difference between nil slice and empty slice in Go. Watch method overriding in Go video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-01-01T00:00:00Z","image":"https://victoronsoftware.com/posts/method-overriding-in-go/method-overriding-headline_hu_6ad0be7b0518e4ee.png","permalink":"https://victoronsoftware.com/posts/method-overriding-in-go/","title":"How to override methods in Go"},{"content":"Engineering metrics are essential for tracking your team\u0026rsquo;s progress and productivity and identifying areas for improvement. However, manually collecting and updating these metrics can be time-consuming and error-prone. In this article, we will show you how to automate the tracking of engineering metrics and visualize them in the Google Office suite.\nSome standard engineering metrics include:\nNumber of bugs Lead time for changes (or bug fixes) Code coverage Build/test success rate Deployment frequency Number of incidents Mean time to recovery Delivered story points and many more Engineering metrics can be further sliced and diced in various ways. For example, you can track bugs by severity or on a per-team basis.\nBuilding an engineering metrics tracker For our example metrics tracker, we will gather the number of GitHub open bugs for a team, update the numbers in a Google Sheet, automate the process with GitHub Actions, and display the data in Google Docs.\nAll the tools for this flow are freely available, and this process does not rely on costly third-party metrics-gathering services. We will use the Go programming language in our example.\nGathering the number of open bugs The GitHub API is a well-documented way to query issues in a repository.\nThere are also many quality client libraries for the API. We will use the go-github client.\nCreate a git repository and set up a new Go module. Here is our code snippet to get the number of open bugs:\nimport ( \u0026#34;github.com/google/go-github/v67/github\u0026#34; ) func getGitHubIssues(ctx context.Context) ([]*github.Issue, error) { githubToken := os.Getenv(\u0026#34;GITHUB_TOKEN\u0026#34;) client := github.NewClient(nil).WithAuthToken(githubToken) // Get issues. var allIssues []*github.Issue opts := \u0026amp;github.IssueListByRepoOptions{ State: \u0026#34;open\u0026#34;, Labels: []string{\u0026#34;#g-mdm\u0026#34;, \u0026#34;:release\u0026#34;, \u0026#34;bug\u0026#34;}, } for { issues, resp, err := client.Issues.ListByRepo(ctx, \u0026#34;fleetdm\u0026#34;, \u0026#34;fleet\u0026#34;, opts) if err != nil { return nil, err } allIssues = append(allIssues, issues...) if resp.NextPage == 0 { break } opts.Page = resp.NextPage } return allIssues, nil } The above code snippet uses a GITHUB_TOKEN environment variable to authenticate with the GitHub API. You can create a personal access token in your GitHub account settings. Later, we will show how to set this token in GitHub Actions automatically. The token is optional for public repositories but required for private repositories.\nThe code snippet queries the open issues in the public fleetdm/fleet repository with the labels #g-mdm, :release, and bug. Fleet\u0026rsquo;s MDM product team currently uses these labels for bugs in progress or ready to be worked on.\nUpdating the Google Sheets spreadsheet To update the Google Sheets spreadsheet, we will use the Google Sheets API with the Google\u0026rsquo;s Go client library.\nFor instructions on getting a Google Sheets API key, sharing the spreadsheet with a service account, and editing the spreadsheet using the API, see our previous article: How to quickly edit Google Sheets spreadsheet using the API.\nSee our integrated function to update the Google Sheets spreadsheet with the number of open bugs on GitHub.\nIn our example, we get the spreadsheet ID and the service account key from environment variables. When running locally, you must set the SPREADSHEET_ID and GOOGLE_SERVICE_ACCOUNT_KEY environment variables.\nspreadsheetId := os.Getenv(\u0026#34;SPREADSHEET_ID\u0026#34;) serviceAccountKey = []byte(os.Getenv(\u0026#34;GOOGLE_SERVICE_ACCOUNT_KEY\u0026#34;)) The glue code combining the above two functions is straightforward.\nfunc main() { ctx := context.Background() allIssues, err := getGitHubIssues(ctx) if err != nil { log.Fatalf(\u0026#34;Unable to get GitHub issues: %v\u0026#34;, err) } fmt.Printf(\u0026#34;Total issues: %d\\n\u0026#34;, len(allIssues)) err = updateSpreadsheet(len(allIssues)) if err != nil { log.Fatalf(\u0026#34;Unable to update spreadsheet: %v\u0026#34;, err) } } We can manually run our script to gather the metrics and update the Google Sheets spreadsheet. However, we want to automate this process so that the metrics are always up to date and we have a consistent historical record.\nAutomating the metric-gathering process with GitHub Actions GitHub Actions allows you to automate, customize, and execute your software development workflows in your GitHub repository. We will use GitHub Actions to run our script on a schedule and update the Google Sheets spreadsheet with the latest metrics.\nCreate a .github/workflows/update-spreadsheet.yml file in your repository with the following content:\nname: Update spreadsheet with latest metrics on: workflow_dispatch: # Manual schedule: - cron: \u0026#39;0 */12 * * *\u0026#39; # At 00:00 and 12:00 UTC env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # automatically generated GOOGLE_SERVICE_ACCOUNT_KEY: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_KEY }} SPREADSHEET_ID: ${{ secrets.SPREADSHEET_ID }} jobs: update-spreadsheet: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2 with: fetch-depth: 0 - name: Setup Go uses: actions/setup-go@3041bf56c941b39c61721a86cd11f3bb1338122a # v5.2.0 with: go-version-file: \u0026#39;go.mod\u0026#39; - name: Run run: go run main.go The above GitHub Actions workflow runs the main.go script every 12 hours. GitHub automatically generates the GITHUB_TOKEN secret. The GOOGLE_SERVICE_ACCOUNT_KEY and SPREADSHEET_ID secrets must be set up manually in the repository settings.\nThe workflow checks out the code, sets up Go, and runs the script. After pushing the workflow file to GitHub, you can manually run the workflow to test it.\nDisplay the metrics in Google Docs To see the metrics in Google Docs or Google Slides, you can copy and paste the relevant cells from the Google Sheets spreadsheet. This operation will create a one-way link from Google Sheets to the document. You can refresh the data by clicking Tools \u0026gt; Linked objects \u0026gt; Update All.\nFurther reading Free, flexible, and insightful: engineering metrics done right\nDiscover how to monitor and visualize your dev team\u0026rsquo;s performance with Grafana and an analytical database.\nRecently, we explained how to measure unreadable code and turn it into clean code, as well as how to make incremental improvements to your codebase with evolutionary architecture.\nPreviously, we showed how to reuse workflows and steps in GitHub Actions.\nWe also covered measuring the execution time of Go tests.\nWe also described inefficiencies in the GitHub code review process.\nCode on GitHub For the complete code, see the GitHub repository: github-metrics.\nWatch Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-12-26T00:00:00Z","image":"https://victoronsoftware.com/posts/track-engineering-metrics/engineering-metrics_hu_4b5d14f945936369.png","permalink":"https://victoronsoftware.com/posts/track-engineering-metrics/","title":"How to easily track engineering metrics"},{"content":" Get a Google Sheets API key Share a Google Sheets spreadsheet with the service account Edit a Google Sheets spreadsheet using the API When you need to edit a Google Sheets spreadsheet quickly, you can use the Google Sheets API. The API allows you to programmatically read, write, and update data in a Google Sheets spreadsheet. However, following the Google Sheets API documentation can be overwhelming. In this article, we will show you how to get a Google Sheets API key and edit a Google Sheets spreadsheet using the API.\nUser authentication (OAuth) vs. API key (JWT) The problem is that the Google API documentation focuses on the OAuth 2.0 user authentication flow. This flow is useful when you need to access Google Sheets on behalf of a user. For example, you\u0026rsquo;re creating your web app, and you need to read or write data in a Google Sheets spreadsheet owned by your web app user. The OAuth standard allows Google to authenticate the user and authorize your app to access the user\u0026rsquo;s data without exposing the user\u0026rsquo;s credentials to your web app. The OAuth flow interacts with three parties \u0026ndash; the user, your web app, and Google.\nIn our case, we want to access a specific Google Sheets spreadsheet programmatically without user interaction. We can use the API key (JWT) authentication method. JWT stands for JSON Web Token, a standard for securely transmitting information between two parties. This method allows us to access Google Sheets programmatically without user interaction. The API key (JWT) method interacts with two parties: your app and Google.\nGet a Google Sheets API key To get a Google Sheets API key, follow these steps:\nGo to the Google Cloud Console. Create a new project or select an existing project. In the new project, go to APIs \u0026amp; Services and enable the Google Sheets API. After enabling it, you should see it in the Enabled APIs \u0026amp; services list. Go to APIs \u0026amp; Services \u0026gt; Credentials and create a new Service Account. This account does not need any optional permissions. Create a new JSON key for the service account. After creating the key, the JSON file will be automatically downloaded to your computer. This file contains the credentials for your service account. Keep it secure. For example, we received axial-paratext-444915-f9-8ef1de636587.json with the following content: { \u0026#34;type\u0026#34;: \u0026#34;service_account\u0026#34;, \u0026#34;project_id\u0026#34;: \u0026#34;axial-paratext-444915-f9\u0026#34;, \u0026#34;private_key_id\u0026#34;: \u0026#34;8ef1de636587238a028addaa8be9dbdf1d406420\u0026#34;, \u0026#34;private_key\u0026#34;: \u0026#34;-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDGFXmEc6VK0TO9\\n2E/LDel4gTYl1u8uZGtX16B7Lo4ufM7ics3h9Gyi1lJMcGrHruGEzatDeTRclILd\\nLhLwrckfl3IF9MIsqwaEkHk7YnUXj9zGl+v8LTGJL0ycQ9hVdoD6cCOOAmghLj8F\\n9Sl6KQ5PHGbBUUL4qi8uExKY4tQOrqol1Pi3RPpAOCR6BLC/ZFPp+4e4HRhF+DMD\\nI1QX8QwPit9XdIomnZPUL5sGD+q4cp1gHLBuBp2ehyiFI5MGhgzvCIQzaTExw7GK\\nqrjYMjBKXaFRqGpZJWJMdVmGGHpmZLCL/wQmhujlThrF0FO8BHAGriAvUgDbH7m/\\nMzGRk/KzAgMBAAECggEAE7LwBkWP8xxR9nfMG6fzB3pmHaY93BG9gRtfCNEM77+W\\nvXtoUSfDJACHZ7WoUNpp8BCaDxg/JlPYndFmrcvCnCMuAjygkNujRsytWcQFXAYB\\nETjrjYUbD4cGKeYvXfRuiDldt9Iyc9ZLCzch3FW36BMtft0reVpHXeAksdKg/yKf\\nhM4jw3tQMu5JR3trLHtqwaA8VUav7I2Qn8nxEbB/0+AUatqpDOp/hQNTN7MGZ/i7\\n4V0538U7C9RYDzk9hBBT2/IegGixlL0lX2V6LjYlRcEgC0PLuKF7gM/RRnorNPnv\\nHfHxyZt6/MyI8RLRwwv05ZSITaOj66lXxReVsMXxhQKBgQD+vJPrvKD7mGiXRQto\\nh1LJqPcyknzLzxf2OX5vZyF8asdroU0sRy8pYYHy8JCPlkOJ0fj9kq6R79W+rmn3\\npFkvwRY9dUcJLpoMAMEfO4wQp3QKpxdkjMS8xGcEVOIZacAHCof7uUwrHUUcqRIq\\nwrgZcj5P8ZjwsLmuqLNeXqFYNwKBgQDHEPf4PCyjieF+aGvaOPLfn/lRBbEQGlrg\\n9Z09UXpxcW39RMq7MkS+U9m88Kn9MsEK3umJdP+s5m8ddVdIVgZLj96Ufn52RzRT\\ne8crSjCVC3oQaloScvOBSQA1Z3Bn+QstIko042i2qTNJWMArdCJe9uRbwL1hqEvx\\n+LtNPniDZQKBgQDX4g9maFzx/G8fS+doNc8mkmi01kqnGyJOjNknJnrNi1zoTTIv\\nBUDly/oqXk/VMF6ajXV7yPTjPyOhTwUFV6Yx/2yOtzZ1hKYO6BDDHF8Ouitw37zG\\nfTo6VCSOGjXnnaSdEwK9hYMUwuCQcoSv8oe9IQHIFJMt4EfsypIAtyf7rwKBgFtC\\ntzvRcnGC+6K1AoTnyMimkWkIn/UO8Azj7TM4UFcDtnX+/KY3VHahAFhzSKswgnmW\\nWiBPSAufFN+/dMVP0tD/Yv5Ww2k8GYwQWe3JtF4QBeTSrPp6QpJJwlO5WToBXZNS\\nfgyjGNVs2ntMucTyF/PLYkOCKBBGVJLZAh1Wf29VAoGADf7a1l8kDKgokm6pc4qG\\nzb97GMk1CpE0dGl31dvx2ilckDVP354yfWEwVXWWVfVSq/LQdJVgkdArYbAPdsPb\\nYuUfNwXMSp/OjmEL2QyC2zRm+2ZZZt5bcnPRbYETzb2An8kDYX49vwgBLJXpLOmt\\nlCvxUDyoASHgAMu+OlqHIh4=\\n-----END PRIVATE KEY-----\\n\u0026#34;, \u0026#34;client_email\u0026#34;: \u0026#34;example@axial-paratext-444915-f9.iam.gserviceaccount.com\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;114906617333001451487\u0026#34;, \u0026#34;auth_uri\u0026#34;: \u0026#34;https://accounts.google.com/o/oauth2/auth\u0026#34;, \u0026#34;token_uri\u0026#34;: \u0026#34;https://oauth2.googleapis.com/token\u0026#34;, \u0026#34;auth_provider_x509_cert_url\u0026#34;: \u0026#34;https://www.googleapis.com/oauth2/v1/certs\u0026#34;, \u0026#34;client_x509_cert_url\u0026#34;: \u0026#34;https://www.googleapis.com/robot/v1/metadata/x509/example%40axial-paratext-444915-f9.iam.gserviceaccount.com\u0026#34;, \u0026#34;universe_domain\u0026#34;: \u0026#34;googleapis.com\u0026#34; } Share a Google Sheets spreadsheet with the service account Share the spreadsheet with the service account email address to allow the account to access a Google Sheets spreadsheet. In our case, the service account email is example@axial-paratext-444915-f9.iam.gserviceaccount.com.\nNote the spreadsheet ID from the URL. For example, in the URL https://docs.google.com/spreadsheets/d/1QCtnB6MXfFJLZsBE1E2vq5FxKBgh1Q0s727wRxFkmX4/edit, the spreadsheet ID is 1QCtnB6MXfFJLZsBE1E2vq5FxKBgh1Q0s727wRxFkmX4.\nEdit a Google Sheets spreadsheet using the API Now that we have the Google Sheets API key and editor permissions for the target spreadsheet, we can edit it using the API. For this example, we will use the Go programming language.\nIn an empty directory, create a Go project and get the necessary dependencies:\ngo mod init google-sheets-api go get golang.org/x/oauth2@v0.24.0 go get google.golang.org/api@v0.211.0 Copy the JSON key file to the project directory and rename it to credentials.json.\nCreate a new Go file, main.go, with the following content:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;golang.org/x/oauth2/google\u0026#34; \u0026#34;google.golang.org/api/option\u0026#34; \u0026#34;google.golang.org/api/sheets/v4\u0026#34; ) const spreadsheetId = \u0026#34;1QCtnB6MXfFJLZsBE1E2vq5FxKBgh1Q0s727wRxFkmX4\u0026#34; func main() { ctx := context.Background() serviceAccountKey, err := os.ReadFile(\u0026#34;credentials.json\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Unable to read client secret file: %v\u0026#34;, err) } cfg, err := google.JWTConfigFromJSON(serviceAccountKey, sheets.SpreadsheetsScope) if err != nil { log.Fatalf(\u0026#34;Unable to parse client secret file to config: %v\u0026#34;, err) } client := cfg.Client(ctx) srv, err := sheets.NewService(ctx, option.WithHTTPClient(client)) if err != nil { log.Fatalf(\u0026#34;Unable to retrieve Sheets client: %v\u0026#34;, err) } readRange := \u0026#34;Sheet1!A2:B2\u0026#34; resp, err := srv.Spreadsheets.Values.Get(spreadsheetId, readRange).Do() if err != nil { log.Fatalf(\u0026#34;Unable to retrieve data from sheet: %v\u0026#34;, err) } if len(resp.Values) == 0 { fmt.Println(\u0026#34;No data found.\u0026#34;) } else { fmt.Println(\u0026#34;Date, Value:\u0026#34;) for _, row := range resp.Values { fmt.Printf(\u0026#34;%s, %s\\n\u0026#34;, row[0], row[1]) } } } Replace the spreadsheetId constant with the ID of your target spreadsheet.\nThis code authenticates with the Google Sheets API using the service account key and reads the data from cells A2 and B2 in the Sheet1 sheet of the target spreadsheet.\nUpdate the dependencies and run the program:\ngo mod tidy go run main.go The result should look like:\nDate, Value: 2024-12-16 10:00:00, 10 To write data to a Google Sheets spreadsheet, use the spreadsheets.Values.Update and Spreadsheets.BatchUpdate methods. For example, the following modified code inserts a new row above other rows with the current date and an incremented value:\nWe can review the spreadsheet to verify that our code added the new row.\nNext steps Automate tracking of engineering metrics. Further reading Recently, we covered how to set up a remote development environment. Previously, we showed how to build a webhook flow with Tines. Watch how to edit Google Sheets spreadsheet using the API Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-12-18T00:00:00Z","image":"https://victoronsoftware.com/posts/google-sheets-api/google-sheets-headline_hu_2dc0f36bd926bc9c.png","permalink":"https://victoronsoftware.com/posts/google-sheets-api/","title":"How to quickly edit Google Sheets spreadsheet using the API"},{"content":" How to find the plist file for a running process Create and edit .plist files with PlistBuddy What is launchd? launchd is a macOS system service manager that starts, stops, and manages daemons, agents, and other processes. It is the first process the kernel starts and is responsible for starting all other processes on the system.\nIf you go to Activity Monitor on your Mac and View \u0026gt; All Processes, Hierarchically, you will see that all processes are children of launchd.\nWhat are launchd agents and daemons? launchd can start and manage agents and daemons.\nDaemons Daemons are background processes that run without a user interface. They typically start at boot time and run continuously in the background. One example of a daemon is Apple\u0026rsquo;s timed time synchronization daemon, which maintains system clock accuracy by synchronizing the clock with reference clocks over the network. Another example is a device management daemon, such as Fleet\u0026rsquo;s orbit, which manages the device\u0026rsquo;s configuration and security settings.\nAgents Agents are similar to daemons but run in the context of a user session. They are started when a user logs in and can interact with the user interface. Agents are helpful for tasks that need to run in the background but also need to communicate with the user. For example, a security agent can check the system\u0026rsquo;s state and notify the user if they fail a corporate security policy.\nAgents may or may not have a user interface. Many 3rd party agents run in the background and provide a menu bar icon to configure the agent\u0026rsquo;s behavior.\nHow are agents and daemons configured with plist? launchd uses property list (.plist) files to define the configuration of agents and daemons. These files specify the program to run, the arguments to pass, the environment variables to set, and other settings.\nHere is an example of a .plist file for a launchd daemon:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;EnvironmentVariables\u0026lt;/key\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;ORBIT_ENROLL_SECRET_PATH\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;/opt/orbit/secret.txt\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ORBIT_FLEET_URL\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;https://dogfood.fleetdm.com\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ORBIT_ENABLE_SCRIPTS\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;true\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ORBIT_ORBIT_CHANNEL\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;stable\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ORBIT_OSQUERYD_CHANNEL\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;stable\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ORBIT_UPDATE_URL\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;https://updates.fleetdm.com\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ORBIT_FLEET_DESKTOP\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;true\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ORBIT_DESKTOP_CHANNEL\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;stable\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ORBIT_UPDATE_INTERVAL\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;15m0s\u0026lt;/string\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;key\u0026gt;KeepAlive\u0026lt;/key\u0026gt; \u0026lt;true/\u0026gt; \u0026lt;key\u0026gt;Label\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;com.fleetdm.orbit\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ProgramArguments\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;string\u0026gt;/opt/orbit/bin/orbit/orbit\u0026lt;/string\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;key\u0026gt;RunAtLoad\u0026lt;/key\u0026gt; \u0026lt;true/\u0026gt; \u0026lt;key\u0026gt;StandardErrorPath\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;/var/log/orbit/orbit.stderr.log\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;StandardOutPath\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;/var/log/orbit/orbit.stdout.log\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ThrottleInterval\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;10\u0026lt;/integer\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt; The typical locations for agent and daemon .plist files are:\nType Location User Agents ~/Library/LaunchAgents Global Agents /Library/LaunchAgents System Agents /System/Library/LaunchAgents Global Daemons /Library/LaunchDaemons System Daemons /System/Library/LaunchDaemons Note: In rare cases, the .plist files may be located in other directories or missing entirely.\nHow to view the contents of a .plist file .plist files come in several formats, including binary, XML, and JSON.\nYou can view the contents of a .plist file using the plutil (property list utility) command. For example:\nplutil -p /System/Library/LaunchDaemons/com.apple.analyticsd.plist plutil can also convert between different .plist formats. For example, to convert a binary .plist file to XML, run:\ncp /System/Library/LaunchDaemons/com.apple.analyticsd.plist my.plist plutil -convert xml1 my.plist Can I use a .plist file for cron-like scheduling? Yes, you can use launchd to schedule tasks in a .plist file. launchd is the recommended alternative to cron on macOS. The StartCalendarInterval key specifies when the task should run. For example, to run a task every day at 5 AM, you can add the following to your .plist file:\n\u0026lt;key\u0026gt;StartCalendarInterval\u0026lt;/key\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;Hour\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;5\u0026lt;/integer\u0026gt; \u0026lt;key\u0026gt;Minute\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;0\u0026lt;/integer\u0026gt; \u0026lt;/dict\u0026gt; How to find the plist file for a running process Suppose you identified a process running on your Mac from Activity Monitor and want to find the .plist file that started it. The process should be a child of launchd.\nTo find the identifier of a running process, you can use the launchctl command. The launchctl list command lists all agents and daemons started by the user, while the sudo launchctl list lists all agents and daemons started by the system.\nFor example, to find the identifier of the process with PID 62303, run:\n( /usr/bin/sudo launchctl list; launchctl list ) | grep 62303 The output will show the identifier, such as:\n62303 0 com.fleetdm.orbit You can now look in the standard locations for the com.fleetdm.orbit.plist file. Alternatively, you can use the launchctl dumpstate command to dump the state of all launchd jobs, including the .plist files that started them. For example, in a macOS system with Fleet\u0026rsquo;s orbit running, you can run:\nlaunchctl dumpstate | grep -B 1 -A 4 -E \u0026#34;active count = [1-9]\u0026#34; | grep com.fleetdm.orbit And the output will show the path to the .plist file:\nsystem/com.fleetdm.orbit = { path = /Library/LaunchDaemons/com.fleetdm.orbit.plist You can now view the contents of the .plist file to understand how the process was started.\nplutil -p /Library/LaunchDaemons/com.fleetdm.orbit.plist Create and edit .plist files with PlistBuddy PlistBuddy is a powerful built-in macOS tool for creating and editing .plist files. You can use it to automate the creation and modification of launchd agents and daemons.\nYou can create and edit .plist files using the PlistBuddy. For example, to create a new .plist file with a key-value pair, run:\n/usr/libexec/PlistBuddy -c \u0026#34;Add :Label string com.fleetdm.orbit\u0026#34; com.fleetdm.orbit.plist To edit an existing .plist file, use the -c flag with the Set command. For example, to change the above Label key to com.fleetdm.orbit2, run:\n/usr/libexec/PlistBuddy -c \u0026#34;Set :Label com.fleetdm.orbit2\u0026#34; com.fleetdm.orbit.plist Run /usr/libexec/PlistBuddy --help for more information on using PlistBuddy.\nFurther reading Recently, we showed two ways to turn a script into a macOS install package. Previously, we explained how to configure mTLS using the macOS keychain. We also covered how to create signed URLs with AWS CloudFront. Watch the video on launchd agents and daemons, and how to find the plist file for a running process Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-12-11T00:00:00Z","image":"https://victoronsoftware.com/posts/macos-launchd-agents-and-daemons/macos-agents-and-daemons_hu_b2be6db1d8fc0835.png","permalink":"https://victoronsoftware.com/posts/macos-launchd-agents-and-daemons/","title":"What are launchd agents and daemons on macOS?"},{"content":"What is a Go package? A Go package is a collection of Go source files in the same directory that are compiled together. It can contain functions, types, and variables. Go packages organize code and provide a way to share code between different program parts.\nBy convention, the package name is the same as the last element of the import path. For example, the package name for the fmt package in the fmt directory is fmt. You can name the package differently from the directory name, but it is not recommended.\nWhen a Go application is compiled, the Go compiler compiles all the packages imported by the main package. The main package contains the main function and is the entry point of the program. However, all the imported packages can be compiled separately as well, even if they do not contain the main function. That\u0026rsquo;s what happens when we run unit tests for all packages in a Go project ‚Äî each package is compiled separately and can be tested independently.\nThe directory structure does not have to match the package dependencies. For example, given the following directory structure:\nservice/ service.go api/ api.go db/ db.go The packages service, api, and db can be completely independent. The service package does not include the api.go and db.go files because they are in a different directory. Go\u0026rsquo;s package dependency graph has no relation to the directory structure.\nWhat is a Go module? A Go module, introduced in Go 1.11, is a collection of Go packages that are versioned together. A Go module is defined by a go.mod file located at the module\u0026rsquo;s root. The go.mod file contains the module name and the versions of the dependencies the module uses.\nA Go module can contain multiple packages, and the packages do not need to be related to each other.\nSample project with two modules A Go module can exist at any level of the directory structure, even nested within another module. The Go toolchain treats all Go modules as independent entities, regardless of their location in the directory structure.\nHow to use multiple modules in one Go workarea 1. Create a new Go workarea: mkdir -p go-modules cd go-modules go mod init example.com/go-modules 2. Create a simple Go file in the root of the workarea: package main import \u0026#34;fmt\u0026#34; func main() { fmt.Printf(\u0026#34;Hello, world.\\n\u0026#34;) } 3. Create a new Go module in a subdirectory: mkdir -p adder cd adder go mod init example.com/go-modules/adder With the following Go file in the adder directory:\npackage adder func Add(a, b int) int { return a + b } 4. Use the adder package in the main package: You cannot simply import the adder package in the main package because they are in different modules.\nOne way to use the adder package is to publish it and use go get to download it as the main dependency. However, this is impractical for local development and doesn\u0026rsquo;t make sense when we explicitly want to use multiple modules in one repository.\nThe proper way is to use a workspace, declared in the go.work file, introduced in Go 1.18.\nIn the top level of the workarea, create a go.work file with all the modules in the subdirectories:\ngo work use -r . The resulting go.work file will look like this:\ngo 1.23.2 use ( . ./adder ) Now, you can import the adder package in the main package:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;example.com/go-modules/adder\u0026#34; ) func main() { fmt.Printf(\u0026#34;Hello, world.\\n\u0026#34;) fmt.Print(myadder.Add(1, 2)) } Should I use multiple Go modules in one repository? Generally, it is not recommended to use multiple Go modules in one repository. However, there are some cases where it makes sense, like:\nTemporarily pull in a third-party module to add a feature before this feature is merged upstream. Work on multiple interdependent modules that can be versioned and released independently. Usually, it is better to use packages within a single module to organize and decouple code.\nFurther reading Read the follow-up article on how to find package dependencies of a Go package. We also explained why modularity is important in software. Recently, we covered method overriding in Go. We also wrote about using the staticcheck linter on a large Go project. Previously, we described how to use Go to unmarshal JSON null, set, and missing fields. We also published an article on accurately measuring the execution time of Go tests. Multiple modules in one Go project on GitHub Example Go project with multiple modules\nWatch an explanation of Go modules and packages, along with an example of using multiple modules in one repository Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-12-04T00:00:00Z","image":"https://victoronsoftware.com/posts/go-modules-and-packages/go-modules-and-packages_hu_85866624f192ce69.png","permalink":"https://victoronsoftware.com/posts/go-modules-and-packages/","title":"Go modules and packages ‚Äî which one to use and when"},{"content":"What is a VLAN? VLAN (Virtual Local Area Network) technology allows you to create multiple isolated networks on a single physical network. For example, a single ethernet wire or a WLAN (wireless LAN) can support multiple VLANs. VLANs improve network security, performance, and scalability.\nHow does VLAN improve network security? VLAN improves network security by isolating devices into separate networks. This isolation prevents devices in one VLAN from communicating with devices in another VLAN. For example, you can create a separate VLAN for your IoT (Internet of Things) devices, such as smart light bulbs and thermostats, to prevent them from accessing your primary network. You can also create a separate VLAN for guest devices to prevent them from accessing your main network and other VLANs.\nFor example, if a hacker gains access to a device in the IoT VLAN, they won\u0026rsquo;t be able to access devices in the office VLAN or the guest VLAN. This isolation limits the damage that a hacker can do to your network.\nHow does VLAN work? VLAN works by adding a VLAN tag to each network packet. The VLAN tag contains the VLAN ID, which identifies the VLAN to which the packet belongs. Network switches use the VLAN tag to forward packets only to devices in the same VLAN. Routers can route packets between different VLANs based on their VLAN tags.\nHow to set up VLANs in your home network Unfortunately, setting up a VLAN in your home network is not as simple as flipping a switch. Multiple parts of your home network need to be configured, and some older or cheaper hardware, such as no-configuration network switches, may not support VLANs.\nSelecting VLAN tags and IP ranges Before configuring VLANs, decide how many VLAN tags you need and what each tag will represent. Also, determine what IP ranges will map to each VLAN. Some people map the VLAN ID to the third octet of the IP address. For example, VLAN 333 may use the IP range 10.0.333.0/24.\nOn the other hand, there is a security argument for using random VLAN IDs. If a hacker gets access to your network, they won\u0026rsquo;t know what each VLAN ID represents and may even have difficulty figuring out which VLAN IDs are active. This security approach is often referred to as security through obscurity.\nSome common VLANs are:\nOffice IoT Guest Media Router interfaces The network router interface is the first place to configure VLANs. A router interface is a physical or virtual router port connecting to a network. So, you must configure the router interface to support your VLAN-selected tags.\nDHCP Dynamic Host Configuration Protocol (DHCP) is a network protocol that automatically assigns IP addresses to devices on a network.\nYou need to configure a DHCP server to assign IP addresses to devices in each VLAN. You can use the same DHCP server for all VLANs but must configure it to assign IP addresses from different ranges for each VLAN.\nFirewall You need to configure firewall rules for each VLAN to control what traffic is allowed in and out of the VLAN. For example, you may not allow devices on the Guest VLAN to access devices on the other VLANs.\nBelow is an example of our firewall rules for the GUEST VLAN.\nThe rules allow access to our local DNS server to block inappropriate content. The rules block all private networking IPs (as defined by RFC 1918) except the VLAN\u0026rsquo;s subnet.\n10.0.0.0/8 (10.0.0.0 - 10.255.255.255) 172.16.0.0/12 (172.16.0.0 - 172.31.255.255) 192.168.0.0/16 (192.168.0.0 - 192.168.255.255) Network switch You need to configure the VLAN tags for each port on the network switch. For example, you may configure port 1 to be part of the Office VLAN and port 2 to be part of the IoT VLAN. A single port can be part of multiple VLANs, often required for a wireless access point.\nBelow is an example of the network switch configuration for our GUEST VLAN.\nThe tagged ports include three wireless access points and the router port.\nWireless Access Point (WAP) You need to configure the SSIDs (Service Set Identifiers) for each VLAN on your wireless access point. In our case, we created a WLAN for each VLAN‚ÄîOffice, IoT, and Guest. Each WLAN is associated with a VLAN tag, which we set in the advanced options of the WLAN configuration, as shown below.\nGuest network Before adding a VLAN for our guest network, we used the \u0026ldquo;Guest Mode\u0026rdquo; feature on our WAP (Wireless Access Point). This feature was secure because it isolated guest devices from our primary network. However, the user experience for our guests was terrible.\nThe guest network directed users to a captive portal before granting them Internet access. Some child guests could not access the captive portal due to parental device restrictions. Guests' devices also had trouble reconnecting to the guest network on a subsequent visit \u0026ndash; they were not automatically reconnected.\nSwitching to a VLAN-based guest network significantly improved the user experience.\nHow to specify VLAN on a wired connection A single wired ethernet connection may be part of multiple VLANs. You can connect your computer to different VLANs for testing or security reasons.\nOn a wired connection, you can specify the VLAN ID in your device\u0026rsquo;s network settings. For example, you can add a virtual interface with a specific VLAN tag on macOS.\nDebugging notes While setting up our VLANs, we encountered the issue of our computer not getting an IP address from the DHCP. After reviewing the settings on our router and switch, we found that our settings did not save for some reason. Make sure to reload your settings after making changes to ensure they stick.\nFurther reading In the past, we discussed how to set up a virtual router.\nWe also covered how to create an IPv6-only linux server.\nSecuring Private Keys with TPM 2.0: A Developer‚Äôs Guide A hands-on walkthrough of using TPM 2.0 for hardware-backed key protection, featuring code examples and practical usage patterns.\nWatch us discuss what is a VLAN and why you need it in your home network Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-11-27T00:00:00Z","image":"https://victoronsoftware.com/posts/why-you-need-vlan/vlan-house_hu_67f10a9ee27478b2.png","permalink":"https://victoronsoftware.com/posts/why-you-need-vlan/","title":"What is a VLAN and why you need it in your home network"},{"content":" Obtain VPN connection details Point your computer to the remote Active Directory DNS server Join your computer to the Active Directory domain Log in with your Active Directory credentials What is Active Directory? Active Directory is a directory service developed by Microsoft for Windows domain networks. It provides authentication and authorization services and a framework for organizing and managing resources in a networked environment. Active Directory stores information about users, computers, and other network objects, making managing and securing your network easier.\nActive Directory runs on Windows Server and is the central component of many Windows-based networks. It is the central piece for a variety of services, including:\nCertificate Services (AD CS) Lightweight Directory Services (AD LDS) Federation Services (AD FS) Rights Management Services (AD RMS) and many others Why connect to a remote Active Directory server? With the rise of remote work and distributed teams, you may need to connect your home computer to a remote Active Directory server. This connection allows you to access network resources, authenticate with your company\u0026rsquo;s domain, and use services that rely on Active Directory.\nThe recommended way to connect to a remote Active Directory server is through a VPN (Virtual Private Network). A VPN creates a secure connection between your computer and the remote network, allowing you to access resources as if you were physically connected.\nSteps to connect to a remote Active Directory server 1. Obtain VPN connection details Contact your IT department or network administrator to obtain the VPN connection details. You will need the following information:\nVPN server address VPN type (e.g., PPTP, L2TP/IPsec, OpenVPN) VPN username and password Any additional settings or requirements, such as a private key or certificate For our example, we are using WireGuard, a modern VPN protocol known for its simplicity and security.\nNote: The Allowed IPs field above specifies which IP addresses will be routed through the VPN. Make sure to include the IP addresses of the remote Active Directory server. Also, ensure the IP addresses do not conflict with your local network.\nInstall the VPN client on your computer and activate the VPN connection using the provided details.\nTest the VPN connection by pinging the remote Active Directory server.\nPS C:\\Users\\victor\u0026gt; ping 10.98.1.1 Pinging 10.98.1.1 with 32 bytes of data: Reply from 10.98.1.1: bytes=32 time=155ms TTL=127 Reply from 10.98.1.1: bytes=32 time=156ms TTL=127 Reply from 10.98.1.1: bytes=32 time=156ms TTL=127 Reply from 10.98.1.1: bytes=32 time=156ms TTL=127 Ping statistics for 10.98.1.1: Packets: Sent = 4, Received = 4, Lost = 0 (0% loss), Approximate round trip times in milli-seconds: Minimum = 155ms, Maximum = 156ms, Average = 155ms 2. Point your computer to the remote Active Directory DNS server To use Active Directory, your computer must know where to find the domain controller. This information is stored in the DNS (Domain Name System) settings.\nSince we are using a VPN to connect to the remote Active Directory server, we need to update the DNS settings of our VPN connection.\nIn the VPN settings above, we specified the DNS server as part of the WireGuard configuration.\nDNS = 10.98.1.1 Alternatively, we can manually set the DNS server for the VPN connection in Control Panel \u0026gt; Network and Sharing Center \u0026gt; your VPN connection \u0026gt; Properties \u0026gt; Networking \u0026gt; Internet Protocol Version 4 (TCP/IPv4) \u0026gt; Properties.\n3. Join your computer to the Active Directory domain Open Control Panel \u0026gt; System and Security \u0026gt; System \u0026gt; Domain or workgroup \u0026gt; Change\u0026hellip;.\nEnter the domain name provided by your IT department. You can also change your computer name if necessary.\nClick OK and enter your domain credentials when prompted. You must have permission from Active Directory to join a computer to the domain.\nAfter a few seconds, you should see a message indicating that your computer has successfully joined the domain.\nYou must restart your computer for the changes to take effect.\n4. Log in with your Active Directory credentials Your VPN connection must be active to log in with your Active Directory credentials after joining the domain. Some VPN clients allow you to connect before logging in to Windows. This feature ensures that your computer can reach the domain controller during the login process.\nIf your VPN client does not support connecting before logging in, you may need to log in with a local account first and then connect to the VPN. Then, you can switch users and login with your Active Directory credentials.\nAdditional information The local computer caches credentials for Active Directory users. You can log in to your computer with your Active Directory credentials on subsequent logins, even when you are not connected to the domain. For example, you can log in and then connect to the VPN.\nAfter joining the domain, we found that our local computer refused SSH connections from other local computers. We resolved this issue by allowing SSH access in Active Directory settings.\nFurther reading Recently, we covered how to test a Windows NDES SCEP server. Previously, we explained how to code sign a Windows application. Watch how to connect to remote Active Directory server Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-11-20T00:00:00Z","image":"https://victoronsoftware.com/posts/connect-to-remote-active-directory/connect-to-ad-headline_hu_34208139348c5ed8.png","permalink":"https://victoronsoftware.com/posts/connect-to-remote-active-directory/","title":"How to connect to remote Active Directory server in 4 steps"},{"content":" Create a script-only install package using the pkgbuild command Create a script-only install package using the Packages app What is a macOS install package? A macOS install package is a file that contains the files and scripts needed to install an application on a macOS system. It is commonly used to distribute software to macOS users and can contain multiple files, scripts, and metadata.\nWhy create a macOS install package that only runs a script? Sometimes, you must distribute a script that performs a specific task on a macOS system, such as fixing a known issue. You can create a macOS install package that contains the script and any other files needed to run that script. This workflow allows you to distribute the script as an install package that users can easily install on their macOS systems.\nAnother reason to create a macOS install package that only runs a script is to use a third-party installer instead of the built-in macOS installer. This custom installer can provide additional features and customization options.\nCreate a script-only install package using the pkgbuild command The pkgbuild command is a command-line tool included with macOS. It allows you to create macOS install packages from the command line.\nFirst, create a directory for your script files:\nmkdir Scripts Create a script file in the above directory called postinstall that contains the script you want to run. Below is an example script for testing:\n#!/bin/bash # installer script variables: # $0 = path to the script # $1 = path to the package # $2 = target location, i.e., /Applications # $3 = target volume, i.e., /Volumes/Macintosh HD # $4 = \u0026#34;/\u0026#34; if this is the startup disk mkdir -p /opt/hello target=\u0026#34;/opt/hello/hello.txt\u0026#34; echo \u0026#34;\\$0=$0\u0026#34; \u0026gt; $target echo \u0026#34;\\$1=$1\u0026#34; \u0026gt;\u0026gt; $target echo \u0026#34;\\$2=$2\u0026#34; \u0026gt;\u0026gt; $target echo \u0026#34;\\$3=$3\u0026#34; \u0026gt;\u0026gt; $target echo \u0026#34;\\$4=$4\u0026#34; \u0026gt;\u0026gt; $target echo \u0026#34;\\$INSTALL_PKG_SESSION_ID=$INSTALL_PKG_SESSION_ID\u0026#34; \u0026gt;\u0026gt; $target echo \u0026#34;\\$USER=$USER\u0026#34; \u0026gt;\u0026gt; $target echo \u0026#34;\\$HOME=$HOME\u0026#34; \u0026gt;\u0026gt; $target # Always succeed exit 0 The above script creates a directory /opt/hello and writes various script variables to a file /opt/hello/hello.txt.\nMake sure the script is executable:\nchmod +x Scripts/postinstall Create the installation package using the pkgbuild command:\npkgbuild --nopayload --scripts Scripts --identifier com.victoronsoftware.pkgbuild-demo --version 1.0 PkgbuildDemo.pkg The above command creates an install package PkgbuildDemo.pkg.\nThe --nopayload flag tells pkgbuild that there are no application files to include in the package. The --scripts flag specifies the directory containing the scripts to run during the installation. The scripts directory may also contain additional files needed by the script.\nAt this point, you can try installing the package on a test macOS system:\nsudo installer -pkg PkgbuildDemo.pkg -target / Create a script-only install package using the Packages app One popular GUI tool for creating macOS installer packages is the Packages app.\nDownload and install the Packages app.\nCreate a new project in the Packages app using the Raw Package template.\nChoose the name and location of your project.\nIn the Scripts tab, choose a Post-installation script. Add additional script resource files if needed for the script.\nSave your project with File \u0026gt; Save and build the package with Build \u0026gt; Build.\nThe tool will save the new PKG file to your project directory.\nAnalyze the install package To analyze the install package, you can use a tool like Suspicious Package\nSign and notarize the install package Before distributing the package to users, you may need to sign and notarize it.\nTo sign the package, you need a Developer ID Installer certificate. The Apple Developer Program currently costs 99 USD per membership year. To sign your package, place the certificate and corresponding private key (together called an \u0026ldquo;identity\u0026rdquo;) into your keychain. Then, you can sign the package using the productsign command-line utility:\nproductsign --sign \u0026#34;Developer ID Installer: ********\u0026#34; ~/PkgbuildDemo.pkg ~/PkgbuildDemo-signed.pkg You can notarize your package with Apple using the notarytool command-line utility. For more information, see Notarizing macOS software before distribution.\nDistribute the install package You can distribute the package by posting a download link on your website, through a package manager, or using your MDM tool.\nIf you\u0026rsquo;re using a macOS MDM platform such as Fleet, you can upload the package to the MDM and deploy it to your managed devices.\nFurther reading Recently, we explained launchd agents and daemons on macOS. In the past, we showed how to create an EXE installer for Windows and code sign a Windows application. We also covered using Mutual TLS (mTLS) with macOS keychain. Watch how to create a script-only macOS install package Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-11-13T00:00:00Z","image":"https://victoronsoftware.com/posts/script-only-macos-install-package/script-package-headline_hu_25e92af6247d7164.png","permalink":"https://victoronsoftware.com/posts/script-only-macos-install-package/","title":"2 ways to turn a script into a macOS install package"},{"content":"What is staticcheck? Staticcheck is a Go linter that checks your Go code for bugs and performance issues. It is a powerful tool that can help you find issues in your code before they become problematic. Staticcheck is one of the default linters in the golangci-lint tool.\nRun staticcheck on your Go project In this example, we will enable staticcheck via the golangci-lint tool in a large Go project. The golangci-lint tool is a lint runner that runs many linters in parallel. It is a great tool to use in your CI/CD pipeline to catch issues early.\nInstall golangci-lint To install the golangci-lint tool, you can use one of the options in golangci-lint install documentation. We install it using the following command:\ngo install github.com/golangci/golangci-lint/cmd/golangci-lint@v1.61.0 Although the documentation does not recommend this way of installing from source, we use it to ensure that our version of golangci-lint is compiled using the same Go version as our project. We previously encountered issues with golangci-lint compiled with a different Go version.\nCheck the version of golangci-lint:\ngolangci-lint --version Sample output:\ngolangci-lint has version v1.61.0 built with go1.23.1 from (unknown, modified: ?, mod sum: \u0026#34;h1:VvbOLaRVWmyxCnUIMTbf1kDsaJbTzH20FAMXTAlQGu8=\u0026#34;) on (unknown) Run golangci-lint with staticcheck You can run staticcheck using the golangci-lint tool. In the root of your Go project, run the following command:\ngolangci-lint run --disable-all --enable staticcheck This command turns off all default linters and enables only the staticcheck linter. You can view the complete list of run options with:\ngolangci-lint run --help For our project, we add a few more flags to the golangci-lint run command:\ngolangci-lint run --disable-all --enable staticcheck --timeout 10m --max-same-issues 0 --max-issues-per-linter 0 --exclude-dirs ./node_modules Analyze and fix staticcheck issues SA1019 - Using a deprecated function, variable, constant or field After running the linter, the first thing we notice is a considerable number of SA1019 fails flagging deprecations, such as:\ncmd/osquery-perf/agent.go:2574:2: SA1019: rand.Seed has been deprecated since Go 1.20 and an alternative has been available since Go 1.0: As of Go 1.20 there is no reason to call Seed with a random value. Programs that call Seed with a known value to get a specific sequence of results should use New(NewSource(seed)) to obtain a local random generator. (staticcheck) rand.Seed(*randSeed) ^ or\nserver/service/appconfig.go:970:5: SA1019: customSettings[i].Labels is deprecated: the Labels field is now deprecated, it is superseded by LabelsIncludeAll, so any value set via this field will be transferred to LabelsIncludeAll. (staticcheck) customSettings[i].Labels = nil ^ The first fail flags a Go library depreciation issue. Although we could fix it, we are not worried because of Go\u0026rsquo;s commitment to backward compatibility.\nThe second SA1019 deprecation fail flags an internal depreciation within our app. However, we must maintain many deprecated functions within our app for backward compatibility until they can be removed with the next major release. So, many of these failures cannot be fixed. We could waive each one, but that would be a lot of busy work.\nEnabling SA1019 as a default staticcheck rule is a mistake. We suspect many potential users of staticcheck will be turned off by the sheer number of these fails and will simply turn off staticcheck in their projects.\nWe decide to suppress them for now by creating a custom configuration file:\nlinters-settings: staticcheck: checks: [\u0026#34;all\u0026#34;, \u0026#34;-ST1000\u0026#34;, \u0026#34;-ST1003\u0026#34;, \u0026#34;-ST1016\u0026#34;, \u0026#34;-ST1020\u0026#34;, \u0026#34;-ST1021\u0026#34;, \u0026#34;-ST1022\u0026#34;, \u0026#34;-SA1019\u0026#34;] We use the default staticcheck checks and turn off the SA1019 check.\nWe then run golangci-lint with the custom configuration file:\ngolangci-lint run --disable-all --enable staticcheck --config staticcheck.yml SA1032 - Wrong order of arguments to errors.Is After rerunning the linter, we saw a SA1032 fail:\nserver/datastore/mysql/vpp.go:1090:6: SA1032: arguments have the wrong order (staticcheck) if errors.Is(sql.ErrNoRows, err) { ^ This failure is a good catch and a potential bug. We fix it by swapping the arguments:\nif errors.Is(err, sql.ErrNoRows) { SA4005 - Field assignment that will never be observed. Did you mean to use a pointer receiver? Another fail we saw was SA4005:\nserver/mail/users.go:44:2: SA4005: ineffective assignment to field PasswordResetMailer.CurrentYear (staticcheck) r.CurrentYear = time.Now().Year() ^ The relevant Go code is:\nr.CurrentYear = time.Now().Year() t, err := server.GetTemplate(\u0026#34;server/mail/templates/password_reset.html\u0026#34;, \u0026#34;email_template\u0026#34;) if err != nil { return nil, err } var msg bytes.Buffer if err = t.Execute(\u0026amp;msg, r); err != nil { return nil, err } In this case, the CurrentYear field was used in our template, but the linter could not detect it. We spent a few minutes testing the template to ensure that the CurrentYear field was being populated correctly. To waive this failure, we add a comment:\nr.CurrentYear = time.Now().Year() // nolint:staticcheck // SA4005 false positive for Go templates SA4006 - A value assigned to a variable is never read before being overwritten. Forgotten error check or dead code? We saw a lot of SA4006 fails in our codebase. It was the most common staticcheck fail we encountered. Here is an example:\nee/fleetctl/updates_test.go:455:2: SA4006: this value of `repo` is never used (staticcheck) repo, err = openRepo(tmpDir) ^ This is a bug or a potential bug. The developer assigned a value to repo but never used it. We fix it by removing the assignment:\n_, err = openRepo(tmpDir) SA4009 - A function argument is overwritten before its first use Another fail we saw was SA4009. Here is an example:\norbit/pkg/installer/installer.go:288:37: SA4009: argument ctx is overwritten before first use (staticcheck) func (r *Runner) runInstallerScript(ctx context.Context, scriptContents string, installerPath string, fileName string) (string, int, error) { ^ This is another bug or potential bug. A function argument is passed in but then immediately overwritten and never used. This issue could be challenging to fix because it requires specific code knowledge.\nOther fails We found a few other fails that were not as critical as the ones mentioned above. We fixed them as we went along. See the video below for more details.\nOverall impressions Overall, we like the staticcheck linter. It found many bugs or potential bugs and provided a lot of value.\nWe did have to ignore the SA1019 check and encountered an SA4005 false positive.\nWe will enable it in our CI/CD pipeline and continue to use it in our project.\nFurther reading See how to use linting to improve Go code complexity. Recently, we wrote about finding performance issues with OpenTelemetry and Jaeger in your Go project. We also wrote about optimizing the performance of your Go code. We also published an article on Go modules and packages. Example code on GitHub Fleet repo we used when enabling staticcheck (as of this writing)\nWatch us enable staticcheck in our Go project Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-11-06T00:00:00Z","image":"https://victoronsoftware.com/posts/staticcheck-go-linter/staticcheck-go-linter-headline_hu_2be66890d9a88d57.png","permalink":"https://victoronsoftware.com/posts/staticcheck-go-linter/","title":"Is staticcheck linter useful for my Go project?"},{"content":"This article discusses our first impressions of using OpenTelemetry with Jaeger.\nUse cases for OpenTelemetry and Jaeger Problems with OpenTelemetry and Jaeger What is OpenTelemetry? OpenTelemetry is a set of APIs, libraries, agents, and instrumentation for collecting distributed traces and metrics from your applications. It provides a standardized way to instrument your code and collect telemetry data. OpenTelemetry supports programming languages like Java, Python, Go, JavaScript, etc.\nTracing is a method of monitoring and profiling your application to understand how requests flow through your system. For example, you can view the associated database calls and requests to other services for a single API request. Tracing allows you to identify bottlenecks, latency issues, and other performance problems.\nWhat is Jaeger? Jaeger is an open-source, end-to-end distributed tracing system. Jaeger is popular for tracing applications because of its scalability, ease of use, and integration with other tools. Jaeger provides a web-based UI for viewing traces and analyzing performance data.\nAdd OpenTelemetry instrumentation to your application To start with OpenTelemetry and Jaeger, you must instrument your application with OpenTelemetry libraries.\nIn our case, we used the OpenTelemetry Go SDK to instrument our Go application. We added the necessary dependencies to our project.\ngo get go.opentelemetry.io/otel@v1.31.0 go get go.opentelemetry.io/otel/exporters/otlp/otlptrace@v1.31.0 go get go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc@v1.31.0 go get go.opentelemetry.io/otel/sdk@v1.31.0 go get go.opentelemetry.io/contrib/instrumentation/github.com/gorilla/mux/otelmux@v0.56.0 go get github.com/XSAM/otelsql@v0.35.0 The go.opentelemetry.io/contrib/instrumentation/github.com/gorilla/mux/otelmux package is needed to instrument our gorilla/mux HTTP router.\nr := mux.NewRouter() r.Use(otelmux.Middleware(\u0026#34;fleet\u0026#34;)) The github.com/XSAM/otelsql package is needed to instrument our SQL database queries.\n// ... import \u0026#34;github.com/XSAM/otelsql\u0026#34; import semconv \u0026#34;go.opentelemetry.io/otel/semconv/v1.26.0\u0026#34; // ... var otelTracedDriverName string func init() { var err error otelTracedDriverName, err = otelsql.Register(\u0026#34;mysql\u0026#34;, otelsql.WithAttributes(semconv.DBSystemMySQL), otelsql.WithSpanOptions(otelsql.SpanOptions{ // DisableErrSkip ignores driver.ErrSkip errors, which are frequently returned by the MySQL // driver when certain optional methods or paths are not implemented/taken. // For example, interpolateParams=false (the secure default) will not do a parametrized // sql.conn.query directly without preparing it first, causing driver.ErrSkip DisableErrSkip: true, // Omitting span for sql.conn.reset_session since it takes ~1us and doesn\u0026#39;t provide useful // information OmitConnResetSession: true, // Omitting span for sql.rows since it is very quick and typically doesn\u0026#39;t provide useful // information beyond what\u0026#39;s already reported by prepare/exec/query OmitRows: true, }), // WithSpanNameFormatter allows us to customize the span name, which is especially useful for SQL // queries run outside an HTTPS transaction, which do not belong to a parent span, show up as their // own trace, and would otherwise be named \u0026#34;sql.conn.query\u0026#34; or \u0026#34;sql.conn.exec\u0026#34;. otelsql.WithSpanNameFormatter(func(ctx context.Context, method otelsql.Method, query string) string { if query == \u0026#34;\u0026#34; { return string(method) } // Append query with extra whitespaces removed query = strings.Join(strings.Fields(query), \u0026#34; \u0026#34;) if len(query) \u0026gt; 100 { query = query[:100] + \u0026#34;...\u0026#34; } return string(method) + \u0026#34;: \u0026#34; + query }), ) if err != nil { panic(err) } } Then, use otelTracedDriverName to open a connection to your database.\ndb, err := sql.Open(otelTracedDriverName, \u0026#34;user:password@tcp(localhost:3306)/database\u0026#34;) When starting your application, you must create an OpenTelemetry exporter and a trace provider.\nctx := context.Background() client := otlptracegrpc.NewClient() otlpTraceExporter, err := otlptrace.New(ctx, client) if err != nil { panic(\u0026#34;Failed to initialize tracing\u0026#34;) } batchSpanProcessor := trace.NewBatchSpanProcessor(otlpTraceExporter) tracerProvider := trace.NewTracerProvider(trace.WithSpanProcessor(batchSpanProcessor)) otel.SetTracerProvider(tracerProvider) Launch Jaeger To view traces, you need to launch Jaeger. You can run Jaeger locally using Docker. Based on the Jaeger 1.62 Getting Started guide, you can run the following command:\ndocker run --rm --name jaeger \\ -p 16686:16686 \\ -p 4317:4317 \\ jaegertracing/all-in-one:1.62.0 In our example, we are only exposing two ports:\n4317 for the Jaeger collector, which receives trace data using OpenTelemetry Protocol (OTLP) over gRPC 16686 for the Jaeger UI Launch your application Before starting your application, you must set the OpenTelemetry endpoint to send traces to Jaeger. For example:\nexport OTEL_SERVICE_NAME=fleet export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317 Now, you can start your application.\nView traces in Jaeger Open your browser and navigate to http://localhost:16686 to view traces in the Jaeger UI. Select your Service name and click Find Traces.\nYou can click into a trace to view the details of each span. You can see the duration, logs, and tags for each span. The example below shows the HTTP request details and multiple SQL queries.\nUse cases for OpenTelemetry and Jaeger In a local software development environment, OpenTelemetry and Jaeger can be used to:\nFix bottlenecks and latency issues Understand how requests flow through your system If a bottleneck is known or suspected, Jaeger can help you identify the root cause. For example, you can see which database queries are taking the most time and optimize them.\nWhen developing new features, Jaeger can help you understand how requests flow through your system. This telemetry data provides a quick check to ensure your new feature works as expected.\nIn a production environment, OpenTelemetry and Jaeger can be used to:\nMonitor and profile your applications Troubleshoot performance issues Optimize your applications and improve user experience Ensure your applications meet service level objectives (SLOs) Problems with OpenTelemetry and Jaeger OpenTelemetry and Jaeger are powerful tools, yet their development use seems limited to fixing performance bottlenecks. They cannot be used for general debugging out of the box since they don\u0026rsquo;t provide enough detail for each specific request, such as the request body.\nIn addition, missing spans can be a problem. If your application is not instrumented correctly, you may not see all the spans you expect or know about in Jaeger. Our application lacks spans for some API endpoints, Redis transactions, outbound HTTP requests, and asynchronous processes. Adding all of these spans requires additional development and QA efforts.\nThe Jaeger UI itself is basic and lacks some features. For example, regex search is missing out of the box, unless Elasticsearch/OpenSearch storage is added.\nOur chosen SQL instrumentation library, github.com/XSAM/otelsql, could be better. It does not provide a way to trace the transaction lifecycle, and it creates many spans at the root level, clogging the Jaeger UI.\nFurther reading OpenTelemetry: A developer\u0026rsquo;s best friend for production-ready code\nHow we changed our minds about OpenTelemetry and now advocate for developers to use it during development.\nTop 5 metrics for software load testing performance\nEssential metrics to track when evaluating your application\u0026rsquo;s performance under load.\nHow to benchmark performance of Go serializers\nLearn effective techniques for measuring and optimizing Go code performance.\nExample code on GitHub Fleet Device Management repo with OpenTelemetry instrumentation (as of this writing)\nWatch OpenTelemetry with Jaeger video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-10-30T00:00:00Z","image":"https://victoronsoftware.com/posts/opentelemetry-with-jaeger/opentelemetry-with-jaeger-headline_hu_d4aa9472c6a8d09e.png","permalink":"https://victoronsoftware.com/posts/opentelemetry-with-jaeger/","title":"Is OpenTelemetry useful for the average software developer?"},{"content":"In this article, we\u0026rsquo;ll create a custom reusable GitHub Action using TypeScript. As covered in our article on reusing GitHub workflows and steps, GitHub Actions allow you to automate your software development workflows. By creating a custom GitHub Action, you can extend the functionality of GitHub Actions to suit your specific needs.\nWe will create a simple GitHub Action to replace GitHub\u0026rsquo;s broken Pull Request review process. This custom GitHub Action will automatically approve Pull Requests that meet specific criteria.\nStart with GitHub\u0026rsquo;s Action template GitHub provides a template for creating a new GitHub Action using TypeScript. You can use this template to get started quickly. The template includes the necessary files and structure to create a new GitHub Action. Anything unnecessary can be removed or modified to suit your requirements.\nFollow the instructions in the template\u0026rsquo;s README to set up your action.\nClick the Use this template button at the top of the repository Select Create a new repository Select an owner and name for your new repository Click Create repository Clone your new repository Check out your new repository, go to the cloned repo directory, and make sure your node version matches the one in the .node-version file. If you\u0026rsquo;re using nvm (Node Version Manager), you can run:\ncp .node-version .nvmrc nvm install node --version Note: Our example is based on this commit of the template repository.\nImplement your custom GitHub Action Install the template\u0026rsquo;s dependencies with npm install. In addition, install the @actions/github package with npm install @actions/github.\nThe template provides a basic structure for your GitHub Action. Update the action.yml file with your action\u0026rsquo;s name, description, author, and updated inputs/outputs.\nname: \u0026#39;Code review\u0026#39; description: \u0026#39;Improved code review process\u0026#39; author: \u0026#39;Victor on Software\u0026#39; # Add your action\u0026#39;s branding here. This will appear on the GitHub Marketplace. branding: icon: \u0026#39;heart\u0026#39; color: \u0026#39;red\u0026#39; # Define your inputs here. inputs: github-token: description: \u0026#39;GitHub secret token\u0026#39; required: true runs: using: node20 main: dist/index.js In this example, our new action will have a single input, github-token, the GitHub secret token to use the GitHub API.\nNext, we can modify the code in the src/main.ts file to implement our custom logic.\nimport * as core from \u0026#39;@actions/core\u0026#39; import * as github from \u0026#39;@actions/github\u0026#39; import { readFileSync } from \u0026#39;fs\u0026#39; /** * The main function of the action. * @returns {Promise\u0026lt;void\u0026gt;} Resolves when the action is complete. */ export async function run(): Promise\u0026lt;void\u0026gt; { try { // Get the PR number from the payload. This action is only intended for PRs. const prNumber = github.context.payload.pull_request?.number if (!prNumber) { return } // Get the required reviewer from the REVIEWERS file. // This simplified example assumes that the REVIEWERS file contains a single reviewer. // In a real-world scenario, we would need to parse the REVIEWERS file at the top directory // of our changed files to get the reviewers. const reviewer = readFileSync(\u0026#39;REVIEWERS\u0026#39;, \u0026#39;utf8\u0026#39;).trim() if (!reviewer) { core.setFailed(\u0026#39;No reviewer found in REVIEWERS file\u0026#39;) return } // Get all the reviews for this PR. const githubToken = core.getInput(\u0026#39;github-token\u0026#39;) const octokit = github.getOctokit(githubToken) const reviews = await octokit.rest.pulls.listReviews({ owner: github.context.repo.owner, repo: github.context.repo.repo, pull_number: prNumber }) // Check if the required reviewer has approved the PR. // This action does not require the reviewer to re-approve the PR if new changes are pushed. let approved = false reviews.data.forEach(review =\u0026gt; { if (review.user?.login === reviewer \u0026amp;\u0026amp; review.state === \u0026#39;APPROVED\u0026#39;) { approved = true } }) // Fail the workflow run if the required reviewer has not approved the PR. if (!approved) { core.setFailed(`Reviewer ${reviewer} needs to approve the PR`) return } } catch (error) { // Fail the workflow run if an error occurs if (error instanceof Error) core.setFailed(error.message) } } We read the REVIEWERS file to get the required reviewer for the PR. We then retrieve all the reviews for the pull request and check if the needed reviewer has approved the PR. If the reviewer has not approved the PR, we fail the run.\nBuild your custom GitHub Action To build your action, run npm run bundle. This will compile the TypeScript code in the src/ directory and output the JavaScript code in the dist/ directory. The exact command for bundle is defined in the package.json file.\nThe bundle step is required before you can test your action locally or use it in a workflow because the action.yml file points at the dist/index.js bundled version of your code.\nThis step can easily be forgotten, and you may wonder why your changes are not reflected in the action. You can configure our IDE to do npm run bundle automatically on save, create a check as a git pre-commit hook to ensure the dist/ directory is up-to-date, or rely on the Check Transpiled Javascript workflow in the .github/workflows/ directory.\nNow, commit your changes and push them to your repository.\nNote: We will not cover testing in this article, but you should add tests to the __tests__/ directory for your source code. For this example, we must refactor the code to make it more testable.\nTest your custom GitHub Action in another repository Add workflows and REVIEWERS file You can use the uses keyword in a workflow file to try your action in another repository. Create a new workflow file in the repository\u0026rsquo;s .github/workflows/ directory where you want to test your action. For example:\nname: Code review on: workflow_dispatch: # Manual (for debug) pull_request: jobs: code-review: name: Code review runs-on: ubuntu-latest steps: - name: Checkout id: checkout uses: actions/checkout@v4 - name: Code review action id: test-action uses: getvictor/code-review-demo@main # Your action goes here with: github-token: ${{ secrets.GITHUB_TOKEN }} We are using @main as the version of the action to test the latest code on the main branch. You can replace this with a specific version tag or branch name.\nIn addition, create a REVIEWERS file at the repository\u0026rsquo;s root with the required reviewer\u0026rsquo;s GitHub username.\nNotice that the above workflow runs on pull_request events but not on pull_request_review events. This is because GitHub Actions treats the pull_request workflow runs and the pull_request_review workflow runs as distinct. Instead, we want the same workflow run to be triggered by both events. This is a common pitfall when working with GitHub Actions. We need to add another workflow file that listens for pull_request_review events and triggers the pull_request workflow run to fix this.\nname: Rerun checks after review on: pull_request_review: types: - submitted - dismissed jobs: rerun_checks: name: Rerun specified checks permissions: actions: write runs-on: ubuntu-latest steps: - name: Rerun Checks uses: shqear93/rerun-checks@v3 with: github-token: ${{ secrets.GITHUB_TOKEN }} check-names: \u0026#39;Code review\u0026#39; Configure a GitHub rule to require code review Create a branch protection rule in the repository settings that requires the above Code review workflow to pass in a PR before merging to your default branch.\nCreate a pull request Commit your changes to a new branch and create a pull request. The Code review workflow should run automatically and show the Failed status.\nOnce the required reviewer approves the PR, the Rerun checks after review workflow will run and trigger the Code review workflow. The rerun should pass, and you should be able to merge the PR.\nClean up your custom GitHub Action repo As an optional step, you can clean up your custom GitHub Action repository:\nUpdate README.md with instructions on how to use your new action Remove the src/wait.ts file and associated tests Update workflows in the .github/workflows/ directory Further reading In a previous article, we described what happens in a GitHub pull request after a git merge.\nIn another article, we covered how to use GitHub Actions for general-purpose tasks.\nExample code on GitHub The code for our simple GitHub Action is available on GitHub: https://github.com/getvictor/code-review-demo\nWatch how to create a custom GitHub Action using TypeScript Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-10-23T00:00:00Z","image":"https://victoronsoftware.com/posts/typescript-github-action/typescript-github-action-headline_hu_8d56de5f8631c5e1.png","permalink":"https://victoronsoftware.com/posts/typescript-github-action/","title":"How to create a custom GitHub Action using TypeScript"},{"content":"JSON unmarshalling use cases When passing a JSON payload to a Go application, you may encounter situations where you must tell the difference between set, missing, or null fields.\nFor example, consider the following JSON payload:\n{ \u0026#34;name\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;age\u0026#34;: 30, \u0026#34;address\u0026#34;: { \u0026#34;street\u0026#34;: \u0026#34;123 Main St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Springfield\u0026#34; } } We can unmarshal this JSON payload using JSON tags and the following Go structs:\ntype Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` Address Address `json:\u0026#34;address\u0026#34;` } type Address struct { Street string `json:\u0026#34;street\u0026#34;` City string `json:\u0026#34;city\u0026#34;` } However, we will not be able to tell the difference between these two JSON payloads:\n{ \u0026quot;name\u0026quot;: null } { \u0026quot;name\u0026quot;: \u0026quot;\u0026quot; } Go\u0026rsquo;s zero values are not distinguishable from missing fields when unmarshalling JSON.\nWe can change the above struct to use pointers to identify null fields:\ntype Person struct { Name *string `json:\u0026#34;name\u0026#34;` Age *int `json:\u0026#34;age\u0026#34;` Address *Address `json:\u0026#34;address\u0026#34;` } However, we will still not be able to tell the difference between these two JSON payloads:\n{ \u0026quot;name\u0026quot;: null } { } Both of these payloads will unmarshal into a Person struct with all fields set to nil, and we cannot distinguish between a missing field and a field set to null.\nOne reason to distinguish between missing and null fields is to avoid overwriting existing values with null values. For example, when name is not specified in the JSON payload, we may want to keep the existing name value in the Person struct. But we may want to clear the name when name is defined as null.\nDetecting null, set, and missing JSON fields with Go We can use custom unmarshalling logic by implementing the Unmarshaler interface to detect the difference between null and missing fields. The UnmarshalJSON method allows us to inspect the JSON token stream and decide how to unmarshal the JSON payload. The critical insight is that UnmarshalJSON is only called when the field is present in the JSON payload. So, we can mark a Set flag as true when the field is present and false when it is not.\nHere is an example implementation:\ntype Any[T any] struct { Set bool Valid bool Value T } // MarshalJSON implements the json.Marshaler interface. // Only Value is marshaled, and only if Valid is true. func (s Any[T]) MarshalJSON() ([]byte, error) { if !s.Valid { return []byte(\u0026#34;null\u0026#34;), nil } return json.Marshal(s.Value) } // UnmarshalJSON implements the json.Unmarshaler interface. // Set is always set to true, even if the JSON data was set to null. // Valid is set if the JSON data is not set to null. func (s *Any[T]) UnmarshalJSON(data []byte) error { s.Set = true s.Valid = false if bytes.Equal(data, []byte(\u0026#34;null\u0026#34;)) { // The key was set to null, set value to zero/default value var zero T s.Value = zero return nil } // The key isn\u0026#39;t set to null var v T if err := json.Unmarshal(data, \u0026amp;v); err != nil { return err } s.Value = v s.Valid = true return nil } We used a generic type T to allow Any to work with any type. The Valid flag distinguishes between nil and non-nil values. The Set flag is set to true only when the field is present in the JSON payload.\nHere is how we can use the Any type in a Person struct:\ntype Person struct { Name Any[string] `json:\u0026#34;name\u0026#34;` Age Any[int] `json:\u0026#34;age\u0026#34;` Address Any[Address] `json:\u0026#34;address\u0026#34;` } Testing the custom unmarshalling logic The following example demonstrates how the Any type works:\ntype Form struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` } type Config struct { Form Any[Form] `json:\u0026#34;form\u0026#34;` Member Any[bool] `json:\u0026#34;member\u0026#34;` } func main() { tests := []struct { description string JSON string }{ { \u0026#34;Nothing set\u0026#34;, `{}`, }, { \u0026#34;Set all fields\u0026#34;, `{\u0026#34;form\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;age\u0026#34;: 30}, \u0026#34;member\u0026#34;: false}`, }, { \u0026#34;Set only member field, and leave form fields unchanged\u0026#34;, `{\u0026#34;member\u0026#34;: true}`, }, { \u0026#34;Set only the form field, and leave the member field unchanged\u0026#34;, `{\u0026#34;form\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;Jane\u0026#34;, \u0026#34;age\u0026#34;: 25}}`, }, { \u0026#34;Leave all fields unchanged\u0026#34;, `{}`, }, { \u0026#34;Clear all fields\u0026#34;, `{\u0026#34;form\u0026#34;: null, \u0026#34;member\u0026#34;: null}`, }, { \u0026#34;Set only form field\u0026#34;, `{\u0026#34;form\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;Chris\u0026#34;, \u0026#34;age\u0026#34;: 35}}`, }, } c := Config{} for _, test := range tests { fmt.Printf(\u0026#34;\\nTest: %s\\n\u0026#34;, test.description) _ = json.Unmarshal([]byte(test.JSON), \u0026amp;c) fmt.Printf(\u0026#34;Input: %s\\n\u0026#34;, test.JSON) fmt.Printf(\u0026#34;%+v\\n\u0026#34;, c) data, _ := json.Marshal(c) fmt.Printf(\u0026#34;Output: %s\\n\u0026#34;, data) } } The test output will show how the Any type behaves when unmarshalling JSON payloads with different fields set, missing, or set to null.\nTest: Nothing set Input: {} {Form:{Set:false Valid:false Value:{Name: Age:0}} Member:{Set:false Valid:false Value:false}} Output: {\u0026#34;form\u0026#34;:null,\u0026#34;member\u0026#34;:null} Test: Set all fields Input: {\u0026#34;form\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;age\u0026#34;: 30}, \u0026#34;member\u0026#34;: false} {Form:{Set:true Valid:true Value:{Name:John Age:30}} Member:{Set:true Valid:true Value:false}} Output: {\u0026#34;form\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;John\u0026#34;,\u0026#34;age\u0026#34;:30},\u0026#34;member\u0026#34;:false} Test: Set only member field, and leave form fields unchanged Input: {\u0026#34;member\u0026#34;: true} {Form:{Set:true Valid:true Value:{Name:John Age:30}} Member:{Set:true Valid:true Value:true}} Output: {\u0026#34;form\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;John\u0026#34;,\u0026#34;age\u0026#34;:30},\u0026#34;member\u0026#34;:true} Test: Set only the form field, and leave the member field unchanged Input: {\u0026#34;form\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;Jane\u0026#34;, \u0026#34;age\u0026#34;: 25}} {Form:{Set:true Valid:true Value:{Name:Jane Age:25}} Member:{Set:true Valid:true Value:true}} Output: {\u0026#34;form\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Jane\u0026#34;,\u0026#34;age\u0026#34;:25},\u0026#34;member\u0026#34;:true} Test: Leave all fields unchanged Input: {} {Form:{Set:true Valid:true Value:{Name:Jane Age:25}} Member:{Set:true Valid:true Value:true}} Output: {\u0026#34;form\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Jane\u0026#34;,\u0026#34;age\u0026#34;:25},\u0026#34;member\u0026#34;:true} Test: Clear all fields Input: {\u0026#34;form\u0026#34;: null, \u0026#34;member\u0026#34;: null} {Form:{Set:true Valid:false Value:{Name: Age:0}} Member:{Set:true Valid:false Value:false}} Output: {\u0026#34;form\u0026#34;:null,\u0026#34;member\u0026#34;:null} Test: Set only form field Input: {\u0026#34;form\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;Chris\u0026#34;, \u0026#34;age\u0026#34;: 35}} {Form:{Set:true Valid:true Value:{Name:Chris Age:35}} Member:{Set:true Valid:false Value:false}} Output: {\u0026#34;form\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Chris\u0026#34;,\u0026#34;age\u0026#34;:35},\u0026#34;member\u0026#34;:null} Complete code on Go Playground The complete Go code for unmarshalling JSON null, set, and missing fields is available on the Go Playground.\nFurther reading Recently, we published an article on how to optimize the performance of a Go application. We benchmarked JSON decoding vs gob decoding in that article. In addition, we wrote about how to read program arguments from STDIN with Go, which is more secure than using environment variables or command-line arguments. Also, we explained the difference between Go modules and packages, which is essential for organizing and managing Go code. In addition, we explained method overriding in Go. Watch how to use Go to unmarshal JSON null, set, and missing fields accurately Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-10-09T00:00:00Z","image":"https://victoronsoftware.com/posts/go-json-unmarshal/go-json-unmarshal-headline_hu_c883e153a4331d6.png","permalink":"https://victoronsoftware.com/posts/go-json-unmarshal/","title":"Use Go to unmarshal JSON null, set, and missing fields"},{"content":" Test NDES using PowerShell Test NDES using a SCEP client Test NDES using Apple MDM profile What is a Windows NDES SCEP server? SCEP (Simple Certificate Enrollment Protocol) is a protocol used to issue certificates with a Certificate Authority (CA) in a Public Key Infrastructure (PKI). It allows devices to request and receive certificates over a secure channel without user interaction. IT admins use SCEP for network devices, mobile devices, and other endpoints that need to authenticate themselves. The issued certificates can be used for various purposes, such as Wi-Fi authentication, VPN access, email encryption, etc. For example, a new mobile device can request a certificate from the SCEP server to authenticate on the corporate Wi-Fi network.\nNDES (Network Device Enrollment Service) is a Microsoft implementation of the SCEP protocol. NDES is part of the Active Directory Certificate Services (AD CS) role in Windows Server.\nSetting up a Windows NDES SCEP server Before testing your Windows NDES SCEP server, you must set it up. Numerous articles and guides cover the installation and configuration of NDES. This article will focus on testing the NDES SCEP server to ensure the correct setup. We wrote this article because we could not find a comprehensive guide on how to test the NDES SCEP server.\nHere are the high-level steps to configure a Windows NDES SCEP server:\nCreate or use an existing Windows AD (Active Directory) server and domain. Install the Active Directory Certificate Services (AD CS) role on a Windows Server that is part of the AD domain. Configure the Enterprise NDES role service within AD CS. (Optional) Configure the certificate templates for NDES. We used Windows Server 2022 for our tests, and we will update this article once we test with Windows Server 2025.\nTest NDES using a web browser First, we must make sure the NDES server is accessible via a web browser. If the server should be accessible outside the corporate network, test it using the public URL or IP address.\nThe NDES server has an admin web interface for retrieving the SCEP challenge. The URL typically looks like http://ndes-server/certsrv/mscep_admin/ and requires authentication. The username must use the Windows name format, like username@example.domain.com. Accessing this URL should prompt you to log in and display the SCEP challenge.\nNote: The above admin page is encoded as UTF-16, as opposed to the more popular UTF-8 encoding. This encoding must considered when parsing this page with a script.\nThe other URL to test is the actual SCEP enrollment URL, typically http://ndes-server/certsrv/mscep/mscep.dll. It returns the following.\nTest NDES using PowerShell For our first test, we will use PowerShell to request a certificate from another Windows machine in the same AD domain.\nBelow is a sample PowerShell script that requests a certificate from the NDES server. Update the URL and the challenge password.\nAfter running the script, check that NDES issued a certificate.\nTest NDES using a SCEP client For our next test, we will use an SCEP client to request a certificate from the NDES server. Several SCEP clients are available, but many have been abandoned and do not work with NDES.\nWe will use micromdm/scep, a Go-based open-source SCEP server and client. We will use the latest code from the main branch, with the following commit hash: 781f8042a79cabcf61a5e6c01affdbadcb785932.\nFollow the instructions from the above URL to install the scep client. We built it for macOS M1 using the following command:\nmake scepclient-darwin-arm64 After building the client, obtain a new enrollment challenge password and run the following command to request a certificate from the NDES server:\nmkdir test cd test ../scepclient-darwin-arm64 -key-encipherment-selector -cn \u0026#34;ScepClient\u0026#34; -challenge \u0026#34;ABBFE34CF11C2C04\u0026#34; -server-url \u0026#34;https://my-ndes.com/certsrv/mscep/mscep.dll\u0026#34; -debug -private-key ./ndes-pk Note: We recommend running the above command in a separate directory because the SCEP client generates several intermediate files during the certificate request process. If you don\u0026rsquo;t clean them up, the client may reuse them instead of generating new ones from the command line flags.\nThe above command will generate a new certificate request and send it to the NDES server. The server will respond with a signed certificate, which the client will save to the current directory as a client.pem file.\nAs a final step, verify that the certificate and the private key match by building a PKCS#12 file:\n/usr/bin/openssl pkcs12 -export -inkey ndes-pk -in client.pem -out client.p12 Test NDES using Apple MDM profile For our final test, we will use an Apple MDM profile to request a certificate from the NDES server. We will use a macOS VM enrolled in Fleet Device Management\u0026rsquo;s MDM server. However, adding the MDM profile manually via System Settings -\u0026gt; Profiles should also work.\nFirst, create a new Device Management SCEP payload with the NDES server\u0026rsquo;s URL and the challenge password. Then, assign the SCEP payload to your device. Here\u0026rsquo;s an example payload:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;PayloadContent\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;PayloadContent\u0026lt;/key\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;Challenge\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;8E6D19CAEC9411CC\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;Key Type\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;RSA\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;Key Usage\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;5\u0026lt;/integer\u0026gt; \u0026lt;key\u0026gt;Keysize\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;2048\u0026lt;/integer\u0026gt; \u0026lt;key\u0026gt;Retries\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;3\u0026lt;/integer\u0026gt; \u0026lt;key\u0026gt;RetryDelay\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;10\u0026lt;/integer\u0026gt; \u0026lt;key\u0026gt;Subject\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;array\u0026gt; \u0026lt;array\u0026gt; \u0026lt;string\u0026gt;CN\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;MDM TEST VM\u0026lt;/string\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;array\u0026gt; \u0026lt;array\u0026gt; \u0026lt;string\u0026gt;OU\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;FLEET DEVICE MANAGEMENT\u0026lt;/string\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;key\u0026gt;URL\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;https://my-ndes.com/certsrv/mscep/mscep.dll\u0026lt;/string\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;key\u0026gt;PayloadDisplayName\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;SCEP #1\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;PayloadIdentifier\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;com.apple.security.scep.9DCC35A5-72F9-42B7-9A98-7AD9A9CCA3AA\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;PayloadType\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;com.apple.security.scep\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;PayloadUUID\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;9DCC35A5-72F9-42B7-9A98-7AD9A9CCA3AA\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;PayloadVersion\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;1\u0026lt;/integer\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;key\u0026gt;PayloadDisplayName\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;SCEP cert\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;PayloadIdentifier\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;Victors-Fleet-MBP.4CD1BD65-1D2C-4E9E-9E18-9BCD400CDEDB\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;PayloadType\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;Configuration\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;PayloadUUID\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;4CD1BD65-1D2C-4E9E-9E18-9BCD400CDEDB\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;PayloadVersion\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;1\u0026lt;/integer\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt; Once the device receives the payload, it immediately requests a certificate from the NDES server. The server responds with a signed certificate, which the device saves to the keychain.\nFurther reading Recently, we covered how to connect to a remote Active Directory server.\nWe also wrote a series of articles on building a mutual TLS client which uses a system keystore, such as a Windows certificate store.\nIn addition, we presented an example of code signing a Windows application.\nWatch how to test a Windows NDES SCEP server Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-10-02T00:00:00Z","image":"https://victoronsoftware.com/posts/test-ndes-scep-server/windows-security-headline_hu_c084f5c3e01ef57e.png","permalink":"https://victoronsoftware.com/posts/test-ndes-scep-server/","title":"How to test a Windows NDES SCEP server"},{"content":"Why set up a remote development environment? A remote development environment can be beneficial for several reasons:\nOffload processing power: Your local machine may not have enough processing power to run resource-intensive tasks. By using a remote development environment, you can use more powerful hardware. Consistent environment: A remote development environment ensures all team members can work in the same environment, reducing configuration issues and ensuring consistent behavior across different machines. For example, developers may be using a mix of macOS, Windows, and Linux machines, which can lead to differences in behavior due to operating system-specific issues. Multiple environments: You can set up multiple environments for different projects or tasks without cluttering your local machine. Access from anywhere: A remote development environment allows you to access your work from any device with an internet connection. Collaboration: You can easily collaborate with team members by sharing the same development environment. For example, after coding a feature, the developer can hand off the environment to another engineer for review or QA. Security: Keeping your code and development environment on a remote server reduces the risk of data loss in case of local hardware failure or theft. Scalability: You can quickly scale your development environment up or down based on your needs without affecting your local machine. Cost-effective: A remote development environment can be more cost-effective than purchasing and maintaining high-end hardware for your local machine. Setting up a remote development environment For our development example, we will use a standalone server application connected to a database and a Redis cache. The application uses a monolith repo with a frontend and a backend codebase.\nChoose a cloud provider We used a Digital Ocean VM with 8GB of RAM, 4 CPUs, and a 160 GB disk, running Ubuntu 24.04 LTS for our remote development environment. We found that Digital Ocean provides VMs that are generally cheaper than other cloud providers.\nAny other cloud provider, such as AWS, Google Cloud, or Azure, can also be used. Your choice of provider depends on your specific requirements, budget, and familiarity with the platform.\nAfter spinning up the VM, we SSH\u0026rsquo;ed into the server, installed the necessary software for our application, and launched the server.\nSince our server required multiple running processes, we used tmux to manage multiple terminal sessions. Tmux allowed us to create numerous panes and windows within a single terminal session, making it easier to manage the server processes. We could disconnect from the server, and the tmux processes ran in the background. When reconnected to the server, we could easily reattach to the tmux session and resume work. Additionally, we used iTerm2 tmux integration to enhance our terminal experience.\ntmux running on remote dev server Connect your IDE to the remote development environment We used JetBrains IDE for development work and JetBrains Gateway to connect to our remote development server using SSH. JetBrains Gateway automatically installed the IDE backend on the remote server and brought up a local client of the IDE.\nIn our case, we wanted to use one IDE for backend development (GoLand) and another IDE for frontend development (WebStorm).\nWe had trouble starting them up and could not run both IDEs simultaneously. Either one or both of them would disconnect from the remote development server without an obvious way to fix the issue. We suspect the issue was due to insufficient memory on the machine \u0026ndash; try to plan for around 4 GB of memory per IDE.\nHowever, we could use one of the IDEs at a time, which was sufficient for most of our needs.\nUsing a remote development environment After setting up the remote development environment, we reviewed common development use cases to ensure that everything was working as expected.\nMake a code change and restart the application server We made a simple code change in the backend service, saved the file, and restarted the application server. We verified that the change was reflected in the application.\nThe compile time was slower than on our local machine, likely due to the remote server\u0026rsquo;s lower CPU count and total RAM compared to our local machine.\nRun unit tests We ran the unit tests for the backend service. The tests passed successfully.\nConnect to the database and Redis cache From our local development machine, we connected to the development server\u0026rsquo;s database and Redis cache to verify that the services were running correctly.\nReconnecting to remote development environment After opening up our local computer the next day, we found that the JetBrains Gateway and the IDE has disconnected from the remote server. Refreshing the Gateway re-established the connection, and the IDE also showed as connected within 60 seconds or so.\nSecurity considerations When setting up a remote development environment, consider the following security best practices:\nSSH key authentication: For secure access to the remote server, use SSH key authentication instead of passwords. Firewall rules: Configure firewall rules to restrict access to the server to only necessary IP addresses. Secure connections: Use HTTPS for web applications and encrypted connections for database access. Data encryption: Encrypt sensitive data at rest and in transit. Always encrypt sensitive data in the database. Docker firewall rules Docker containers use iptables rules to open ports for incoming traffic. We can restrict external connections to containers by adding rules to the DOCKER-USER chain, such as:\niptables -I DOCKER-USER -i eth0 ! -s \u0026lt;your local IP\u0026gt; -j DROP Where eth0 is the network interface connected to the internet and \u0026lt;your local IP\u0026gt; is the IP address of your local machine. This rule blocks all incoming traffic to Docker from the internet except for your local IP.\nAfter setting up and testing your rules, you can persist them across restarts with the iptables-persistent package or other methods.\nOverall impressions After using the remote development environment for a few days, we found it usable but not as smooth as working on a local machine. For our use case, it is an excellent option for a secondary development environment or for working on a resource-intensive feature.\nSome issues we encountered included:\nLatency: Occasionally, clicking on an element or using a keyboard shortcut had a noticeable delay. Missing features: Some features, such as only searching inside text strings, were not available in the remote development environment. Issues with plugins: GitHub Copilot did not work out of the box; it did not provide suggestions in the editor. We did not drill down to the issue, but a potential workaround is to use JetBrains\u0026rsquo;s code assistant plugin. Further reading We recently explained how to secure a MySQL Docker container for Zero Trust. We also discussed the issues with GitHub\u0026rsquo;s code review process. We wrote about quickly editing a Google Sheets spreadsheet via the API. We also covered creating secure signed URLs with AWS CloudFront. Watch how to set up a remote development environment Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-09-22T00:00:00Z","image":"https://victoronsoftware.com/posts/remote-development-environment/remote-dev-environment-headline_hu_ac22e8baf05be8aa.png","permalink":"https://victoronsoftware.com/posts/remote-development-environment/","title":"How to set up a remote development environment"},{"content":" CODEOWNERS is not scalable Re-approvals for every push Impractical for protected feature branches Our team has been using GitHub to review the code for our open-source product. We have encountered several issues with GitHub code reviews. The default GitHub code review process is not scalable and provides a poor developer experience.\nHow to set up a GitHub code review process GitHub admins can create a branch protection rule that requires a code review before merging the code to the main branch.\nHere\u0026rsquo;s a representative branch protection rule:\nPR branch protection rule When a developer creates a pull request, GitHub requires code reviews from all relevant owners specified by the CODEOWNERS file in the repository. If someone makes a new push to the PR, all the owners need to re-approve the PR.\nIn a previous article, we covered how to find the required code owners for a PR. This is another issue, but we will not discuss it in this article.\nIssue 1: CODEOWNERS is not scalable GitHub uses a CODEOWNERS file to define individuals or teams responsible for each file in the repository.\nThe CODEOWNERS file format favors fine-grained code ownership, where the last matching pattern takes precedence over previous patterns. Here is an example from the GitHub documentation:\n# In this example, @octocat owns any file in the `/apps` # directory in the root of your repository except for the `/apps/github` # subdirectory, as this subdirectory has its own owner @doctocat /apps/ @octocat /apps/github @doctocat The CODEOWNERS file is not scalable for medium-to-large and even small organizations. As the number of code owners grows, each pull request is likely to require approval from more code owners. Each code owner may request changes, potentially leading to cycles and cycles of re-approvals.\nTracking down multiple people to approve and re-approve a PR can be time-consuming and frustrating for developers. This results in longer PR turnaround times, slower development velocity, and missed commitments.\nFrom a developer experience perspective, we want to make the code review process as smooth and efficient as possible, which means one reviewer for one PR. This approach is feasible by manually inverting the last matching pattern takes precedence rule in the CODEOWNERS file by always including the owner(s) from the previous pattern. For example, we would rewrite the above owners as:\n/apps/ @octocat /apps/github @octocat @doctocat Keeping the CODEOWNERS file in this format may be cumbersome to do manually, but it can be done with a script.\nIssue 2: Re-approvals for every push When a developer makes a new push to a PR, all the code owners need to re-approve it. This is a poor developer experience, as it requires the code owners to review potentially the same code changes multiple times.\nThe issue stems from the lack of fine-grained control over the following option:\nWith multiple code owners, every code owner must re-approve every change.\nA code owner should not need to re-review code that didn\u0026rsquo;t change \u0026ndash; this is a waste of time and effort.\nWith a single code owner, the reviewer must re-approve trivial or irrelevant changes, such as:\nfixing a typo in a comment fully accepting a suggestion from the reviewer re-generating an auto-generated file, such as documentation The required re-approvals can be frustrating and time-consuming for developers and code owners. They make developers feel untrusted and inefficient.\nThe main argument for requiring re-approvals is security‚Äîwe don\u0026rsquo;t want to merge potentially malicious code. If that\u0026rsquo;s the case, we should have a security review process in place, not a code review process. A security review can be done by a separate individual and improved by automated tools.\nIn addition, we should be able to completely exclude some files/directories from the code review process. For example, generated files, such as documentation based on code changes, should not require code review. Other generated files, such as testing mocks, may have CI/CD checks that ensure they are generated correctly, and they should not require code review either.\nIssue 3: Impractical to maintain a protected feature branch A protected feature branch requires code reviews before merging. Since all the commits on the feature branch have already been reviewed and approved, it is considered safe to merge into the main branch.\nThe main issue is that the developer cannot simply update this feature branch with the latest changes on the main branch. They need PR approval from all the code owners who have already approved the same changes on the main branch. This busy work is another example of a waste of time and effort.\nIn addition, a feature branch may be long-lived and introduce changes across multiple areas of the code base. This means that it may require approval from many code owners, which can be time-consuming and frustrating.\nSolution: Custom GitHub Action to manage code reviews Instead of relying on the default GitHub code review process, we can create a custom GitHub Action to manage code reviews. The custom GitHub Action can:\nautomatically identify a single reviewer for a PR (or identify a small group of reviewers, each of whom can approve the PR) automatically exclude specific files/directories from the code review process automatically maintain the approval state of the PR when new commits meeting explicit criteria are pushed enable a usable and practical protected feature branch Here is an example GitHub Action to replace GitHub\u0026rsquo;s pull request review process.\nFurther reading Why transparency beats everything else in engineering\nHow making work visible transforms teams from frustrated to high-performing through organizational transparency.\nHow to track engineering metrics with GitHub Actions\nImplement automated tracking of engineering performance metrics using GitHub\u0026rsquo;s built-in tools and APIs.\nHow git merge works with PRs\nUnderstand the mechanics behind pull request merges and how to optimize your git workflow.\nHow to reuse GitHub workflows and steps\nBuild maintainable CI/CD pipelines by creating reusable components for your GitHub Actions.\nWhat is clean, readable code and why it matters?\nDiscover why code readability directly impacts team productivity and how to measure it effectively.\nHow to scale your codebase with evolutionary architecture\nLearn architectural patterns that help teams maintain velocity as codebases and organizations grow.\nSet up a remote dev environment\nConfigure development environments that enhance team collaboration and reduce onboarding friction.\nWatch the top 3 issues with GitHub code reviews Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-09-15T00:00:00Z","image":"https://victoronsoftware.com/posts/github-code-review-issues/developer-on-tightrope-headline_hu_8292ddb59e762702.png","permalink":"https://victoronsoftware.com/posts/github-code-review-issues/","title":"Top 3 issues with GitHub code review process"},{"content":"MSI versus EXE installers When distributing software for Windows, you have two main options for installers: MSI and EXE. An MSI installer is a Windows Installer package that contains installation information and files. It uses the Windows Installer service. On the other hand, an EXE installer is a self-extracting executable file containing the installation files and an installation program. EXE installers are more customizable and do not depend on the built-in Windows Installer technology.\nThis article will show how to create an EXE installer for a program using the Inno Setup tool.\nBuild your program We will create a simple Hello World program using the Go programming language for this example.\nWith Go installed, we can build our program using the go build command. For example, given the source code in main.go:\npackage main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;hello world\u0026#34;) } We can build the program for Windows using:\nGOOS=windows GOARCH=amd64 go build -o hello-world.exe main.go Download and install Inno Setup We will need to use a Windows machine to create an EXE installer.\nInno Setup is a free installer for Windows programs. You can download it from the official website. Once you have downloaded the installer, run it and follow the installation instructions.\nCreate an EXE installer Launch the Inno Setup Compiler application. The main window will appear, with a toolbar and a script editor.\nOn the Welcome modal, choose Create a new script file using the Script Wizard and click OK.\nFollow the instructions on several subsequent screens.\nOn the Application Files screen, add your program executable file and any other files, such as a README.\nContinue following the instructions.\nOn the Compiler Settings screen, select the file name for your installer.\nFinally, click\u0026rsquo; Finish\u0026rsquo; after a couple more screens to generate the script.\nClick Yes to compile the script.\nClick No to save the script before compiling. If needed, it can be saved later.\nThe Inno Setup Compiler will create an EXE installer for your program and put it in the Documents/Output folder.\nYou can try running the installer to make sure it works as expected.\nFurther reading In the past, we demonstrated how to code sign a Windows application Recently, we explained how to create a script-only macOS install package Related Go articles How to measure Go test execution time and derive actionable insights Go benchmarks Watch how to create an EXE installer Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-09-11T00:00:00Z","image":"https://victoronsoftware.com/posts/exe-installer/exe-installer-headline_hu_a2497754ae7527ce.png","permalink":"https://victoronsoftware.com/posts/exe-installer/","title":"How to create an EXE installer for your program"},{"content":" Accurately measuring test execution time Why measure test execution time? By speeding up your test suite, you\u0026rsquo;re improving developer experience and productivity. Faster tests mean faster feedback, which leads to quicker iterations and better code quality.\nWhen you run tests, you want to know how long they take to execute. This information can help you optimize your test suite and make it run faster. By measuring the execution time of your tests, you can identify slow tests and improve their performance.\nProblems with current measurement tools We have yet to find a tool that provides detailed, actionable insights into the performance of Go tests.\nFor example, running the gotestsum tool slowest command from the gotestsum tool gave us the following output for our test suite:\ngithub.com/fleetdm/fleet/v4/server/datastore/mysql TestMDMApple 6m9.65s github.com/fleetdm/fleet/v4/server/datastore/mysql TestSoftware 4m8.9s github.com/fleetdm/fleet/v4/server/datastore/mysql TestPolicies 3m31s github.com/fleetdm/fleet/v4/server/datastore/mysql TestActivity 2m16.67s github.com/fleetdm/fleet/v4/server/datastore/mysql TestMDMWindows 2m14.85s github.com/fleetdm/fleet/v4/server/datastore/mysql TestMDMShared 2m10.27s github.com/fleetdm/fleet/v4/server/datastore/mysql TestVulnerabilities 2m7.98s github.com/fleetdm/fleet/v4/server/datastore/mysql TestPacks 1m59.2s github.com/fleetdm/fleet/v4/server/worker TestAppleMDM 1m55.11s github.com/fleetdm/fleet/v4/server/datastore/mysql TestTeams 1m47.82s github.com/fleetdm/fleet/v4/server/datastore/mysql TestAppConfig 1m42.81s github.com/fleetdm/fleet/v4/server/datastore/mysql TestHosts 1m41.79s github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240709183940 1m36.43s github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240709132642 1m36.34s github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240725182118 1m35.95s github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240730171504 1m35.73s github.com/fleetdm/fleet/v4/server/vulnerabilities/nvd TestTranslateCPEToCVE/recent_vulns 1m34.87s ... The first thing to notice is that the numbers don\u0026rsquo;t add up. Our test suite takes around 14 minutes to run, but the times in the report add up to more than 14 minutes. This discrepancy makes it hard to identify the slowest tests.\nThe second thing to notice is that our tests contain many subtests. The TestMDMApple test contains over 40 subtests. We want to know the execution time of each subtest, not just the total time for the test.\nThe third thing to notice is that the output does not provide any information regarding parallelism. We want to know if our tests run in parallel and how many run concurrently. We want to run tests in parallel when possible to speed up the test suite.\nUnderstanding parallelism in Go tests Before measuring the execution time of our tests, we need to understand how Go tests run in parallel.\nWhen you run go test, Go compiles each package in your test suite in a separate binary. It then runs each binary in parallel. The tests in different packages run concurrently. This behavior is controlled by the -p flag, which defaults to GOMAXPROCS, the number of CPUs on your machine.\nWithin a package, tests run sequentially by default \u0026ndash; the tests in the same package run one after the other. However, you can run tests in parallel within a package by calling t.Parallel() in your test functions. This behavior is controlled by the -parallel flag, which also defaults to GOMAXPROCS. So, in a system with 8 CPUs, running a test suite with many packages and parallel tests will run 8 packages concurrently and 8 tests within each package concurrently, for a total of 64 tests running concurrently.\nEach test function may have multiple subtests, which may have their own subtests, and so on. Subtests run sequentially by default. However, you can also run subtests in parallel by calling t.Parallel() in your subtest functions.\nAccurately measuring test execution time To measure the execution time of your tests, we must use the -json flag with the go test command. This flag outputs test results in JSON format, which we can parse and analyze.\nThe Action field in the JSON output shows the start and end times of each test and subtest.\n{ \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:51.969606869Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;run\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB\u0026#34; } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:51.96984165Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;run\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB/test1\u0026#34; } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:51.969928132Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;pause\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB/test1\u0026#34; } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:51.969983777Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;run\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB/test2\u0026#34; } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:51.970052987Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;pause\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB/test2\u0026#34; } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:51.970090377Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;cont\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB/test1\u0026#34; } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:51.973464469Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;cont\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB/test2\u0026#34; } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:52.015505184Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;pass\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB/test1\u0026#34;, \u0026#34;Elapsed\u0026#34;: 0.04 } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:52.015523238Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;pass\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB/test2\u0026#34;, \u0026#34;Elapsed\u0026#34;: 0.04 } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:52.015527907Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;pass\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB\u0026#34;, \u0026#34;Elapsed\u0026#34;: 0 } While parsing the JSON output, we can track how many tests are running in parallel. We can then adjust the execution time of each test by dividing the total time by the number of tests running concurrently. Since we don\u0026rsquo;t have access to the actual CPU time each test used, this is the best approximation we can get.\nWhen tests run in parallel, we typically see the pause and cont actions. If we see these actions, we know that the test or subtest is running in parallel.\nWe created a parser called goteststats that does these calculations.\nAccurate test execution time measurement in practice By running our goteststats parser on the JSON output of our test suite, we gained actionable insights into our tests\u0026rsquo; performance.\nWARNING: Stopped test not found in running tests: TestGenerateMDMApple/successful_run github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240709132642: 8.142s (total: 2m12.806s parallel: 16) github.com/fleetdm/fleet/v4/server/cron TestCalendarEvents1KHosts: 7.853s (total: 36.158s parallel: 4) github.com/fleetdm/fleet/v4/server/cron TestEventForDifferentHost: 7.853s (total: 36.158s parallel: 4) github.com/fleetdm/fleet/v4/cmd/fleet TestCronVulnerabilitiesCreatesDatabasesPath: 6.878s (total: 30.232s parallel: 4) github.com/fleetdm/fleet/v4/server/vulnerabilities/nvd TestTranslateCPEToCVE/find_vulns_on_cpes: 6.849s (total: 1m34.89s parallel: 13) github.com/fleetdm/fleet/v4/server/vulnerabilities/nvd TestTranslateCPEToCVE/recent_vulns: 6.849s (total: 1m34.89s parallel: 13) github.com/fleetdm/fleet/v4/server/vulnerabilities/oval TestOvalAnalyzer/#load/invalid_vuln_path: 5.844s (total: 1m25.152s parallel: 14) github.com/fleetdm/fleet/v4/server/vulnerabilities/oval TestOvalAnalyzer/analyzing_RHEL_software: 5.844s (total: 1m25.152s parallel: 14) github.com/fleetdm/fleet/v4/server/vulnerabilities/oval TestOvalAnalyzer/analyzing_Ubuntu_software: 5.844s (total: 1m25.151s parallel: 14) github.com/fleetdm/fleet/v4/cmd/fleet TestAutomationsSchedule: 5.699s (total: 14.213s parallel: 2) github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240725182118: 5.623s (total: 1m37.577s parallel: 17) github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240709183940: 5.588s (total: 1m36.771s parallel: 17) github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240709124958: 5.52s (total: 1m35.622s parallel: 17) github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240730171504: 5.517s (total: 1m35.74s parallel: 17) github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240726100517: 5.418s (total: 1m33.987s parallel: 17) ... For a given test, we provide the adjusted time, the total time, and the average number of tests running concurrently with this test. The adjusted time is the time the test took to execute, which is also the time saved if we removed this test from the suite.\nThe first thing to notice is that the numbers add up. The total time for the test suite is around 14 minutes, and the times in the report add up to around 14 minutes.\nThe second thing to notice is that we now have the execution time of each subtest. This information is crucial for identifying slow tests and improving their performance.\nThe third thing to notice is that we now have information about parallelism. We can see how many tests are running concurrently and how many tests are running in parallel. If we see a test with a low parallelism number, we know that this test is a bottleneck and should parallelized.\nThe WARNING message indicates that the JSON output did not contain the start time of the test. This issue can happen if the console output of the code under test does not include a new line and gets mixed with the output of Go\u0026rsquo;s testing package. For example:\n{ \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:23:17.8084601Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;output\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/fleetctl\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestGenerateMDMApple/CSR_API_call_fails\u0026#34;, \u0026#34;Output\u0026#34;: \u0026#34;requesting APNs CSR: GET /api/latest/fleet/mdm/apple/request_csr received status 502 Bad Gateway: FleetDM CSR request failed: bad request=== RUN TestGenerateMDMApple/successful_run\\n\u0026#34; } goteststats on GitHub goteststats is available on GitHub. You can use it to get detailed performance data for your Go test suite.\nFurther reading Recently, we wrote about optimizing the performance of Go applications and analyzing Go build times. And how to measure and fix unreadable code. We also explored fuzz testing with Go. In addition, we showed how to create an EXE installer for a Go program. We also published an article on using Go modules and packages. And we wrote about automatically tracking engineering metrics with Go. Watch how to measure the execution time of Go tests accurately Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-09-04T00:00:00Z","image":"https://victoronsoftware.com/posts/go-test-execution-time/crash-test-dummy-headline_hu_ceb34e7ab7732ad1.png","permalink":"https://victoronsoftware.com/posts/go-test-execution-time/","title":"How to measure the execution time of Go tests accurately"},{"content":" Creating a Go benchmark Running Go benchmarks What is benchmarking? Performance optimization is a critical part of software development. Once your application has been released and is being used by real users, you may need to optimize its performance. One way to do this is to benchmark your code to identify bottlenecks and improve its performance. Benchmarking provides you with data to make informed decisions about what parts of your code can be sped up and by how much.\nBenchmarking is the process of measuring your code\u0026rsquo;s performance. It involves running your code multiple times and measuring how long it takes to execute. By running your code multiple times, you can get an average execution time, which is more reliable than a one-off report.\nIdentifying the bottlenecks In our application, we deserialize and process large amounts of JSON data once every hour. We noticed that this process was taking a long time for some of our users. First, we used Go pprof to enable profiling and generated a flame graph to identify the bottlenecks in our code.\nGo pprof flame graph The flame graph showed us that the JSON decoding process took the most time. We benchmarked different serialization libraries to find the fastest one for our use case.\nCreating a Go benchmark In Go, you can write benchmarks using the built-in testing package. Benchmarks are written similarly to unit tests but with the Benchmark prefix instead of the Test prefix.\nBefore creating and running the benchmark, we generated 1000 test JSON files in the testdata directory.\nTo benchmark JSON decoding, we created the following benchmark.\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;testing\u0026#34; ) const files = 1000 const itemsPerFile = 1000 func BenchmarkJSONImport(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { // Read in the file (do not time) b.StopTimer() fileNumber := i % files data, err := os.ReadFile(fmt.Sprintf(\u0026#34;testdata/sample_%d.json\u0026#34;, fileNumber)) if err != nil { b.Fatal(err) } b.StartTimer() var samples []Sample dec := json.NewDecoder(bytes.NewReader(data)) if err := dec.Decode(\u0026amp;samples); err != nil { b.Fatal(err) } if len(samples) != itemsPerFile { b.Fatalf(\u0026#34;expected %d samples, got %d\u0026#34;, itemsPerFile, len(samples)) } } } Starting the function name with Benchmark indicates to go test that this is a benchmark.\nThe testing package adjusts the number of iterations through the for i := 0; i \u0026lt; b.N; i++ loop until the function lasts long enough to be timed reliably.\nThe b.StopTimer() and b.StartTimer() calls exclude part of the code from the benchmark.\nRunning Go benchmarks To run all benchmarks, add -bench=. flag to go test:\ngo test -bench=. The result will look like this:\ngoos: darwin goarch: arm64 pkg: serializer cpu: Apple M2 Pro BenchmarkJSONImport-12 357 3324997 ns/op PASS ok serializer 1.868s It tells us that unmarshalling a single file with json.Decode takes an average of 3.3 milliseconds. The benchmark ran the loop 357 times.\nBenchmarking encoding/gob Next, we will benchmark the built-in encoding/gob library.\nfunc BenchmarkGobImport(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { // Read in the file (do not time) b.StopTimer() fileNumber := i % files data, err := os.ReadFile(fmt.Sprintf(\u0026#34;testdata/sample_%d.bin\u0026#34;, fileNumber)) if err != nil { b.Fatal(err) } b.StartTimer() // decode gob var samples []Sample dec := gob.NewDecoder(bytes.NewReader(data)) if err := dec.Decode(\u0026amp;samples); err != nil { b.Fatal(err) } if len(samples) != itemsPerFile { b.Fatalf(\u0026#34;expected %d samples, got %d\u0026#34;, itemsPerFile, len(samples)) } } } Running the two benchmarks gives us:\nBenchmarkJSONImport-12 360 3279579 ns/op BenchmarkGobImport-12 2262 475469 ns/op The benchmark data shows that decoding with encoding/gob takes almost 7 times faster than using encoding/json. This gives sufficient data to present to our management and argue for switching from JSON. In addition, we can benchmark other serialization libraries to see if any of them are even faster.\nFor additional data, we included reading the file in our benchmark numbers for a complete picture of the expected speedup:\nBenchmarkJSONImport-12 360 3374064 ns/op BenchmarkGobImport-12 2710 481935 ns/op BenchmarkJSONImportFile-12 306 3746758 ns/op BenchmarkGobImportFile-12 2176 554254 ns/op Go benchmark code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/go-benchmark-serializers\nFurther reading OpenTelemetry for developers: From skeptic to advocate\nLearn how using OpenTelemetry during development leads to better production instrumentation.\nIs OpenTelemetry useful for the average software developer?\nExplore the practical benefits and limitations of OpenTelemetry with Jaeger for development.\nTop 5 metrics for software load testing performance\nDiscover the critical metrics to monitor when load testing your applications.\nAccurately measuring Go test execution time\nMaster techniques for precise test timing measurements in Go projects.\nGo JSON unmarshaling with null, set, and missing fields\nHandle complex JSON payloads effectively in your Go applications.\nCreating fuzz tests in Go\nStrengthen your code with automated fuzz testing techniques.\nWatch how to benchmark Go serializers Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-08-28T00:00:00Z","image":"https://victoronsoftware.com/posts/optimizing-performance-of-go-app/race-cars-headline_hu_a076cfe676e1bed4.png","permalink":"https://victoronsoftware.com/posts/optimizing-performance-of-go-app/","title":"How to benchmark performance of Go serializers"},{"content":" Securing database secrets with Docker secrets Securing database secrets with SQL commands What is a Zero Trust development environment? Zero Trust is a security model that assumes no trust, even inside the network. Every request is authenticated, authorized, and encrypted in a Zero Trust environment. This approach helps protect against data breaches and insider threats.\nIn our example use case, we create a development environment in a cloud instance, which includes a MySQL database running in a Docker container. We need to be able to access the MySQL database from our local machine for development purposes. However, the database may contain sensitive data, such as API keys or user passwords. We want to secure the MySQL database to prevent unauthorized access.\nWe want to make sure that the MySQL database is not easily accessible from the internet. In addition, we want to limit the exposure of database credentials.\nLaunching MySQL Docker container We can run a MySQL database in a Docker container using the official MySQL Docker image. We create docker-compose.yml like:\nservices: mysql: image: mysql:8.0 command: [ \u0026#34;mysqld\u0026#34;, \u0026#34;--datadir=/tmp/mysqldata\u0026#34;, ] environment: MYSQL_ROOT_PASSWORD: toor MYSQL_DATABASE: fleet MYSQL_USER: fleet MYSQL_PASSWORD: insecure ports: - \u0026#34;3306:3306\u0026#34; And we run docker-compose up to start the MySQL database.\nWe can access the MySQL database by using the MySQL client:\nmysql -h 127.0.0.1 -P 3306 -uroot -ptoor As we can see, the passwords are stored in plain text in the docker-compose.yml file. We want to avoid storing sensitive data in plain text.\nSecuring database secrets with Docker secrets Docker secrets allow us to store sensitive data, such as passwords, securely. We can create secrets and use them in the docker-compose.yml file.\nsecrets: mysql_root_password: file: ./mysql_root_password.txt mysql_password: environment: MYSQL_PASSWORD services: mysql: image: mysql:8.0 command: [ \u0026#34;mysqld\u0026#34;, \u0026#34;--datadir=/tmp/mysqldata\u0026#34;, ] secrets: - mysql_root_password - mysql_password environment: MYSQL_ROOT_PASSWORD_FILE: /run/secrets/mysql_root_password MYSQL_DATABASE: fleet MYSQL_USER: fleet MYSQL_PASSWORD_FILE: /run/secrets/mysql_password ports: - \u0026#34;3306:3306\u0026#34; We create a mysql_root_password.txt file and run MYSQL_PASSWORD=insecure docker-compose up to start the MySQL database.\nThe above example shows that the MySQL root password is stored in a file, and the MySQL password is passed as an environment variable. Although this approach may be an improvement, it is not secure for a Zero Trust environment. A user with access to the file system can read the secrets, and environment variables can be read by anyone who can run the ps command, like: ps eww \u0026lt;docker compose process ID\u0026gt;.\nIn addition, a user can dump the secrets from the Docker container by running:\ndocker exec \u0026lt;container ID\u0026gt; cat /run/secrets/mysql_root_password Securing database secrets with SQL commands To secure the MySQL database without exposing the secrets on the server, we can use MySQL commands to set the passwords. We spin up MySQL with the following docker-compose.yml:\nservices: mysql: image: mysql:8.0 command: [ \u0026#34;mysqld\u0026#34;, \u0026#34;--datadir=/tmp/mysqldata\u0026#34;, ] environment: MYSQL_ROOT_PASSWORD: toor MYSQL_ONETIME_PASSWORD: true ports: - \u0026#34;3306:3306\u0026#34; We set the root password and marked the root user as expired with MYSQL_ONETIME_PASSWORD: true.\nNow, as the second step, we can run the following commands to set the passwords:\necho \\ \u0026#34;ALTER USER root IDENTIFIED BY \u0026#39;$(op read op://employee/DEMO_SERVER/MYSQL_ROOT_PASSWORD)\u0026#39;;\u0026#34; \\ \u0026#34;CREATE DATABASE fleet;\u0026#34; \\ \u0026#34;CREATE USER \u0026#39;fleet\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;$(op read op://employee/DEMO_SERVER/MYSQL_PASSWORD)\u0026#39;;\u0026#34; \\ \u0026#34;GRANT ALL PRIVILEGES ON fleet.* TO \u0026#39;fleet\u0026#39;@\u0026#39;%\u0026#39;;\u0026#34; \\ \u0026#34;FLUSH PRIVILEGES;\u0026#34; \\ | mysql -h 127.0.0.1 -P 3306 -uroot -ptoor --connect-expired-password In the above command, we use 1Password as our secrets manager. We read the secrets from 1Password and pass them to the MySQL client to set the passwords.\nAdditional security considerations This article focused on securing the MySQL passwords. However, there are additional security considerations when running MySQL in a Zero Trust environment:\nEncrypting sensitive data \u0026ndash; all sensitive data should be encrypted when stored in the database Limiting access to specific IPs \u0026ndash; we can add a server firewall to restrict access to the MySQL port Further reading Recently, we wrote about setting up a remote development environment.\nWe also explained how to use STDIN to read your program arguments.\nWe\u0026rsquo;ve previously written about MySQL master-slave replication. You can use MySQL replication to create a high-availability setup for your MySQL databases.\nWatch how to secure a MySQL Docker container for Zero Trust Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-08-20T00:00:00Z","image":"https://victoronsoftware.com/posts/secure-mysql-docker/mysql-docker-headline_hu_e55bbd55daceafb0.png","permalink":"https://victoronsoftware.com/posts/secure-mysql-docker/","title":"How to secure MySQL Docker container for Zero Trust"},{"content":"STDIN is more secure than environment variables or command-line arguments When you pass command-line arguments to a program, they are visible to anyone who can run the ps command. Allowing others to read arguments is a security risk if the arguments contain sensitive information like passwords or API keys.\nEnvironment variables are also visible to anyone who can run the ps command. They are also globally visible to the program, so any arbitrary code in your application can extract the environment variables.\nTo get the environment variables of a process, run ps eww \u0026lt;PID\u0026gt;. For example:\n$ ps eww 1710 PID TTY STAT TIME COMMAND 1710 pts/0 Ss+ 0:00 bash SYSTEMD_EXEC_PID=1209 SSH_AUTH_SOCK=/run/user/1000/keyring/ssh SESSION_MANAGER=local/victor-ubuntu:@/tmp/.ICE-unix/1176,unix/victor-ubuntu:/tmp/.ICE-unix/1176 GNOME_TERMINAL_SCREEN=/org/gnome/Terminal/screen/ab0b9d6a_a699_4bc5_bb53_628be016afa5 LANG=en_US.UTF-8 XDG_CURRENT_DESKTOP=ubuntu:GNOME PWD=/home/victor WAYLAND_DISPLAY=wayland-0 DISPLAY=:0 QT_IM_MODULE=ibus USER=victor DESKTOP_SESSION=ubuntu XDG_MENU_PREFIX=gnome- HOME=/home/victor DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus SSH_AGENT_LAUNCHER=gnome-keyring _=/usr/bin/gnome-session XDG_CONFIG_DIRS=/etc/xdg/xdg-ubuntu:/etc/xdg VTE_VERSION=6800 XDG_SESSION_DESKTOP=ubuntu QT_ACCESSIBILITY=1 GNOME_DESKTOP_SESSION_ID=this-is-deprecated GNOME_SETUP_DISPLAY=:1 GTK_MODULES=gail:atk-bridge LOGNAME=victor GNOME_TERMINAL_SERVICE=:1.83 GNOME_SHELL_SESSION_MODE=ubuntu XDG_RUNTIME_DIR=/run/user/1000 XMODIFIERS=@im=ibus SHELL=/bin/bash XDG_SESSION_TYPE=wayland PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin USERNAME=victor COLORTERM=truecolor XAUTHORITY=/run/user/1000/.mutter-Xwaylandauth.R816R2 XDG_DATA_DIRS=/usr/share/ubuntu:/usr/local/share/:/usr/share/:/var/lib/snapd/desktop IM_CONFIG_PHASE=1 TERM=xterm-256color GDMSESSION=ubuntu XDG_SESSION_CLASS=user STDIN is more secure because it is not visible to the ps command and is not globally visible to the program. Thus, only the parts of the program that explicitly read from STDIN can access this data.\nHow to read program arguments from STDIN with Go In the code example below, we check if any data is being piped in from STDIN with os.ModeNamedPipe. Then, we wait to read all the data from STDIN with ioutil.ReadAll. Finally, we parse the STDIN data just like a shell would using the github.com/kballard/go-shellquote library and append it to any existing command-line arguments.\nHow to integrate with a secret manager One way to securely pass sensitive information to a program is to store it in a secret manager like 1Password. Then, you can read the secret from the secret manager and pass it to the program via STDIN. For example:\necho --secret $(op read op://employee/example_server/secret) | go run read-args-from-stdin.go Further reading Recently, we discussed how to unmarshal JSON payloads with null, set, and missing keys using Go.\nPreviously, we wrote how we catch missed authorization checks in our Go application.\nWatch how to read program arguments from STDIN Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-08-12T00:00:00Z","image":"https://victoronsoftware.com/posts/get-args-from-stdin/stdin-headline_hu_d34d818ae2ec114f.png","permalink":"https://victoronsoftware.com/posts/get-args-from-stdin/","title":"Why you should use STDIN to read your program arguments"},{"content":" Find code owners from the command line What are GitHub code owners? GitHub CODEOWNERS is a file that defines the individuals or teams responsible for code in a repository. When a user creates a pull request, GitHub uses the CODEOWNERS file to suggest the appropriate reviewers for the pull request. This process helps ensure that the right people review the code changes.\nRepository owners can enable branch protection rules that require the code owner of each changed file to approve the pull request.\nThe problem: too many files and too many code owners It can be challenging to determine who needs to review a pull request in a large repository with many files and many code owners. This challenge is especially true when the pull request touches many files.\nMany times, I\u0026rsquo;ve asked another engineer to approve my PR, and they approved it, but GitHub said that the PR still needed approval from another code owner. I needed another approval because my PR changed another file with another code owner, and I didn\u0026rsquo;t know about it.\nSteps to find the minimum required code owners To find the minimum required code owners for a pull request, we can use these steps:\nGet the list of changed files in the pull request. For each changed file, get the list of code owners. Find the minimum set of code owners that covers all the above lists. Finding the code owners manually The above steps can be done manually by opening the pull request in GitHub and hovering over the blue CODEOWNERS icon for each changed file to see the code owners. However, this can be time-consuming and error-prone.\nSee the code owners on hover Finding the code owners from the command line To automate the above steps, we will need the following prerequisites:\nGitHub CLI installed and logged in Command-line JSON processor jq installed A CODEOWNERS parser installed, such as https://github.com/hmarr/codeowners From the top directory containing your git repository, run the following:\ngh pr view $MY_PR_NUMBER --json files | jq -r \u0026#39;.files[] .path\u0026#39; \\ | xargs codeowners | tr -s \u0026#39; \u0026#39; | cut -f2- -d \u0026#39; \u0026#39; | sort -u Where $MY_PR_NUMBER is the number of your pull request.\nThe first part of the command, gh pr view $MY_PR_NUMBER --json files, gets the list of changed files in the pull request in JSON format. The second part, jq -r '.files[] .path', extracts the file paths from the JSON. The third part, xargs codeowners, runs the codeowners command for each file. The fourth optional part, tr -s ' ', removes extra spaces. The fifth part, cut -f2- -d ' ', removes the first column. The last part, sort -u, sorts and removes duplicates.\nThe output will be the list of code owners for the changed files in the pull request. For example:\n(unowned) @fleetdm/go @getvictor @lucasmrod @roperzh @mostlikelee @lucasmrod @getvictor @jacobshandling @roperzh @gillespi314 @lucasmrod @getvictor At this point, we can do additional processing, such as excluding some code owners.\nBy visually inspecting the output, we can determine the minimum set of code owners that need to review the pull request.\nFurther reading Recently, we covered top 3 issues with GitHub code review process.\nWe also wrote about how merges work with GitHub pull requests.\nPreviously, we explained how to create reusable workflows and steps in GitHub Actions.\nWatch how to find the minimum required code owners for a pull request Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-08-06T00:00:00Z","image":"https://victoronsoftware.com/posts/find-code-owners-for-pull-request/codeowners-headline_hu_f73c50cc04e718c6.png","permalink":"https://victoronsoftware.com/posts/find-code-owners-for-pull-request/","title":"Find required code owner approvers for a PR in 3 steps"},{"content":" Add Jest testing framework Write a unit test Review unit test coverage This article is part of our series on building a Chrome extension.\nWhy add unit tests? Unit tests help us catch bugs early, ensure our extension continues to work as expected in different scenarios, and make it easier to refactor our code. In this article, we will add unit tests to our Chrome extension.\nAdd Jest to the project Jest is a popular JavaScript testing framework. We will use Jest to write and run unit tests for our Chrome extension.\nTo install Jest, run:\nnpm install --save-dev jest jest-environment-jsdom ts-jest @types/jest jest is the testing framework jest-environment-jsdom simulates a browser environment for Jest tests ts-jest allows Jest to work with TypeScript @types/jest provides TypeScript definitions for Jest Configure Jest Create a jest.config.ts file in the root of the project with the following content:\nimport type { JestConfigWithTsJest } from \u0026#34;ts-jest\u0026#34; const config: JestConfigWithTsJest = { setupFiles: [\u0026#34;./__mocks__/chrome.ts\u0026#34;], testEnvironment: \u0026#34;jsdom\u0026#34;, transform: { \u0026#34;^.+.ts$\u0026#34;: [\u0026#34;ts-jest\u0026#34;, {}], }, } export default config The setupFiles option loads a mock for the Chrome API. In the next step, we will create this mock.\nThe testEnvironment option sets a browser testing environment by default. We can override the environment at the top of each test file:\n/** * @jest-environment jsdom */ The transform option specifies to process TypeScript test files with ts-jest.\nCreate a mock for the Chrome API Our extension code relies on the Chrome API, which is unavailable in our unit test environment. We will create a mock for the Chrome API to simulate its behavior in our tests.\nA mock is a fake implementation of a function or object that allows us to test our code in isolation. Mocks are helpful for testing code that depends on external services or APIs.\nCreate a __mocks__ folder in the root of the project. The __mocks__ name is a Jest convention for mock files. In that folder, add a chrome.ts file with the following content:\n// eslint-disable-next-line @typescript-eslint/ban-ts-comment -- disable ESLint check for the next line // @ts-nocheck -- this TS comment turns off TypeScript type checking for this file because we do not // mock the entire Chrome API, but only the parts we need global.chrome = { runtime: { onInstalled: { addListener: jest.fn(), }, onMessage: { addListener: jest.fn(), }, onStartup: { addListener: jest.fn(), }, sendMessage: jest.fn(), }, storage: { sync: { get: jest.fn(), set: jest.fn(), }, }, } The empty jest.fn() implementations can be replaced during testing with custom behavior using Jest\u0026rsquo;s mocking functions using jest.spyOn.\nWrite a unit test We will test the content.ts file in our first unit test. This file contains the logic for the content script that runs on web pages when the extension is active. The content script blurs a page element that contains a user-defined keyword.\nCreate a content.test.ts file in the src folder with the following content:\nimport { blurFilter, observe, config } from \u0026#34;./content\u0026#34; describe(\u0026#34;blur\u0026#34;, () =\u0026gt; { test(\u0026#34;blur a secret\u0026#34;, () =\u0026gt; { // Define the document (web page) that we will test against document.body.innerHTML = ` \u0026lt;div id=\u0026#34;testDiv\u0026#34;\u0026gt; \u0026#34;My secret\u0026#34; \u0026lt;/div\u0026gt;` // Set value to blur config.item = \u0026#34;secret\u0026#34; // Start observing the document. observe() // Make sure the element is blurred as expected const testDiv = document.getElementById(\u0026#34;testDiv\u0026#34;) as HTMLInputElement expect(testDiv).toBeDefined() expect(testDiv.style.filter).toBe(blurFilter) }) }) In the above test, the Jest functions describe and test define a test suite and a test case, respectively. The expect function checks whether the test results match the expected values.\nRun the unit tests The Jest unit test can be run using the following command:\nnpx jest The result of the test should look like:\nconsole.debug blurred id:testDiv class: tag:DIV text: \u0026#34;My secret\u0026#34; at blurElement (src/content.ts:36:11) at Array.forEach (\u0026lt;anonymous\u0026gt;) at Array.forEach (\u0026lt;anonymous\u0026gt;) at Array.forEach (\u0026lt;anonymous\u0026gt;) at Array.forEach (\u0026lt;anonymous\u0026gt;) PASS src/content.test.ts blur ‚úì blur a secret (15 ms) Test Suites: 1 passed, 1 total Tests: 1 passed, 1 total Snapshots: 0 total Time: 1.149 s Ran all test suites. Add the following script to the package.json file:\n\u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;jest\u0026#34;, } Now you can run the tests using the npm test or npm run test.\nReview unit test coverage Code coverage measures how much of the code is tested by the unit tests. A high percentage indicates that most of the code is tested and less likely to contain bugs. Code coverage is an important metric for assessing the quality of the code. A common target for code coverage is 80% or higher.\nJest can generate a code coverage report to show which parts of the code are covered by the unit tests. To create a coverage report, add the --coverage flag to the Jest command:\nnpx jest --coverage The terminal output will include the code coverage summary:\n------------|---------|----------|---------|---------|-------------------------- File | % Stmts | % Branch | % Funcs | % Lines | Uncovered Line #s ------------|---------|----------|---------|---------|-------------------------- All files | 45.23 | 48.57 | 42.85 | 45.23 | content.ts | 45.23 | 48.57 | 42.85 | 45.23 | 22,26,50-54,62-69,88-117 ------------|---------|----------|---------|---------|-------------------------- The full report is available in the coverage folder. Open the coverage/lcov-report/index.html file in a browser to view the detailed coverage report.\nNote that the code coverage report only includes the files in the test run. If you want to include all files in the coverage report, we can add the collectCoverageFrom option to the jest.config.ts Jest configuration file:\ncollectCoverageFrom: [\u0026#34;src/**/*.ts\u0026#34;], Now, the report shows a complete picture:\n---------------|---------|----------|---------|---------|-------------------------- File | % Stmts | % Branch | % Funcs | % Lines | Uncovered Line #s ---------------|---------|----------|---------|---------|-------------------------- All files | 16.96 | 30.9 | 10.71 | 16.96 | background.ts | 0 | 0 | 0 | 0 | 1-21 common.ts | 0 | 0 | 0 | 0 | 17-19 content.ts | 45.23 | 48.57 | 42.85 | 45.23 | 22,26,50-54,62-69,88-117 options.ts | 0 | 0 | 0 | 0 | 2-31 popup.ts | 0 | 0 | 0 | 0 | 1-86 ---------------|---------|----------|---------|---------|-------------------------- HTML coverage report Adding unit tests to GitHub Actions To make sure that our unit tests are run automatically on every push to the repository, we can add them to a GitHub Actions workflow. In the Linting and formatting TypeScript article, we added ESLint to GitHub Actions. We can add a step to run the Jest tests in the same workflow.\n- name: Test run: | npm run test Unit test code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/7-unit-tests\nOther articles on Unit Testing Explore fuzz testing with Go Watch how we set up unit testing for our Chrome extension Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-07-26T00:00:00Z","image":"https://victoronsoftware.com/posts/add-unit-tests-to-chrome-extension/chrome-jest-headline_hu_acca9c60d21bac2c.png","permalink":"https://victoronsoftware.com/posts/add-unit-tests-to-chrome-extension/","title":"Add unit tests to Chrome extension (2024)"},{"content":"This article will present a problem we encountered in our production distributed system and how we solved it using a distributed lock.\nThe problem \u0026ndash; data inconsistency Recently, we started using the Google Calendar API to monitor calendar changes. However, we noticed that it is possible to receive a second callback while processing the first one. This second callback can lead to data inconsistency, race conditions, and deadlocks.\nData inconsistency In the above diagram, two servers, A and B, are processing calendar events. Server B receives a callback from Google Calendar API stating that something has changed in the calendar. Google does not provide information about what event changed, so server B must fetch the event of interest from the calendar. While server B is fetching the event, server A also receives a callback. Server A also fetches the event from the calendar. Both servers now have the same event but are unaware of each other\u0026rsquo;s actions. Server B updates the event with new information. Server A also updates the event with different details, potentially overwriting or duplicating Server B\u0026rsquo;s changes. The calendar event is now in an inconsistent state, as is the data in our database.\nWhat is a distributed lock? A distributed lock is a mechanism that allows multiple servers to coordinate access to a shared resource. This mechanism is widely used across the software industry to ensure data consistency in distributed systems.\nIn our case, we need to make sure that only one server is processing a calendar event at a time. The distributed lock will prevent the second server from processing the event until the first server completes.\nImplementation of distributed lock We implemented a distributed lock using Redis. Redis is an in-memory data structure store that can be used as a database, cache, and message broker. To acquire the lock, our server sets a key in Redis with a unique value using the Redis SET command.\nSET mykey \u0026#34;myvalue\u0026#34; NX PX 60000 The NX option only sets the key if it does not exist. The PX 60000 option sets the key\u0026rsquo;s expiration time to 60 seconds. This ensures that the lock is released if the server crashes or does not release it in a timely manner.\nTo release the lock, we EVAL a Lua script that checks if the key\u0026rsquo;s value matches the unique value set by the server. If the values match, the script deletes the key using the Redis DEL command.\nif redis.call(\u0026#34;get\u0026#34;, KEYS[1]) == ARGV[1] then return redis.call(\u0026#34;del\u0026#34;, KEYS[1]) else return 0 end We only release the lock if the value matches, ensuring that the server that acquired the lock releases it.\nDistributed lock solution With the distributed lock in place, we can make sure that only one server is processing a calendar event at a time.\nUsing distributed lock with a processing queue In the above diagram, server B receives a calendar callback and acquires a lock from Redis. Server A also gets a callback but cannot acquire the lock since it has already been taken by Server B. Instead of waiting, Server A puts the event in a processing queue. Once Server B finishes processing the event, it releases the lock. Server B then checks the queue. Finding an event in the queue, the server starts a new worker process to process the events. The worker processes all outstanding events in the queue and exits on completion.\nWaiting to acquire the lock One issue we encountered was that another system process needed to acquire the lock. The process could keep trying to obtain the lock, but there was no guarantee that it would be successful in a reasonable amount of time because it could compete with other servers.\nFairness in acquiring the lock We implemented a fairness mechanism to ensure a priority process could acquire the lock.\nCron job is guaranteed to acquire the lock In the above diagram, the worker process has acquired the lock. Another process, a cron job, also needs to acquire the lock. The cron job is a priority process that needs to run at a specific time. The cron job tries to acquire the lock but fails because the worker process has it. The cron job sets a key in Redis that indicates that it wants to acquire the lock next. This action tells the other servers not to acquire the lock. The cron job then retries acquiring the lock until it is successful.\nDistributed lock code on GitHub We implemented the distributed lock logic in Go. The crucial part of the code is in the redis_lock.go file.\nDistributed lock video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-07-18T00:00:00Z","image":"https://victoronsoftware.com/posts/distributed-lock/distributed-lock-headline_hu_17089af628d186e7.png","permalink":"https://victoronsoftware.com/posts/distributed-lock/","title":"Using a distributed lock in production distributed systems"},{"content":" Add CSS to the webpack bundle Add Tailwind CSS Use Tailwind CSS utility classes This article is part of our complete guide to building a Chrome extension.\nIn the previous articles on creating a Chrome extension and adding an options page to a Chrome extension, we built two user interface pages \u0026ndash; the popup and the options page. In this article, we will improve the look and maintainability of our extension by adding a CSS framework.\nWhy add a CSS framework? Using a CSS framework like Tailwind CSS or Bootstrap can help you:\nQuickly style your extension Make your extension look professional Save time on writing custom CSS Improve the maintainability of your code Use pre-built and optimized components Add CSS to the webpack bundle Before we can use a CSS framework, we need to add CSS integration to our webpack bundle. In adding webpack to a Chrome extension, we set up webpack with a static CSS file. We will include the CSS file as part of our JavaScript bundle. Although the CSS files can be kept separate, we will include them in the JavaScript bundle since this is currently the best practice in web development.\nWe will use style-loader to inject CSS into our JavaScript and css-loader to convert our CSS file to a string. First, install the packages:\nnpm install --save-dev style-loader css-loader Then, update the webpack.common.ts to include the rule for CSS files:\n{ test: /\\.css$/, use: [ \u0026#34;style-loader\u0026#34;, \u0026#34;css-loader\u0026#34;, ], }, Move the CSS files from the static folder to the src folder, and update the popup.ts and options.ts to import the CSS files:\nimport \u0026#34;./popup.css\u0026#34;; import \u0026#34;./options.css\u0026#34;; And remove the link CSS references from the static/popup.html and static/options.html HTML files.\nWhen you run npm run build, the CSS file content will be included in the JavaScript bundle. You can verify this by inspecting the dist folder and looking at the popup.js and options.js files.\nAdd Tailwind CSS We will use Tailwind CSS for our extension. It is a popular CSS framework focused on providing CSS utility classes. First, install these packages and generate the TypeScript config file:\nnpm install --save-dev tailwindcss postcss-loader npx tailwindcss init --ts postcss-loader is a webpack loader that processes CSS with PostCSS. PostCSS is a plugin-based CSS transformer recommended for integrating Tailwind CSS with the webpack build flow.\nUpdate the webpack.common.ts CSS rule to include the loader for PostCSS with the tailwindcss plugin:\n{ test: /\\.css$/, use: [ \u0026#34;style-loader\u0026#34;, \u0026#34;css-loader\u0026#34;, { loader: \u0026#34;postcss-loader\u0026#34;, options: { postcssOptions: { plugins: [\u0026#34;postcss-import\u0026#34;, \u0026#34;tailwindcss\u0026#34;], }, }, }, ], }, The postcss-import plugin processes @import statements in CSS files, which we will use in the next step.\nUpdate the generated tailwind.config.ts file to point to our HTML files:\nimport type { Config } from \u0026#39;tailwindcss\u0026#39; export default { content: [\u0026#34;./static/*.html\u0026#34;], theme: { extend: {}, }, plugins: [], } satisfies Config Use Tailwind CSS utility classes in the HTML and CSS files This section will update our CSS to use Tailwind CSS utility classes. Using Tailwind CSS utility classes is a slightly different approach to writing CSS. Instead of writing custom CSS classes, we use pre-built utility classes to style our elements. It is a small step above writing raw CSS, but it is more maintainable and easier to read. Utility classes are documented in the Tailwind CSS documentation.\nFirst, we create a new CSS file, src/common.css to contain styles shared by both pages:\n@tailwind base; @tailwind components; @tailwind utilities; .my-input { @apply block m-1.5 p-2 bg-gray-50 border border-gray-300 text-gray-900 text-sm rounded-lg focus:ring-blue-500 focus:border-blue-500; } The @apply directive is a Tailwind CSS feature that allows us to create a custom CSS class using Tailwind CSS utility classes.\nThen, update the popup.css and options.css files to use the Tailwind CSS classes. popup.css will use Tailwind CSS utility classes:\n@import \u0026#34;./common.css\u0026#34;; .my-center { @apply w-full inline-flex items-center justify-center; } .my-button-link { @apply text-indigo-400 m-1.5 px-3 py-1 text-xs outline-none focus:outline-none ease-linear transition-all duration-150; } options.css will only use our common CSS file:\n@import \u0026#34;./common.css\u0026#34;; Next, we update static/popup.html to use our new CSS classes as well as other Tailwind CSS classes:\n\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;My popup\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;label class=\u0026#34;my-center cursor-pointer\u0026#34;\u0026gt; \u0026lt;input id=\u0026#34;enabled\u0026#34; type=\u0026#34;checkbox\u0026#34; class=\u0026#34;sr-only peer\u0026#34; /\u0026gt; \u0026lt;span class=\u0026#34;m-1.5 relative w-16 h-8 bg-gray-200 rounded-full after:content-[\u0026#39;\u0026#39;] after:absolute after:top-0.5 after:start-[4px] after:bg-white after:border-gray-300 after:border after:rounded-full after:h-7 after:w-7 after:transition-all peer-focus:outline-none peer-focus:ring-4 peer-focus:ring-blue-300 peer-checked:after:translate-x-full rtl:peer-checked:after:-translate-x-full peer-checked:after:border-white peer-checked:bg-blue-600\u0026#34; \u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;label for=\u0026#34;item\u0026#34; class=\u0026#34;invisible\u0026#34;\u0026gt;\u0026lt;/label\u0026gt; \u0026lt;input class=\u0026#34;my-input w-52\u0026#34; id=\u0026#34;item\u0026#34; type=\u0026#34;text\u0026#34; /\u0026gt; \u0026lt;div class=\u0026#34;my-center\u0026#34;\u0026gt; \u0026lt;button id=\u0026#34;go-to-options\u0026#34; class=\u0026#34;my-button-link\u0026#34;\u0026gt;Advanced options\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;popup.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; We built a custom checkbox using Tailwind CSS utility classes. The peer class styles the checkbox and the label together.\nAnd static/options.html:\n\u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Advanced options\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body class=\u0026#34;m-1.5\u0026#34;\u0026gt; \u0026lt;h2 class=\u0026#34;text-3xl m-1.5 font-extrabold\u0026#34;\u0026gt;Advanced options\u0026lt;/h2\u0026gt; \u0026lt;br /\u0026gt; \u0026lt;h3 class=\u0026#34;text-xl m-1.5 font-bold\u0026#34;\u0026gt;Web host to exclude\u0026lt;/h3\u0026gt; \u0026lt;label for=\u0026#34;exclude_host\u0026#34; class=\u0026#34;invisible\u0026#34;\u0026gt;\u0026lt;/label\u0026gt; \u0026lt;input id=\u0026#34;exclude_host\u0026#34; class=\u0026#34;my-input w-96\u0026#34; type=\u0026#34;text\u0026#34; placeholder=\u0026#34;example.com\u0026#34;/\u0026gt; \u0026lt;script src=\u0026#34;options.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Testing the extension, we should see a nicer-looking popup page:\nPopup with Tailwind CSS classes And a nicer-looking options page:\nOptions with Tailwind CSS classes Next steps In the next part of this series, we will add unit tests to our Chrome extension. Unit tests help us catch bugs early, ensure our extension continues to work as expected in different scenarios and make it easier to refactor our code.\nCSS framework code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/6-css-framework\nCSS framework video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-07-10T00:00:00Z","image":"https://victoronsoftware.com/posts/add-css-framework-to-chrome-extension/chrome-tailwind-headline_hu_693c0d39d71548c2.png","permalink":"https://victoronsoftware.com/posts/add-css-framework-to-chrome-extension/","title":"Add CSS framework to Chrome extension (2024)"},{"content":"This article is part of a series on building a complete production-ready Chrome extension.\nIn the first article of the series, we introduced the main parts of a Chrome extension \u0026ndash; the service worker (background script), content script, and popup. This article will add a fourth part to our Chrome extension \u0026ndash; an options page. This page will allow users to configure the extension\u0026rsquo;s behavior and settings.\nWhy add an options page? An options page is a user-friendly way for users to customize the extension to their needs. It can be as simple as a few checkboxes or as complex as a full settings page with multiple tabs. Users can access the options page from the Chrome extension\u0026rsquo;s popup or the Chrome extension\u0026rsquo;s context menu.\nAdding an options page To add an options page to our Chrome extension, we need to create a new HTML file and add it to the extension\u0026rsquo;s manifest. Our options page will be a simple HTML file with some JavaScript to handle user interactions.\nOur example options page will allow a user to exclude a web host, like victoronsoftware.com, from the extension\u0026rsquo;s functionality. The extension will store the excluded host in the extension\u0026rsquo;s local storage.\nWe will add several new files, update the extension\u0026rsquo;s configuration, and update the existing files.\nAdd a new file options.html to the static folder:\n\u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Advanced options\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;options.css\u0026#34;/\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h2\u0026gt;Advanced options\u0026lt;/h2\u0026gt; \u0026lt;h3\u0026gt;Web host to exclude\u0026lt;/h3\u0026gt; \u0026lt;input id=\u0026#34;exclude_host\u0026#34; type=\u0026#34;text\u0026#34;/\u0026gt; \u0026lt;script src=\u0026#34;options.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Add a new file options.css to the static folder:\n#exclude_host { margin: 5px; width: 500px; } Add a new file options.ts to the src folder, which will watch for changes on the options page:\nimport {Message, StoredConfig} from \u0026#34;./common\u0026#34; chrome.storage.sync.get(null, (data) =\u0026gt; { const config = data as StoredConfig const excludeHost = config.excludeHost ?? \u0026#34;\u0026#34; const input = document.getElementById( `exclude_host`, ) as HTMLInputElement input.value = excludeHost input.addEventListener(\u0026#34;change\u0026#34;, (event) =\u0026gt; { if (event.target instanceof HTMLInputElement) { const updatedExcludeWebsite = event.target.value const updatedConfig: StoredConfig = {excludeHost: updatedExcludeWebsite} void chrome.storage.sync.set(updatedConfig) // Send message to content script in all tabs void chrome.tabs .query({}) .then((tabs) =\u0026gt; { const message: Message = {excludeHost: updatedExcludeWebsite} for (const tab of tabs) { if (tab.id !== undefined) { chrome.tabs .sendMessage(tab.id, message) .catch(() =\u0026gt; { // We ignore tabs without a proper URL, like chrome://extensions/ // Do nothing }) } } }) .catch((error: unknown) =\u0026gt; { console.error(\u0026#34;Could not query tabs\u0026#34;, error) }) } }) }) The types in src/common.ts need to be updated to include the new excludeHost field:\nexport interface Message { enabled?: boolean excludeHost?: string } export interface StoredConfig { enabled?: boolean item?: string excludeHost?: string } The content script src/content.ts needs to be updated to handle the new excludeHost setting. See the updated file here.\nWe need to update the extension\u0026rsquo;s manifest to include the new options page. Add the following to the manifest.json file:\n{ \u0026#34;options_page\u0026#34;: \u0026#34;static/options.html\u0026#34; } In addition, we need to tell webpack to compile the new options.ts file. Update the webpack.common.ts file to include the new entry point:\nentry: { background: \u0026#34;./src/background.ts\u0026#34;, content: \u0026#34;./src/content.ts\u0026#34;, popup: \u0026#34;./src/popup.ts\u0026#34;, options: \u0026#34;./src/options.ts\u0026#34;, }, Testing the options page To test the options page, load the extension in Chrome and right-click on its icon. You should see a new Options item. Clicking on this item will open the options page.\nNew Options selection Adding a link to the options page We can add a link to the popup to make it easier for users to access the options page. Update the popup.html file to include a link to the options page:\n\u0026lt;button id=\u0026#34;go-to-options\u0026#34; class=\u0026#34;button-link\u0026#34;\u0026gt;Advanced options\u0026lt;/button\u0026gt; Add the CSS for the link:\n.button-link { margin: 10px; background: none !important; border: none; padding: 0 !important; font-family: arial, sans-serif; color: #069; text-decoration: underline; cursor: pointer; } And add the TypeScript in popup.ts to handle the link click:\n// Options page const optionsElement = document.querySelector(\u0026#34;#go-to-options\u0026#34;) if (!optionsElement) { console.error(\u0026#34;Could not find options element\u0026#34;) } else { optionsElement.addEventListener(\u0026#34;click\u0026#34;, function () { // This code is based on Chrome for Developers documentation // eslint-disable-next-line @typescript-eslint/no-unnecessary-condition if (chrome.runtime.openOptionsPage) { chrome.runtime.openOptionsPage().catch((error: unknown) =\u0026gt; { console.error(\u0026#34;Could not open options page\u0026#34;, error) }) } else { window.open(chrome.runtime.getURL(\u0026#34;options.html\u0026#34;)) } }) } We will now see the Advanced options link in the popup. Clicking on the link will take the user to the options page.\nPopup with Advanced options link Embedded options page Instead of a full options page, Chrome extensions can use an embedded options page. However, this approach was confusing and not user-friendly because Chrome takes the user to the extension details page. We recommend using a dedicated options page. To try an embedded options page, add the following to the manifest.json file:\n{ \u0026#34;options_ui\u0026#34;: { \u0026#34;page\u0026#34;: \u0026#34;options.html\u0026#34;, \u0026#34;open_in_tab\u0026#34;: false } } Next steps In the next part of this series, we will focus on the look of our popup and options page. We will add CSS to make Chrome extension pages visually appealing and user-friendly.\nOptions page code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/5-options-page\nOptions page video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-07-03T00:00:00Z","image":"https://victoronsoftware.com/posts/add-options-to-chrome-extension/chrome-extension-options-headline_hu_ea69e055dfdcc0a4.png","permalink":"https://victoronsoftware.com/posts/add-options-to-chrome-extension/","title":"Adding options page to Chrome extension (2024)"},{"content":"This article covers how git merge works with GitHub pull requests. We will focus on the use case where developers want to keep their feature branches updated with the main branch. After completing the feature work, developers create a pull request to merge their feature branch into the main branch.\ngit merge Pull request after a merge Updating a protected feature branch with a pull request What is a merge in version control? Git is a distributed version control system that allows multiple developers to work on the same codebase. When developers work on different branches, they must merge their changes into the main branch. A merge is the process of combining changes from one branch into another branch, resulting in a single branch that contains the changes from both branches.\ngit merge The standard git merge command takes each commit from one branch and applies it to another. The final commit has two parent commits: one from the current branch and one from the merged branch.\nIn the following example, we have a branch that we want to merge into the main branch:\ngit merge of two branches before merge The git log of the main branch shows the commit history:\ncommit e493ac8fea4e0efe125a561b9014313bec41a489 (HEAD -\u0026gt; main, origin/main) Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:31:12 2024 -0500 m3 commit b79e810cb86405061dc979ce4fc05fe36a724256 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:29:41 2024 -0500 m2 commit eaccad9476b472dbfb3cdfbd17088425be75b7b1 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:26:55 2024 -0500 m1 commit 4265c0a30b7b6f03d93331bc112261393c97ee1d Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:25:40 2024 -0500 first commit And the git log of the branch shows the commit history:\ncommit 2afb078875a84095327ab2ef7c83711534c5eef8 (HEAD -\u0026gt; branch, origin/branch) Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:30:20 2024 -0500 b2 commit 9fc53c2ca9a58637a5d433de4c6150b832d4d275 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:28:25 2024 -0500 b1 commit eaccad9476b472dbfb3cdfbd17088425be75b7b1 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:26:55 2024 -0500 m1 commit 4265c0a30b7b6f03d93331bc112261393c97ee1d Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:25:40 2024 -0500 first commit We merge the branch into the main branch:\ngit checkout main git merge branch The resulting commit history shows all the commits from both branched as well as the final empty merge commit pointing to the two parent commits: e493ac8 2afb078:\ncommit 09d569bd079162643462dde112246f4167f14889 (HEAD -\u0026gt; main) Merge: e493ac8 2afb078 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 21:03:01 2024 -0500 Merge branch \u0026#39;branch\u0026#39; commit e493ac8fea4e0efe125a561b9014313bec41a489 (origin/main) Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:31:12 2024 -0500 m3 commit 2afb078875a84095327ab2ef7c83711534c5eef8 (origin/branch, branch) Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:30:20 2024 -0500 b2 commit b79e810cb86405061dc979ce4fc05fe36a724256 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:29:41 2024 -0500 m2 commit 9fc53c2ca9a58637a5d433de4c6150b832d4d275 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:28:25 2024 -0500 b1 commit eaccad9476b472dbfb3cdfbd17088425be75b7b1 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:26:55 2024 -0500 m1 commit 4265c0a30b7b6f03d93331bc112261393c97ee1d Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:25:40 2024 -0500 first commit git merge of two branches after merge The result would be the same if instead we merged the main branch into the branch branch:\ngit checkout branch git merge main except the final merge commit would be slightly different:\ncommit 06713a38ed38c12f599c6e810ee50d4cacfe2de7 (HEAD -\u0026gt; branch) Merge: 2afb078 e493ac8 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:32:01 2024 -0500 Merge branch \u0026#39;main\u0026#39; into branch The merged changes on branch can be pushed to the remote repository without issues because the remote branch can be fast-forwarded to the new commit.\ngit push origin branch What is a fast-forward merge? A fast-forward merge is a merge where the base branch (target branch) has no new commits. In this case, git moves the target branch to the commit of the source branch. It is a fast-forward merge because the target branch is moved forward to the new commit.\nA fast-forward merge does not lose any history \u0026ndash; it is always possible to undo a fast-forward merge.\ngit push does not, by default, allow a merge that is not a fast-forward. Use the\u0026rsquo;- force\u0026rsquo; option to enable a merge that is not a fast-forward.\nUndo a git merge The above merge can be undone by resetting the branch to the commit before the merge, which is one of the parent commits of the merge commit.\nThis command resets the branch to the commit before the merge:\ngit reset --hard 2afb078875a84095327ab2ef7c83711534c5eef8 git rebase Another way to combine changes from one branch into another is to use git rebase. This command applies the changes from the source branch to the target branch by reapplying the commits from the source branch to the target branch.\nThe git rebase command will modify the commit history of the source branch. In our pull request examples, we will use git merge instead of git rebase to preserve all the commit histories.\nPull request after a merge When working on a feature branch, developers often want to update their branch with the latest changes from the main to make sure their feature works with the newest code. We start this process with the above-described git merge command, where we merge the main branch into the branch branch.\nAfter the merge, the developer can create a GitHub pull request to merge the branch into the main branch.\nGitHub pull request after merge Note that the commit history only shows the commits from the branch and the merge commit. The main commits are not shown in the pull request.\nGitHub shows a few options for merging the pull request:\nMerge pull request Create a merge commit: This option creates a new merge commit combining the changes from the branch and the main branches. This is the default option. Squash and merge: This option combines all the commits from the branch into a single commit and merges that commit into the main branch. Rebase and merge: This option applies the changes from the branch onto the main branch by rebasing the commits from the branch onto the main branch. Selecting Create a merge commit results in the following commit history:\nCommit history after pull request The last two commits are both merge commits.\ncommit 1eb0af8c0e9ad16a0267d8abd1ce667f125ab7e8 (HEAD -\u0026gt; main, origin/main) Merge: e493ac8 22d2107 Author: Victor Lyuboslavsky \u0026lt;******\u0026gt; Date: Sun Jun 23 07:35:57 2024 -0500 Merge pull request #1 from getvictor/branch My pull request commit 22d2107b49bca56e67b7d4e800d93f93378a0956 (origin/branch, branch) Merge: 2afb078 e493ac8 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 21:25:39 2024 -0500 Merge branch \u0026#39;main\u0026#39; into branch The PR merge commit points to the previous merge commit and the last commit on main.\nDiagram of commit history after pull request Updating a protected feature branch We have two branches in this example: main and feature. Both branches are protected, meaning that changes to them must be made through a pull request. We want to update the feature branch with the latest changes from the main branch.\nWe can do this by merging the main branch into the feature branch, creating a new branch, and creating a pull request.\ngit checkout feature git merge main git checkout -b feature-update git push origin feature-update And create a pull request to merge feature-update into feature.\nCreate a PR to merge into feature branch This pull request shows all the commits from the main branch and the merge commit. This commit history is problematic because the PR may trigger a code review from the code owners of the files that were already reviewed in previous pull requests to the main branch.\nCommits from feature-update branch After the merge, the feature branch commit history looks like:\nCommit history of feature branch after PR Now, we create a pull request to merge the update feature branch into the main branch.\nPR to merge feature branch into main After the merge, the main branch commit history looks like:\nCommit history of main after PR from feature branch The last three commits are merge commits.\ncommit 59caaf1cc5103099f850c32f1729c5ffe3525404 (HEAD -\u0026gt; main, origin/main) Merge: e493ac8 dcbc117 Author: Victor Lyuboslavsky \u0026lt;******\u0026gt; Date: Sun Jun 23 08:56:53 2024 -0500 Merge pull request #3 from getvictor/feature Feature commit dcbc117e5811e683f1074d947bc25da21b5fa5f6 (origin/feature) Merge: 2afb078 373dd82 Author: Victor Lyuboslavsky \u0026lt;******\u0026gt; Date: Sun Jun 23 08:45:44 2024 -0500 Merge pull request #2 from getvictor/feature-update Feature update commit 373dd82672a4879bfcf3b29c4feb97004359adfe (origin/feature-update, feature-update, feature) Merge: 2afb078 e493ac8 Author: Victor on Software \u0026lt;\u0026gt; Date: Sun Jun 23 08:03:22 2024 -0500 Merge branch \u0026#39;main\u0026#39; into feature Diagram of commit history after two pull requests Merging a pull request with Squash and merge If the final pull request is merged with Squash and merge, the commit history will look like:\nCommit history of main after squash and merge The last commit is a single commit that combines all the changes from the feature branch. The merge commits and all other commits are eliminated.\nThe downside of Squash and merge is that the commit history is lost. The commit history is useful for debugging, understanding the changes made, and keeping ownership of the changes when multiple developers work on the same feature branch.\nFurther reading Recently, we covered why GitHub code review process is broken for our organization.\nPreviously, we explained how to find the minimum required code owner approvers for a pull request.\nWatch how git merge works with GitHub pull requests Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-06-26T00:00:00Z","image":"https://victoronsoftware.com/posts/git-merges-and-pull-requests/git-merges-and-pull-requests-feature_hu_aeafd121b1f10dc.png","permalink":"https://victoronsoftware.com/posts/git-merges-and-pull-requests/","title":"How git merge works with GitHub pull requests"},{"content":" Setting up linting with ESLint and typescript-eslint Setting up formatting with Prettier Adding linting and formatting to CI This article is part of a series on building a maintainable Chrome extension.\nIn the previous article, we added TypeScript code for communicating between parts of a Chrome extension. This check-in will be our starting point for this article. This article will add linting and formatting to our TypeScript code, which will help us catch errors and enforce a consistent code style for larger teams.\nWhat is linting? Linting is the process of running a static code analysis program to analyze code for potential errors. Linters can catch syntax errors, typos, and other common mistakes that can lead to bugs. They can also enforce coding standards, such as indentation, variable naming, and other style rules.\nWhat is formatting? Formatting automatically changes the code\u0026rsquo;s appearance to match a specific style guide. Formatting tools can automatically add or remove whitespace, change indentation, and reformat code to make it more readable. Formatting tools can enforce a consistent code style across a project.\nWhy use linting and formatting tools? Linters and formatters work together to help developers write better code and accelerate the development process \u0026ndash; linters flag errors, while formatters automatically enforce a consistent code style.\nTogether, they can help prevent bugs, improve code quality, and make it easier for developers to read and understand the code. The result is cleaner, more maintainable code that uses many coding best practices and is easier to work with.\nLinting can also teach developers about best practices and help them avoid common pitfalls. For example, a linter can flag misused promises, such as missing await or uncaught errors.\nSetting up linting with ESLint and typescript-eslint To set up linting for TypeScript code, we will use ESLint with the typescript-eslint plugin. ESLint is a popular linter that can analyze JavaScript and TypeScript code. The typescript-eslint plugin adds TypeScript-specific rules to ESLint.\nTo set up ESLint with typescript-eslint, we need to install the following packages:\nnpm install --save-dev eslint @eslint/js @types/eslint__js typescript-eslint Next, we need to create an ESLint configuration file. We will create an eslint.config.mjs file at the root of our project:\n// @ts-check import eslint from \u0026#39;@eslint/js\u0026#39; import tseslint from \u0026#34;typescript-eslint\u0026#34; const config = tseslint.config( eslint.configs.recommended, ...tseslint.configs.recommendedTypeChecked, ...tseslint.configs.stylisticTypeChecked, { ignores: [\u0026#34;dist/**/*\u0026#34;, \u0026#34;eslint.config.mjs\u0026#34;], }, { languageOptions: { parserOptions: { project: true, tsconfigRootDir: import.meta.dirname, }, }, }, ) export default config This configuration file sets up ESLint with the recommended TypeScript type-checked rules and ignores our dist directory containing the webpack-generated bundles. We also ignore the config file because we do not want to apply TypeScript linting to it.\nWhy use the .mjs extension instead of .js for the configuration file? We are using .mjs extension for the configuration file to take advantage of ECMAScript modules. Using ES modules allows us to import and export modules using the import and export keywords. There are other ways to enable ECMAScript modules in JavaScript for our project, but this is the simplest way for just one JavaScript file. Our TypeScript files already use ECMAScript modules via these included recommended tsconfig.json settings:\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;module\u0026#34;: \u0026#34;commonjs\u0026#34;, \u0026#34;esModuleInterop\u0026#34;: true, If we used .js extension for the configuration file, we would need to use require and module.exports syntax. Otherwise, we would get an error like this:\n/Users/victor/work/create-chrome-extension/4-linting-and-formatting/eslint.config.js:3 import eslint from \u0026#39;@eslint/js\u0026#39; ^^^^^^ SyntaxError: Cannot use import statement outside a module at internalCompileFunction (node:internal/vm:77:18) at wrapSafe (node:internal/modules/cjs/loader:1288:20) at Module._compile (node:internal/modules/cjs/loader:1340:27) at Module._extensions..js (node:internal/modules/cjs/loader:1435:10) at Module.load (node:internal/modules/cjs/loader:1207:32) at Module._load (node:internal/modules/cjs/loader:1023:12) at cjsLoader (node:internal/modules/esm/translators:356:17) at ModuleWrap.\u0026lt;anonymous\u0026gt; (node:internal/modules/esm/translators:305:7) at ModuleJob.run (node:internal/modules/esm/module_job:218:25) at async ModuleLoader.import (node:internal/modules/esm/loader:329:24) Running ESLint We can run ESLint from the command line using the following command:\n./node_modules/.bin/eslint . Alternatively, we can use npx, which is a package runner tool that comes with npm:\nnpx eslint . This command will run ESLint on all TypeScript files in the current directory and subdirectories. ESLint will output any errors or warnings it finds in the code, such as:\n/Users/victor/work/create-chrome-extension/4-linting-and-formatting/src/background.ts 14:17 error Unsafe member access .enabled on an `any` value @typescript-eslint/no-unsafe-member-access /Users/victor/work/create-chrome-extension/4-linting-and-formatting/src/content.ts 51:9 error Unsafe assignment of an `any` value @typescript-eslint/no-unsafe-assignment 74:17 error Unsafe member access .enabled on an `any` value @typescript-eslint/no-unsafe-member-access 76:9 error Unsafe assignment of an `any` value @typescript-eslint/no-unsafe-assignment 76:27 error Unsafe member access .enabled on an `any` value @typescript-eslint/no-unsafe-member-access /Users/victor/work/create-chrome-extension/4-linting-and-formatting/src/popup.ts 9:23 error Unsafe argument of type `any` assigned to a parameter of type `boolean` @typescript-eslint/no-unsafe-argument 11:37 error Promise returned in function argument where a void return was expected @typescript-eslint/no-misused-promises 23:34 error Unsafe member access .title on an `any` value @typescript-eslint/no-unsafe-member-access 23:50 error Unsafe member access .url on an `any` value @typescript-eslint/no-unsafe-member-access 43:5 error Unsafe assignment of an `any` value @typescript-eslint/no-unsafe-assignment ‚úñ 10 problems (10 errors, 0 warnings) At this point, we should fix the errors and warnings that ESLint has found in our code.\nWe can also update the scripts section of our package.json file to run ESLint with npm run:\n\u0026#34;scripts\u0026#34;: { \u0026#34;lint\u0026#34;: \u0026#34;eslint .\u0026#34;, Now we can run ESLint with the following command:\nnpm run lint Setting up formatting with Prettier To format TypeScript code, we will use Prettier. Prettier is a popular code formatter that automatically formats code to match a specific style guide.\nTo set up Prettier, we need to install the following package:\nnpm install --save-dev --save-exact prettier Next, create a .prettierignore file in the root of our project to ignore the dist directory:\n/dist By default, Prettier ignores the node_modules directory.\nNext, create a .prettierrc file in the root of our project to configure Prettier:\n{ \u0026#34;semi\u0026#34;: false } We will use the default Prettier settings but turn off the semi rule to remove semicolons from the end of TypeScript lines. Removing semicolons is a common style choice in modern JavaScript and TypeScript code.\nRunning Prettier We can run Prettier from the command line using the following command:\nnpx prettier --write . This command will format all eligible files in the current directory and subdirectories.\nWe can also update the scripts section of our package.json file to run Prettier with the following command:\n\u0026#34;scripts\u0026#34;: { \u0026#34;format\u0026#34;: \u0026#34;prettier --write .\u0026#34;, \u0026#34;format-check\u0026#34;: \u0026#34;prettier --check .\u0026#34;, npm run format will format all eligible files, while npm run format-check will check if the files are formatted.\nAdding linting and formatting to continuous integration (CI) We will use GitHub Actions to automate linting and formatting checks on every pull request and commit to our main branch. This will make sure all code changes are linted and formatted correctly on the main branch.\nThis automatic check means that all contributors can expect that the code they are working on uses a consistent style and meets a quality standard. Consistency is beneficial for open-source projects where contributors may not be familiar with the codebase.\nTo set up GitHub Actions, create a .github/workflows/lint-and-format.yml file in the root of our git repository:\nname: Lint check, format check, and build on: push: branches: - main paths: # We only run the workflow if the code in these files/directories changes - \u0026#39;.github/workflows/lint-and-format.yml\u0026#39; # This file - \u0026#39;4-linting-and-formatting/**\u0026#39; # The working directory for this article pull_request: paths: - \u0026#39;.github/workflows/lint-and-format.yml\u0026#39; - \u0026#39;4-linting-and-formatting/**\u0026#39; # This allows a subsequently queued workflow run to interrupt previous runs concurrency: group: ${{ github.workflow }}-${{ github.head_ref || github.run_id}} cancel-in-progress: true defaults: run: shell: bash working-directory: ./4-linting-and-formatting permissions: contents: read jobs: lint-format-build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v4 - name: Install dependencies run: | npm install --no-save - name: Format check and lint run: | npm run format-check npm run lint - name: Build run: | npm run build Since our git repository is shared by multiple projects (from various articles), we use the paths key to only run the workflow when the code in the 4-linting-and-formatting directory changes.\nAfter pushing our code to GitHub and waiting for the GitHub Actions workflow to run, we can see the results in the Actions tab of our repository. We can see the linting and formatting checks, as well as the build step:\nGitHub Actions workflow results For more details on GitHub Actions workflows, see our article on reusing GitHub Actions workflows and steps.\nAdding stricter linting rules to typescript-eslint The recommended ruleset is a good starting point for linting TypeScript code. However, we can add stricter rules to catch even more potential issues in our code. It is easiest to start with strict rules early in the project when fixing issues is relatively painless. Otherwise, it is a good idea to gradually add stricter rules to avoid overwhelming developers with too many errors and warnings.\nTo switch to a stricter, more opinionated ruleset, replace tseslint.configs.recommendedTypeChecked with tseslint.configs.strictTypeChecked in the eslint.config.mjs file.\nESLint rules can be configured or disabled using configuration comments in the code or the ESLint configuration file. For more details, see the ESLint configure rules.\nNext steps In the next part of this series, we will add an options page to our Chrome extension. This page will allow users to configure the extension\u0026rsquo;s behavior and settings.\nFurther reading We recently wrote about enabling staticcheck linter in a large Go project Linting and formatting TypeScript code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/4-linting-and-formatting\nLinting and formatting TypeScript video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-06-19T00:00:00Z","image":"https://victoronsoftware.com/posts/linting-and-formatting-typescript/linting-and-formatting-headline_hu_e9db304e6b3e549.png","permalink":"https://victoronsoftware.com/posts/linting-and-formatting-typescript/","title":"Linting and formatting TypeScript in Chrome extension (2024)"},{"content":"This article is part of a series on building a production-ready Chrome extension.\nIn the previous article, we set up our Chrome extension with TypeScript support and the webpack bundler. This article will build on that code, dive into the new APIs, and cover message-passing communication between different parts of our Chrome extension.\nCommunication between parts of a Chrome extension As we covered in the first article, a Chrome extension consists of three main parts:\nservice worker (background script) content script popup These parts need to communicate with each other. For example, a popup needs to send a message to a content script to change the appearance of a webpage. Or a background script needs to send a message to a popup to update the user interface based on the page that\u0026rsquo;s being visited.\nCommunication in a Chrome extension One way to communicate between these parts is to use the local storage via the chrome.storage APIs. We do not recommend this method because it is slow and can cause performance issues. This method is slow because it is not synchronous \u0026ndash; the scripts need to check the storage for changes periodically. A better way to communicate between extension parts is to use message passing.\nWhat is message passing? In computer science, message passing is a method for communicating between different processes or threads. A process or thread sends a message to another process or thread, which receives the message and acts on it. This method is often used in distributed systems, where processes run on different machines and need to communicate with each other. The sender sends a message, and the receiver decodes it and executes the appropriate code.\nMessage passing in a Chrome extension Message passing is a way to communicate between different parts of a Chrome extension. Its main advantage is that it\u0026rsquo;s fast and efficient. When a message is sent, the receiver gets it immediately and can respond to it right away.\nMessage passing is done in Chrome extensions using the chrome.runtime.sendMessage, chrome.tabs.sendMessage and chrome.runtime.onMessage functions. Here\u0026rsquo;s how it works:\nThe sender calls chrome.runtime.sendMessage or chrome.tabs.sendMessage with the message to send. The receiver listens for messages using chrome.runtime.onMessage.addListener The receiver processes the incoming message and, optionally, responds to the message. Message passing from a popup to a content script Let\u0026rsquo;s see how we can use message passing to communicate between a popup and a content script. We will send a message when the user toggles the enable slider in the popup, which will enable or disable the content script\u0026rsquo;s processing. We will use the chrome.tabs.sendMessage function to send a message to a specific tab ID.\nIn the popup script (popup.ts), we send a message to all the tabs when we detect a change in the top slider:\n// Send message to content script in all tabs const tabs = await chrome.tabs.query({}) for (const tab of tabs) { // Note: sensitive tab properties such as tab.title or tab.url can only be accessed for // URLs in the host_permissions section of manifest.json chrome.tabs.sendMessage(tab.id!, {enabled: event.target.checked}) .then((response) =\u0026gt; { console.info(\u0026#34;Popup received response from tab with title \u0026#39;%s\u0026#39; and url %s\u0026#34;, response.title, response.url) }) .catch((error) =\u0026gt; { console.warn(\u0026#34;Popup could not send message to tab %d\u0026#34;, tab.id, error) }) } In the content script (content.ts), we listen for the message and process it:\n// Listen for messages from popup. chrome.runtime.onMessage.addListener((request, sender, sendResponse) =\u0026gt; { if (request.enabled !== undefined) { console.log(\u0026#34;Received message from sender %s\u0026#34;, sender.id, request) enabled = request.enabled if (enabled) { observe() } else { observer.disconnect() } sendResponse({title: document.title, url: window.location.href}) } }) When the user toggles the slider in the popup, the popup sends a message to all tabs. The receiving tab will print this message to the Chrome Developer Tools console.\nContent script received message Then, the popup will receive a response from the content script with the tab\u0026rsquo;s title and URL. This response prints to the Inspect Popup console. System tabs like chrome://extensions/ will not respond to messages.\nPopup received response Message passing from a popup to the service worker (background script) To send a message to the service worker, we must use the chrome.runtime.sendMessage function instead of chrome.tabs.sendMessage. The service worker does not have a tab ID, so we cannot use chrome.tabs.sendMessage.\nchrome.runtime.sendMessage({enabled: event.target.checked}) .then((response) =\u0026gt; { console.info(\u0026#34;Popup received response\u0026#34;, response) }) .catch((error) =\u0026gt; { console.warn(\u0026#34;Popup could not send message\u0026#34;, error) }) In the service worker script (background.ts), we listen for the message and process it:\nchrome.runtime.onMessage.addListener((request, sender, sendResponse) =\u0026gt; { if (request.enabled !== undefined) { console.log(\u0026#34;Service worker received message from sender %s\u0026#34;, sender.id, request) sendResponse({message: \u0026#34;Service worker processed the message\u0026#34;}) } }) Message passing from a content script to the popup and service worker To send a message from the content script, use the chrome.runtime.sendMessage function. The popup and service worker can listen and receive this message.\nMessage passing from the service worker (background script) to a content script and the popup Use the chrome.tabs.sendMessage function to send a message to the content script. Use the chrome.runtime.sendMessage function to send a message to the popup.\nThe code for sending a message from the service worker is the same as the code for sending a message from the popup. The receiving code in the content and popup scripts is also the same.\nNext steps In the next part of this series, we will add linting and formatting tools to our Chrome extension. These tools increase the quality of our code and increase engineering velocity for projects with multiple developers.\nChrome extension with webpack and TypeScript code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/3-message-passing\nMessage passing in a Chrome extension video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-06-12T00:00:00Z","image":"https://victoronsoftware.com/posts/message-passing-in-chrome-extension/message-passing-headline_hu_f787375be03f5119.png","permalink":"https://victoronsoftware.com/posts/message-passing-in-chrome-extension/","title":"Message passing in Chrome extension (2024)"},{"content":"We are creating a series of articles on building a production-ready Chrome extension. In this series, we cover the basics of building a Chrome extension, how to set up industry leading development tooling, and how to test and deploy your extension. Our goal is to show you how to build a Chrome extension that is easy to maintain, test, and deploy for a software development team.\n1. Create a Chrome extension from scratch Create a basic Chrome extension without any development tools. We cover the basics such as the major parts of the extension, manifest.json, and manually testing the extension in the Chrome browser.\n2. Add webpack and TypeScript to a Chrome extension We add support for TypeScript (which replaces JavaScript) and webpack (which bundles the extension) to our Chrome extension.\n3. Message passing in a Chrome extension We cover message passing communication between different parts of a Chrome extension. We dive into the code and show how to communicate between the service worker (background script), content scripts, and the popup.\n4. Linting and formatting TypeScript in a Chrome extension We set up ESLint and Prettier to lint and format our TypeScript code. This ensures our code is consistent and follows best practices.\n5. Add options page to Chrome extension We add an advanced options page to our Chrome extension. This page allows users to configure the extension\u0026rsquo;s behavior and settings.\n6. Add CSS framework to Chrome extension We improve the look and maintainability of our extension by adding a CSS framework. We use Tailwind CSS to quickly style our extension and make it look professional.\n7. Add unit tests to Chrome extension We write and run unit tests for our Chrome extension using the Jest testing framework. Unit tests help us catch bugs early, ensure our extension continues to work as expected, and make improve our code quality.\nBuild a production-ready Chrome extension video playlist ","date":"2024-06-11T00:00:00Z","image":"https://victoronsoftware.com/posts/chrome-extension/chrome-extension-headline_hu_a0b5db252f228762.png","permalink":"https://victoronsoftware.com/posts/chrome-extension/","title":"Build a production-ready Chrome extension in 7 steps"},{"content":"What is a webhook? A webhook is a way for one application to send data to another application in real time. It is a simple way to trigger an action based on an event. In other words, a webhook is a custom HTTP callback.\nWhat is Tines? Tines is a no-code automation platform that allows you to automate repetitive tasks. It is a powerful tool that can be used to automate workflows, such as sending emails, creating tickets, and updating databases.\nWhat is Fleet? Fleet is an open-source platform for managing and gathering telemetry from devices such as laptops, desktops, VMs, etc. Osquery agents run on these devices and report to the Fleet server.\nOur example IT workflow In this article, we will build a webhook flow with Tines. When a device has an outdated OS version, Tines will receive a webhook callback from Fleet. Tines will then send an MDM (Mobile Device Management) command to the device to update the device\u0026rsquo;s OS version.\nFleet will send a callback via its calendar integration feature. Fleet can put a \u0026ldquo;System Maintenance\u0026rdquo; event on the device user\u0026rsquo;s calendar. This event warns the device owner that their computer will be restarted to remediate one or more failing policies. During the calendar event time, Fleet sends a webhook. The IT admin must set up a flow to remediate the failing policy. This article is an example of one such flow.\nGetting started \u0026ndash; webhook action First, we create a new Tines story. A story is a sequence of actions that are executed in order. Next, we add a webhook action to the story. The webhook action listens for incoming webhooks. The webhook will contain a JSON body.\nTines webhook action Handling errors Often, webhooks may contain error messages if there is an issue with the configuration, flow, etc. In this example, we add a trigger action that checks whether the webhook body contains an error. Specifically, our action checks whether the webhook body contains a non-empty \u0026ldquo;error\u0026rdquo; field.\nTines trigger action checking for an error We leave this error-handling portion of the story as a stub. In the future, we can expand it by sending an email or triggering other actions.\nChecking whether webhook indicates an outdated OS At the same time, we also check whether the webhook was triggered by a policy indicating an outdated OS. From previous testing, we know that the webhook payload will look like this:\n{ \u0026#34;timestamp\u0026#34;: \u0026#34;2024-03-28T13:57:31.668954-05:00\u0026#34;, \u0026#34;host_id\u0026#34;: 11058, \u0026#34;host_display_name\u0026#34;: \u0026#34;Victor\u0026#39;s Virtual Machine (2)\u0026#34;, \u0026#34;host_serial_number\u0026#34;: \u0026#34;Z5C4L7GKY0\u0026#34;, \u0026#34;failing_policies\u0026#34;: [ { \u0026#34;id\u0026#34;: 479, \u0026#34;name\u0026#34;: \u0026#34;macOS - OS version up to date\u0026#34; } ] } The payload contains:\nThe device\u0026rsquo;s ID (host ID). Display name. Serial number. A list of failing policies. We are interested in the failing policies. When one of the failing policies contains a policy named \u0026ldquo;macOS - OS version up to date,\u0026rdquo; we know that the device\u0026rsquo;s OS is outdated. Hence, we create a trigger that looks for this policy.\nTines trigger action checking for an outdated OS We use the following formula, which loops over all policies and will only allow the workflow to proceed if true:\nIF(FIND(calendar_webhook.body.failing_policies, LAMBDA(item, item.name = \u0026#34;macOS - OS version up to date\u0026#34;)).id \u0026gt; 0, TRUE) Getting device details from Fleet Next, we need to get more details about the device from Fleet. Devices are called hosts in Fleet. We add an \u0026ldquo;HTTP Request\u0026rdquo; action to the story. The action makes a GET request to the Fleet API to get the device details. We use the host ID from the webhook payload. We are looking for the device\u0026rsquo;s UUID, which we need to send the OS update MDM command.\nTines HTTP Request action to get Fleet device details To access Fleet\u0026rsquo;s API, we need to provide an API key. We store the API key as a CREDENTIAL in the current story. The API key should belong to an API-only user in Fleet so that the key does not reset when the user logs out.\nAdd credential to Tines story Creating MDM command payload to update OS version We can create the MDM payload now that we have the device\u0026rsquo;s UUID. The payload contains the command to update the OS version. We use the ScheduleOSUpdate command from Apple\u0026rsquo;s MDM protocol.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;Command\u0026lt;/key\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;RequestType\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;ScheduleOSUpdate\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;Updates\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;InstallAction\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;InstallASAP\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ProductVersion\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;14.4.1\u0026lt;/string\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;key\u0026gt;CommandUUID\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;\u0026lt;\u0026lt;UUID()\u0026gt;\u0026gt;\u0026lt;/string\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt; This command will download macOS 14.4.1, install it, and pop up a 60-second countdown dialog box before restarting the device. Note that the \u0026lt;\u0026lt;UUID()\u0026gt;\u0026gt; Tines function creates a unique UUID for this MDM command.\nTines event to create ScheduleOSUpdate MDM command The Fleet API requires the command to be sent as a base64-encoded string. We add a \u0026ldquo;Base64 Encode\u0026rdquo; action to the story to encode the XML payload. It uses the Tines BASE64_ENCODE function.\nTines Base64 Encode event Run MDM command on device Finally, we send the MDM command to the device. We add another \u0026ldquo;HTTP Request\u0026rdquo; action to the story. The action makes a POST request to the Fleet API to send the MDM command to the device.\nTines HTTP Request action to run MDM command on device The MDM command will run on the device, downloading and installing the OS update.\nmacOS restart notification after OS update Conclusion In this article we built a webhook flow with Tines. We received a webhook callback from Fleet when a device had an outdated OS version. We then sent an MDM command to the device to update the OS version. This example demonstrates how Tines can automate workflows and tasks in IT environments.\nFurther reading Recently, we explained how to quickly get started with Google Sheets API for your development scripts. Building a webhook flow with Tines video Note: If you want to comment on this article, please do so on the YouTube video.\nThis article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2024-06-05T00:00:00Z","image":"https://victoronsoftware.com/posts/webhook-flow-with-tines/tines-fleet-webhook-workflow_hu_7c6c1d70ffb9ce23.png","permalink":"https://victoronsoftware.com/posts/webhook-flow-with-tines/","title":"Building a webhook flow with Tines"},{"content":" Excessive database locks Read-after-write consistency Index limitations When building an application, the database is often an afterthought. The database used in a development environment often contains limited data with little traffic. However, when the application is deployed to production, real-world traffic can expose issues that were not caught in development or testing.\nIn this article, we cover issues we ran into with our customers. We assume the production application is deployed with one master and one or more read replicas. See this article on creating a MySQL slave replica in dev environment.\nExcessive database locks One write query can bring your database to its knees if it locks too many rows.\nConsider this simplified INSERT with a subquery transaction:\nINSERT INTO software_counts (host_id, count) SELECT host_id, COUNT(*) as count FROM host_software GROUP BY host_software.host_id; Simplified INSERT with a subquery The above query scans the entire host_software table index to create a count. While the database is doing the scan and the INSERT, it locks the host_software table, preventing other transactions from writing to that table. If the table and insert are large, the query can hold the lock for a long time. In production, we saw a lock time of over 30 seconds, creating a bottleneck and spiking DB resource usage.\nPay special attention to the following queries, as they can cause performance issues:\nCOUNT(*) Using a non-indexed column, like WHERE non_indexed_column = value Returning a large number of rows, like SELECT * FROM table One way to solve the above performance issue is to separate the SELECT and INSERT queries. First, run the SELECT query on the replica to get the data, then run the INSERT query on the master to insert the data. We completely eliminate the lock since the read is done on the replica. This article goes through a specific example of optimizing an INSERT with subqueries.\nAs general advice, avoid running SELECT queries and subqueries on the master, especially if they scan the entire table.\nRead-after-write consistency When you write to the master and read from the replica, you might not see the data you wrote. The replica is not in sync with the master in real time. In our production, the replica is usually less than 30 milliseconds behind the master.\nRead-after-write database issue These issues are typically not caught in development since dev environments usually have one database instance. Unit or integration tests might not even see these issues if they run on a single database instance. Even in testing or small production environments, you might only see these issues if the replica sync time is high. Customers with large deployments may be experiencing these consistency issues without the development team knowing about it.\nOne way to solve this issue is to read from the master after writing to it. This way, you are guaranteed to see the data you just wrote. In our Go backend, forcing reads from the master can be done by updating the Context:\nctxUsePrimary := ctxdb.RequirePrimary(ctx, true) However, additional master reads increase the load on the master, defeating the purpose of having a replica for read scaling.\nIn addition, what about expensive read queries, like COUNT(*) and calculations, which we don\u0026rsquo;t want to run on the master? In this case, we can wait for the replica to catch up with the master.\nOne generic approach to waiting for the replica is to read the last written data from the replica and retry the read if the data is not found. The app could check the updated_at column to see if the data is recent. If the data is not found, the app can sleep for a few milliseconds and retry the read. This approach is imperfect but a good compromise between read consistency and performance.\nNote: The default precision of MySQL date and time data types is 1 second (0 fractional seconds).\nIndex limitations What are SQL indexes? Indexes are a way to optimize read queries. They are a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure. Indexes are created using one or more database columns and are stored and sorted using a B-tree or a similar data structure. The goal is to reduce the number of data comparisons needed to find the data.\nDatabase index Indexes are generally beneficial. They speed up read queries but slightly slow down write queries. Indexes can also be large and take up a lot of disk space.\nIndex size is limited As the product grows with more features, the number of columns in a specific table can also increase. Sometimes, the new columns need to be part of a unique index. However, the maximum index size in MySQL is 3072 bytes. This limit can be quickly reached if columns are of type VARCHAR or TEXT.\nCREATE TABLE `activities` ( `user_name` VARCHAR(255) NOT NULL, One way to solve the issue of hitting the index size limit is to create a new column that makes the hash of the other relevant column(s), and use that as the unique index. For example, in our backend we use a checksum column in the software table to create a unique index for a software item.\nForeign keys may cause performance issues If a table has a foreign key, any insert, update, or delete with a constraint on the foreign key column will lock the corresponding row in the parent table. This locking can lead to performance issues when\nthe parent table is large the parent has many foreign key constraints the parent table or child tables are frequently updated The performance issue manifests as excessive lock wait times for queries. One way to solve this issue is to remove the foreign key constraint. Instead, the application code can handle the data integrity checks that the foreign key constraint provides. In our application, we run a regular clean-up job to remove orphaned child rows.\nBonus database gotchas Additional database gotchas that we have seen in production include:\nPrepared statements consuming too much memory Deadlocks caused by using an UPDATE/INSERT upsert pattern Also, we recently solved a problem in production with distributed lock.\n3 database gotchas video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-29T00:00:00Z","image":"https://victoronsoftware.com/posts/database-gotchas-when-scaling-apps/database-thumbnail_hu_86c34d7127c945c8.png","permalink":"https://victoronsoftware.com/posts/database-gotchas-when-scaling-apps/","title":"3 database gotchas when building apps for scale"},{"content":"This article is part of a series on creating a production-ready Chrome extension. The previous article covered creating a basic Chrome extension without any tooling. For a list of all articles in the series, see the Chrome extension series overview.\nAdd webpack bundler Add TypeScript Convert webpack configuration from JavaScript to TypeScript Introduction This article will add the webpack module bundler and TypeScript support to the Chrome extension we created in the previous article. This software tooling will allow us to use modern JavaScript features and development tools.\nA module bundler and TypeScript are essential tools for modern web development. They improve the development experience for large or long-running projects.\nPrerequisites - Node.js and npm Before we start, make sure you have Node.js and npm installed. Node.js is a JavaScript runtime. We will use it to run webpack and future development tools. npm is a JavaScript package manager.\nYou can check if you have them installed by running the following commands:\nnode -v npm -v package.json First, we will create a package.json file containing project and dependency info. We can use the npm init command to create the file. Or manually create one containing something like:\n{ \u0026#34;name\u0026#34;: \u0026#34;my-chrome-extension\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/create-chrome-extension\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34; } What is webpack? Webpack is a module bundler for JavaScript applications. It takes modules with dependencies and generates static assets representing those modules. We will use webpack to bundle multiple JavaScript files into a single file.\nA module bundler allows you to write modular code and bundle it into a single file. TypeScript is a superset of JavaScript that adds static typing and other features to the language.\nWe will install webpack with npm:\nnpm install --save-dev webpack webpack-cli webpack-merge copy-webpack-plugin webpack is the core module bundler webpack-cli is the command-line interface for webpack webpack-merge is a utility to merge multiple webpack configurations, which we will use to differentiate development and production configs copy-webpack-plugin is a plugin to copy files and directories in webpack The above npm command will install the packages, create a package-lock.json file, and add them to the devDependencies section of the package.json file. The updated package.json should look like this:\n{ \u0026#34;name\u0026#34;: \u0026#34;my-chrome-extension\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/create-chrome-extension\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;copy-webpack-plugin\u0026#34;: \u0026#34;^12.0.2\u0026#34;, \u0026#34;webpack\u0026#34;: \u0026#34;^5.91.0\u0026#34;, \u0026#34;webpack-cli\u0026#34;: \u0026#34;^5.1.4\u0026#34;, \u0026#34;webpack-merge\u0026#34;: \u0026#34;^5.10.0\u0026#34; } } webpack configuration Next, we will create webpack configuration files. Webpack uses a configuration file to define how to bundle the project. We will create two configurations: one for development and one for production. Initially, we will use JavaScript for the configuration files, but we will convert them to TypeScript later.\nCreate a webpack.common.js file with the shared configuration:\nconst path = require(\u0026#39;path\u0026#39;) const CopyWebpackPlugin = require(\u0026#39;copy-webpack-plugin\u0026#39;); module.exports = { entry: { background: \u0026#39;./src/background.js\u0026#39;, content: \u0026#39;./src/content.js\u0026#39;, popup: \u0026#39;./src/popup.js\u0026#39;, }, output: { filename: \u0026#39;[name].js\u0026#39;, path: path.resolve(__dirname, \u0026#39;dist\u0026#39;), clean: true, // Clean the output directory before emit. }, plugins: [ new CopyWebpackPlugin({ patterns: [{ from: \u0026#39;static\u0026#39; }], }), ] } Create a webpack.dev.js file with the development configuration:\nconst { merge } = require(\u0026#39;webpack-merge\u0026#39;) const common = require(\u0026#39;./webpack.common.js\u0026#39;) module.exports = merge(common, { mode: \u0026#39;development\u0026#39;, devtool: \u0026#39;inline-source-map\u0026#39;, }) Create a webpack.prod.js file with the production configuration:\nconst { merge } = require(\u0026#39;webpack-merge\u0026#39;) const common = require(\u0026#39;./webpack.common.js\u0026#39;) module.exports = merge(common, { mode: \u0026#39;production\u0026#39;, devtool: \u0026#39;source-map\u0026#39;, }) Refactoring directory structure We will refactor the directory structure to separate the source code from the static files. Create a src directory and move the JavaScript files (background.js, content.js, popup.js) into it. Create a static directory and move the manifest.json, popup.html, and popup.css file into it.\nThe directory structure should look like this (running tree . -I node_modules):\n. ‚îú‚îÄ‚îÄ package-lock.json ‚îú‚îÄ‚îÄ package.json ‚îú‚îÄ‚îÄ src ‚îÇ¬†‚îú‚îÄ‚îÄ background.js ‚îÇ¬†‚îú‚îÄ‚îÄ content.js ‚îÇ¬†‚îî‚îÄ‚îÄ popup.js ‚îú‚îÄ‚îÄ static ‚îÇ¬†‚îú‚îÄ‚îÄ manifest.json ‚îÇ¬†‚îú‚îÄ‚îÄ popup.css ‚îÇ¬†‚îî‚îÄ‚îÄ popup.html ‚îú‚îÄ‚îÄ webpack.common.js ‚îú‚îÄ‚îÄ webpack.dev.js ‚îî‚îÄ‚îÄ webpack.prod.js Running webpack Now, we can run the webpack bundler using the following command:\n./node_modules/.bin/webpack --watch --config webpack.dev.js This command creates a dist directory with the bundled files. The --watch flag tells webpack to continue running, watch for changes, and recompile the files when changes occur. This recompilation is crucial for development, as it allows us to see our code changes in real time.\nWe can run the production build with:\n./node_modules/.bin/webpack --config webpack.prod.js Now, we can add scripts to the package.json file to simplify how we run webpack:\n... \u0026#34;scripts\u0026#34;: { \u0026#34;build\u0026#34;: \u0026#34;webpack --config webpack.prod.js\u0026#34;, \u0026#34;start\u0026#34;: \u0026#34;webpack --watch --config webpack.dev.js\u0026#34; } These scripts allow us to run npm run build to build the production version and npm start (or npm run start) to start the development version.\nAt this point, we can test the browser extension to ensure it is still working as before. Open the Chrome browser, go to chrome://extensions, enable Developer mode, click on Load unpacked, and select the dist directory.\nWhat is TypeScript? TypeScript is a superset of JavaScript that adds static typing and other features to the language. It compiles to plain JavaScript and can be used in any browser or JavaScript engine. Although TypeScript is not required for writing Chrome extensions, it is highly recommended as it can help catch errors early and improve code quality.\nWe install TypeScript with:\nnpm install --save-dev typescript @tsconfig/recommended ts-node ts-loader @types/chrome typescript is the core TypeScript compiler @tsconfig/recommended is a recommended TypeScript configuration, which we will use ts-node is a TypeScript execution environment for Node.js, which is needed for converting the webpack configuration to TypeScript ts-loader is a TypeScript loader for webpack, which is needed for webpack to understand TypeScript source files @types/chrome is the TypeScript type definitions for the Chrome extension API What are TypeScript type definitions? We loaded the @types/chrome packages to provide TypeScript type definitions for the Chrome extension API.\nTypeScript type definitions are files that describe the shape of a JavaScript library. They provide type information for JavaScript libraries that were not written in TypeScript. This information allows TypeScript to understand the library\u0026rsquo;s API and provide type checking. With this information, TypeScript can check our code.\n@types/chrome provides a global chrome object representing the Chrome extension API. No additional code is needed to use it from the command line, as TypeScript automatically loads it. However, IDEs may need to be configured to recognize this global type definition.\ntsconfig.json Next, we will create a tsconfig.json file to configure TypeScript. This file tells the TypeScript compiler how to compile the project. Create a tsconfig.json file with the recommended config:\n{ \u0026#34;extends\u0026#34;: \u0026#34;@tsconfig/recommended/tsconfig.json\u0026#34;, \u0026#34;compilerOptions\u0026#34;: { \u0026#34;sourceMap\u0026#34;: true } } We added the sourceMap option to generate source maps, which help debug TypeScript code in the browser.\nConvert webpack configuration from JavaScript to TypeScript First, rename the webpack configuration files to TypeScript files by changing the extension from .js to .ts. For example, webpack.common.js becomes webpack.common.ts. Then, update the contents of the files to TypeScript syntax.\nwebpack.common.ts:\nimport path from \u0026#39;path\u0026#39; import webpack from \u0026#39;webpack\u0026#39; import CopyWebpackPlugin from \u0026#39;copy-webpack-plugin\u0026#39; const config: webpack.Configuration = { entry: { background: \u0026#39;./src/background.ts\u0026#39;, content: \u0026#39;./src/content.ts\u0026#39;, popup: \u0026#39;./src/popup.ts\u0026#39;, }, output: { filename: \u0026#39;[name].js\u0026#39;, path: path.resolve(__dirname, \u0026#39;dist\u0026#39;), clean: true, // Clean the output directory before emit. }, plugins: [ new CopyWebpackPlugin({ patterns: [{from: \u0026#39;static\u0026#39;}], }), ] } export default config We made the following changes to the shared config:\nWe changed the require statements to import statements We changed the module.exports to export default We added the webpack.Configuration type from the webpack package webpack.dev.ts:\nimport {Configuration} from \u0026#39;webpack\u0026#39; import {merge} from \u0026#39;webpack-merge\u0026#39; import config from \u0026#39;./webpack.common\u0026#39; const merged = merge\u0026lt;Configuration\u0026gt;(config,{ mode: \u0026#39;development\u0026#39;, devtool: \u0026#39;inline-source-map\u0026#39;, }) export default merged webpack.prod.ts:\nimport {Configuration} from \u0026#39;webpack\u0026#39; import {merge} from \u0026#39;webpack-merge\u0026#39; import config from \u0026#39;./webpack.common\u0026#39; const merged = merge\u0026lt;Configuration\u0026gt;(config,{ mode: \u0026#39;production\u0026#39;, devtool: \u0026#39;source-map\u0026#39;, }) export default merged And update the package.json scripts to use the TypeScript configuration files:\n... \u0026#34;scripts\u0026#34;: { \u0026#34;build\u0026#34;: \u0026#34;webpack --config webpack.prod.ts\u0026#34;, \u0026#34;start\u0026#34;: \u0026#34;webpack --watch --config webpack.dev.ts\u0026#34; } We can test npm run start and npm run build to ensure the new webpack Typescript configurations are working correctly.\nConvert JavaScript source files to TypeScript Finally, we will convert the JavaScript source files to TypeScript. Rename the .js files to .ts files. For example, background.js becomes background.ts. Update the contents of the files to TypeScript syntax.\nAlso, we will refactor the common setBadgeText function to a shared common.ts file:\nexport function setBadgeText(enabled: boolean) { const text = enabled ? \u0026#34;ON\u0026#34; : \u0026#34;OFF\u0026#34; void chrome.action.setBadgeText({text: text}) } Updated background.ts:\nimport {setBadgeText} from \u0026#34;./common\u0026#34; function startUp() { chrome.storage.sync.get(\u0026#34;enabled\u0026#34;, (data) =\u0026gt; { setBadgeText(!!data.enabled) }) } // Ensure the background script always runs. chrome.runtime.onStartup.addListener(startUp) chrome.runtime.onInstalled.addListener(startUp) Updated content.ts:\nconst blurFilter = \u0026#34;blur(6px)\u0026#34; let textToBlur = \u0026#34;\u0026#34; // Search this DOM node for text to blur and blur the parent element if found. function processNode(node: Node) { if (node.childNodes.length \u0026gt; 0) { Array.from(node.childNodes).forEach(processNode) } if (node.nodeType === Node.TEXT_NODE \u0026amp;\u0026amp; node.textContent !== null \u0026amp;\u0026amp; node.textContent.trim().length \u0026gt; 0) { const parent = node.parentElement if (parent == null) { return } if (parent.tagName === \u0026#39;SCRIPT\u0026#39; || parent.style.filter === blurFilter) { // Already blurred return } if (node.textContent.includes(textToBlur)) { blurElement(parent) } } } function blurElement(elem: HTMLElement) { elem.style.filter = blurFilter console.debug(\u0026#34;blurred id:\u0026#34; + elem.id + \u0026#34; class:\u0026#34; + elem.className + \u0026#34; tag:\u0026#34; + elem.tagName + \u0026#34; text:\u0026#34; + elem.textContent) } // Create a MutationObserver to watch for changes to the DOM. const observer = new MutationObserver((mutations) =\u0026gt; { mutations.forEach((mutation) =\u0026gt; { if (mutation.addedNodes.length \u0026gt; 0) { mutation.addedNodes.forEach(processNode) } else { processNode(mutation.target) } }) }) // Enable the content script by default. let enabled = true const keys = [\u0026#34;enabled\u0026#34;, \u0026#34;item\u0026#34;] chrome.storage.sync.get(keys, (data) =\u0026gt; { if (data.enabled === false) { enabled = false } if (data.item) { textToBlur = data.item } // Only start observing the DOM if the extension is enabled and there is text to blur. if (enabled \u0026amp;\u0026amp; textToBlur.trim().length \u0026gt; 0) { observer.observe(document, { attributes: false, characterData: true, childList: true, subtree: true, }) // Loop through all elements on the page for initial processing. processNode(document) } }) Updated popup.ts:\nimport {setBadgeText} from \u0026#34;./common\u0026#34; console.log(\u0026#34;Hello, world from popup!\u0026#34;) // Handle the ON/OFF switch const checkbox = document.getElementById(\u0026#34;enabled\u0026#34;) as HTMLInputElement chrome.storage.sync.get(\u0026#34;enabled\u0026#34;, (data) =\u0026gt; { checkbox.checked = !!data.enabled void setBadgeText(data.enabled) }) checkbox.addEventListener(\u0026#34;change\u0026#34;, (event) =\u0026gt; { if (event.target instanceof HTMLInputElement) { void chrome.storage.sync.set({\u0026#34;enabled\u0026#34;: event.target.checked}) void setBadgeText(event.target.checked) } }) // Handle the input field const input = document.getElementById(\u0026#34;item\u0026#34;) as HTMLInputElement chrome.storage.sync.get(\u0026#34;item\u0026#34;, (data) =\u0026gt; { input.value = data.item || \u0026#34;\u0026#34; }); input.addEventListener(\u0026#34;change\u0026#34;, (event) =\u0026gt; { if (event.target instanceof HTMLInputElement) { void chrome.storage.sync.set({\u0026#34;item\u0026#34;: event.target.value}) } }) Update webpack configuration to handle TypeScript source files Update webpack.common.ts to use the new TypeScript source files and add the ts-loader to the webpack configuration:\nimport path from \u0026#39;path\u0026#39; import webpack from \u0026#39;webpack\u0026#39; import CopyWebpackPlugin from \u0026#39;copy-webpack-plugin\u0026#39; const config: webpack.Configuration = { entry: { background: \u0026#39;./src/background.ts\u0026#39;, content: \u0026#39;./src/content.ts\u0026#39;, popup: \u0026#39;./src/popup.ts\u0026#39;, }, resolve: { extensions: [\u0026#34;.ts\u0026#34;], }, module: { rules: [ { test: /\\.ts$/, loader: \u0026#34;ts-loader\u0026#34;, exclude: /node_modules/, }, ], }, output: { filename: \u0026#39;[name].js\u0026#39;, path: path.resolve(__dirname, \u0026#39;dist\u0026#39;), clean: true, // Clean the output directory before emit. }, plugins: [ new CopyWebpackPlugin({ patterns: [{from: \u0026#39;static\u0026#39;}], }), ] } export default config Debug our TypeScript extension in Chrome Build the extension with npm run start and load it in Chrome.\nRight-click the extension icon (M) and select Inspect popup to open the Chrome Developer Tools. By default, you can see the console logs from the popup.ts file.\nGo to the Sources tab in the Chrome Developer Tools and open the top/my-chrome-extension/src/popup.ts file. You can set breakpoints and debug the popup script.\nDebugging Chrome extension popup The popup.ts file should exactly match the TypeScript code we wrote. You can set breakpoints, inspect variables, and step through the code.\nNext steps In the next part of this series, we will add message passing between the content script, the background script, and the popup script. This communication will allow us to make real-time changes across all parts of our Chrome extension.\nChrome extension with webpack and TypeScript code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/2-webpack-typescript\nAdd webpack and TypeScript to a Chrome extension video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-23T00:00:00Z","image":"https://victoronsoftware.com/posts/add-webpack-and-typescript-to-chrome-extension/chrome-typescript-webpack_hu_b7e6ec8db32c6378.png","permalink":"https://victoronsoftware.com/posts/add-webpack-and-typescript-to-chrome-extension/","title":"Add webpack and TypeScript to a Chrome extension (2024)"},{"content":"In this series, we will be building a production-ready Chrome extension. We will start with a basic extension and then add more features.\nWhat is a Chrome extension? A Chrome extension is a small software program that customizes the browsing experience. It can modify and enhance the functionality of the Chrome browser. Extensions are written using web technologies such as HTML, CSS, and JavaScript.\nWhy build a Chrome extension? Users can utilize Chrome extensions to:\nModify web pages Automate tasks Integrate with third-party services Add new features to the browser And much more Prerequisites For this tutorial, no additional tools are required. We will create the extension using a text editor and the Chrome browser.\nThree parts of a Chrome extension The three main parts of a Chrome extension are the background script, content script(s), and popup. All these parts are optional.\nParts of a Chrome extension background script: Also known as a service worker, this is a long-running script that runs in the background. It can listen for events and perform tasks. content script(s): This script runs in the context of a web page. It can interact with the DOM and modify the page, including adding UI elements. The extension can statically inject this script or dynamically inject it by the background script or the popup. popup: This small HTML page appears when a user clicks the extension icon. It can contain buttons, forms, and other UI elements. This is the extension\u0026rsquo;s user interface. These three parts of the extension run independently but can communicate with each other using message passing, events, and storage.\nOur first extension will have a popup with a turn-on/off switch and an input field. The extension will blur the page elements containing the text in the input field.\nmanifest.json configuration file Create a src directory for the extension. This directory will contain all the extension files.\nThe manifest.json file is the configuration file of a Chrome extension. It contains metadata about the extension, such as its name, version, permissions, and scripts.\nCreating the popup Add a manifest.json file with the following content:\n{ \u0026#34;manifest_version\u0026#34;: 3, \u0026#34;name\u0026#34;: \u0026#34;My Chrome Extension\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;My first Chrome extension.\u0026#34;, \u0026#34;action\u0026#34;: { \u0026#34;default_popup\u0026#34;: \u0026#34;popup.html\u0026#34; }, \u0026#34;permissions\u0026#34;: [ \u0026#34;storage\u0026#34; ] } The permissions specify the permissions required by the extension. In this case, we need the storage permission to store data in the Chrome storage so that the extension can remember the state of its configuration.\nCreate popup.html with the content below.\n\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;My popup\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;popup.css\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;label class=\u0026#34;switch\u0026#34;\u0026gt; \u0026lt;input id=\u0026#34;enabled\u0026#34; type=\u0026#34;checkbox\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;slider round\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;input class=\u0026#34;secret\u0026#34; id=\u0026#34;item\u0026#34; type=\u0026#34;text\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;popup.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Our popup.html includes a CSS file and a script. Create popup.js with the following content:\n\u0026#34;use strict\u0026#34;; console.log(\u0026#34;Hello, world from popup!\u0026#34;) function setBadgeText(enabled) { const text = enabled ? \u0026#34;ON\u0026#34; : \u0026#34;OFF\u0026#34; void chrome.action.setBadgeText({text: text}) } // Handle the ON/OFF switch const checkbox = document.getElementById(\u0026#34;enabled\u0026#34;) chrome.storage.sync.get(\u0026#34;enabled\u0026#34;, (data) =\u0026gt; { checkbox.checked = !!data.enabled void setBadgeText(data.enabled) }) checkbox.addEventListener(\u0026#34;change\u0026#34;, (event) =\u0026gt; { if (event.target instanceof HTMLInputElement) { void chrome.storage.sync.set({\u0026#34;enabled\u0026#34;: event.target.checked}) void setBadgeText(event.target.checked) } }) // Handle the input field const input = document.getElementById(\u0026#34;item\u0026#34;) chrome.storage.sync.get(\u0026#34;item\u0026#34;, (data) =\u0026gt; { input.value = data.item || \u0026#34;\u0026#34; }); input.addEventListener(\u0026#34;change\u0026#34;, (event) =\u0026gt; { if (event.target instanceof HTMLInputElement) { void chrome.storage.sync.set({\u0026#34;item\u0026#34;: event.target.value}) } }) The script listens for changes in the switch and the input field. It saves the switch\u0026rsquo;s state and the input field\u0026rsquo;s value in Chrome storage.\nCreate popup.css with the following content to style the switch and the input field:\n/* The switch - the box around the slider */ .switch { margin-left: 30%; /* Center the switch */ position: relative; display: inline-block; width: 60px; height: 34px; } /* Hide default HTML checkbox */ .switch input { opacity: 0; width: 0; height: 0; } /* The slider */ .slider { position: absolute; cursor: pointer; top: 0; left: 0; right: 0; bottom: 0; background-color: #ccc; } .slider::before { position: absolute; content: \u0026#34;\u0026#34;; height: 26px; width: 26px; left: 4px; bottom: 4px; background-color: white; } input:checked + .slider { background-color: #2196F3; } input:checked + .slider:before { transform: translateX(26px); /* Move the slider to the right when checked */ } /* Rounded sliders */ .slider.round { border-radius: 34px; } .slider.round::before { border-radius: 50%; } .secret { margin: 5px; } Loading and testing the extension in Chrome Even though we have not added the background script and content script, we can load the extension in Chrome.\nOpen the Chrome browser. Go to chrome://extensions/. Enable the Developer mode. Click on Load unpacked. Select the src directory containing the extension files. Click Select Folder. The extension will be loaded. Pin the extension to the toolbar by clicking the pin button in the extension dropdown. This pin will make it easier to test the extension. The popup page will appear when you click the M extension icon. Chrome extension popup We can now do some basic testing:\nTest the switch and the input field. The state of the switch and the value of the input field should be saved in the Chrome storage. The values should persist even after restarting the browser. The badge text of the extension icon should change to \u0026ldquo;ON\u0026rdquo; or \u0026ldquo;OFF\u0026rdquo; based on the state of the switch. To inspect the extension, right-click the extension icon and select Inspect popup. You should see a \u0026ldquo;Hello, world\u0026rdquo; message in the Console tab. Creating the content script Update the manifest.json file to include the content_scripts section. The entire file should look like this:\n{ \u0026#34;manifest_version\u0026#34;: 3, \u0026#34;name\u0026#34;: \u0026#34;My Chrome Extension\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;My first Chrome extension.\u0026#34;, \u0026#34;action\u0026#34;: { \u0026#34;default_popup\u0026#34;: \u0026#34;popup.html\u0026#34; }, \u0026#34;permissions\u0026#34;: [ \u0026#34;storage\u0026#34; ], \u0026#34;content_scripts\u0026#34;: [ { \u0026#34;matches\u0026#34;: [\u0026#34;\u0026lt;all_urls\u0026gt;\u0026#34;], \u0026#34;js\u0026#34;: [\u0026#34;content.js\u0026#34;] } ] } Create the new file content.js with the following content:\n\u0026#34;use strict\u0026#34; const blurFilter = \u0026#34;blur(6px)\u0026#34; let textToBlur = \u0026#34;\u0026#34; // Search this DOM node for text to blur and blur the parent element if found. function processNode(node) { if (node.childNodes.length \u0026gt; 0) { Array.from(node.childNodes).forEach(processNode) } if (node.nodeType === Node.TEXT_NODE \u0026amp;\u0026amp; node.textContent !== null \u0026amp;\u0026amp; node.textContent.trim().length \u0026gt; 0) { const parent = node.parentElement if (parent !== null \u0026amp;\u0026amp; (parent.tagName === \u0026#39;SCRIPT\u0026#39; || parent.style.filter === blurFilter)) { // Already blurred return } if (node.textContent.includes(textToBlur)) { blurElement(parent) } } } function blurElement(elem) { elem.style.filter = blurFilter console.debug(\u0026#34;blurred id:\u0026#34; + elem.id + \u0026#34; class:\u0026#34; + elem.className + \u0026#34; tag:\u0026#34; + elem.tagName + \u0026#34; text:\u0026#34; + elem.textContent) } // Create a MutationObserver to watch for changes to the DOM. const observer = new MutationObserver((mutations) =\u0026gt; { mutations.forEach((mutation) =\u0026gt; { if (mutation.addedNodes.length \u0026gt; 0) { mutation.addedNodes.forEach(processNode) } else { processNode(mutation.target) } }) }) // Enable the content script by default. let enabled = true const keys = [\u0026#34;enabled\u0026#34;, \u0026#34;item\u0026#34;] chrome.storage.sync.get(keys, (data) =\u0026gt; { if (data.enabled === false) { enabled = false } if (data.item) { textToBlur = data.item } // Only start observing the DOM if the extension is enabled and there is text to blur. if (enabled \u0026amp;\u0026amp; textToBlur.trim().length \u0026gt; 0) { observer.observe(document, { attributes: false, characterData: true, childList: true, subtree: true, }) // Loop through all elements on the page for initial processing. processNode(document) } }) The script listens for changes in the DOM and blurs elements that contain the text specified in the input field of the popup.\nAt this point, we can test the extension by entering text in the input field and enabling it. After reloading the page, the extension should blur elements that contain the text.\nCreating the background script Our background script will update the badge text of the extension icon on startup.\nUpdate the manifest.json file to include the background section. The complete file should look like this:\n{ \u0026#34;manifest_version\u0026#34;: 3, \u0026#34;name\u0026#34;: \u0026#34;My Chrome Extension\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;My first Chrome extension.\u0026#34;, \u0026#34;action\u0026#34;: { \u0026#34;default_popup\u0026#34;: \u0026#34;popup.html\u0026#34; }, \u0026#34;permissions\u0026#34;: [ \u0026#34;storage\u0026#34; ], \u0026#34;content_scripts\u0026#34;: [ { \u0026#34;matches\u0026#34;: [\u0026#34;\u0026lt;all_urls\u0026gt;\u0026#34;], \u0026#34;js\u0026#34;: [\u0026#34;content.js\u0026#34;] } ], \u0026#34;background\u0026#34;: { \u0026#34;service_worker\u0026#34;: \u0026#34;background.js\u0026#34; } } Create a new file background.js with the following content:\n\u0026#34;use strict\u0026#34; function setBadgeText(enabled) { const text = enabled ? \u0026#34;ON\u0026#34; : \u0026#34;OFF\u0026#34; void chrome.action.setBadgeText({text: text}) } function startUp() { chrome.storage.sync.get(\u0026#34;enabled\u0026#34;, (data) =\u0026gt; { setBadgeText(!!data.enabled) }) } // Ensure the background script always runs. chrome.runtime.onStartup.addListener(startUp) chrome.runtime.onInstalled.addListener(startUp) The script listens for the startup and installation events and sets the badge text based on the extension\u0026rsquo;s saved state.\nAt this point, our basic extension is complete. We can test the extension.\nNext steps In the next part of this series, we will add development tooling to the Chrome extension, such as TypeScript support, a bundling tool called webpack, and a development mode that will reload the extension automatically when changes are made.\nFor a list of all articles in the series, see the production-ready Chrome extension series overview.\nOther getting started guides Recently, we wrote about creating a React application from scratch while minimizing the amount of tools used. We also have a guide on getting started with CGO in Go. Basic extension code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/1-basic-extension\nCreate a Chrome extension from scratch step-by-step video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-15T00:00:00Z","image":"https://victoronsoftware.com/posts/create-chrome-extension/chrome-extension-headline_hu_a0b5db252f228762.png","permalink":"https://victoronsoftware.com/posts/create-chrome-extension/","title":"Create a Chrome extension from scratch step-by-step (2024)"},{"content":"Introduction IPv6 is the latest version of the Internet Protocol. It provides a larger address space than IPv4, which is running out of addresses. IPv6 is essential for the future of the Internet, and many cloud providers support it.\nIn addition, IPv6 is more secure than IPv4. It has built-in security features like IPsec, which is optional in IPv4. IPv6 also has a simplified header, which makes it faster than IPv4.\nMany corporations use IPv6 internally, and some have even disabled IPv4. This tutorial will create a Linux VM using IPv6, with IPv4 disabled.\nThe steps are:\nCreate droplets with IPv6 enabled SSH from IPv4 client to IPv6-only server Disable IPv4 on the Linux server Prerequisites We will use Digital Ocean as our cloud provider. Their IPv6 documentation is available at https://docs.digitalocean.com/products/networking/ipv6/.\nDroplets are Digital Ocean\u0026rsquo;s virtual private servers. They run on virtualized hardware and are available in various sizes. We will create a new droplet with IPv6.\nStep 1: Create droplets with IPv6 enabled We will create two Digital Ocean droplets. The first droplet will have only IPv6 enabled, and the second droplet will have both IPv4 and IPv6 enabled. We only need the second droplet to SSH into the first droplet because our client machine uses IPv4 only.\nBoth droplets will use Ubuntu 24.04 (LTS), although any Linux distribution should work. Both droplets should have IPv6 enabled in Advanced Options.\nThe first droplet will use the Password authentication method.\nThe second droplet can have either Password or SSH authentication.\nStep 2: SSH from IPv4 client to IPv6-only server You can find the Droplet IPv4 and IPv6 addresses in the Droplet details.\nNow, we connect to the second droplet using SSH.\nssh root@143.198.235.211 From there, we can SSH into the first droplet using its IPv6 address.\nssh root@2604:a880:4:1d0::4d3:3000 Install the net-tools package to use the ifconfig command.\nsudo apt update sudo apt install net-tools Step 3: Disable IPv4 on the Linux server To disable IPv4 on the first droplet, edit the /etc/netplan/50-cloud-init.yaml network configuration file by removing all the IPv4 addresses and routes, and adding the IPv6 nameservers, as shown below.\nnetwork: version: 2 ethernets: eth0: accept-ra: false addresses: - 2604:a880:4:1d0::4d3:3000/64 match: macaddress: da:a1:07:89:d9:a1 mtu: 1500 nameservers: addresses: - 2001:4860:4860::8844 - 2001:4860:4860::8888 search: [] routes: - to: ::/0 via: 2604:a880:4:1d0::1 set-name: eth0 Apply the changes.\nsudo netplan apply --debug Now, you can view the network configuration using the ifconfig command. It should look like:\neth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet6 fe80::d8a1:7ff:fe89:d9a1 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; inet6 2604:a880:4:1d0::4d3:3000 prefixlen 64 scopeid 0x0\u0026lt;global\u0026gt; ether da:a1:07:89:d9:a1 txqueuelen 1000 (Ethernet) RX packets 5179 bytes 3832240 (3.8 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 5099 bytes 696019 (696.0 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eth1: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet6 fe80::e826:4cff:feb7:6659 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether ea:26:4c:b7:66:59 txqueuelen 1000 (Ethernet) RX packets 12 bytes 916 (916.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 43 bytes 2266 (2.2 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 233 bytes 22136 (22.1 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 233 bytes 22136 (22.1 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 You can see that the eth0 interface has an IPv6 address but no IPv4 address. The eth1 interface also has an IPv6 address. The lo interface is the loopback interface and still uses the IPv4 127.0.0.1 address. We will not disable IPv4 on the loopback interface at this point since many tools may break.\nTransfer files between IPv4 and IPv6-only servers To transfer files between the IPv4 and IPv6-only servers, you can use the scp command. First, transfer to the droplet that supports both IPv4 and IPv6, like:\nscp fleet-osquery_1.24.0_amd64.deb root@143.198.235.211:~ Then, SSH into that droplet and transfer the file to the IPv6-only droplet:\nscp fleet-osquery_1.24.0_amd64.deb root@\\[2604:a880:4:1d0::4d3:3000\\]:~ Conclusion In this tutorial, we created a Linux VM using IPv6, with IPv4 disabled. We also transferred files between an IPv4 and an IPv6-only server. IPv6 is the future of the Internet, and learning how to use it is essential. You can now create your own IPv6-only servers and experiment with them.\nFurther reading Recently, we discussed why you need VLANs in your home network.\nCreate an IPv6-only Linux server video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-08T00:00:00Z","image":"https://victoronsoftware.com/posts/create-ipv6-only-linux-server/ipv6-only_hu_356df42add42edc9.png","permalink":"https://victoronsoftware.com/posts/create-ipv6-only-linux-server/","title":"Create an IPv6-only Linux server in 3 easy steps"},{"content":"Introduction We recently encountered a performance issue in production. Once an hour, we saw a spike in average DB lock time, along with occasional deadlocks and server errors. We identified the problematic query using Amazon RDS logs. It was an INSERT statement with subqueries.\nINSERT INTO policy_stats (policy_id, inherited_team_id, passing_host_count, failing_host_count) SELECT p.id, t.id AS inherited_team_id, ( SELECT COUNT(*) FROM policy_membership pm INNER JOIN hosts h ON pm.host_id = h.id WHERE pm.policy_id = p.id AND pm.passes = true AND h.team_id = t.id ) AS passing_host_count, ( SELECT COUNT(*) FROM policy_membership pm INNER JOIN hosts h ON pm.host_id = h.id WHERE pm.policy_id = p.id AND pm.passes = false AND h.team_id = t.id ) AS failing_host_count FROM policies p CROSS JOIN teams t WHERE p.team_id IS NULL GROUP BY p.id, t.id ON DUPLICATE KEY UPDATE updated_at = NOW(), passing_host_count = VALUES(passing_host_count), failing_host_count = VALUES(failing_host_count); This statement calculated passing/failing results and inserted them into a policy_stats summary table. Unfortunately, this query took over 30 seconds to execute. During this time, it locked the important policy_membership table, preventing other threads from writing to it.\nReproducing slow SQL queries Since we saw the issue in production, we needed to reproduce it in a test environment. We created a similar schema and loaded it with data. We used a Go script to populate the tables with dummy data: https://github.com/getvictor/mysql/blob/main/insert-with-subqueries-perf/main.go.\nInitially, we used ten policies and ten teams with 10,000 hosts each, resulting in 100 inserted rows with the above query. However, the performance was only three to six seconds. Then, we increased the number of policies to 50, resulting in 500 inserted rows. The performance dropped to 30 to 60 seconds.\nThe above data made it clear that this query needed to be more scalable. As the GROUP BY p.id, t.id clause demonstrates, performance exponentially degrades with the number of policies and teams.\nDebugging slow SQL queries MySQL has powerful tools called EXPLAIN and EXPLAIN ANALYSE. These tools show how MySQL executes a query and help identify performance bottlenecks. We ran EXPLAIN ANALYSE on the problematic query and viewed the results as a tree and a diagram.\nMySQL EXPLAIN result in TREE format MySQL EXPLAIN result as a diagram Although the EXPLAIN output was complex, it was clear that the SELECT subqueries were executing too many times.\nFixing INSERT with subqueries performance The first step was to separate the INSERT from the SELECT. The top SELECT subquery took most of the time. But, more importantly, the SELECT does not block other threads from updating the policy_membership table.\nHowever, the single standalone SELECT subquery was still slow. In addition, memory usage could be high for many teams and policies.\nWe decided to process one policy row at a time. This reduced the time to complete an individual SELECT query to less than two seconds and limited the memory usage. We did not use a transaction to minimize locks. Not utilizing a transaction meant that the INSERT could fail if a parallel process deleted the policy. Also, the INSERT could overwrite a clearing of the policy_stats row. These drawbacks were acceptable, as they were rare cases.\nSELECT p.id as policy_id, t.id AS inherited_team_id, ( SELECT COUNT(*) FROM policy_membership pm INNER JOIN hosts h ON pm.host_id = h.id WHERE pm.policy_id = p.id AND pm.passes = true AND h.team_id = t.id ) AS passing_host_count, ( SELECT COUNT(*) FROM policy_membership pm INNER JOIN hosts h ON pm.host_id = h.id WHERE pm.policy_id = p.id AND pm.passes = false AND h.team_id = t.id ) AS failing_host_count FROM policies p CROSS JOIN teams t WHERE p.team_id IS NULL AND p.id = ? GROUP BY t.id, p.id; After each SELECT, we inserted the results into the policy_stats table.\nINSERT INTO policy_stats (policy_id, inherited_team_id, passing_host_count, failing_host_count) VALUES (?, ?, ?, ?), ... ON DUPLICATE KEY UPDATE updated_at = NOW(), passing_host_count = VALUES(passing_host_count), failing_host_count = VALUES(failing_host_count); Further reading about MySQL MySQL deadlock on UPDATE/INSERT upsert pattern Scaling DB performance using master slave replication Fully supporting Unicode and emojis in your app SQL prepared statements are broken when scaling applications MySQL code to populate DB on GitHub The code to populate our test DB is available on GitHub at: https://github.com/getvictor/mysql/tree/main/insert-with-subqueries-perf\nMySQL query performance: INSERT with subqueries video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-06T00:00:00Z","image":"https://victoronsoftware.com/posts/mysql-query-performance-insert-subqueries/INSERT%20with%20subqueries_hu_e678b21b78faffd9.png","permalink":"https://victoronsoftware.com/posts/mysql-query-performance-insert-subqueries/","title":"Optimize MySQL query performance: INSERT with subqueries"},{"content":" GitHub reusable workflows GitHub reusable steps (composite action) Introduction GitHub Actions is a way to automate your software development workflows. The approach is similar to CI/CD tools like Jenkins, CircleCI, and TravisCI. However, GitHub Actions are built into GitHub.\nHigh level diagram of GitHub Actions The entry point for GitHub Actions is the .github/workflows directory in your repository. This directory contains one or more YAML files that define your workflows. A workflow is an automated process made up of one or more jobs. Each job runs on a separate runner. A runner is a server that runs the job. A job contains one or more steps. Each step runs a separate command.\nWhy reuse? Code reuse is a fundamental principle of software development. Reusing GitHub Actions code allows you to:\nImprove maintainability by keeping common code in one place and reducing the amount of code Increase consistency since multiple workflows can use the same code Promote best practices Increase productivity Reduce errors Examples of reusable GitHub Actions code include:\nCode signing Uploading artifacts to cloud services Security checks Notifications and reports Data processing and many others Reusable workflows A reusable workflow replaces a job in the main workflow.\nGitHub Actions reusable workflow A reusable workflow may be shared across repositories and run on a different platform than the main workflow.\nFor file sharing, \u0026lsquo;build artifacts\u0026rsquo; must be used to share files with the main workflow. The reusable workflow does not inherit environment variables. However, it accepts inputs and secrets from the calling workflow and may use outputs to pass data back to the main workflow.\nHere is an example of a reusable workflow. It uses the same schema as a regular workflow.\nname: Reusable workflow on: workflow_call: inputs: reusable_input: description: \u0026#39;Input to the reusable workflow\u0026#39; required: true type: string filename: required: true type: string secrets: HELLO_WORLD_SECRET: required: true outputs: # Map the workflow output(s) to job output(s) reusable_output: description: \u0026#39;Output from the reusable workflow\u0026#39; value: ${{ jobs.reusable-workflow-job.outputs.job_output }} defaults: run: shell: bash jobs: reusable-workflow-job: runs-on: ubuntu-20.04 # Map the job output(s) to step output(s) outputs: job_output: ${{ steps.process-step.outputs.step_output }} steps: - name: Process reusable input id: process-step env: HELLO_WORLD_SECRET: ${{ secrets.HELLO_WORLD_SECRET }} run: | echo \u0026#34;reusable_input=${{ inputs.reusable_input }}\u0026#34; echo \u0026#34;HELLO_WORLD_SECRET=${HELLO_WORLD_SECRET}\u0026#34; echo \u0026#34;step_output=${{ inputs.reusable_input }}_processed\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT - uses: actions/download-artifact@v4 with: name: input_file - name: Process file run: | echo \u0026#34;Processing file: ${{ inputs.filename }}\u0026#34; echo \u0026#34;file processed\u0026#34; \u0026gt;\u0026gt; ${{ inputs.filename }} - uses: actions/upload-artifact@v4 with: name: output_file path: ${{ inputs.filename }} The reusable workflow is triggered on: workflow_call. It accepts an input called reusable_input and generates an output called reusable_output. It also downloads an artifact called input_file, processes a file, and uploads an artifact called output_file.\nThe main workflow calls the reusable workflow using the uses keyword.\njob-2: needs: job-1 # We do not need to check out the repository to use the reusable workflow uses: ./.github/workflows/reusable-workflow.yml with: reusable_input: \u0026#34;job-2-input\u0026#34; filename: \u0026#34;input.txt\u0026#34; secrets: # Can also implicitly pass the secrets with: secrets: inherit HELLO_WORLD_SECRET: TERCES_DLROW_OLLEH A successful run of the main workflow looks like this on GitHub:\nGitHub Actions reusable workflow success Reusable steps (composite action) Reusable steps replace a regular step in a job. We will use a composite action for reusable steps in our example.\nGitHub Actions reusable steps (composite action) Like a reusable workflow, a composite action may be shared across repositories, it accepts inputs, and it may use outputs to pass data back to the main workflow.\nUnlike a reusable workflow, a composite action inherits environment variables. However, it does not inherit secrets. Secrets must be passed explicitly as inputs or environment variables. Also, there is no need to use \u0026lsquo;build artifacts\u0026rsquo; to share files since the reusable steps run on the same runner and in the same work area as the main job.\nHere is an example of a composite action. It uses a different schema than a workflow. Also, the file must be named action.yml or similar.\nname: Reusable steps (AKA composite action) description: Demonstrate how to use reusable steps in a workflow # Schema: https://json.schemastore.org/github-action.json inputs: reusable_input: description: \u0026#39;Input to the reusable workflow\u0026#39; required: true filename: required: true outputs: # Map the action output(s) to step output(s) reusable_output: description: \u0026#39;Output from the reusable workflow\u0026#39; value: ${{ steps.process-step.outputs.step_output }} runs: using: \u0026#39;composite\u0026#39; steps: - name: Process reusable input id: process-step # Shell must explicitly specify the shell for each step. https://github.com/orgs/community/discussions/18597 shell: bash run: | echo \u0026#34;reusable_input=${{ inputs.reusable_input }}\u0026#34; echo \u0026#34;HELLO_WORLD_SECRET=${HELLO_WORLD_SECRET}\u0026#34; echo \u0026#34;step_output=${{ inputs.reusable_input }}_processed\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT - name: Process file shell: bash run: | echo \u0026#34;Processing file: ${{ inputs.filename }}\u0026#34; echo \u0026#34;file processed\u0026#34; \u0026gt;\u0026gt; ${{ inputs.filename }} The composite action is called via the uses setting on a step. Our action accepts an input called reusable_input and generates an output called reusable_output. It also processes a file called filename.\nThe following code snippet shows how to use the composite action in a job.\n- name: Use reusable steps id: reusable-steps uses: ./.github/reusable-steps # To use this syntax, we must have the repository checked out with: reusable_input: \u0026#34;job-2-input\u0026#34; filename: \u0026#34;input.txt\u0026#34; env: HELLO_WORLD_SECRET: TERCES_DLROW_OLLEH A successful run of the main workflow with reusable steps looks like this on GitHub:\nGitHub Actions composite action success For a reusable TypeScript action example, see the How to create a custom GitHub Action using TypeScript article.\nConclusion Reusable workflows and steps are powerful tools for improving the maintainability, consistency, and productivity of your GitHub Actions. They allow you to reuse code across repositories and workflows and promote best practices. They are a great way to reduce errors and increase productivity.\nFor larger units of work, a reusable workflow should be used. A composite action should be used for smaller units of work that may run on the same runner and share the same work area.\nExample code on GitHub The example code is available on GitHub at: https://github.com/getvictor/github-reusable-workflows-and-steps\nOther articles related to GitHub Automate tracking of engineering metrics with GitHub Actions Is GitHub code review process broken in your company? git merge and GitHub pull requests explained Finding the minimum required code owner approvers for pull request Use GitHub actions for general-purpose tasks GitHub Actions reusable workflows and steps video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-01T00:00:00Z","image":"https://victoronsoftware.com/posts/github-reusable-workflows-and-steps/GitHub%20Actions%20thumbnail_hu_976f041deee6d2a9.png","permalink":"https://victoronsoftware.com/posts/github-reusable-workflows-and-steps/","title":"How to reuse workflows and steps in GitHub Actions (2024)"},{"content":"What is an SQL deadlock? A deadlock occurs when two or more SQL transactions are waiting for each other to release locks. This can occur when two transactions have locks on separate resources and each is waiting for the other to release its lock.\nWhat is an upsert? An upsert combines the words \u0026ldquo;update\u0026rdquo; and \u0026ldquo;insert.\u0026rdquo; It is a database operation that inserts a new row into a table if the row does not exist or updates the row if it already exists.\nINSERT/UPDATE upsert pattern One common way to implement an upsert operation in MySQL is to use the following pattern:\nUPDATE table_name SET column1 = value1, column2 = value2 WHERE id = ?; -- If the UPDATE statement does not affect any rows, insert a new row: INSERT INTO table_name (id, column1, column2) VALUES (?, value1, value2); UPDATE returns the number of rows that were actually changed.\nThis UPDATE/INSERT pattern is optimized for frequent updates and rare inserts. However, it can lead to deadlocks when multiple transactions try to insert simultaneously.\nMySQL deadlock example We assume the default transaction isolation level of REPEATABLE READ. Given the following table with one row:\nCREATE TABLE my_table ( id int(10) unsigned NOT NULL, amount int(10) unsigned NOT NULL, PRIMARY KEY (id) ); INSERT INTO my_table (id, amount) VALUES (1, 1); One transaction executes the following SQL:\nUPDATE my_table SET amount = 2 WHERE id = 2; Another transaction executes the following SQL:\nUPDATE my_table SET amount = 3 WHERE id = 3; INSERT INTO my_table (id, amount) VALUES (3, 3); At this point, the second transaction is waiting for the first transaction to release the lock.\nThe first transaction then executes the following SQL:\nINSERT INTO my_table (id, amount) VALUES (2, 2); Causing a deadlock:\n[40001][1213] Deadlock found when trying to get lock; try restarting transaction Why does the deadlock occur? The deadlock occurs because both transactions set next-key locks on the rows they attempted to update. Since the rows they attempted to update did not exist, the lock is set on the \u0026ldquo;supremum\u0026rdquo; pseudo-record. This pseudo-record has a value higher than any value actually in the index. This lock prevents the other transaction from inserting the row it needs.\nDebugging MySQL deadlocks To view the last deadlock detected by MySQL, you can use:\nSHOW ENGINE INNODB STATUS; The output will contain a section like this:\n------------------------ LATEST DETECTED DEADLOCK ------------------------ 2024-04-28 12:29:17 281472351068032 *** (1) TRANSACTION: TRANSACTION 1638819, ACTIVE 7 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 3 lock struct(s), heap size 1128, 2 row lock(s) MySQL thread id 97926, OS thread handle 281471580295040, query id 24192112 192.168.65.1 root update /* ApplicationName=DataGrip 2024.1 */ INSERT INTO my_table (id, amount) VALUES (3, 3) *** (1) HOLDS THE LOCK(S): RECORD LOCKS space id 158 page no 4 n bits 72 index PRIMARY of table `test`.`my_table` trx id 1638819 lock_mode X Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 158 page no 4 n bits 72 index PRIMARY of table `test`.`my_table` trx id 1638819 lock_mode X insert intention waiting Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (2) TRANSACTION: TRANSACTION 1638812, ACTIVE 13 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 3 lock struct(s), heap size 1128, 2 row lock(s) MySQL thread id 97875, OS thread handle 281471585578880, query id 24192285 192.168.65.1 root update /* ApplicationName=DataGrip 2024.1 */ INSERT INTO my_table (id, amount) VALUES (2, 2) *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 158 page no 4 n bits 72 index PRIMARY of table `test`.`my_table` trx id 1638812 lock_mode X Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 158 page no 4 n bits 72 index PRIMARY of table `test`.`my_table` trx id 1638812 lock_mode X insert intention waiting Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; We can see the supremum locks held by both transactions: 0: len 8; hex 73757072656d756d; asc supremum;;.\nAnother way to debug MySQL deadlocks is to enable the innodb_print_all_deadlocks option. This option prints all deadlocks to the error log.\nPreventing the UPDATE/INSERT deadlock One way to prevent the deadlock is to use the INSERT \u0026hellip; ON DUPLICATE KEY UPDATE pattern. This syntax allows you to insert a new row or update an existing row in a single statement. However, it is slower than an UPDATE and always increments the auto-increment value if present.\nAnother way is to roll back the transaction once we know that the UPDATE did not affect any rows. This avoids the deadlock by not holding the lock while we insert the new row. After the rollback, we can retry the transaction using the above INSERT ... ON DUPLICATE KEY UPDATE pattern.\nA third way is not to use transactions. In this case, the locks are released immediately after the statement is executed. However, this approach may not be suitable for all use cases.\nConclusion The UPDATE/INSERT upsert pattern can lead to MySQL deadlocks when multiple transactions try to insert simultaneously. To prevent deadlocks, consider using the INSERT ... ON DUPLICATE KEY UPDATE pattern, rolling back the transaction, or not using transactions.\nMySQL deadlock on UPDATE/INSERT upsert pattern video Other articles related to MySQL Optimize MySQL query performance: INSERT with subqueries Master slave replication in MySQL Fully supporting Unicode and emojis in your app SQL prepared statements are broken when scaling applications Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-04-28T00:00:00Z","image":"https://victoronsoftware.com/posts/mysql-upsert-deadlock/MySQL%20deadlock_hu_76d5d11012123a5d.png","permalink":"https://victoronsoftware.com/posts/mysql-upsert-deadlock/","title":"MySQL deadlock on UPDATE/INSERT upsert pattern"},{"content":"Introduction In this article, we will create a simple React app from scratch. We will not use any templates or helper scripts. We aim to reduce tool usage and fully understand each step of the process.\nWhat is React? React is a popular JavaScript library for building user interfaces. It was created by Meta (Facebook) and is maintained by Meta and a community of developers. React is used to build single-page applications (SPAs) and dynamic web applications.\nPrerequisites \u0026ndash; Node.js and npm Node.js and npm are the most popular tools for working with React. Node.js is a JavaScript runtime. npm is a package manager for Node.js. These two tools are essential for modern web development.\npackage.json We will start by creating a package.json file. This file contains metadata about the project and its dependencies. You can use the npm init command to create the package.json file. Or create one yourself containing something like:\n{ \u0026#34;name\u0026#34;: \u0026#34;react-hello-world\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Hello world app using React\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/react\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34; } TypeScript Next, we will add TypeScript to our project. TypeScript is a superset of JavaScript that adds static types to the language. It helps catch errors early in the development process and improves code quality.\nAlthough TypeScript is not required to build a React app, it is strongly recommended. TypeScript is widely used in the React community and provides many benefits. Modern IDEs, such as Visual Studio Code and WebStorm, support TypeScript, making development and learning easier.\nnpm install --save-dev typescript This command updates the package.json file with the TypeScript dependency.\n{ \u0026#34;name\u0026#34;: \u0026#34;react-hello-world\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Hello world app using React\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/react\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;typescript\u0026#34;: \u0026#34;^5.4.5\u0026#34; } } It also creates a node_modules directory. This directory contains the packages installed by npm.\nFinally, the command creates a package-lock.json file. This file locks the dependencies to specific versions, ensuring that the project is built with the same versions of the dependencies across different machines.\nThe --save-dev flag tells npm to save the package as a development dependency. Development dependencies are not required for the production build of the app.\ntsconfig.json We need to create a tsconfig.json file to configure TypeScript. This file specifies the root files and compiler options for the TypeScript compiler. We will extend the recommended base configuration.\nInstall the recommended configuration with the following:\nnpm install --save-dev @tsconfig/recommended Then, create a tsconfig.json file with the following content:\n{ \u0026#34;extends\u0026#34;: \u0026#34;@tsconfig/recommended/tsconfig.json\u0026#34;, \u0026#34;compilerOptions\u0026#34;: { \u0026#34;jsx\u0026#34;: \u0026#34;react-jsx\u0026#34; } } What is JSX? In our tsconfig.json file, we set the jsx option to react-jsx. This option tells TypeScript to treat JSX as React JSX.\nJSX is a syntax extension for JavaScript. It allows you to write HTML-like code in JavaScript. JSX is used in React. It is syntactic sugar that is generally transpiled into JavaScript by the build tool.\nReact and ReactDOM Next, we will add React and ReactDOM to our project. React is the base library. ReactDOM is the package that provides DOM-specific methods for React.\nnpm install react react-dom Since we are using TypeScript, we must also install type definitions for React and ReactDOM. The TypeScript compiler uses these definitions for type checking.\nnpm install --save-dev @types/react @types/react-dom What is Webpack? Webpack is a module bundler for JavaScript. It takes modules with dependencies and generates static assets representing those modules. We will use Webpack as the build tool for our React app.\nWe will install the Webpack packages:\nnpm install --save-dev webpack webpack-cli webpack-dev-server html-webpack-plugin ts-loader webpack is the core package webpack-cli provides the command-line interface, which we will use to run Webpack commands webpack-dev-server is a development server that serves the app html-webpack-plugin will generate the index.html file to serve our app ts-loader is a TypeScript loader for Webpack. It allows Webpack to compile TypeScript files. webpack.config.ts By default, Webpack does not need a configuration file. However, since we use TypeScript, we must create a webpack.config.ts file to configure Webpack.\nNote that we use the .ts extension for the configuration file. The TypeScript compiler will compile this file. Using a .js file is also possible, but we prefer TypeScript for type safety.\nNo additional type definitions are required for our Webpack configuration at this time.\nimport HtmlWebpackPlugin from \u0026#39;html-webpack-plugin\u0026#39;; module.exports = { entry: \u0026#39;./src/index.tsx\u0026#39;, module: { rules: [ { test: /\\.(ts|tsx)$/, loader: \u0026#34;ts-loader\u0026#34;, exclude: /node_modules/, }, ], }, plugins: [new HtmlWebpackPlugin()], } We specify src/index.tsx as our app\u0026rsquo;s top-level file. By default, the build\u0026rsquo;s output will go to the dist directory.\nWe configure the TypeScript loader to compile .ts and .tsx files.\nWe also use the html-webpack-plugin to generate an index.html file. This file will load the Webpack bundle.\nWe need to add a TypeScript execution engine to the Node.js runtime so that it can understand the above TypeScript configuration file. We will use ts-node for this purpose.\nnpm install --save-dev ts-node Final package.json After all the installations, our package.json file should look similar to this:\n{ \u0026#34;name\u0026#34;: \u0026#34;react-hello-world\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Hello world app using React\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/react\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;@tsconfig/recommended\u0026#34;: \u0026#34;^1.0.6\u0026#34;, \u0026#34;@types/react\u0026#34;: \u0026#34;^18.2.79\u0026#34;, \u0026#34;@types/react-dom\u0026#34;: \u0026#34;^18.2.25\u0026#34;, \u0026#34;html-webpack-plugin\u0026#34;: \u0026#34;^5.6.0\u0026#34;, \u0026#34;ts-loader\u0026#34;: \u0026#34;^9.5.1\u0026#34;, \u0026#34;ts-node\u0026#34;: \u0026#34;^10.9.2\u0026#34;, \u0026#34;typescript\u0026#34;: \u0026#34;^5.4.5\u0026#34;, \u0026#34;webpack\u0026#34;: \u0026#34;^5.91.0\u0026#34;, \u0026#34;webpack-cli\u0026#34;: \u0026#34;^5.1.4\u0026#34;, \u0026#34;webpack-dev-server\u0026#34;: \u0026#34;^5.0.4\u0026#34; }, \u0026#34;dependencies\u0026#34;: { \u0026#34;react\u0026#34;: \u0026#34;^18.2.0\u0026#34;, \u0026#34;react-dom\u0026#34;: \u0026#34;^18.2.0\u0026#34; } } src/index.tsx We are finally ready to write some React code. TSX files are TypeScript files that contain JSX.\nWe will create the src/index.tsx file. It will render a simple React component. React components are the reusable building blocks of React apps.\nimport React from \u0026#34;react\u0026#34;; import {createRoot} from \u0026#34;react-dom/client\u0026#34; // A simple Class component class HelloWorld extends React.Component { render() { return \u0026lt;h1\u0026gt;Hello world!\u0026lt;/h1\u0026gt; } } // Use traditional DOM manipulation to create a root element for React document.body.innerHTML = \u0026#39;\u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt;\u0026lt;/div\u0026gt;\u0026#39; // Create a root element for React const app = createRoot(document.getElementById(\u0026#34;app\u0026#34;)!) // Render our HelloWorld component app.render(\u0026lt;HelloWorld/\u0026gt;) Running the app on the Webpack development server Now, we can run the app on the Webpack development server. This server will serve the app and automatically reload the page when the code changes.\nnode_modules/.bin/webpack serve --mode development --open The --mode development flag tells Webpack to build the app in development mode. The --open flag tells Webpack to open the app in the default browser.\nThe browser should show the following:\nReact app served by Webpack dev server package.json scripts Instead of remembering the above webpack command, we can add a script to the package.json file to run the Webpack development server.\n\u0026#34;scripts\u0026#34;: { \u0026#34;start\u0026#34;: \u0026#34;webpack serve --mode development --open\u0026#34; } start is a special script name that maps to the npm start command. Now, we can run the development server with:\nnpm start or\nnpm run start Building the app for production To build the app for production, we can run:\nnode_modules/.bin/webpack --mode production This command will create a dist directory with the app\u0026rsquo;s production build. The directory will contain the index.html file and the main.js JavaScript bundle. The production files are optimized for performance, and they are minified and compressed to reduce their size.\nIt is possible to host these production files on a local HTTP server like Apache or Nginx, or deploy the app to cloud providers such as AWS, Cloudflare Pages, Netlify, Render, or Vercel.\nOther getting started guides Recently, we wrote about creating a Chrome extension from scratch without any additional tooling. As part of that series, we covered adding linting and formatting tooling for TypeScript. We also have a guide on using CGO in Go programming language. Example code on GitHub The example code is available on GitHub at https://github.com/getvictor/react/tree/main/1-hello-world\nReact Hello World video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-04-24T00:00:00Z","image":"https://victoronsoftware.com/posts/react-hello-world/react-hello-world_hu_f7ec4c1ee10d39e7.png","permalink":"https://victoronsoftware.com/posts/react-hello-world/","title":"Build a React app from scratch: getting started (2024)"},{"content":"Why fix security vulnerabilities? Security vulnerabilities are a common issue in software development. They can lead to data breaches, unauthorized access, and other security incidents. It is important to fix security vulnerabilities as soon as possible to protect your data and users.\nFinding vulnerabilities Nowadays, it is possible to integrate various vulnerability scanning tools into your CI/CD pipeline. These tools can help you identify security vulnerabilities in your code and dependencies. One such tool is OpenSSF Scorecard, which combines multiple other tools into a single GitHub action. It uses the OSV service to find vulnerabilities affecting your project\u0026rsquo;s dependencies. OSV (Open Source Vulnerabilities) is a Google-based vulnerability database providing information about open-source projects' vulnerabilities.\nIn this article, we will focus on fixing a few recent real-world security vulnerabilities in our yarn.lock dependencies.\nscore is 3: 6 existing vulnerabilities detected: Warn: Project is vulnerable to: GHSA-crh6-fp67-6883 Warn: Project is vulnerable to: GHSA-wf5p-g6vw-rhxx Warn: Project is vulnerable to: GHSA-p6mc-m468-83gw Warn: Project is vulnerable to: GHSA-566m-qj78-rww5 Warn: Project is vulnerable to: GHSA-7fh5-64p2-3v2j Warn: Project is vulnerable to: GHSA-4wf5-vphf-c2xc Click Remediation section below to solve this issue Using local tools to find vulnerabilities In a local environment, we can use OSV-Scanner to find vulnerabilities in our dependencies. Running:\nosv-scanner scan --lockfile yarn.lock It will output the same vulnerabilities mentioned above but with additional details.\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ ‚îÇ OSV URL ‚îÇ CVSS ‚îÇ ECOSYSTEM ‚îÇ PACKAGE ‚îÇ VERSION ‚îÇ SOURCE ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ https://osv.dev/GHSA-crh6-fp67-6883 ‚îÇ 9.8 ‚îÇ npm ‚îÇ @xmldom/xmldom ‚îÇ 0.8.3 ‚îÇ yarn.lock ‚îÇ ‚îÇ https://osv.dev/GHSA-wf5p-g6vw-rhxx ‚îÇ 6.5 ‚îÇ npm ‚îÇ axios ‚îÇ 0.21.4 ‚îÇ yarn.lock ‚îÇ ‚îÇ https://osv.dev/GHSA-p6mc-m468-83gw ‚îÇ 7.4 ‚îÇ npm ‚îÇ lodash.set ‚îÇ 4.3.2 ‚îÇ yarn.lock ‚îÇ ‚îÇ https://osv.dev/GHSA-566m-qj78-rww5 ‚îÇ 5.3 ‚îÇ npm ‚îÇ postcss ‚îÇ 6.0.23 ‚îÇ yarn.lock ‚îÇ ‚îÇ https://osv.dev/GHSA-7fh5-64p2-3v2j ‚îÇ 5.3 ‚îÇ npm ‚îÇ postcss ‚îÇ 6.0.23 ‚îÇ yarn.lock ‚îÇ ‚îÇ https://osv.dev/GHSA-7fh5-64p2-3v2j ‚îÇ 5.3 ‚îÇ npm ‚îÇ postcss ‚îÇ 7.0.39 ‚îÇ yarn.lock ‚îÇ ‚îÇ https://osv.dev/GHSA-7fh5-64p2-3v2j ‚îÇ 5.3 ‚îÇ npm ‚îÇ postcss ‚îÇ 8.4.21 ‚îÇ yarn.lock ‚îÇ ‚îÇ https://osv.dev/GHSA-4wf5-vphf-c2xc ‚îÇ 7.5 ‚îÇ npm ‚îÇ terser ‚îÇ 5.12.1 ‚îÇ yarn.lock ‚îÇ ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ Another way to find these vulnerabilities is by using the built-in yarn audit command.\nWaiving vulnerabilities In some cases, you may decide to waive a vulnerability. This approach means that you examine the vulnerability documentation and acknowledge it but decide not to fix it.\nTo waive a vulnerability for the OSV flow, you can create an osv-scanner.toml file in the root of your project. For example, to waive the GHSA-crh6-fp67-6883 vulnerability, you can add the following to the osv-scanner.toml file:\n[[IgnoredVulns]] id = \u0026#34;GHSA-crh6-fp67-6883\u0026#34; reason = \u0026#34;We examined this vulnerability and concluded that it does not affect our project for a very good reason.\u0026#34; In our example, we will not waive any vulnerabilities, but we will fix them by updating the dependencies.\nUpdating an inner dependency In our example, we have a vulnerability in the @xmldom/xmldom package. From the vulnerability URL, we know we must update this package to 0.8.4 or later.\nRunning yarn why @xmldom/xmldom will show that it is an inner dependency of another package:\n=\u0026gt; Found \u0026#34;@xmldom/xmldom@0.8.3\u0026#34; info Reasons this module exists - \u0026#34;msw#@mswjs#interceptors\u0026#34; depends on it - Hoisted from \u0026#34;msw#@mswjs#interceptors#@xmldom#xmldom\u0026#34; Looking at yarn.lock shows:\n\u0026#34;@xmldom/xmldom@^0.8.3\u0026#34;: version \u0026#34;0.8.3\u0026#34; resolved \u0026#34;https://registry.yarnpkg.com/@xmldom/xmldom/-/xmldom-0.8.3.tgz#beaf980612532aa9a3004aff7e428943aeaa0711\u0026#34; integrity sha512-Lv2vySXypg4nfa51LY1nU8yDAGo/5YwF+EY/rUZgIbfvwVARcd67ttCM8SMsTeJy51YhHYavEq+FS6R0hW9PFQ== We see that 0.8.4 will satisfy the dependency requirement of ^0.8.3. We can update the package by deleting the above section from yarn.lock and running yarn install\nWe will then see the update:\n\u0026#34;@xmldom/xmldom@^0.8.3\u0026#34;: version \u0026#34;0.8.10\u0026#34; resolved \u0026#34;https://registry.yarnpkg.com/@xmldom/xmldom/-/xmldom-0.8.10.tgz#a1337ca426aa61cef9fe15b5b28e340a72f6fa99\u0026#34; integrity sha512-2WALfTl4xo2SkGCYRt6rDTFfk9R1czmBvUQy12gK2KuRKIpWEhcbbzy8EZXtz/jkRqHX8bFEc6FC1HjX4TUWYw== Upgrading an inner dependency by overriding the version Our following vulnerability is in the axios package. We need to update it to 0.28.0 or later. By running yarn why axios we see that this package is part of a deep dependency chain:\n=\u0026gt; Found \u0026#34;wait-on#axios@0.21.4\u0026#34; info This module exists because \u0026#34;@storybook#test-runner#jest-playwright-preset#jest-process-manager#wait-on\u0026#34; depends on it. The needed version 0.28.0 does not satisfy the ^0.21.4 requirement. We can override the version by adding the following to the package.json file:\n\u0026#34;resolutions\u0026#34;: { \u0026#34;**/wait-on/axios\u0026#34;: \u0026#34;^0.28.0\u0026#34; }, Upgrading the parent dependency The following vulnerability is in the lodash.set package. The vulnerability URL shows that there is no fix for this vulnerability. We also see at npmjs.com that this package was last updated eight years ago.\nWe need to update the parent package that uses lodash.set. Running yarn why lodash.set shows:\ninfo Reasons this module exists - \u0026#34;nock\u0026#34; depends on it - Hoisted from \u0026#34;nock#lodash.set\u0026#34; We update the parent by running yarn upgrade nock@latest. Luckily, the latest version of nock does not depend on lodash.set, and lodash.set is removed from yarn.lock.\nRemoving a dependency Sometimes the best way to fix a vulnerability is to remove the vulnerable dependency. This can be done with the yarn remove \u0026lt;dependency\u0026gt; command. However, this requires code changes. You must find a different library or implement the removed functionality yourself.\nConclusion We use the above strategies to fix the vulnerabilities in our project.\nUpdating an inner dependency Upgrading an inner dependency by overriding the version Upgrading the parent dependency Removing a dependency We can now rerun the vulnerability scanner to verify that we fixed the vulnerabilities.\nIn addition, we must run our unit test and integration test suite to ensure that the updates do not break our application.\nFix security vulnerabilities in Yarn video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-04-10T00:00:00Z","image":"https://victoronsoftware.com/posts/fix-security-vulnerabilities-yarn/cover_hu_c56039400bd50d01.png","permalink":"https://victoronsoftware.com/posts/fix-security-vulnerabilities-yarn/","title":"Fix security vulnerabilities in Yarn"},{"content":"We recently completed a series of articles on mutual TLS (mTLS). In this series, we covered the basics of mTLS, how to use macOS keychain and Windows certificate store, and how to build an mTLS Go client. Our goal was to show you how to use mTLS in your applications and securely store your mTLS certificates and keys without exposing them to the filesystem.\nHere is a summary of the articles in the series:\nMutual TLS intro and hands-on example An introduction to mTLS and a hands-on example of using an mTLS client to connect to an mTLS server.\nUsing mTLS with the macOS keychain A guide on how to use the macOS system keystore to store your mTLS certificates and keys. We connect to an mTLS server with applications that use the macOS system keychain to find the mTLS certificates.\nCreate an mTLS Go client We create a standard mTLS client in Go using the crypto/tls library. This client loads the client certificate and private key from the filesystem.\nAdd a custom certificate signer to the mTLS Go client We implement a custom crypto.Signer to sign a client certificate during the mTLS handshake. Thus, we are a step closer to removing our client certificate and private key from the filesystem.\nA complete mTLS Go client using the macOS keychain In this article, we continue the previous article by connecting our custom signer to the macOS keychain using CGO and Apple APIs.\nUsing mTLS with the Windows certificate store Switching to Windows, we learn how to use the Windows system keystore to store your mTLS certificates and keys. We connect to an mTLS server with applications that use the Windows certificate store to find the mTLS certificates.\nCreate an mTLS Go client using the Windows certificate store Using the software pattern from the previous articles on the macOS keychain, we build an mTLS client in Go integrated with the Windows certificate store to store the mTLS certificates and keys.\nmTLS vs HTTP signature faceoff: securing your APIs Where do mTLS and HTTP message signatures fit, and how to choose the right one for your architecture.\nMutual TLS (mTLS): building a client using the system keystore video playlist ","date":"2024-04-01T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls/mtls-handshake_hu_87e12984511b4ef4.png","permalink":"https://victoronsoftware.com/posts/mtls/","title":"Mutual TLS (mTLS): building a client using the system keystore"},{"content":"What is code signing? Code signing is the process of digitally signing executables and scripts to confirm the software author and guarantee that the code has not been altered or corrupted since it was signed. The method employs a cryptographic hash to validate the authenticity and integrity of the code.\nThe benefits of code signing Code signing provides several benefits:\nUser trust: Users are likelier to trust signed software because they can verify its origin. Security: Code signing helps prevent tampering and makes sure that bad actors have not altered the software. Malware protection: Code signing helps protect users from malware by verifying the software\u0026rsquo;s authenticity. Software updates: Code signing helps users verify that software updates are legitimate and not malicious. Windows Defender: Code signing helps prevent Windows Defender warnings. Code signing process for Windows The code signing process for Windows involves the following steps:\nObtain a code signing certificate: Purchase a code signing certificate from a trusted certificate authority (CA) or use a self-signed certificate. Sign the code: Use a code signing tool to sign the code with the code signing certificate. Timestamp the signature: Timestamp the signature to make sure that the signature remains valid even after the certificate expires. Distribute the signed code: Distribute the signed code to users. Verify the signature: Users can verify the signature to confirm the software\u0026rsquo;s authenticity. Obtaining a code signing certificate In our example, we will use a self-signed certificate. This approach is suitable for internal business applications. For public applications, you should obtain a code signing certificate from a trusted CA.\nWe will use the OpenSSL command line tool to generate the certificates. OpenSSL is a popular open-source library for TLS and SSL protocols.\nThe following script generates the certificate and key needed for code signing. It also generates a certificate authority (CA) and signs the code signing certificate with the CA.\n#!/usr/bin/env bash # -e: Immediately exit if any command has a non-zero exit status. # -x: Print all executed commands to the terminal. # -u: Exit if an undefined variable is used. # -o pipefail: Exit if any command in a pipeline fails. set -exuo pipefail # This script generates certificates and keys needed for code signing. mkdir -p certs # Certificate authority (CA) openssl genrsa -out certs/ca.key 2048 openssl req -new -x509 -nodes -days 1000 -key certs/ca.key -out certs/ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testCodeSignCA\u0026#34; # Generate a certificate for code signing, signed by the CA openssl req -newkey rsa:2048 -nodes -keyout certs/sign.key -out certs/sign.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testCodeSignCert\u0026#34; openssl x509 -req -in certs/sign.req -days 398 -CA certs/ca.crt -CAkey certs/ca.key -set_serial 01 -out certs/sign.crt # Clean up rm certs/sign.req Building the application We will build a simple \u0026ldquo;Hello World\u0026rdquo; Windows application using the Go programming language for this example. We compile the application with:\nexport GOOS=windows export GOARCH=amd64 go build ./hello-world.go The Go build process generates the hello-world.exe Windows executable.\nSigning and timestamping the code To sign the code, we will use osslsigncode, an open-source code signing tool that uses OpenSSL to sign Windows executables. Unlike Microsoft\u0026rsquo;s signtool, osslsigncode is cross-platform and can be used on Linux and macOS.\nTo sign the code, we use the following script:\n#!/usr/bin/env bash # -e: Immediately exit if any command has a non-zero exit status. # -x: Print all executed commands to the terminal. # -u: Exit if an undefined variable is used. # -o pipefail: Exit if any command in a pipeline fails. set -exuo pipefail input_file=$1 if [ ! -f \u0026#34;$input_file\u0026#34; ] then echo \u0026#39;First argument must be path to binary\u0026#39; exit 1 fi # Check that input file is a windows PE (Portable Executable) if ! ( file \u0026#34;$input_file\u0026#34; | grep -q PE ) then echo \u0026#39;File must be a Portable Executable (PE) file.\u0026#39; exit 0 fi # Check that osslsigncode is installed if ! command -v osslsigncode \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 ; then echo \u0026#34;osslsigncode utility is not present or missing from PATH. Binary cannot be signed.\u0026#34; exit 1 fi orig_file=\u0026#34;${input_file}_unsigned\u0026#34; mv \u0026#34;$input_file\u0026#34; \u0026#34;$orig_file\u0026#34; osslsigncode sign -certs \u0026#34;./certs/sign.crt\u0026#34; -key \u0026#34;./certs/sign.key\u0026#34; -n \u0026#34;Hello Windows code signing\u0026#34; -i \u0026#34;https://victoronsoftware.com/\u0026#34; -t \u0026#34;http://timestamp.comodoca.com/authenticode\u0026#34; -in \u0026#34;$orig_file\u0026#34; -out \u0026#34;$input_file\u0026#34; rm \u0026#34;$orig_file\u0026#34; In addition to signing the code, we timestamp the signature using the Comodo server. Timestamping makes sure the signature remains valid even after the certificate expires or is invalidated.\nWe can use osslsigncode to verify the signature:\n#!/usr/bin/env bash input_file=$1 osslsigncode verify -CAfile ./certs/ca.crt \u0026#34;$input_file\u0026#34; Distributing and manually verifying the signed code After signing the code, we can distribute the signed executable to users. Users can manually verify the signature by right-clicking the executable, selecting \u0026ldquo;Properties,\u0026rdquo; and navigating to the \u0026ldquo;Digital Signatures\u0026rdquo; tab. The user can then view the certificate details and verify that the signature is valid.\nHowever, since we are using the self-signed certificate, users will see a warning that the certificate is not trusted. Our self-signed certificate is not trusted because the certificate authority is not part of the Windows trusted root certificate store.\nCertificate in code signature cannot be verified We can add the certificate authority to the Windows trusted root certificate store with the following Powershell command:\nImport-Certificate -FilePath \u0026#34;certs\\ca.crt\u0026#34; -CertStoreLocation Cert:\\LocalMachine\\Root After adding the certificate authority to the trusted root certificate store, users will see that the certificate is trusted and the signature is valid.\nCertificate in code signature is be verified Code signing using a certificate from a public CA To sign public applications, we must obtain a code signing certificate from a trusted CA. The latest industry standards require private keys for code signing certificates to be stored in hardware security modules (HSMs) to prevent unauthorized access. This security requirement means certificates for code signing in CI/CD pipelines must use a cloud HSM vendor or a private pipeline runner with an HSM.\nIn a future article, we will explore signing a Windows application using a cloud HSM vendor.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/code-sign-windows\nFurther reading Recently, we explained how to create an EXE installer.\nWe also discussed connecting your local machine to remote Active Directory and covered how to test a Windows NDES SCEP server.\nCode signing a Windows application video ","date":"2024-03-27T00:00:00Z","image":"https://victoronsoftware.com/posts/code-signing-windows/digital-signature-ok_hu_f6994734badb4536.png","permalink":"https://victoronsoftware.com/posts/code-signing-windows/","title":"Code signing a Windows application"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain mTLS Go client mTLS Go client with custom certificate signer mTLS Go client using macOS keychain mTLS with Windows certificate store Why use the Windows certificate store? Keeping the mTLS client private key on the filesystem is insecure and not recommended. In the mTLS Go client using macOS keychain, we demonstrated achieving greater mTLS security with macOS keychain. In this article, we reach a similar level of protection with the Windows certificate store.\nThe Windows certificate store is a secure location where certificates and keys can be stored. Many applications, such as Edge and Powershell, use it. The Windows certificate store is an excellent place to store mTLS client certificates and keys.\nBuilding a custom tls.Certificate for the Windows certificate store This work builds on the mTLS Go client with custom certificate signer article. We will use the CustomSigner from that article to build a custom tls.Certificate that uses the Windows certificate store.\nHowever, before the application uses the Public and Sign methods of the CustomSigner, we must retrieve the client certificate using Windows APIs.\nRetrieving mTLS client certificate from Windows certificate store using Go We will use the golang.org/x/sys/windows package to access the Windows APIs. We use the windows package to call the CertOpenStore, CertFindCertificateInStore, and CryptAcquireCertificatePrivateKey functions from the crypt32 DLL (dynamic link library).\nFirst, we open the MY store, which is the personal store for the current user. This store contains our client mTLS certificate.\n// Open the certificate store storePtr, err := windows.UTF16PtrFromString(windowsStoreName) if err != nil { return nil, err } store, err := windows.CertOpenStore( windows.CERT_STORE_PROV_SYSTEM, 0, uintptr(0), windows.CERT_SYSTEM_STORE_CURRENT_USER, uintptr(unsafe.Pointer(storePtr)), ) if err != nil { return nil, err } Next, we find the certificate by the common name.\n// Find the certificate var pPrevCertContext *windows.CertContext var certContext *windows.CertContext commonNamePtr, err := windows.UTF16PtrFromString(commonName) for { certContext, err = windows.CertFindCertificateInStore( store, windows.X509_ASN_ENCODING, 0, windows.CERT_FIND_SUBJECT_STR, unsafe.Pointer(commonNamePtr), pPrevCertContext, ) if err != nil { return nil, err } // We can extract the certificate chain and further filter the certificate // we want here. break } Converting the Windows certificate to a Go x509.Certificate After retrieving the certificate from the Windows certificate store, we convert it to a Go x509.Certificate.\n// Copy the certificate data so that we have our own copy outside the windows context encodedCert := unsafe.Slice(certContext.EncodedCert, certContext.Length) buf := bytes.Clone(encodedCert) foundCert, err := x509.ParseCertificate(buf) if err != nil { return nil, err } Building the custom tls.Certificate Finally, we put together the custom tls.Certificate using the x509.Certificate. We hold on to the certContext pointer to get the private key later.\ncustomSigner := \u0026amp;CustomSigner{ store: store, windowsCertContext: certContext, } customSigner.x509Cert = foundCert certificate := tls.Certificate{ Certificate: [][]byte{foundCert.Raw}, PrivateKey: customSigner, SupportedSignatureAlgorithms: []tls.SignatureScheme{supportedAlgorithm}, } Our example only supports the tls.PSSWithSHA256 signature algorithm to keep the code simple.\nSigning the mTLS digest with the Windows certificate store As discussed in the previous mTLS Go client with custom certificate signer article, we must sign the CertificateVerify message during the TLS handshake. We will use the CustomSigner to sign the digest, which implements the crypto.Signer interface as defined in the Go standard library\u0026rsquo;s crypto package.\n// CustomSigner is a crypto.Signer that uses the client certificate and key to sign type CustomSigner struct { store windows.Handle windowsCertContext *windows.CertContext x509Cert *x509.Certificate } func (k *CustomSigner) Public() crypto.PublicKey { fmt.Printf(\u0026#34;crypto.Signer.Public\\n\u0026#34;) return k.x509Cert.PublicKey } func (k *CustomSigner) Sign(_ io.Reader, digest []byte, opts crypto.SignerOpts ) (signature []byte, err error) { ... Retrieve the private key reference from the Windows certificate store We retrieve the private key reference from the Windows certificate store using the CryptAcquireCertificatePrivateKey function.\n// Get private key var ( privateKey windows.Handle pdwKeySpec uint32 pfCallerFreeProvOrNCryptKey bool ) err = windows.CryptAcquireCertificatePrivateKey( k.windowsCertContext, windows.CRYPT_ACQUIRE_CACHE_FLAG|windows.CRYPT_ACQUIRE_SILENT_FLAG| windows.CRYPT_ACQUIRE_ONLY_NCRYPT_KEY_FLAG, nil, \u0026amp;privateKey, \u0026amp;pdwKeySpec, \u0026amp;pfCallerFreeProvOrNCryptKey, ) if err != nil { return nil, err } Signing the mTLS digest We will use the NCryptSignHash function from ncrypt.dll to sign the digest.\nvar ( nCrypt = windows.MustLoadDLL(\u0026#34;ncrypt.dll\u0026#34;) nCryptSignHash = nCrypt.MustFindProc(\u0026#34;NCryptSignHash\u0026#34;) ) But before we do that, we must create a BCRYPT_PSS_PADDING_INFO structure for our supported RSA-PSS algorithm.\nflags := nCryptSilentFlag | bCryptPadPss pPaddingInfo, err := getRsaPssPadding(opts) if err != nil { return nil, err } Where getRsaPssPadding is a helper function:\nfunc getRsaPssPadding(opts crypto.SignerOpts) (unsafe.Pointer, error) { pssOpts, ok := opts.(*rsa.PSSOptions) if !ok || pssOpts.Hash != crypto.SHA256 { return nil, fmt.Errorf(\u0026#34;unsupported hash function %s\u0026#34;, opts.HashFunc().String()) } if pssOpts.SaltLength != rsa.PSSSaltLengthEqualsHash { return nil, fmt.Errorf(\u0026#34;unsupported salt length %d\u0026#34;, pssOpts.SaltLength) } sha256, _ := windows.UTF16PtrFromString(\u0026#34;SHA256\u0026#34;) // Create BCRYPT_PSS_PADDING_INFO structure: // typedef struct _BCRYPT_PSS_PADDING_INFO { // LPCWSTR pszAlgId; // ULONG cbSalt; // } BCRYPT_PSS_PADDING_INFO; return unsafe.Pointer( \u0026amp;struct { pszAlgId *uint16 cbSalt uint32 }{ pszAlgId: sha256, cbSalt: uint32(pssOpts.HashFunc().Size()), }, ), nil } Finally, we sign the digest using the NCryptSignHash function.\n// Sign the digest // The first call to NCryptSignHash retrieves the size of the signature var size uint32 success, _, _ := nCryptSignHash.Call( uintptr(privateKey), uintptr(pPaddingInfo), uintptr(unsafe.Pointer(\u0026amp;digest[0])), uintptr(len(digest)), uintptr(0), uintptr(0), uintptr(unsafe.Pointer(\u0026amp;size)), uintptr(flags), ) if success != 0 { return nil, fmt.Errorf(\u0026#34;NCryptSignHash: failed to get signature length: %#x\u0026#34;, success) } // The second call to NCryptSignHash retrieves the signature signature = make([]byte, size) success, _, _ = nCryptSignHash.Call( uintptr(privateKey), uintptr(pPaddingInfo), uintptr(unsafe.Pointer(\u0026amp;digest[0])), uintptr(len(digest)), uintptr(unsafe.Pointer(\u0026amp;signature[0])), uintptr(size), uintptr(unsafe.Pointer(\u0026amp;size)), uintptr(flags), ) if success != 0 { return nil, fmt.Errorf(\u0026#34;NCryptSignHash: failed to generate signature: %#x\u0026#34;, success) } return signature, nil Putting it all together With the above code, we can create our new Go mTLS client that uses the Windows certificate store.\nfunc main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } client := http.Client{ Transport: \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ GetClientCertificate: signer.GetClientCertificate, MinVersion: tls.VersionTLS13, MaxVersion: tls.VersionTLS13, }, }, } // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } We limit the scope of this example to TLS 1.3\nSetting up the environment The next step is to use the Windows certificate store to store the client certificate and private key. We will use the certificates and keys scripts from the previous mTLS with Windows certificate store article. If you still need to generate the certificates and keys, please follow the instructions in that article.\nFinally, as in the mTLS with Windows certificate store article, we start two nginx servers:\nhttps://\u0026lt;your_host\u0026gt;:8888 for TLS https://\u0026lt;your_host\u0026gt;:8889 for mTLS Running the Go mTLS client using the Windows certificate store We can run our mTLS client without pointing to certificate/key files and retrieving everything from the Windows certificate store. Hitting the ordinary TLS server:\ngo run .\\client-signer.go --url https://myhost:8888/hello-world.txt Returns the expected:\nTLS Hello World! While hitting the mTLS server:\ngo run .\\client-signer.go --url https://myhost:8889/hello-world.txt Returns a more detailed message, including the print statements in our custom code:\nServer requested certificate Found certificate with common name testClientTLS crypto.Signer.Public crypto.Signer.Public crypto.Signer.Sign with key type *rsa.PublicKey, opts type *rsa.PSSOptions, hash SHA-256 mTLS Hello World! Example code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-go-windows\nmTLS Go client using Windows certificate store video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-03-20T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-go-client-windows-certificate-store/mtls-go-windows_hu_fe4de4fef5202383.png","permalink":"https://victoronsoftware.com/posts/mtls-go-client-windows-certificate-store/","title":"Mutual TLS (mTLS) Go client using Windows certificate store"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain mTLS Go client mTLS Go client with custom certificate signer mTLS Go client using macOS keychain Why use Windows certificate store? In our previous articles, we introduced mTLS and demonstrated how to use mTLS client certificates and keys. Keeping the mTLS client private key on the filesystem is insecure and not recommended. In the mTLS Go client using macOS keychain, we demonstrated achieving greater mTLS security with macOS keychain. In this article, we start exploring how to achieve the same level of protection with Windows certificate store.\nThe Windows certificate store is a secure location to store certificates and keys. Many applications, such as Edge and Powershell use it. The Windows certificate store is an excellent place to store mTLS client certificates and keys.\nThe Windows certificate stores have two types:\nUser certificate store: Certificates and keys are stored for the current user, local to a user account. Local machine certificate store: Certificates and keys are stored for all users on the computer. We will store our client mTLS certificate in the user certificate store and the other certificates in the local machine certificate store.\nGenerating mTLS certificates and keys We will use the following Powershell script to generate the mTLS certificates and keys. OpenSSL must be installed on your computer.\nNew-Item -ItemType Directory -Force certs # Private keys for CAs openssl genrsa -out certs/server-ca.key 2048 openssl genrsa -out certs/client-ca.key 2048 # Generate CA certificates openssl req -new -x509 -nodes -days 1000 -key certs/server-ca.key -out certs/server-ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testServerCA\u0026#34; openssl req -new -x509 -nodes -days 1000 -key certs/client-ca.key -out certs/client-ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testClientCA\u0026#34; # Generate a certificate signing request openssl req -newkey rsa:2048 -nodes -keyout certs/server.key -out certs/server.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testServerTLS\u0026#34; openssl req -newkey rsa:2048 -nodes -keyout certs/client.key -out certs/client.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testClientTLS\u0026#34; # Have the CA sign the certificate requests and output the certificates. openssl x509 -req -in certs/server.req -days 398 -CA certs/server-ca.crt -CAkey certs/server-ca.key -set_serial 01 -out certs/server.crt -extfile localhost.ext openssl x509 -req -in certs/client.req -days 398 -CA certs/client-ca.crt -CAkey certs/client-ca.key -set_serial 01 -out certs/client.crt # Create PFX file for importing to certificate store openssl pkcs12 -export -out certs\\client.pfx -inkey certs\\client.key -in certs\\client.crt -passout pass: # Clean up Remove-Item certs/server.req Remove-Item certs/client.req The maximum validity period for a TLS certificate is 398 days.\nThe localhost.ext file is used to specify the subject alternative name (SAN) for the server certificate. The localhost.ext file contains the following:\n[alt_names] DNS.1 = localhost DNS.2 = myhost We can access the server using either localhost or myhost names.\nThe above script generates the following files:\ncerts/server-ca.crt: Server CA certificate certs/server-ca.key: Server CA private key certs/client-ca.crt: Client CA certificate certs/client-ca.key: Client CA private key certs/server.crt: Server certificate certs/server.key: Server private key certs/client.crt: Client certificate certs/client.key: Client private key certs/client.pfx: Client certificate and private key in PFX format, needed for importing into the Windows certificate store Importing the client certificate and key into the Windows certificate store We will import the client certificate and key into the user certificate store using the following powershell script.\n# Import the server CA Import-Certificate -FilePath \u0026#34;certs\\server-ca.crt\u0026#34; -CertStoreLocation Cert:\\LocalMachine\\Root # Import the client CA so that client TLS certificates can be verified Import-Certificate -FilePath \u0026#34;certs\\client-ca.crt\u0026#34; -CertStoreLocation Cert:\\LocalMachine\\Root # Import the client TLS certificate and key Import-PfxCertificate -FilePath \u0026#34;certs\\client.pfx\u0026#34; -CertStoreLocation Cert:\\CurrentUser\\My The command result should be similar to the following:\nPSParentPath: Microsoft.PowerShell.Security\\Certificate::LocalMachine\\Root Thumbprint Subject ---------- ------- 0A31BF3C48A3D98A91A2F63B5BD286818311A707 CN=testServerCA, OU=Your Unit, O=Your Organization, L=Austin, S=Texas, C=US 7F7E5612F3A90B9EB246762358251F98911A9D1A CN=testClientCA, OU=Your Unit, O=Your Organization, L=Austin, S=Texas, C=US PSParentPath: Microsoft.PowerShell.Security\\Certificate::CurrentUser\\My Thumbprint Subject ---------- ------- E2EBB991E3849E32E934D8465FAE42787D34C9ED CN=testClientTLS, OU=Your Unit, O=Your Organization, L=Austin, S=Texas, C=US By default, the private key is marked as non-exportable. A user or an application cannot export the private key from the certificate store. They can only access the private key via Windows APIs. Using a non-exportable private key is the recommended security approach. You can use the -Exportable parameter if you need to export the private key.\nVerifying imported certificates and keys As an extra step, we can verify that the certificates and keys exist in the Windows certificate store. We can use the certlm Local Machine Certificate Manager GUI, certmgr User Certificate Manager GUI, or the Get-ChildItem powershell command.\nGet-ChildItem -Path Cert:\\LocalMachine\\Root | Where-Object{$_.Subject -match \u0026#39;testServerCA\u0026#39;} | Test-Certificate -Policy SSL Get-ChildItem -Path Cert:\\CurrentUser\\My | Where-Object{$_.Subject -match \u0026#39;testClientTLS\u0026#39;} Running the mTLS server We will use the same docker-compose.yml file from the mTLS Hello World article. The docker-compose.yml file starts two nginx servers:\nhttps://\u0026lt;your_host\u0026gt;:8888 for TLS https://\u0026lt;your_host\u0026gt;:8889 for mTLS We can run Docker on WSL (Windows Subsystem for Linux) or another machine. We will run it on a different machine, so we need to copy the certs directory to the machine running Docker. When running the server on a different machine, we must update the C:\\Windows\\System32\\drivers\\etc\\hosts file to point to the other machine.\n10.0.0.5 myhost Connecting to the TLS and mTLS servers with clients Because we added the server CA to the root certificate store, we can now access the TLS server without any additional flags:\nInvoke-WebRequest -Uri https://myhost:8888/hello-world.txt Result:\nStatusCode : 200 StatusDescription : OK Content : TLS Hello World! RawContent : HTTP/1.1 200 OK Connection: keep-alive Accept-Ranges: bytes Content-Length: 17 Content-Type: text/plain Date: Sun, 03 Mar 2024 17:28:29 GMT ETag: \u0026#34;65b29c19-11\u0026#34; Last-Modified: Thu, 25 Jan 2024 1... Forms : {} Headers : {[Connection, keep-alive], [Accept-Ranges, bytes], [Content-Length, 17], [Content-Type, text/plain]...} Images : {} InputFields : {} Links : {} ParsedHtml : System.__ComObject RawContentLength : 17 However, we cannot access the mTLS server directly.\nInvoke-WebRequest -Uri https://myhost:8889/hello-world.txt The client attempted the TLS handshake, but the server rejected the connection because the client did not provide a certificate. Result:\nInvoke-WebRequest : 400 Bad Request No required SSL certificate was sent nginx/1.25.3 At line:1 char:1 + Invoke-WebRequest -Uri https://myhost:8889/hello-world.txt + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : InvalidOperation: (System.Net.HttpWebRequest:HttpWebRequest) [Invoke-WebRequest], WebException + FullyQualifiedErrorId : WebCmdletWebResponseException,Microsoft.PowerShell.Commands.InvokeWebRequestCommand We can, however, provide the client certificate thumbprint to access the mTLS server. We saw the thumbprint of the client certificate earlier when we imported it into the Windows certificate store.\nInvoke-WebRequest -Uri https://myhost:8889/hello-world.txt -CertificateThumbprint E2EBB991E3849E32E934D8465FAE42787D34C9ED Result:\nStatusCode : 200 StatusDescription : OK Content : mTLS Hello World! RawContent : HTTP/1.1 200 OK Connection: keep-alive Accept-Ranges: bytes Content-Length: 18 Content-Type: text/plain Date: Sun, 03 Mar 2024 17:31:55 GMT ETag: \u0026#34;65b29c19-12\u0026#34; Last-Modified: Thu, 25 Jan 2024 1... Forms : {} Headers : {[Connection, keep-alive], [Accept-Ranges, bytes], [Content-Length, 18], [Content-Type, text/plain]...} Images : {} InputFields : {} Links : {} ParsedHtml : System.__ComObject RawContentLength : 18 Edge browser can access the mTLS server. We can verify this by opening the following URL:\nhttps://myhost:8889/hello-world.txt We see the following popup:\nEdge mTLS popup\nWe can click OK to connect to the mTLS server. Future connections will not show the popup and will automatically use the client certificate.\nNote: Here is a helpful link that may resolve issues trying to use mTLS client certificates on Windows 10: https://superuser.com/questions/1181163/unable-to-use-client-certificates-in-chrome-or-ie-on-windows-10\nExample code on Github The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-with-windows\nCreating our own Windows mTLS client In the following article, we will create a custom Windows mTLS client using the Windows certificate store.\nFurther reading Recently, we wrote an article on testing a Windows NDES SCEP server.\nmTLS with Windows certificate store video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-03-06T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-with-windows/mtls-edge_hu_e5f37290d112f7f1.png","permalink":"https://victoronsoftware.com/posts/mtls-with-windows/","title":"Mutual TLS (mTLS) with Windows certificate store"},{"content":"Introduction Any app aiming to reach an international audience must support Unicode. Emojis, which are based on Unicode, are everywhere. They are used in text messages, social media, and programming languages. Supporting Unicode and emojis in your app can be tricky. This article will cover common Unicode and emoji support issues and how to fix them.\nWhat is Unicode? Unicode is a standard for encoding, representing, and handling text. It is a character set that assigns a unique number to every character. The most common encoding for Unicode is UTF-8, which stands for Unicode Transformation Format 8-bit. UTF-8 is a variable-width encoding that can represent every character in the Unicode character set.\nUTF-8 format can take one to four bytes to represent a code point. Multiple code points can be combined to form a single character. For example, the emoji \u0026ldquo;üëç\u0026rdquo; is represented by the code point U+1F44D. In UTF-8, it is represented by the bytes F0 9F 91 8D. The same emoji with skin tone \u0026ldquo;üëçüèΩ\u0026rdquo; is represented by the code point U+1F44D U+1F3FD. In UTF-8, that emoji is represented by the bytes F0 9F 91 8D F0 9F 8F BD. Generally, emojis take up at least four bytes in UTF-8.\nUnicode equivalence Our first gotcha is unicode equivalence.\nUnicode equivalence is the concept that two different sequences of code points can represent the same character. For example, the character √© can be represented by the code point U+00E9 or by the sequence of code points U+0065 U+0301. The first representation is the composed form, and the second is the decomposed form. Unicode equivalence is essential when comparing strings or searching for a string character.\nDatabases typically do not support Unicode equivalence out of the box. For example, given this table using MySQL 5.7:\nCREATE TABLE test ( id INT UNSIGNED NOT NULL AUTO_INCREMENT, name VARCHAR(255) NOT NULL, PRIMARY KEY (id)) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; INSERT INTO test (name) VALUES (\u0026#39;Í∞Ä\u0026#39;), (CONCAT(\u0026#39;·ÑÄ\u0026#39;, \u0026#39;·Ö°\u0026#39;)); SELECT * from test WHERE name = \u0026#39;Í∞Ä\u0026#39;; The query will return a single row, even though the Korean character Í∞Ä and character sequence ·ÑÄ + ·Ö° are equivalent. The incorrect result is because the utf8mb4_unicode_ci collation does not support Unicode equivalence. One way to fix this is to use the utf8mb4_0900_ai_ci collation, which supports Unicode equivalence. However, this requires updating the database to MySQL 8.0 or later, which may not be possible in some cases.\nEmoji equivalence Our second gotcha is emoji equivalence.\nSome databases may not support emoji equivalence out of the box. For example, given this table using MySQL 5.7:\nCREATE TABLE test ( id INT UNSIGNED NOT NULL AUTO_INCREMENT, name VARCHAR(255) NOT NULL, PRIMARY KEY (id)) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; INSERT INTO test (name) VALUES (\u0026#39;üî•\u0026#39;), (\u0026#39;üî•üî•\u0026#39;), (\u0026#39;üëç\u0026#39;), (\u0026#39;üëçüèΩ\u0026#39;); SELECT * from test WHERE name = \u0026#39;üî•\u0026#39;; The query will return:\n1,üî• 3,üëç And the following query:\nSELECT * from test WHERE name LIKE \u0026#39;%üî•%\u0026#39;; Will return:\n1,üî• 2,üî•üî• The utf8mb4_unicode_ci collation does not support emoji equivalence, and the behavior of = differs from LIKE.\nOne way to fix the problem of emoji equivalence is to use a different collation during the = comparison. For example:\nSELECT * from test WHERE name COLLATE utf8mb4_bin = \u0026#39;üî•\u0026#39;; Will return the single correct result:\n1,üî• However, this solution is not ideal because it requires the developer to remember to use the utf8mb4_bin collation for emoji equivalence. There is also a slight performance impact when using a different collation.\nCase-insensitive sorting Our third gotcha is sorting.\nTypically, app users want to see case-insensitive sorting of strings. For example, the strings \u0026ldquo;apple\u0026rdquo;, \u0026ldquo;Banana\u0026rdquo;, and \u0026ldquo;cherry\u0026rdquo; should be sorted as \u0026ldquo;apple\u0026rdquo;, \u0026ldquo;Banana\u0026rdquo;, and \u0026ldquo;cherry\u0026rdquo;. The utf8mb4_unicode_ci collation used above supports case-insensitive sorting. However, switching to another collation, such as utf8mb4_bin, to support emoji equivalence will break case-insensitive sorting. Hence, whatever solution you develop for full Unicode support should also support case-insensitive sorting.\nSolving our gotchas with normalization A partial solution to the above gotchas is to use normalization. Normalization is the process of transforming text into a standard form. Unicode defines four normalization forms: NFC, NFD, NFKC, and NFKD. The most common normalization form is NFC, which is the composed form. NFC is the standard form for most text processing.\nFor example, in the following Go code:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;golang.org/x/text/unicode/norm\u0026#34; \u0026#34;strconv\u0026#34; ) func main() { str1, _ := strconv.Unquote(`\u0026#34;\\uAC00\u0026#34;`) // Í∞Ä str2, _ := strconv.Unquote(`\u0026#34;\\u1100\\u1161\u0026#34;`) // ·ÑÄ + ·Ö° fmt.Println(str1) fmt.Println(str2) if str1 == str2 { fmt.Println(\u0026#34;raw equal\u0026#34;) } else { fmt.Println(\u0026#34;raw not equal\u0026#34;) } strNorm1 := norm.NFC.String(str1) strNorm2 := norm.NFC.String(str2) if strNorm1 == strNorm2 { fmt.Println(\u0026#34;normalized equal\u0026#34;) } else { fmt.Println(\u0026#34;normalized not equal\u0026#34;) } } The two strings are not equal in their raw form but equal after normalization. Normalizing before inserting, updating, and searching in the database can solve the Unicode equivalence issue while allowing the user to keep the case-insensitive sorting.\nTo solve emoji equivalence, we can use the utf8mb4_bin collation for the = comparison. However, if our column is indexed, we may need to use the utf8mb4_bin collation for the index. We cannot have a different collation for the column and the index, but we could use a second generated column with the utf8mb4_bin collation and index that column.\nConclusion Unicode and emoji support is essential for any app aiming to reach an international audience. Unicode equivalence, emoji equivalence, and case-insensitive sorting are common issues with Unicode and emoji support. Normalization can solve the Unicode equivalence issue while allowing the user to keep the case-insensitive sorting. Using the utf8mb4_bin collation for the = comparison can solve the emoji equivalence issue.\nFully supporting Unicode and emojis in your app video Other articles related to MySQL Optimize MySQL query performance: INSERT with subqueries MySQL deadlock on UPDATE/INSERT upsert pattern SQL prepared statements are broken when scaling applications Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-02-29T00:00:00Z","image":"https://victoronsoftware.com/posts/unicode-and-emoji-gotchas/unicode-emoji_hu_1a354683464348c.png","permalink":"https://victoronsoftware.com/posts/unicode-and-emoji-gotchas/","title":"Fully supporting Unicode and emojis in your app"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain mTLS Go client mTLS Go client with custom certificate signer Why use macOS keychain? In the mTLS Go client article, we built a simple Go client that uses mTLS. Our client used Go standard library methods and loaded the client certificate and private key from the filesystem. However, keeping the private key on the filesystem is insecure and not recommended. We aim to build an mTLS client fully integrated with the operating system\u0026rsquo;s keystore.\nThe macOS keychain is a secure storage system for passwords and other confidential information. It is used by many Apple applications, such as Safari, Mail, and iCloud, to store the user\u0026rsquo;s passwords and additional sensitive information.\nBuilding a custom tls.Certificate for macOS keychain This work builds on the mTLS Go client with custom certificate signer article. We will use the CustomSigner from that article to build a custom tls.Certificate that uses the macOS keychain.\nHowever, before the application uses the Public and Sign methods of the CustomSigner, we need to retrieve the certificate from the keychain using Apple\u0026rsquo;s API.\nRetrieving certificate from macOS keychain with CGO We will use CGO to call the macOS keychain API to retrieve the client certificate. To set up CGO, we include the following code above our imports:\n/* #cgo LDFLAGS: -framework CoreFoundation -framework Security #include \u0026lt;CoreFoundation/CoreFoundation.h\u0026gt; #include \u0026lt;Security/Security.h\u0026gt; */ import \u0026#34;C\u0026#34; To find the identities from the keychain, we use SecItemCopyMatching. An identity is a certificate and its associated private key.\nidentitySearch := C.CFDictionaryCreateMutable( C.kCFAllocatorDefault, maxCertificatesNum, \u0026amp;C.kCFTypeDictionaryKeyCallBacks, \u0026amp;C.kCFTypeDictionaryValueCallBacks, ) defer C.CFRelease(C.CFTypeRef(unsafe.Pointer(identitySearch))) const commonName = \u0026#34;testClientTLS\u0026#34; var commonNameCFString = stringToCFString(commonName) defer C.CFRelease(C.CFTypeRef(commonNameCFString)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecClass), unsafe.Pointer(C.kSecClassIdentity)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecAttrCanSign), unsafe.Pointer(C.kCFBooleanTrue)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecMatchSubjectWholeString), unsafe.Pointer(commonNameCFString)) // To filter by issuers, we must provide a CFDataRef array of DER-encoded ASN.1 items. // C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecMatchIssuers), unsafe.Pointer(issuerCFArray)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecReturnRef), unsafe.Pointer(C.kCFBooleanTrue)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecMatchLimit), unsafe.Pointer(C.kSecMatchLimitAll)) var identityMatches C.CFTypeRef if status := C.SecItemCopyMatching(C.CFDictionaryRef(identitySearch), \u0026amp;identityMatches); status != C.errSecSuccess { return nil, fmt.Errorf(\u0026#34;failed to find client certificate: %v\u0026#34;, status) } defer C.CFRelease(identityMatches) In our example, we find the identities by a common name, which we hardcode for demonstration purposes. We can filter by the certificate issuer, as shown in the commented-out code. Filtering by issuer requires an array of DER-encoded ASN.1 items, which can be created from the tls.CertificateRequestInfo object. Another approach to finding the proper certificate is to retrieve all the keychain certificates and filter them in Go code.\nConverting the Apple identity to a Go x509.Certificate After we retrieve the array of identities from the keychain, we convert them to Go x509.Certificate objects and pick the first one that is not expired.\nvar foundCert *x509.Certificate var foundIdentity C.SecIdentityRef identityMatchesArrayRef := C.CFArrayRef(identityMatches) numIdentities := int(C.CFArrayGetCount(identityMatchesArrayRef)) fmt.Printf(\u0026#34;Found %d identities\\n\u0026#34;, numIdentities) for i := 0; i \u0026lt; numIdentities; i++ { identityMatch := C.CFArrayGetValueAtIndex(identityMatchesArrayRef, C.CFIndex(i)) x509Cert, err := identityRefToCert(C.SecIdentityRef(identityMatch)) if err != nil { continue } // Make sure certificate is not expired if x509Cert.NotAfter.After(time.Now()) { foundCert = x509Cert foundIdentity = C.SecIdentityRef(identityMatch) fmt.Printf(\u0026#34;Found certificate from issuer %s with public key type %T\\n\u0026#34;, x509Cert.Issuer.String(), x509Cert.PublicKey) break } } The identityRefToCert function converts the SecIdentityRef to a Go x509.Certificate object. It exports the certificate to PEM format using SecItemExport and then parses the PEM to get the x509.Certificate object.\nfunc identityRefToCert(identityRef C.SecIdentityRef) (*x509.Certificate, error) { // Convert the identity to a certificate var certificateRef C.SecCertificateRef if status := C.SecIdentityCopyCertificate(identityRef, \u0026amp;certificateRef); status != 0 { return nil, fmt.Errorf(\u0026#34;failed to get certificate from identity: %v\u0026#34;, status) } defer C.CFRelease(C.CFTypeRef(certificateRef)) // Export the certificate to PEM // SecItemExport: https://developer.apple.com/documentation/security/1394828-secitemexport var pemDataRef C.CFDataRef if status := C.SecItemExport( C.CFTypeRef(certificateRef), C.kSecFormatPEMSequence, C.kSecItemPemArmour, nil, \u0026amp;pemDataRef, ); status != 0 { return nil, fmt.Errorf(\u0026#34;failed to export certificate to PEM: %v\u0026#34;, status) } defer C.CFRelease(C.CFTypeRef(pemDataRef)) certPEM := C.GoBytes(unsafe.Pointer(C.CFDataGetBytePtr(pemDataRef)), C.int(C.CFDataGetLength(pemDataRef))) var x509Cert *x509.Certificate for block, rest := pem.Decode(certPEM); block != nil; block, rest = pem.Decode(rest) { if block.Type == \u0026#34;CERTIFICATE\u0026#34; { var err error x509Cert, err = x509.ParseCertificate(block.Bytes) if err != nil { return nil, fmt.Errorf(\u0026#34;error parsing client certificate: %v\u0026#34;, err) } } } return x509Cert, nil } Retrieve the private key reference from the keychain At this point, we also retrieve the private key reference from the keychain. We will use the private key reference to sign the CertificateVerify message during the TLS handshake. The reference does not contain the private key. When importing private keys to the keychain, they should be marked as non-exportable so that no one can retrieve the private key cleartext from the keychain.\nvar privateKey C.SecKeyRef if status := C.SecIdentityCopyPrivateKey(C.SecIdentityRef(foundIdentity), \u0026amp;privateKey); status != 0 { return nil, fmt.Errorf(\u0026#34;failed to copy private key ref from identity: %v\u0026#34;, status) } Building the custom tls.Certificate Finally, we put together the custom tls.Certificate using the x509.Certificate and the private key reference.\ncustomSigner := \u0026amp;CustomSigner{ x509Cert: foundCert, privateKey: privateKey, } certificate := tls.Certificate{ Certificate: [][]byte{foundCert.Raw}, PrivateKey: customSigner, SupportedSignatureAlgorithms: []tls.SignatureScheme{supportedAlgorithm}, } Our example only supports the tls.PSSWithSHA256 signature algorithm to keep the code simple. Adding additional algorithm support is easy since it only requires passing the right parameter to the SecKeyCreateSignature function, which we will review next.\nSigning the mTLS digest with Apple\u0026rsquo;s keychain As discussed in the previous mTLS Go client with custom certificate signer article, we need to sign the CertificateVerify message during the TLS handshake. We will use the CustomSigner to sign the digest, which implements the crypto.Signer interface as defined in the Go standard library\u0026rsquo;s crypto package.\ntype CustomSigner struct { x509Cert *x509.Certificate privateKey C.SecKeyRef } func (k *CustomSigner) Public() crypto.PublicKey { fmt.Printf(\u0026#34;crypto.Signer.Public\\n\u0026#34;) return k.x509Cert.PublicKey } func (k *CustomSigner) Sign(_ io.Reader, digest []byte, opts crypto.SignerOpts) ( signature []byte, err error) { fmt.Printf(\u0026#34;crypto.Signer.Sign with key type %T, opts type %T, hash %s\\n\u0026#34;, k.Public(), opts, opts.HashFunc().String()) // Convert the digest to a CFDataRef digestCFData := C.CFDataCreate(C.kCFAllocatorDefault, (*C.UInt8)(unsafe.Pointer(\u0026amp;digest[0])), C.CFIndex(len(digest))) defer C.CFRelease(C.CFTypeRef(digestCFData)) // SecKeyAlgorithm: https://developer.apple.com/documentation/security/seckeyalgorithm // SecKeyCreateSignature: https://developer.apple.com/documentation/security/1643916-seckeycreatesignature var cfErrorRef C.CFErrorRef signCFData := C.SecKeyCreateSignature( k.privateKey, C.kSecKeyAlgorithmRSASignatureDigestPSSSHA256, C.CFDataRef(digestCFData), \u0026amp;cfErrorRef, ) if cfErrorRef != 0 { return nil, fmt.Errorf(\u0026#34;failed to sign data: %v\u0026#34;, cfErrorRef) } defer C.CFRelease(C.CFTypeRef(signCFData)) // Convert CFDataRef to Go byte slice return C.GoBytes(unsafe.Pointer(C.CFDataGetBytePtr(signCFData)), C.int(C.CFDataGetLength(signCFData))), nil } We use the SecKeyCreateSignature function to sign the digest. The function takes the private key reference, the algorithm, the digest, and a pointer to a CFErrorRef. The function returns a CFDataRef, which we convert to a Go byte slice. Additional algorithms can be supported by passing the proper parameter to the SecKeyCreateSignature function.\nPutting it all together With the above code, we can create our new Go mTLS client that uses the macOS keychain.\nfunc main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } client := http.Client{ Transport: \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ GetClientCertificate: signer.GetClientCertificate, MinVersion: tls.VersionTLS13, MaxVersion: tls.VersionTLS13, }, }, } // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } We limit the scope of this example to TLS 1.3\nBuild the mTLS client With go build client-signer.go, we generate the client-signer executable.\nSetting up the environment The next step is to use the macOS keychain to store the client certificate and private key. We will use the same certificates and keys script from the mTLS with macOS keychain article. If you still need to generate the certificates and keys, please follow the instructions in that article.\nWe must also import the generated certificates and keys into the macOS keychain.\n# Import the server CA security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain certs/server-ca.crt # Import the client CA so that client TLS certificates can be verified security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain certs/client-ca.crt # Import the client TLS certificate and key security import certs/client.crt -k /Library/Keychains/System.keychain security import certs/client.key -k /Library/Keychains/System.keychain -x -T $PWD/client-signer -T /usr/bin/curl -T /Applications/Safari.app -T \u0026#39;/Applications/Google Chrome.app\u0026#39; We specify our application $PWD/client-signer as one of the trusted applications that can access the private key. If we do not select the trusted application, we will get a security pop-up whenever our app tries to access the private key.\nFinally, as in the mTLS Hello World article, we will use docker compose up to start two nginx servers:\nhttps://localhost:8888 for TLS https://localhost:8889 for mTLS Running the Go mTLS client using the macOS keychain We can now run our mTLS client without pointing to certificate and key files. Hitting the ordinary TLS server:\n./client-signer --url https://localhost:8888/hello-world.txt Returns the expected:\nTLS Hello World! While hitting the mTLS server:\n./client-signer --url https://localhost:8889/hello-world.txt Returns a more detailed message, including the print statements in our custom code:\nServer requested certificate Found 1 identities Found certificate from issuer CN=testClientCA,OU=Your Unit,O=Your Organization,L=Austin,ST=Texas,C=US with public key type *rsa.PublicKey crypto.Signer.Public crypto.Signer.Public crypto.Signer.Sign with key type *rsa.PublicKey, opts type *rsa.PSSOptions, hash SHA-256 mTLS Hello World! Using certificate and key from the Windows certificate store The following article will explore using the Windows certificate store to hold the mTLS client certificate and private key.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-go-apple-keychain\nmTLS Go client using macOS keychain video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-02-22T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-go-client-using-apple-keychain/mtls-go-apple-keychain_hu_f384428683025.png","permalink":"https://victoronsoftware.com/posts/mtls-go-client-using-apple-keychain/","title":"Mutual TLS (mTLS) Go client using macOS keychain"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain mTLS Go client Why a custom certificate signer? In the mTLS Go client article, we built a simple Go client that uses mTLS. Our client used Go standard library methods and loaded the client certificate and private key from the filesystem. However, keeping the private key on the filesystem is insecure and not recommended. We aim to build an mTLS client fully integrated with the operating system\u0026rsquo;s keystore.\nThe first step toward that goal is to extract the functionality of the mTLS handshake that requires the private key. Luckily, the client\u0026rsquo;s private key is only needed to sign the CertificateVerify message. The CertificateVerify message is the last in the mTLS handshake. It proves to the server that the client has the private key associated with the client certificate.\nFrom Wikipedia entry on TLS:\nThe client sends a CertificateVerify message, which is a signature over the previous handshake messages using the client\u0026rsquo;s certificate\u0026rsquo;s private key. This signature can be verified by using the client\u0026rsquo;s certificate\u0026rsquo;s public key. This lets the server know that the client has access to the private key of the certificate and thus owns the certificate.\nSetting up the environment We will use the same certificates and keys script from the mTLS with macOS keychain article. If you still need to generate the certificates and keys, please follow the instructions in that article.\nIn addition, we will import the generated certificates and keys into the macOS keychain. (In a future article, we will use the Windows Certificate Store instead.)\nFinally, as in the mTLS Hello World article, we will use docker compose up to start two nginx servers:\nhttps://localhost:8888 for TLS https://localhost:8889 for mTLS Building our crypto.Signer We will build a custom crypto.Signer that signs the CertificateVerify message. The crypto.Signer interface is defined in the Go standard library\u0026rsquo;s crypto package. It is used to sign messages with a private key.\n// CustomSigner is a crypto.Signer that uses the client certificate and key to sign type CustomSigner struct { x509Cert *x509.Certificate clientCertPath string clientKeyPath string } func (k *CustomSigner) Public() crypto.PublicKey { fmt.Printf(\u0026#34;crypto.Signer.Public\\n\u0026#34;) return k.x509Cert.PublicKey } func (k *CustomSigner) Sign(rand io.Reader, digest []byte, opts crypto.SignerOpts) ( signature []byte, err error) { fmt.Printf(\u0026#34;crypto.Signer.Sign\\n\u0026#34;) tlsCert, err := tls.LoadX509KeyPair(k.clientCertPath, k.clientKeyPath) if err != nil { log.Fatalf(\u0026#34;error loading client certificate: %v\u0026#34;, err) } fmt.Printf(\u0026#34;Sign using %T\\n\u0026#34;, tlsCert.PrivateKey) return tlsCert.PrivateKey.(crypto.Signer).Sign(rand, digest, opts) } Although we still use the filesystem to load the client certificate and private key, we now use the crypto.Signer interface to sign the CertificateVerify message. In the future, we will replace this code by calls to the operating system\u0026rsquo;s keystore. The vital thing to note is that we only load the private key when we need to sign the digest and do not load the key during the client configuration.\nGetting the client certificate Besides building a custom crypto.Signer, we will implement a custom GetClientCertificate function. This function will be called during the TLS handshake when the server requests a certificate from the client. The function will load the client certificate and create a CustomSigner instance. It will not load the private key at this time. Once again, the client certificate is only loaded when needed and not during the client\u0026rsquo;s configuration.\nWe set Certificate: [][]byte{cert.Raw}, because the Go implementation of the TLS handshake requires the client certificate here to validate it against the server\u0026rsquo;s CA.\nfunc GetClientCertificate(clientCertPath string, clientKeyPath string) (*tls.Certificate, error) { fmt.Printf(\u0026#34;Server requested certificate\\n\u0026#34;) if clientCertPath == \u0026#34;\u0026#34; || clientKeyPath == \u0026#34;\u0026#34; { return nil, errors.New(\u0026#34;client certificate and key are required\u0026#34;) } clientBytes, err := os.ReadFile(clientCertPath) if err != nil { return nil, fmt.Errorf(\u0026#34;error reading client certificate: %w\u0026#34;, err) } var cert *x509.Certificate for block, rest := pem.Decode(clientBytes); block != nil; block, rest = pem.Decode(rest) { if block.Type == \u0026#34;CERTIFICATE\u0026#34; { cert, err = x509.ParseCertificate(block.Bytes) if err != nil { return nil, fmt.Errorf(\u0026#34;error parsing client certificate: %v\u0026#34;, err) } } } certificate := tls.Certificate{ Certificate: [][]byte{cert.Raw}, PrivateKey: \u0026amp;CustomSigner{ x509Cert: cert, clientCertPath: clientCertPath, clientKeyPath: clientKeyPath, }, } return \u0026amp;certificate, nil } Putting it all together With the above customizations, we create our new Go mTLS client:\npackage main import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/getvictor/mtls/signer\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) clientCert := flag.String(\u0026#34;cert\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Client certificate file\u0026#34;) clientKey := flag.String(\u0026#34;key\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Client key file\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } client := http.Client{ Transport: \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ GetClientCertificate: func(info *tls.CertificateRequestInfo) ( *tls.Certificate, error) { return signer.GetClientCertificate(*clientCert, *clientKey) }, }, }, } // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } Trying to hit the mTLS server with:\ngo run client-signer.go --url https://localhost:8889/hello-world.txt --cert certs/client.crt --key certs/client.key Returns the expected result:\nmTLS Hello World! Using certificate and key from the macOS keychain In the following article, we will use the macOS keychain to load the client certificate and generate the CertificateVerify message without extracting the private key.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-go-custom-signer\nmTLS Go client with custom certificate signer video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-02-14T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-go-custom-signer/signer_hu_c0014d6dcb3e7237.png","permalink":"https://victoronsoftware.com/posts/mtls-go-custom-signer/","title":"Mutual TLS (mTLS) Go client with custom certificate signer"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain What is Go? Go is a statically typed, compiled programming language designed at Google. It is known for its simplicity, efficiency, and ease of use. Go is often used for building web servers, APIs, and command-line tools. We will use Go to make a client that uses mTLS.\nSetting up the environment We will use the same certificates and keys script from the mTLS with macOS keychain article. If you still need to generate the certificates and keys, please follow the instructions in that article.\nIn addition, we will import the generated certificates and keys into the macOS keychain. (In a future article, we will use the Windows Certificate Store instead.) Keeping private keys on the filesystem is insecure and not recommended. We aim to build an mTLS client fully integrated with the operating system\u0026rsquo;s keystore.\nFinally, as in the mTLS Hello World article, we will use docker compose up to start two nginx servers:\nhttps://localhost:8888 for TLS https://localhost:8889 for mTLS Building the TLS Go client Below is a simple Go HTTP client.\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } client := http.Client{} // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } Trying the ordinary TLS server with:\ngo run client.go --url https://localhost:8888/hello-world.txt Gives the expected result:\nTLS Hello World! The Go client is integrated with the system keystore out of the box.\nHowever, when trying the mTLS server with the following:\ngo run client.go --url https://localhost:8889/hello-world.txt We get the error:\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;400 No required SSL certificate was sent\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;400 Bad Request\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;center\u0026gt;No required SSL certificate was sent\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx/1.25.3\u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; The Go libraries are not integrated with the system keystore for using the mTLS client certificate and key.\nModifying the Go client for mTLS We will use the crypto/tls package to build the mTLS client.\npackage main import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) clientCert := flag.String(\u0026#34;cert\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Client certificate file\u0026#34;) clientKey := flag.String(\u0026#34;key\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Client key file\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } var certificate tls.Certificate if *clientCert != \u0026#34;\u0026#34; \u0026amp;\u0026amp; *clientKey != \u0026#34;\u0026#34; { var err error certificate, err = tls.LoadX509KeyPair(*clientCert, *clientKey) if err != nil { log.Fatalf(\u0026#34;error loading client certificate: %v\u0026#34;, err) } } client := http.Client{ Transport: \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ Certificates: []tls.Certificate{certificate}, }, }, } // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } Now, trying the mTLS server with:\ngo run client-mtls.go --url https://localhost:8889/hello-world.txt --cert certs/client.crt --key certs/client.key Returns the expected result:\nmTLS Hello World! However, we pass the client certificate and key as command-line arguments. In a real-world scenario, we want to use the system keystore to manage the client certificate and key.\nUsing a custom signer for the mTLS client certificate The following article will cover creating a custom Go signer for the mTLS client certificate. This work will pave the way for us to use the system keystore to manage the client certificate and key.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-with-go\nmTLS Go client video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-02-07T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-go-client/go-client_hu_5110c87ffac65136.png","permalink":"https://victoronsoftware.com/posts/mtls-go-client/","title":"Mutual TLS (mTLS) Go client"},{"content":"This article is part of a series on mTLS. Check out the previous article: mTLS Hello World.\nSecuring mTLS certificates and keys In the mTLS Hello World article, we generated mTLS certificates and keys for the client and the server. We also created two certificate authorities (CAs) and signed the client and server certificates with their respective CAs. We ended up with the following files:\nserver CA: certs/server-ca.crt server CA private key: certs/server-ca.key TLS certificate for localhost server: certs/server.crt server TLS certificate private key: certs/server.key client CA: certs/client-ca.crt client CA private key: certs/client-ca.key TLS certificate for client: certs/client.crt client TLS certificate private key: certs/client.key In a real-world scenario, we would need to secure these files. The server CA private key and the client CA private key are the most important files to secure. If an attacker gets access to these files, they can create new certificates and impersonate the server or the client. These two files should be secured in a dedicated secure storage.\nThe server will need access to the client CA, the server TLS certificate, and the server TLS certificate private key. The server TLS certificate private key is the most important to secure out of these three files.\nThe client will need access to the server CA, the client TLS certificate, and the client TLS certificate private key. We can use the macOS keychain to secure these files. In a future article, we will show how to secure these on Windows with certificate stores.\nApple\u0026rsquo;s macOS keychain As I\u0026rsquo;ve written in inspecting keychain files on macOS, keychains are the macOS\u0026rsquo;s method to track and protect secure information such as passwords, private keys, and certificates.\nThe system keychain is located at /Library/Keychains/System.keychain. It contains the root certificates and other certificates. The login keychain is located at /Users/\u0026lt;username\u0026gt;/Library/Keychains/login.keychain-db. It contains the user\u0026rsquo;s certificates and private keys. In this example, we will use the system keychain, which all users on the system can access.\nGenerating mTLS certificates and keys We will use the following script to generate the mTLS certificates and keys. It resembles the script from the mTLS Hello World article.\n#!/bin/bash # This script generates certificates and keys needed for mTLS. mkdir -p certs # Private keys for CAs openssl genrsa -out certs/server-ca.key 2048 openssl genrsa -out certs/client-ca.key 2048 # Generate CA certificates openssl req -new -x509 -nodes -days 1000 -key certs/server-ca.key -out certs/server-ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testServerCA\u0026#34; openssl req -new -x509 -nodes -days 1000 -key certs/client-ca.key -out certs/client-ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testClientCA\u0026#34; # Generate a certificate signing request openssl req -newkey rsa:2048 -nodes -keyout certs/server.key -out certs/server.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testServerTLS\u0026#34; openssl req -newkey rsa:2048 -nodes -keyout certs/client.key -out certs/client.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testClientTLS\u0026#34; # Have the CA sign the certificate requests and output the certificates. openssl x509 -req -in certs/server.req -days 398 -CA certs/server-ca.crt -CAkey certs/server-ca.key -set_serial 01 -out certs/server.crt -extfile localhost.ext openssl x509 -req -in certs/client.req -days 398 -CA certs/client-ca.crt -CAkey certs/client-ca.key -set_serial 01 -out certs/client.crt # Clean up rm certs/server.req rm certs/client.req The maximum validity period for a TLS certificate is 398 days. Apple will reject certificates with a more extended validity period.\nImporting client mTLS certificates and keys into the macOS keychain We will import the client mTLS certificates and keys into the macOS keychain using the following script. The script uses the security command line tool. Accessing the system keychain must be run as root (sudo).\n#!/bin/bash # This script imports mTLS certificates and keys into the Apple Keychain. # Import the server CA security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain certs/server-ca.crt # Import the client CA so that client TLS certificates can be verified security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain certs/client-ca.crt # Import the client TLS certificate and key security import certs/client.crt -k /Library/Keychains/System.keychain security import certs/client.key -k /Library/Keychains/System.keychain -x -T /usr/bin/curl -T /Applications/Safari.app -T \u0026#39;/Applications/Google Chrome.app\u0026#39; The -x option marks the imported key as non-extractable. No application or user can view the private key once it is imported. The private key can only be used indirectly via Apple\u0026rsquo;s APIs.\nThe -T option specifies the applications that can access the key. Additional applications may be added later to the access control list.\nVerifying imported certificates and keys As an extra step, we can verify the client and server certificates before using them in an application.\nWe can verify the server certificate by running the following command:\nsecurity verify-cert -c certs/server.crt -p ssl -s localhost -k /Library/Keychains/System.keychain The output should include:\n...certificate verification successful. The Apple keychain automatically combines the certificate and the private key into an identity. We can verify the client identity by running the following command:\nsecurity find-identity -p ssl-client /Library/Keychains/System.keychain The list of identities should include:\nPolicy: SSL (client) Matching identities 1) B307B90CCD374080E74F1B15AF602B35A75D8401 \u0026#34;testClientTLS\u0026#34; 1 identities found Valid identities only 1) B307B90CCD374080E74F1B15AF602B35A75D8401 \u0026#34;testClientTLS\u0026#34; 1 valid identities found macOS can validate the identity because we also imported the client CA into the system keychain.\nRunning the mTLS server As in the mTLS Hello World article, we will use docker compose up to start two nginx servers:\nhttps://localhost:8888 for TLS https://localhost:8889 for mTLS Connecting to the TLS and mTLS servers with clients Because the server CA was added to the system keychain, curl can now access the TLS server without any additional flags:\ncurl https://localhost:8888/hello-world.txt However, the built-in curl client cannot access the mTLS server. We use the -v option for additional information:\ncurl -v https://localhost:8889/hello-world.txt The output:\n* Trying [::1]:8889... * Connected to localhost (::1) port 8889 * ALPN: curl offers h2,http/1.1 * (304) (OUT), TLS handshake, Client hello (1): * CAfile: /etc/ssl/cert.pem * CApath: none * (304) (IN), TLS handshake, Server hello (2): * (304) (IN), TLS handshake, Unknown (8): * (304) (IN), TLS handshake, Request CERT (13): * (304) (IN), TLS handshake, Certificate (11): * (304) (IN), TLS handshake, CERT verify (15): * (304) (IN), TLS handshake, Finished (20): * (304) (OUT), TLS handshake, Certificate (11): * (304) (OUT), TLS handshake, Finished (20): * SSL connection using TLSv1.3 / AEAD-CHACHA20-POLY1305-SHA256 * ALPN: server accepted http/1.1 * Server certificate: * subject: C=US; ST=Texas; L=Austin; O=Your Organization; OU=Your Unit; CN=testServerTLS * start date: Jan 28 17:08:10 2024 GMT * expire date: Mar 1 17:08:10 2025 GMT * subjectAltName: host \u0026#34;localhost\u0026#34; matched cert\u0026#39;s \u0026#34;localhost\u0026#34; * issuer: C=US; ST=Texas; L=Austin; O=Your Organization; OU=Your Unit; CN=testServerCA * SSL certificate verify ok. * using HTTP/1.1 \u0026gt; GET /hello-world.txt HTTP/1.1 \u0026gt; Host: localhost:8889 \u0026gt; User-Agent: curl/8.4.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 400 Bad Request \u0026lt; Server: nginx/1.25.3 \u0026lt; Date: Sun, 28 Jan 2024 18:28:20 GMT \u0026lt; Content-Type: text/html \u0026lt; Content-Length: 237 \u0026lt; Connection: close \u0026lt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;400 No required SSL certificate was sent\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;400 Bad Request\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;center\u0026gt;No required SSL certificate was sent\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx/1.25.3\u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; * Closing connection The client attempted the TLS handshake, but the server rejected the connection because the client did not provide a certificate. Our built-in curl client does not currently support mTLS using the macOS keychain. The client used for this example is:\ncurl 8.4.0 (x86_64-apple-darwin23.0) libcurl/8.4.0 (SecureTransport) LibreSSL/3.3.6 zlib/1.2.12 nghttp2/1.55.1 Release-Date: 2023-10-11 Protocols: dict file ftp ftps gopher gophers http https imap imaps ldap ldaps mqtt pop3 pop3s rtsp smb smbs smtp smtps telnet tftp Features: alt-svc AsynchDNS GSS-API HSTS HTTP2 HTTPS-proxy IPv6 Kerberos Largefile libz MultiSSL NTLM NTLM_WB SPNEGO SSL threadsafe UnixSockets On the other hand, Safari can access the mTLS server. We can verify this by opening the following URL in Safari:\nhttps://localhost:8889/hello-world.txt We see the following popup:\nSafari mTLS popup\nWe can click Continue to connect to the mTLS server. Future connections will not show the popup and will automatically use the client certificate.\nGoogle Chrome\u0026rsquo;s behavior is similar.\nNote: If we did not add Safari as an application that can access the client key, Safari would ask for a username and password to connect to the system keychain.\nCreating our own mTLS client In the following article, we will create our own mTLS client with the Go programming language. This is the first step toward creating an mTLS client integrated with the macOS keychain.\nLater, we will use mTLS with the Windows certificate store and create an mTLS client integrated with the Windows certificate store.\nFurther reading Recently, we explained agents and daemons and plists on macOS. We also showed how to convert a script into a macOS install package. Example code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-with-apple-keychain\nmTLS with macOS keychain video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-01-31T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-with-apple-keychain/mtls-safari_hu_939e70d1d9c590e6.png","permalink":"https://victoronsoftware.com/posts/mtls-with-apple-keychain/","title":"Mutual TLS (mTLS) with macOS keychain"},{"content":"What is mTLS (mutual TLS)? TLS stands for Transport Layer Security. It is a cryptographic protocol that provides privacy and data integrity between two communicating applications. It is the successor to SSL (Secure Sockets Layer).\nIn ordinary (non-mutual) TLS, the client authenticates the server, but the server does not authenticate the client. Most websites use regular TLS. The client (web browser) knows it is talking to the correct server (website), but the server knows very little about the client. Instead, web applications use other client authentication methods, such as passwords, cookies, and session tokens.\nMutual TLS (mTLS) is a way to authenticate both the client and the server in a TLS connection. It is also known as client certificate authentication. In addition to the server authenticating itself to the client, the client also authenticates itself to the server.\nmTLS is helpful as an additional layer of security. It is used in many applications, including:\nVPNs Microservices Service mesh IoT (Internet of Things) Mobile apps How does Fleet Device Management use mTLS? Many of Fleet\u0026rsquo;s customers use mTLS as an additional layer of security to authenticate the Fleet server to the Fleet agent. The Fleet agent is a small program that runs on each host device, such as a corporate laptop. It collects information about the host and sends it to the Fleet server.\nHow does mTLS work? TLS is a complex protocol with multiple versions (1.2, 1.3, etc.). We will only go over the basics to understand how mTLS works.\nTLS uses a handshake protocol to establish a secure connection. The handshake protocol is a series of messages between the client and the server.\nThe client sends a \u0026ldquo;Client Hello\u0026rdquo; message to the server. The server responds with a \u0026ldquo;Server Hello\u0026rdquo; message and sends its certificate to the client. As an additional step for mTLS, the server requests a certificate from the client.\nThe client verifies the server\u0026rsquo;s certificate by checking the certificate\u0026rsquo;s signature and verifying that the certificate is valid and has not expired. The client also checks that the server\u0026rsquo;s hostname matches the hostname in the certificate.\nThe client uses the server\u0026rsquo;s public key to encrypt the messages sent to the server, including the session key and its certificate. The server decrypts these messages with its private key.\nThe client also sends a digital signature, encrypted with its private key, to the server. The server verifies the signature by decrypting it with the client\u0026rsquo;s public key.\nAt this point, both the client and the server have verified each other\u0026rsquo;s identity. They complete the TLS handshake and can exchange encrypted messages using a symmetric session key.\nGenerate certificates and keys We will use the OpenSSL command line tool to generate the certificates. OpenSSL is a popular open-source library for TLS and SSL protocols.\nThe following script generates the certificates and keys for the client and the server. It also creates two certificate authorities (CAs) and signs the client and server certificates with their respective CA. The same CA may sign the certificates, but we will use separate CAs for this example.\n#!/bin/bash # This script generates files needed for mTLS. mkdir -p certs # Private keys for CAs openssl genrsa -out certs/server-ca.key 2048 openssl genrsa -out certs/client-ca.key 2048 # Generate CA certificates openssl req -new -x509 -nodes -days 1000 -key certs/server-ca.key -out certs/server-ca.crt openssl req -new -x509 -nodes -days 1000 -key certs/client-ca.key -out certs/client-ca.crt # Generate a certificate signing request openssl req -newkey rsa:2048 -nodes -keyout certs/server.key -out certs/server.req openssl req -newkey rsa:2048 -nodes -keyout certs/client.key -out certs/client.req # Have the CA sign the certificate requests and output the certificates. openssl x509 -req -in certs/server.req -days 1000 -CA certs/server-ca.crt -CAkey certs/server-ca.key -set_serial 01 -out certs/server.crt -extfile localhost.ext openssl x509 -req -in certs/client.req -days 1000 -CA certs/client-ca.crt -CAkey certs/client-ca.key -set_serial 01 -out certs/client.crt # Clean up rm certs/server.req rm certs/client.req The localhost.ext file is used to specify the hostname for the server certificate. In our example, we will use localhost. The file contains the following:\nauthorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment subjectAltName = @alt_names [alt_names] DNS.1 = localhost Run the mTLS server We will use nginx as our mTLS server. nginx is a popular open-source web server.\nUsing docker compose, we can run two nginx servers. One server will use ordinary TLS, and one will use mutual TLS. We will use the following docker-compose.yml file:\n--- version: \u0026#34;2\u0026#34; services: nginx-tls: image: nginx volumes: - ./certs/server.crt:/etc/nginx/certificates/server.crt - ./certs/server.key:/etc/nginx/certificates/server.key - ./nginx-tls/nginx.conf:/etc/nginx/conf.d/default.conf - ./nginx-tls/hello-world.txt:/www/data/hello-world.txt ports: - \u0026#34;8888:8888\u0026#34; nginx-mtls: image: nginx volumes: - ./certs/server.crt:/etc/nginx/certificates/server.crt - ./certs/server.key:/etc/nginx/certificates/server.key - ./certs/client-ca.crt:/etc/nginx/certificates/client-ca.crt - ./nginx-mtls/nginx.conf:/etc/nginx/conf.d/default.conf - ./nginx-mtls/hello-world.txt:/www/data/hello-world.txt ports: - \u0026#34;8889:8889\u0026#34; The nginx-tls service uses the nginx-tls/nginx.conf file, which contains the following:\nserver { listen 8888 ssl; server_name tls-hello-world; # Server TLS certificate (client must have the CA cert to connect) ssl_certificate /etc/nginx/certificates/server.crt; ssl_certificate_key /etc/nginx/certificates/server.key; location / { root /www/data; } } The nginx-mtls service uses the nginx-mtls/nginx.conf file, which contains the following:\nserver { listen 8889 ssl; server_name mtls-hello-world; # Server TLS certificate (client must have the CA cert to connect) ssl_certificate /etc/nginx/certificates/server.crt; ssl_certificate_key /etc/nginx/certificates/server.key; # Enable mTLS ssl_client_certificate /etc/nginx/certificates/client-ca.crt; ssl_verify_client on; location / { root /www/data; } } The hello-world.txt files contain a simple text message.\nConnect to the mTLS server with curl client We can connect to the mTLS server with the curl command line tool. We will use the following command:\ncurl https://localhost:8889/hello-world.txt --cacert ./certs/server-ca.crt --cert ./certs/client.crt --key ./certs/client.key The --cacert option specifies the CA certificate that signed the server certificate. The --cert and --key options select the client certificate and key.\nTo connect to the ordinary TLS server, we do not need to specify the client certificate and key:\ncurl https://localhost:8888/hello-world.txt --cacert ./certs/server-ca.crt Curl can use --insecure to ignore the server certificate:\ncurl --insecure https://localhost:8888/hello-world.txt However, it is impossible to ignore the client certificate for mTLS. The server will reject the connection if the client does not provide a valid certificate.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/hello-world\nSecuring mTLS certificates and keys In the next article, we will secure the mTLS certificates and keys with the macOS keychain.\nIn a later article, we also secure the mTLS certificates and keys with the Windows certificate store.\nThis article is part of a series on mTLS.\nFurther reading How to Use TPM 2.0 to Secure Private Keys Learn how to create, manage, and use TPM-backed keys, including parent/child key hierarchies and secure signing. mTLS Hello World video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-01-24T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-hello-world/mtls-handshake_hu_87e12984511b4ef4.png","permalink":"https://victoronsoftware.com/posts/mtls-hello-world/","title":"Mutual TLS intro and hands-on example"},{"content":"Simple CGO examples CGO is a way to call C code from Go. It helps call existing C libraries or for performance reasons. CGO is enabled by default but can be disabled with the -cgo build flag.\nBelow is a simple example of calling a C function from Go.\npackage main /* double add(double a, double b) { return a + b; } */ import \u0026#34;C\u0026#34; import \u0026#34;fmt\u0026#34; func main() { fmt.Println(C.add(1, 2)) } The C code is embedded in the Go code as a comment above import \u0026quot;C\u0026quot;. The comment must start with /* and end with */. The C code must be valid. The Go compiler compiles the C code and links the resulting object file with the Go code.\nHere is an example of using an existing C library.\npackage main /* #include \u0026#34;math.h\u0026#34; double add(double a, double b) { return a + b; } */ import \u0026#34;C\u0026#34; import \u0026#34;fmt\u0026#34; func main() { fmt.Println(C.floor(C.add(1, 2.1))) } We call the floor function from the math.h library. The math.h library is included with the C compiler, so we don\u0026rsquo;t need to do anything special to use it.\nCGO Hello World fail Here is another example where we print \u0026ldquo;Hello World\u0026rdquo; from C.\npackage main /* #include \u0026#34;stdio.h\u0026#34; */ import \u0026#34;C\u0026#34; func main() { C.printf(C.CString(\u0026#34;Hello World\\n\u0026#34;)) } However, the above seemingly straightforward example will fail to compile with the following enigmatic error:\ncgo: ./exmaple.go:9:2: unexpected type: ... The problem is that printf is a variadic function that can take a variable number of arguments. CGO does not support variadic functions. Even using Go variadic syntax will not work:\nargs := []interface{}{} C.printf(C.CString(\u0026#34;Hello World\\n\u0026#34;), args...) The workaround for this is to use another non-variadic function, such as vprintf, or to wrap the variadic C function in a non-variadic C function.\npackage main /* #include \u0026#34;stdio.h\u0026#34; void wrapPrintf(const char *s) { printf(\u0026#34;%s\u0026#34;, s); } */ import \u0026#34;C\u0026#34; func main() { C.wrapPrintf(C.CString(\u0026#34;Hello, World\\n\u0026#34;)) } C++ Hello World fail Another issue with CGO is only C code can be called from Go. C++ code cannot be called from Go. The following code will fail to compile:\npackage main /* #include \u0026lt;iostream\u0026gt; void helloWorld() { std::cout \u0026lt;\u0026lt; \u0026#34;Hello, World\u0026#34; \u0026lt;\u0026lt; std::endl; } */ import \u0026#34;C\u0026#34; func main() { C.helloWorld() } However, C++ code can be called from C, so we can write a C wrapper for the C++ code.\nCGO real-world example The following is an example of real-world usage of CGO, which uses Apple\u0026rsquo;s APIs to add a secret to the keychain.\npackage keystore /* #cgo LDFLAGS: -framework CoreFoundation -framework Security #include \u0026lt;CoreFoundation/CoreFoundation.h\u0026gt; #include \u0026lt;Security/Security.h\u0026gt; */ import \u0026#34;C\u0026#34; import ( \u0026#34;fmt\u0026#34; \u0026#34;unsafe\u0026#34; ) const service = \u0026#34;com.fleetdm.fleetd.enroll.secret\u0026#34; var serviceStringRef = stringToCFString(service) // AddSecret will add a secret to the keychain. This application can retrieve this // secret without any user authorization. func AddSecret(secret string) error { query := C.CFDictionaryCreateMutable( C.kCFAllocatorDefault, 0, \u0026amp;C.kCFTypeDictionaryKeyCallBacks, \u0026amp;C.kCFTypeDictionaryValueCallBacks, ) defer C.CFRelease(C.CFTypeRef(query)) data := C.CFDataCreate(C.kCFAllocatorDefault, (*C.UInt8)(unsafe.Pointer(C.CString(secret))), C.CFIndex(len(secret))) defer C.CFRelease(C.CFTypeRef(data)) C.CFDictionaryAddValue(query, unsafe.Pointer(C.kSecClass), unsafe.Pointer(C.kSecClassGenericPassword)) C.CFDictionaryAddValue(query, unsafe.Pointer(C.kSecAttrService), unsafe.Pointer(serviceStringRef)) C.CFDictionaryAddValue(query, unsafe.Pointer(C.kSecValueData), unsafe.Pointer(data)) status := C.SecItemAdd(C.CFDictionaryRef(query), nil) if status != C.errSecSuccess { return fmt.Errorf(\u0026#34;failed to add %v to keychain: %v\u0026#34;, service, status) } return nil } // stringToCFString will return a CFStringRef func stringToCFString(s string) C.CFStringRef { bytes := []byte(s) ptr := (*C.UInt8)(\u0026amp;bytes[0]) return C.CFStringCreateWithBytes(C.kCFAllocatorDefault, ptr, C.CFIndex(len(bytes)), C.kCFStringEncodingUTF8, C.false) } The C linker flags are specified with the #cgo LDFLAGS directive.\nThe CGO code uses a lot of casting and data conversion. Let\u0026rsquo;s break down the following segment:\n(*C.UInt8)(unsafe.Pointer(C.CString(secret))) C.CString converts a Go string to a C string. It is one of the CGO special functions to convert between Go and C types. See cgo documentation for more information.\nunsafe.Pointer converts a C pointer to a generic Go pointer. And (*C.UInt8) casts the Go pointer back to a C pointer.\nUnfortunately, CGO cannot cast a C string to a (*C.UInt8) directly. The following will fail to compile:\n(*C.UInt8)(C.CString(secret)) We must go through an intermediate cast to unsafe.Pointer, representing a void C pointer.\nAdditional topics Our custom C and Go code was always in the same file in the above examples. However, the C code can be in a separate file and linked to our Go executable.\nOther getting started guides Recently, we explained how to build a Chrome extension without any additional tools. Also, we wrote a guide to creating a React Hello World app. CGO Hello World fail video ","date":"2024-01-18T00:00:00Z","image":"https://victoronsoftware.com/posts/using-c-and-go-with-cgo-is-tricky/cgo-hello-world-fail_hu_3ce4f49279202857.png","permalink":"https://victoronsoftware.com/posts/using-c-and-go-with-cgo-is-tricky/","title":"Using C and Go with CGO is tricky"},{"content":"What are GitHub Actions? GitHub Actions are a way to automate your software development workflows. They are similar to CI/CD tools like Jenkins, CircleCI, and TravisCI. However, GitHub Actions are built into GitHub.\nGitHub Actions are not entirely free, but they have very high usage limits for open-source projects. For private repositories, you can run up to 2,000 minutes per month for free. After that, you will be charged.\nGitHub Actions for non-CI/CD tasks However, GitHub Actions are not just for CI/CD. You can use them for many general-purpose tasks. For example, you can use them as an extension of your application to perform tasks such as:\ngenerating aggregate reports updating a database sending notifications general data processing and many others A GitHub Action can run arbitrary code, taking inputs from multiple sources such as API calls, databases, and files.\nYou can use a GitHub Action as a worker for your application. For example, you can use it to process data from a database and then send a notification to a user. Or you can use it to generate a report and upload it to a file server.\nAlthough GitHub Actions in open-source repositories are public, they can still use secrets that are not accessible to the public. For example, secrets can be API keys and database access credentials.\nA real-world GitHub Action doing data processing Below is an example GitHub Action that does general data processing. It uses API calls to download data from NVD (National Vulnerability Database), generates files from this data, and then creates a release. Subsequently, the application can download these files and use them directly without making the API calls or processing the data itself.\nGitHub gist: The GitHub Action does a checkout of our application code and runs a script cmd/cve/generate.go to generate the files. Then, it publishes the generated files as a new release. As a final step, it deletes any old releases.\nA note of caution. GitHub monitors for cryptocurrency mining and other abusive behavior. So, keep that in mind and be careful with process-intensive actions.\nUse GitHub Actions for general-purpose tasks video Other articles related to GitHub How to reuse workflows and steps in GitHub Actions What happens in a GitHub pull request after a git merge How to create a custom GitHub Action using TypeScript Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-01-11T00:00:00Z","image":"https://victoronsoftware.com/posts/use-github-actions-for-general-purpose-tasks/GitHub-action_hu_62538675a4025730.png","permalink":"https://victoronsoftware.com/posts/use-github-actions-for-general-purpose-tasks/","title":"Use GitHub Actions for general-purpose tasks"},{"content":"Fuzz testing is a software automated testing technique where random inputs are provided to the software under test. My background is in hardware verification, which uses sophisticated methodologies for pseudorandom testing, so I wanted to see what the Go library had to offer out of the box.\nA Go fuzz test can run as:\na normal unit test a test with fuzzing A fuzz test is written similarly to a normal unit test in a *_test.go file, with the following changes. It must have a Fuzz prefix and use the testing.F struct instead of the usual testing.T struct.\nfunc FuzzSample(f *testing.F) { Here is a workflow for using fuzz testing. First, you create a fuzz test. Then, you run it with fuzzing to automatically find failing corner cases and make any fixes. Thirdly, you include the test and the corner cases in your continuous integration testing suite.\nCreate a fuzz test When creating a fuzz test, you should provide a corpus of initial seed inputs. These are the inputs the test will use before applying randomization. Add the seed corpus with the Add method. For example:\nf.Add(tc.Num, tc.Name) f.Add(uint8(0), \u0026#34;\u0026#34;) The inputs to the Add method indicate which types will be fuzzed, and these types must match the subsequent call to the Fuzz method:\nf.Fuzz(func(t *testing.T, num uint8, name string) { The fuzz test can randomize any number of inputs, as long as they are one of the supported types.\nRun the test with fuzzing To run the test with fuzzing, use the -fuzz switch, like:\ngo test -fuzz FuzzSample The test will continuously run on all your CPUs until it fails, or you kill it:\n=== RUN FuzzSample fuzz: elapsed: 0s, gathering baseline coverage: 0/11 completed fuzz: elapsed: 0s, gathering baseline coverage: 11/11 completed, now fuzzing with 12 workers fuzz: elapsed: 3s, execs: 432199 (144036/sec), new interesting: 0 (total: 11) fuzz: elapsed: 6s, execs: 871147 (146328/sec), new interesting: 0 (total: 11) A sample failure:\nfailure while testing seed corpus entry: FuzzSample/49232526a5eabbdc fuzz: elapsed: 1s, gathering baseline coverage: 10/11 completed --- FAIL: FuzzSample (1.03s) --- FAIL: FuzzSample (0.00s) fuzz_test.go:21: Found 0 The failures are automatically added to the seed corpus. The seed corpus includes the initial inputs that were added with the Add method as well as any new fails. These new seed corpus files are automatically created in the testdata/fuzz/Fuzz* directory. Sample contents of one such file:\ngo test fuzz v1 byte(\u0026#39;\\x01\u0026#39;) string(\u0026#34;0a0000\u0026#34;) Adding the failure to the seed corpus means that the failing case will always run when this test is run again as a unit test or with fuzzing.\nNow, you must fix the failing test and continue the loop of fuzzing and fixing.\nInclude the test in continuous integration When checking in the test to your repository, you must either include the testdata/fuzz/Fuzz* files or convert those files into individual Add method calls in your test. Once the test is checked in, all the inputs in the seed corpus will run as part of the standard Go unit test flow.\nInitial impressions Fuzz testing appears to be a good approach to help the development of small functions with limited scope. The library documentation mentions the following about the function under test:\nThis function should be fast and deterministic, and its behavior should not depend on shared state.\nI plan to give fuzzing a try the next time I develop such a function. I will share the results on this blog.\nConcerns and Issues Native fuzzing support was added to Go in 1.18 and seems like a good initial approach. However, it feels limited in features and usability. The types of functions, fast and deterministic, that fuzzing is intended for are generally not very interesting when testing real applications. They are good examples for students learning how to code. However, more interesting testing scenarios include:\nFunctions accessing remote resources in parallel, such as APIs or databases Functions with asynchronous code Secondly, the fuzzing library does not provide a good way to guide the randomization of inputs and does not give feedback about the input state space already covered. It does provide line coverage information, but that doesn\u0026rsquo;t help for unknown corner cases.\nIf one of my inputs is intended to be a percentage, then I want most of the fuzzing to concentrate on the legal range of 0-100, as opposed to all numbers. This lack of constraints becomes a problem when adding additional inputs to the fuzzing function, as the available state space of inputs expands exponentially. If the state space of inputs is huge, there is no guarantee that fuzzing accomplished its goal of finding all corner cases, leaving the developer with a false sense of confidence in their code.\nLastly, the fuzz test is hard to maintain. The seed corpus is stored in files without any context regarding what corner case each seed is hitting. Software engineers unfamiliar with fuzz testing will find this extremely confusing. If the fuzz test needs to be extended in the future with additional inputs or different types, the old seed corpus will become useless. It will be worse than useless \u0026ndash; the test will not run, and the developer unfamiliar with fuzz testing will not have a clear idea why.\nfuzz_test.go:16: wrong number of values in corpus entry: 2, want 3 That said, understanding the fuzz testing limitation, I‚Äôm willing to try fuzz testing for more interesting test cases, such as database accesses. I will report my findings in a future post.\nGitHub gist: Further reading Benchmarking performance with Go Measure Go test execution time Speed up Go CI tests by breaking them apart. Unit testing a Chrome Extension Go fuzz testing video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-01-04T00:00:00Z","image":"https://victoronsoftware.com/posts/fuzz-testing-with-go/fuzz_hu_21d109764cbeab5f.png","permalink":"https://victoronsoftware.com/posts/fuzz-testing-with-go/","title":"Fuzz testing in Go"},{"content":"In the ever-evolving landscape of device management and cybersecurity, understanding the mechanics behind tools like Fleet is not just about technical curiosity; it\u0026rsquo;s about empowering IT professionals to safeguard digital assets more effectively. Fleet gathers telemetry from various devices, from laptops to virtual machines, using osquery. At the heart of this system lies a crucial feature: Fleet policies.\nPolicies in Fleet are more than just rules; they are the gatekeepers of your device\u0026rsquo;s security, ensuring stringent adherence to security standards. By dissecting how Fleet policies operate \u0026ldquo;under the hood,\u0026rdquo; IT administrators and security professionals can gain invaluable insights. These insights allow for setting up efficient security protocols and rapid response to potential vulnerabilities, a necessity in a landscape where cyber threats are constantly evolving. This article delves into the inner workings of Fleet policies, providing you with the knowledge to better configure, manage, and leverage these policies for optimal device security and efficiency.\nPolicy creation Policies can be created from the web UI, the command-line interface called fleetctl with config files, or the REST API. The user creates a policy and selects which devices need to be checked using that policy. Policies can be global or team-specific.\nWhen a policy is created, a record for it is stored in the policies table of the MySQL database. A Fleet deployment consists of several servers behind a load balancer, so storing the record in the DB makes all servers aware of the new policy.\nPolicy execution Policies are executed on the devices, which are called hosts in Fleet, according to the FLEET_OSQUERY_POLICY_UPDATE_INTERVAL, which is set to 1 hour by default. This interval can be adjusted with the environment variable or set from the server‚Äôs command line.\nPolicies are simply SQL queries that return a true or false result, so the flow they use on the hosts is the same as other queries. Hosts check in with Fleet servers every 10 seconds (the default) and access the /api/v1/osquery/distributed/read API endpoint. The server checks when the policy was last executed to determine whether it should be executed again. If so, the server adds the policy to its response. For example, this policy in the server response checks if the macOS firewall is enabled:\n{ \u0026#34;queries\u0026#34;: { \u0026#34;fleet_policy_query_9\u0026#34;: \u0026#34;SELECT 1 FROM alf WHERE global_state \u0026gt;= 1;\u0026#34; }, \u0026#34;discovery\u0026#34;: { \u0026#34;fleet_policy_query_9\u0026#34;: \u0026#34;SELECT 1\u0026#34; } } Once the host has executed the policy, it writes the result to the server. The server updates the result in the policy_membership table of the MySQL database. At this point, the Host Details page on the web UI is updated with the policy result.\nForce policy execution on a device The user can force the host to execute all of its policies by clicking the Refetch link:\nPolicy results aggregation However, the main Policies page is not updated. This page shows the counts of all passing and failing hosts for each policy. A worker process on one of the Fleet servers updates it once an hour. The worker calculates the counts and stores them in the policy_stats table in the database. This is done for better performance of the UI. For customers with 100,000s of hosts that asynchronously report their policy results, calculating the passing and failing counts in real time was noticeably slow.\nSummary Understanding the intricacies of Fleet policies is essential for IT professionals managing a fleet of devices. This deep dive into the mechanics of Fleet policies ‚Äî from creation to execution ‚Äî provides you with the necessary insights to optimize your cybersecurity strategy effectively. By leveraging these policies, you can ensure stringent security standards across your network, enhancing your organization\u0026rsquo;s digital defense. As the cyber landscape evolves, tools like Fleet remain crucial in maintaining robust and responsive security protocols. We encourage you to apply these insights in your Fleet usage, and as always, we welcome your feedback and experiences in the Fleet community Slack channels.\nUnderstanding the intricacies of Fleet policies video This article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2023-12-30T00:00:00Z","image":"https://victoronsoftware.com/posts/understanding-the-intricacies-of-fleet-policies/understanding-the-intricacies-of-fleet-policies-main-policies-page-1999x978@2x_hu_fe52665b2f097e05.png","permalink":"https://victoronsoftware.com/posts/understanding-the-intricacies-of-fleet-policies/","title":"Understanding the intricacies of Fleet policies"},{"content":" Fleet is an open-source platform for managing and gathering telemetry from devices such as laptops, desktops, VMs, etc. Osquery agents run on these devices and report to the Fleet server. One of Fleet‚Äôs features is the ability to query information from the devices in near real-time, called live queries. This article discusses how live queries work ‚Äúunder the hood.‚Äù\nWhy a live query? Live queries enable administrators to ask near real-time questions of all online devices, such as checking the encryption status of SSH keys across endpoints, or obtaining the uptime of each server within their purview. This enables them to promptly identify and address any issues, thereby reducing downtime and maintaining operational efficiency. These tasks, which would be time-consuming and complex if done manually, are streamlined through live queries, offering real-time insights into the status and posture of the entire fleet of devices helping IT and security.\nLive queries under the hood Live queries can be run from the web UI, the command-line interface called fleetctl, or the REST API. The user creates a query and selects which devices will run that query. Here is an example using fleetctl to obtain the operating system name and version for all devices:\nfleetctl query --query \u0026#34;select name, version from os_version;\u0026#34; --labels \u0026#34;All Hosts\u0026#34; When a client initiates a live query, the server first creates a Query Campaign record in the MySQL database. A Fleet deployment consists of several servers behind a load balancer, so storing the record in the DB makes all servers aware of the new query campaign.\nQuery campaign As devices called Hosts in Fleet check in with the servers, they receive instructions to run a query. For example:\n{ \u0026#34;queries\u0026#34;: { \u0026#34;fleet_distributed_query_140\u0026#34;: \u0026#34;SELECT name, version FROM os_version;\u0026#34; }, \u0026#34;discovery\u0026#34;: { \u0026#34;fleet_distributed_query_140\u0026#34;: \u0026#34;SELECT 1\u0026#34; } } Then, the osquery agents run the actual query on their host, and write the result back to a Fleet server. As a server receives the result, it publishes it to the common cache using Redis Pub/Sub.\nOnly the one server communicating with the client subscribes to the results. It processes the data from the cache, keeps track of how many hosts reported back, and communicates results back to the client. The web UI and fleetctl interfaces use a WebSockets API, and results are reported as they come in. The REST API, on the other hand, only sends a response after all online hosts have reported their query results.\nDiscover more Fleet‚Äôs live query feature represents a powerful tool in the arsenal of IT and security administrators. By harnessing the capabilities of live queries, tasks that once required extensive manual effort can now be executed swiftly and efficiently. This real-time querying ability enhances operational efficiency and significantly bolsters security and compliance measures across a range of devices.\nThe integration of Fleet with Osquery agents, the flexibility offered by interfaces like the web UI, fleetctl, and the REST API, and the efficient data handling through mechanisms like Redis Pub/Sub and WebSockets API all come together to create a robust, real-time telemetry gathering system. This system is designed to keep you informed about the current state of your device fleet, helping you make informed decisions quickly.\nAs you reflect on the capabilities of live queries with Fleet, consider your network environment\u0026rsquo;s unique challenges and needs. What questions could live queries help you answer about your devices? Whether it\u0026rsquo;s security audits, performance monitoring, or compliance checks, live queries offer a dynamic solution to address these concerns.\nWe encourage you to explore the possibilities and share your thoughts or questions. Perhaps you‚Äôre facing a specific query challenge or an innovative use case you‚Äôve discovered. Whatever it may be, the world of live queries is vast and ripe for exploration. Join us in Fleet‚Äôs Slack forums to engage with a community of like-minded professionals and deepen your understanding of what live queries can achieve in your environment.\nAPI Documentation:\nRun live query with REST API Run live query with WebSockets This article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2023-12-29T00:00:00Z","image":"https://victoronsoftware.com/posts/get-current-telemetry-from-your-devices-with-live-queries/Live%20Query_hu_ff2f4cbef3523a64.png","permalink":"https://victoronsoftware.com/posts/get-current-telemetry-from-your-devices-with-live-queries/","title":"Get current telemetry from your devices with live queries"},{"content":"When starting to code in Go, we encountered the following situation. We needed to create an empty slice, so we did:\nslice := []string{} However, my IDE flagged it as a warning, and pointed me to this Go style guide passage, which recommended using a nil slice instead:\nvar slice []string This recommendation didn\u0026rsquo;t seem right. How can a nil variable be better? Won‚Äôt we run into issues like null pointer exceptions and other annoyances? Well, as it turns out, that‚Äôs not how slices work in Go. When declaring a nil slice, it is not the dreaded null pointer. It is still a slice. This slice includes a slice header, but its value just happens to be nil.\nThe main difference between a nil slice and an empty slice is the following. A nil slice compared to nil will return true. That‚Äôs pretty much it.\nif slice == nil { fmt.Println(\u0026#34;Slice is nil.\u0026#34;) } else { fmt.Println(\u0026#34;Slice is NOT nil.\u0026#34;) } When printing a nil slice, it will print like an empty slice:\nfmt.Printf(\u0026#34;Slice is: %v\\n\u0026#34;, slice) Slice is: [] You can append to a nil slice:\nslice = append(slice, \u0026#34;bozo\u0026#34;) You can loop over a nil slice, and the code will not enter the for loop:\nfor range slice { fmt.Println(\u0026#34;We are in a for loop.\u0026#34;) } The length of a nil slice is 0:\nfmt.Printf(\u0026#34;len: %#v\\n\u0026#34;, len(slice)) len: 0 And, of course, you can pass a nil slice by pointer. That‚Äôs right \u0026ndash; pass a nil slice by pointer.\nfunc passByPointer(slice *[]string) { fmt.Printf(\u0026#34;passByPointer len: %#v\\n\u0026#34;, len(*slice)) *slice = append(*slice, \u0026#34;bozo\u0026#34;) } You will get the updated slice if the underlying slice is reassigned.\npassByPointer(\u0026amp;slice) fmt.Printf(\u0026#34;len after passByPointer: %#v\\n\u0026#34;, len(slice)) len after passByPointer: 1 The code above demonstrates that a nil slice is not a nil pointer. On the other hand, you cannot dereference a nil pointer like you can a nil slice. This code causes a crash:\nvar nullSlice *[]string fmt.Printf(\u0026#34;Crash: %#v\\n\u0026#34;, len(*nullSlice)) Here\u0026rsquo;s the full gist:\nFurther reading Recently, we wrote about overriding methods in Go. Watch nil slice vs empty slice video ","date":"2023-12-28T00:00:00Z","image":"https://victoronsoftware.com/posts/nil-slice-versus-empty-slice-in-go/cover_hu_81820af2b5d05211.png","permalink":"https://victoronsoftware.com/posts/nil-slice-versus-empty-slice-in-go/","title":"Nil slice versus empty slice in Go"},{"content":" Matter is a recent open-source standard for connecting devices such as light switches, door locks, motion sensors, and many others. The major goals of the standard are compatibility and interoperability. This means that you will no longer need to be an expert hacker when trying to control devices from multiple manufacturers under a single application. Apple, Amazon, and Google are some of the major members driving the standard. This is great news for the majority of adopters who haven‚Äôt yet fully embraced home automation and security.\nThe Matter specification is published by the Connectivity Standards Alliance (CSA) and includes a software development kit. Version 1.0 of the specification was released in October of 2022. In 2023, we saw a slew of new devices and software upgrades compatible with Matter. Version 1.2 of the specification was published in October of 2023. However, this latest specification is still missing support for a few important device categories such as cameras and major appliances. Cameras are a top priority for the CSA, and we may see Matter-compatible cameras in 2024.\nMatter is an important step for the management of IoT devices because it finally brings true interoperability where it has been sorely missing for so many years. No longer will device manufacturers need to decide and budget precious software resources to support Amazon Alexa, Google Home, Apple HomeKit, or another connectivity hub. Customers will no longer be locked into using one of the major home automation providers. And home automation solutions from smaller companies will come onto the market.\nAn important feature of Matter is multi-admin, which means that devices can be read and controlled by multiple clients. In Matter terminology, the device, such as a motion sensor, is called a server or node, and the applications controlling it are called clients. For example, a light switch may be simultaneously controlled by the manufacturer‚Äôs app, by Alexa, and by the user\u0026rsquo;s hand-written custom API client.\nMulti-admin support means that a home or business may use one application to control their locks, switches, and security sensors, and another application for reading telemetry from those same devices. Businesses will find it easier to integrate physical security with cyber security. For example, suppose a business‚Äôs device management server uses Matter to subscribe to the office door lock. It receives an alert that User A has entered their code. Afterwards, via regular scheduled telemetry, it notices a successful login to Computer B. The business SIEM (security information and event management) system should immediately flag this suspicious sequence of events.\nOf course, the example above can be accomplished today by writing some custom code or using a third party integration. What Matter brings is scalability to such security approaches. The code and integration will no longer need to be redone for each new device and version that comes onto the market.\n","date":"2023-12-20T00:00:00Z","image":"https://victoronsoftware.com/posts/physical-security-meets-cybersecurity-with-matter/cover_hu_a06383bdd079baf5.png","permalink":"https://victoronsoftware.com/posts/physical-security-meets-cybersecurity-with-matter/","title":"Physical security meets cybersecurity with Matter"},{"content":"A prepared statement is a feature of modern databases intended to help execute the same SQL statement multiple times. For example, the following statement is a prepared statement:\nSELECT id, name FROM users WHERE email = ?; The presence of an unspecified parameter, labeled ‚Äú?‚Äù, makes it a prepared statement. When a prepared statement is sent to the database, it is compiled, optimized, and stored in memory on the database server. Subsequently, the client application may execute the same prepared statement multiple times with different parameter values. This results in a speedup.\nPrepared statements are well suited for long and complex queries that require significant compilation and optimization times. They are kept prepared on the DB server, and the application must only pass the parameters to execute them.\nAnother benefit of using prepared statements is the protection they provide against SQL injection. The application does not need to properly escape the parameter values provided to the statement. Because of this protection, many experts recommend always using prepared statements for accessing the database.\nHowever, by always using prepared statements for accessing the database, we force the SQL driver to send the extra prepare command for every ad-hoc statement we execute. The driver sends the following commands:\nPrepare the statement Execute statement with given parameters Close the statement (and deallocate the prepared statement created above) Another issue with prepared statements is the memory requirement. In large application deployments with large numbers of connections, prepared statements can crash your environment. This issue happened to one of our customers.\nA prepared statement is only valid for a single session, which typically maps to a single database connection. If the application runs multiple servers, with many connections, it may end up storing a prepared statement for each one of those sessions.\nFor example, given 100 servers with 100 connections each, we have 10,000 connections to the database. Assuming a memory requirement of 50 KB per prepared statement (derived from the following article), we arrive at the maximum memory requirement of:\n10,000 * 50 KB = 500 MB per single saved prepared statement Some databases also have limits on the number of prepared statements. MySQL‚Äôs max_prepared_stmt_count defaults to 16,382 for the entire server. Yes, this is a global limit, and not per session. In the above example, if the application uses prepared statements for every database access, then each database connection will always be using up 1 short-lived prepared statement. A short-lived prepared statement is the prepared statement, as we described above, that will be created for the purposes of executing one statement, and then immediately deallocated afterwards. This means the above application running with a default MySQL config cannot explicitly save any prepared statements \u0026ndash; 10,000 transient prepared statements + 10,000 saved prepared statements is greater than the max_prepared_stmt_count of 16,382.\nThis is extremely inconvenient for application developers, because they must keep track of:\nThe number of saved prepared statements they are using How many application servers are running How many database connections each server has The prepared statement limits of the database This detail can easily be overlooked when scaling applications.\nIn the end, is it really worth using prepared statements, and especially saved prepared statements, in your application? Yes, saved prepared statements can offer performance advantages, especially for complex queries executed frequently. However they must also be kept in check.\nA few ways to mitigate prepared statement issues for large application deployments include:\nLimit the number of database connections per application server Increase the prepared statement limit on the database server(s) Limit the maximum lifespan of connections. When closing a connection, the database will deallocate all prepared statements on that connection. SQL prepared statements are broken when scaling applications video Other articles related to MySQL Optimize MySQL query performance: INSERT with subqueries MySQL deadlock on UPDATE/INSERT upsert pattern Fully supporting Unicode and emojis in your app Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2023-12-14T00:00:00Z","image":"https://victoronsoftware.com/posts/sql-prepared-statements-are-broken-when-scaling-applications/cover_hu_8c3275370153d6d9.png","permalink":"https://victoronsoftware.com/posts/sql-prepared-statements-are-broken-when-scaling-applications/","title":"SQL prepared statements are broken when scaling applications"},{"content":" At Fleet, our developer documentation is spread out throughout the codebase, contained in a multitude of README and Markdown files. Much of the documentation is hosted on our webpage, but not all of it.\nAs developers, we need to be able to quickly search project documentation to find answers to specific questions, such as:\nHow to do a database migration How to run integration tests How to deploy a development version of to a specific OS One solution is to use grep or the IDE environment to search for these answers. Unfortunately, such search methods are not optimized for text search \u0026ndash; they frequently generate no relevant results or too many results that we must manually wade through to find the most appropriate. Specialized documentation search tools, on the other hand, prioritize headings and whole words, search for plural versions of the search terms, and offer other conveniences.\nThe lack of good search capability for engineering docs must be solved in order to scale engineering efforts. It is an issue because of the following side effects:\nEngineers are discouraged from writing documentation Documentation may be duplicated Senior developers are frequently interrupted when people can‚Äôt find relevant documentation One solution is to use a documentation service, such as a team wiki, Confluence, or GitBook. GitBook integrates with git repositories, and can push documentation changes. GitBook is free for personal use, which makes it easy to use for open source projects such as fleet and osquery. That said, GitBook is a newcomer to the space, and is still reaching maturity.\nTo set up a personal GitBook, make a fork of the open source projects that contain documentation you‚Äôd like to search, and integrate them into GitBook spaces. After indexing is complete, you‚Äôll be able to effectively search the documentation.\nTo keep the forks in sync with the parent repositories, we use Github Actions. Github Actions are free for open source projects. Searching GitHub for sync-fork returned several examples. We ended up using the following:\nname: Sync Fork on: schedule: - cron: \u0026#39;55 * * * *\u0026#39; workflow_dispatch: # on button click jobs: sync: runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v4 with: token: ${{ secrets.WORKFLOW_TOKEN }} fetch-depth: 0 - name: Configure Git run: | git config --global user.name \u0026#34;GitHub Actions Bot\u0026#34; git config --global user.email \u0026#34;actions@github.com\u0026#34; - name: Merge upstream run: | git remote add upstream https://github.com/fleetdm/fleet.git git fetch upstream main git checkout main git merge upstream/main git push origin main The WORKFLOW_TOKEN above is a GitHub personal access token (PAT) that allows reading and writing workflows in this repository. This token is not needed for repositories without workflows.\nIn addition to project documentation, GitBook can be used to synchronize personal documentation that‚Äôs being held in a private repository. There are several git-based notebook applications on the market. In addition, Markdown notes from the popular note-taking app Obsidian can be kept in GitHub. This turns GitBook into a true personalized developer documentation database \u0026ndash; one place to search through developer docs as well as your own private notes.\n","date":"2023-11-30T00:00:00Z","image":"https://victoronsoftware.com/posts/you-need-a-personal-dev-docs-db-gitbook/cover_hu_891ec5936e84789e.png","permalink":"https://victoronsoftware.com/posts/you-need-a-personal-dev-docs-db-gitbook/","title":"You need a personal dev docs DB (GitBook)"},{"content":"Traditionally, network routers used dedicated bare metal machines. However, in the last several years, we‚Äôve seen a rise in software-based routers that can be deployed either on bare metal, on a VM, or even on a container. This means these virtual routers can be used to replace existing router software on an older router. They can run in the cloud. Or they can be installed on do-it-yourself (DIY) hardware. A couple popular open source software-based routers are pfSense and OPNsense.\nWhy use a virtual router? For one, these routers offer enterprise-level features such as build-in VPN support, traffic analysis, and extensive diagnostics, among others. Another reason is that having a virtual router gives you the ability to experiment \u0026ndash; you can install multiple routers on top of your hypervisor, and try all of them out. A third reason is that the virtual router may be only one of many VMs that you run on your hardware. You can use the same piece of hardware to run a router, an ad-blocking service, a media server, and other applications.\nAdvanced virtual router installation and set up When setting up our virtual router, we chose to use PCI Passthrough to allow the virtual router direct access to the NIC hardware. Direct access to hardware improves the latency of our internet traffic. In addition, we wanted our hypervisor to sit behind the router, and not be exposed to the public. This reduces the attack surface for potential bad agents. However, routing hypervisor traffic through the router made our setup a bit tricker. It is like the chicken or the egg dilemma \u0026ndash; how do you put your hypervisor behind the router when the hypervisor is responsible for managing the router? Below is the approach we used when installing pfSense on top of Proxmox Virtual Environment (PVE).\nFor the initial installation, we did not use PCI Passthrough and instead used a virtual network bridge (vmbr0). We configured the router VM to start on boot.\nInitial virtual router configuration This allowed us to continue controlling the virtual router through the PVE web GUI. We set up the router and enabled access to it through the serial interface, which we used in the next step. Then, we put the system into its final configuration.\nFinal virtual router configuration In order to finish configuring, we had to plug in a monitor and keyboard into our hardware. We accessed the virtual router via the serial interface from the PVE command line:\nqm terminal 100 We updated the WAN interface to use eth0. At this point, the LAN interface eth1 had access to the internet.\nIn addition, we added a second LAN interface for the network bridge (vmbr0). We made sure firewall configurations for both LAN interfaces were the same.\nNext, from the PVE command line, we updated the PVE IP and gateway to point at the router by modifying the following files.\n/etc/network/interfaces /etc/hosts After rebooting PVE, we had access to the internet and to the PVE Web GUI from our new LAN.\nUpdating router software Using a virtual router with PCI Passthrough creates a unique challenge when doing software updates. What if the new version doesn‚Äôt work? What if you lose all internet access.\nWe can mitigate potential issues. First, we recommend always making a backup of the router VM when upgrading. That way we can easily roll back the change. Switching to a backup, however, requires keyboard and monitor access to your hardware, since it must be done via the PVE command line.\nAnother way to safely upgrade is to spin up a second VM running updated router software. The second VM can be either from a backup or brand new. This VM should use virtual network bridges for its connections. Once it is properly configured, we can stop the first router VM and switch the port connections to the second VM. This flow also requires accessing the router via the serial interface to update the WAN/LAN interfaces.\nFurther reading Recently, we have been setting up VLANs on our home network.\nSetting up a virtual router video ","date":"2023-11-22T00:00:00Z","image":"https://victoronsoftware.com/posts/setting-up-a-virtual-router/cover_hu_9b69505134e3fdca.jpeg","permalink":"https://victoronsoftware.com/posts/setting-up-a-virtual-router/","title":"Setting up a virtual router (pfSense on Proxmox)"},{"content":"Keychains are the macOS‚Äôs method to track and protect secure information such as passwords, private keys, and certificates. Traditionally, the keychain information was stored in files, such as:\n/Library/Keychains/System.keychain /Library/Keychains/apsd.keychain /System/Library/Keychains/SystemRootCertificates.keychain /Users/\u0026lt;username\u0026gt;/Library/Keychains/login.keychain-db In the last several years, Apple also introduced data protection keychains, such as the iCloud Keychain. Although the file-based keychains above are on the road to deprecation in favor of data protection keychains, current macOS systems still heavily rely on them. It is unclear when, if ever, these keychains will be replaced by data protection keychains.\nInspecting file-based keychains has gotten more difficult as Apple deprecated many of the APIs associated with them, such as SecKeychainOpen. In addition, excessive use of these deprecated APIs may result in corruption of the Login Keychain, as mentioned in this osquery issue. By NOT using the deprecated APIs, the user only has access to the following keychains from the above list:\n/Library/Keychains/System.keychain /Users/\u0026lt;username\u0026gt;/Library/Keychains/login.keychain-db Root certificates are missing. And the APSD (Apple Push Service Daemon) keychain is missing, which is used for device management, among other things.\nSo, how can app developers and IT professionals continue to have access to ALL of these keychain files?\nOne way is to continue using deprecated APIs until they stop working. We recommend making a secure copy of the keychain files before accessing them with the APIs.\nAnother option is to use the macOS security command line tool. For example, to list root certificates, do the following:\nsudo security find-certificate -a /System/Library/Keychains/SystemRootCertificates.keychain A third, and hardest, option is to parse the keychain files yourself. Some details on the keychain format are available. Please leave a comment if you or someone else has created a tool to parse Apple keychains.\nThe fourth option is to use an existing tool, such as osquery. Osquery is an open-source tool built for security and IT professionals. Osquery developers are working on fixing any issues to continue providing access to macOS keychain files via the following tables:\ncertificates keychain_acls keychain_items Watch how to inspect macOS keychain files Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2023-11-16T00:00:00Z","permalink":"https://victoronsoftware.com/posts/inspecting-keychain-files-on-macos/","title":"Inspecting keychain files on macOS"},{"content":"Authorization is giving permission to a user to do an action on the server. As developers, we must ensure that users are only allowed to do what they are authorized.\nOne way to ensure that authorization has happened is to loudly flag when it hasn\u0026rsquo;t. This is how we do it at Fleet Device Management.\nIn our code base, we use the go-kit library. Most of the general endpoints are created in the handler.go file. For example:\n// user-authenticated endpoints ue := newUserAuthenticatedEndpointer(svc, opts, r, apiVersions...) ue.POST(\u0026#34;/api/_version_/fleet/trigger\u0026#34;, triggerEndpoint, triggerRequest{}) Every endpoint calls kithttp.NewServer and wraps the endpoint with our AuthzCheck. From handler.go:\ne = authzcheck.NewMiddleware().AuthzCheck()(e) return kithttp.NewServer(e, decodeFn, encodeResponse, opts...) This means that after the business logic is processed, the AuthzCheck is called. This check ensures that authorization was checked. Otherwise, an error is returned. From authzcheck.go:\n// If authorization was not checked, return a response that will // marshal to a generic error and log that the check was missed. if !authzctx.Checked() { // Getting to here means there is an authorization-related bug in our code. return nil, authz.CheckMissingWithResponse(response) } This additional check is useful during our development and QA process, to ensure that authorization always happens in our business logic.\nFurther reading Recently, we improved our app\u0026rsquo;s security by reading program arguments from STDIN.\nWatch how we catch missed authorization checks Note: If you want to comment on this article, please do so on the YouTube video.\nThis article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2023-11-10T00:00:00Z","permalink":"https://victoronsoftware.com/posts/catch-missed-authorization-checks-during-software-development/","title":"Catch missed authorization checks during software development"}]