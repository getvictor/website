[{"content":"The AI landscape is changing rapidly. A new tool seems to come out weekly, or we hear about a breakthrough. As a software engineer, it is hard to keep up with all the latest developments and even harder to figure out what is relevant to your day-to-day work. Many tech leaders claim that AI is helping them achieve greater engineering velocity, improving the quality of their code, and enhancing the engineering experience, to name a few. But how can you leverage AI to improve your work as a software engineer? And, most importantly, what is the most effective way to use AI right now?\nWe have followed AI since the early 2000s, when Jeff Hawkins published On Intelligence. AI has come a long way since then. Today, most software engineers can benefit from some AI.\nThe top AI use cases for software engineers are:\nCode completion Code generation Summarizing or explaining code Agentic coding Some software developers have other use cases for AI, such as image generation, but this article will focus on the above ones.\nCode completion The original GitHub Copilot popularized AI code completion. It is a tool that can be integrated into your IDE and can perform multi-line code completions. GitHub Copilot and similar tools have been extremely popular with software developers. We found AI code completion extremely helpful, even though it occasionally generates function signatures and other code that is not quite right.\nFor example, after creating the PubSubMessage struct in our code, Copilot recommended the PubSubMessageAttributes struct. Opening the completions window shows other multi-line suggestions.\nGitHub Copilot code completion example The code completion is not only for software code. We used it for documentation, such as writing a user guide for a new feature. AI is most useful for boilerplate introductory content. However, beware that some search engines may be able to detect the use of AI in your documentation and reduce your search ranking.\nCode generation Code generation is similar to code completion, but the desired result is a complete function or code block. Typically, engineers use a chat interface such as ChatGPT and provide it with a description of the desired function. Providing a detailed description along with inputs and outputs is key to getting the best results.\nFor example, given the following prompt:\nWrite a Golang function that takes path.URL, makes a GET request to that URL, parses the JSON response, and returns the \u0026ldquo;attributes\u0026rdquo; and \u0026ldquo;data\u0026rdquo; fields of the response.\nChatGPT o1 model generated the following code:\n// FetchAttributesAndData makes a GET request to the provided URL, parses // the JSON response, and returns the \u0026#34;attributes\u0026#34; and \u0026#34;data\u0026#34; fields. func FetchAttributesAndData(u *url.URL) (map[string]interface{}, interface{}, error) { resp, err := http.Get(u.String()) if err != nil { return nil, nil, err } defer resp.Body.Close() // You may also check resp.StatusCode for error handling if desired if resp.StatusCode != http.StatusOK { return nil, nil, fmt.Errorf(\u0026#34;unexpected status code: %d\u0026#34;, resp.StatusCode) } // Define a struct to capture the JSON structure var result struct { Attributes map[string]interface{} `json:\u0026#34;attributes\u0026#34;` Data interface{} `json:\u0026#34;data\u0026#34;` } if err := json.NewDecoder(resp.Body).Decode(\u0026amp;result); err != nil { return nil, nil, err } return result.Attributes, result.Data, nil } ChatGPT also provided some documentation and a usage example.\nPreviously, software engineers would accomplish this task by searching Google or StackOverflow for similar code snippets. Now, they can use AI chatbots to generate this code. This approach is most useful when you need to generate code that\u0026rsquo;s not your area of expertise.\nSome programming languages have dedicated AI code-generation tools. For example, you can create Go functions and packages online with sketch.dev.\nSummarizing or explaining code When software engineers work on a large codebase, they often need to understand code written by other engineers, some of whom may have left the company, and they may also need to find a specific piece of code in the codebase.\nAI can help by reading the codebase and adding it to its context. This is an example of Retrieval-Augmented Generation (RAG), where AI combines its general knowledge with the specific context of the codebase.\nTo understand the codebase, the AI tool needs to add the code to its context or to index the codebase. Then, the tool can use a combination of AI and deterministic search to find the relevant code. Below is an example of a question we asked Cursor IDE about our codebase:\nIn which Go files is macOS disk encryption code located?\nCursor provided a list of files and additional information on the top hits.\nCursor codebase search example Codebase-wide queries are a great way to find information yourself before asking the broader team. They are also a decent way to learn the codebase.\nAgentic coding Agentic coding refers to using an AI agent to write code and perform tasks on your behalf. Using agents is a more advanced use case, requiring you to know the AI tools, processes, and LLMs well. A good AI agent can:\nWrite code, including creating and moving files Write and run tests, including Browser UI tests Write, read, and follow documentation Do terminal operations such as installing applications Do Git operations such as pushing Connect to other servers with SSH Currently, the top agentic coding tools are:\nRoo Code (VSCode plugin) Cline (VSCode plugin) Cursor (IDE built on top of VSCode) There are many other tools and platforms available. GitHub Copilot also announced Agent mode, which is available in preview as of this writing. JetBrains has announced Junie, which is only available via the Early Access Program.\nAs the agentic coding tools are still in their early stages, changing rapidly, and require a lot of handholding, it is reasonable to wait 6 to 12 months before revisiting them.\nAI coding agent workflows The following are some workflow suggestions for using an AI coding agent to create a small application.\nFirst, start with some context regarding what you want to build. Create a README or a plan outlining how you want to structure the application and the steps to implement it. You can use another general-purse AI, such as ChatGPT, to help you create the high-level plan.\nFor example, we asked ChatGPT to create a high-level plan with the following prompt:\nWe want to create a mock Google Android Management API server using an AI agent. The server is written in Golang and will interact with our MDM solution during testing. It should hold the state for enterprises, profiles, and fake devices enrolled in it. The server should have a mock PubSub webhook that will push notifications regarding ENROLLMENT and STATUS_REPORTs. Please create a plan that another AI agent can implement in several steps.\nNext, ask the AI agent to read the plan, update it, and create a more detailed plan. It may make sense to break the plan into smaller parts and treat each part as a separate project. In effect, you act as the AI agent\u0026rsquo;s project manager.\nMake sure to have documentation and have the AI agent update it regularly. In addition to the README, you can have API specs, secrets, and other documentation files.\nTell the AI agent to initialize a git repo, create the project structure, and start implementing the plan. For each step, ask the AI agent to create tests. After each step, ask the AI agent to update the documentation and commit the changes. This way, you can easily rollback if the AI agent gets stuck or goes off the rails.\nTry to be as precise as possible in your prompts.\nWhen adding a new feature, you can start a new session with the AI agent and ask it to read all the documentation. This will \u0026ldquo;initialize\u0026rdquo; the AI agent with the project context.\nWork in small development iterations with your AI agent.\nAI agent workflow Learn about your AI agent\u0026rsquo;s specific features to level up your skills. Often, there are ways to provide context to the agent or give special meaning to certain words or files.\nAt some point, you may want to take over the maintenance of the code from the AI agent. For example, check the code into your main repository and maintain it as any other human-written code.\nAI coding agent issues The main issue with AI coding agents is that they make mistakes. If you spot their mistake, you can tell them about it, and they will generally correct it. However, if you can\u0026rsquo;t spot their mistake, the agent may end up in a loop where it keeps trying to fix the issue, but your application still doesn\u0026rsquo;t work. That is why it is essential to work in small iterations where you can roll back and start over.\nThe other issue is that AI agents are slow. Often, they need to take several steps to make progress, and the human is left waiting—being there just in case they need help or go off track. Theoretically, a single human could manage multiple AI agents, but in practice, it is hard for people to frequently switch between multiple cognitively demanding tasks.\nFurther reading We recently discussed how to scale your codebase with incremental design. We also wrote about the importance of readable code. Watch how to use AI for software development Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-02-26T00:00:00Z","image":"https://victoronsoftware.com/posts/ai-for-software-developers/ai-building-software_hu_2cd598135c787927.png","permalink":"https://victoronsoftware.com/posts/ai-for-software-developers/","title":"How to use AI for software development (2025)"},{"content":"Go is designed for fast compilation. However, sometimes, you notice that your builds have gotten slower or that certain code changes cause an unexpectedly long recompile time. In this article, we show how to analyze your compilation times and take steps to improve them.\nTurn on the Go build cache First, you must know that Go is very good at caching build artifacts. If you make a small change and rerun the build, the rerun will be significantly faster because Go will reuse the cached artifacts from the previous build. However, if you update the Go version, change the build flags, or pull in new or different dependencies, Go may rebuild everything from scratch.\nThe first step in improving your build time is to make sure you are using a build cache. The cache is enabled by default on your development machine, but that may not be true on your CI/CD system. Ensure you use a build cache across multiple CI/CD runs. For example, the GitHub Actions setup-go action has caching turned on by default.\nAnalyze a Go build We can clear the build cache with the following:\ngo clean -cache Now, we can run a clean build with the -debug-trace flag:\ntime go build -debug-trace=debug-trace.json ./cmd/fleet We use the time command to measure the time the build takes. It is good practice to always use the time command when measuring performance. The time command is built into our Z shell (zsh), but a similar command is available in other shells and OSes.\nIn the time output, we see how long our build took:\n79.25s user 20.13s system 518% cpu 19.183 total The total time (19.183s) is the wall clock time we waited for the build to finish. The user and system times are spent executing user and system code. They are larger than the wall clock time because we use a multi-core machine.\nThe debug trace is in Trace Event Format and looks like this:\n[ {\u0026#34;name\u0026#34;:\u0026#34;Running build command\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;B\u0026#34;,\u0026#34;ts\u0026#34;:1739801608027038,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ,{\u0026#34;name\u0026#34;:\u0026#34;load.PackagesAndErrors\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;B\u0026#34;,\u0026#34;ts\u0026#34;:1739801608027222,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ,{\u0026#34;name\u0026#34;:\u0026#34;modfetch.download github.com/WatchBeam/clock@v0.0.0-20170901150240-b08e6b4da7ea\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;B\u0026#34;,\u0026#34;ts\u0026#34;:1739801608038996,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ,{\u0026#34;name\u0026#34;:\u0026#34;modfetch.download github.com/WatchBeam/clock@v0.0.0-20170901150240-b08e6b4da7ea\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;E\u0026#34;,\u0026#34;ts\u0026#34;:1739801608039035,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ,{\u0026#34;name\u0026#34;:\u0026#34;modfetch.download github.com/briandowns/spinner@v1.23.1\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;B\u0026#34;,\u0026#34;ts\u0026#34;:1739801608039382,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ,{\u0026#34;name\u0026#34;:\u0026#34;modfetch.download github.com/briandowns/spinner@v1.23.1\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;E\u0026#34;,\u0026#34;ts\u0026#34;:1739801608039410,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ,{\u0026#34;name\u0026#34;:\u0026#34;modfetch.download github.com/e-dard/netbug@v0.0.0-20151029172837-e64d308a0b20\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;B\u0026#34;,\u0026#34;ts\u0026#34;:1739801608039643,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ,{\u0026#34;name\u0026#34;:\u0026#34;modfetch.download github.com/e-dard/netbug@v0.0.0-20151029172837-e64d308a0b20\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;E\u0026#34;,\u0026#34;ts\u0026#34;:1739801608039808,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ,{\u0026#34;name\u0026#34;:\u0026#34;modfetch.download github.com/getsentry/sentry-go@v0.18.0\u0026#34;,\u0026#34;ph\u0026#34;:\u0026#34;B\u0026#34;,\u0026#34;ts\u0026#34;:1739801608053496,\u0026#34;pid\u0026#34;:0,\u0026#34;tid\u0026#34;:0} ... and so on A widespread tool for visualizing Trace Event Format is Perfetto. Click Open trace file and upload your trace. Use Ctrl + Scroll to zoom in and out and Shift + Scroll to move right or left. The WASD keyboard keys also work.\nThe Perfetto tool showing a Go build debug trace The trace shows that our https://github.com/mattn/go-sqlite3 dependency is taking most of the build time. The fact that we have 16 cores doesn\u0026rsquo;t help because Go is not parallelizing the build for this dependency. This dependency uses CGO, so the build takes time to compile C files.\nWe attempted to speed up the build by adding the go-sqlite3 dependency to our top ./cmd/fleet package, assuming the build tool would start compiling it first. However, the total build took longer because the subsequent link step became much slower.\nAs we mentioned above, the initial compile time is usually not a big concern if you are using a build cache. So, let\u0026rsquo;s try making a small change and analyzing the recompile time. We make a change to a frequently modified package.\necho \u0026#39;var _ = \u0026#34;bozo\u0026#34;\u0026#39; \u0026gt;\u0026gt; ./server/datastore/mysql/mysql.go time go build -debug-trace=debug-trace-recompile.json ./cmd/fleet The total recompile time is 1.229s, and the trace looks like this:\nThe Perfetto tool showing a Go recompile debug trace We see that the mysql package we modified is taking about half the recompile time. The load.PackagesAndErrors step takes ~300ms and is not parallelized. This step is part of the Go toolchain. Modifying a smaller package would reduce the recompile time. If you have a large package that is frequently modified, you can improve the build time by splitting it into smaller packages.\nFind why dependencies are included in the build In a previous article, we described how to find Go package dependencies. A way to analyze the build and see why a dependency is being pulled in is to use the -debug-actiongraph flag:\ngo clean -cache time go build -debug-actiongraph=actiongraph.json ./cmd/fleet The resulting actiongraph.json is a JSON file containing an array of entries such as:\n{ \u0026#34;ID\u0026#34;: 27, \u0026#34;Mode\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/server/datastore/filesystem\u0026#34;, \u0026#34;Deps\u0026#34;: [ 4, 11, 23, 33, 93, 97, 180, 103 ], \u0026#34;Objdir\u0026#34;: \u0026#34;/var/folders/r6/br06kz3s6lxb_75zz6dkjvvc0000gn/T/go-build3105381437/b845/\u0026#34;, \u0026#34;Priority\u0026#34;: 843, \u0026#34;NeedBuild\u0026#34;: true, \u0026#34;ActionID\u0026#34;: \u0026#34;JGOAJdypDJJbwlHvaUPE\u0026#34;, \u0026#34;BuildID\u0026#34;: \u0026#34;JGOAJdypDJJbwlHvaUPE/B47ZHL3FCDKdll6TubU2\u0026#34;, \u0026#34;TimeReady\u0026#34;: \u0026#34;2025-02-18T09:02:54.257806-06:00\u0026#34;, \u0026#34;TimeStart\u0026#34;: \u0026#34;2025-02-18T09:02:54.272756-06:00\u0026#34;, \u0026#34;TimeDone\u0026#34;: \u0026#34;2025-02-18T09:02:54.293356-06:00\u0026#34;, \u0026#34;Cmd\u0026#34;: [ \u0026#34;/opt/homebrew/Cellar/go/1.23.4/libexec/pkg/tool/darwin_arm64/compile -o /var/folders/r6/br06kz3s6lxb_75zz6dkjvvc0000gn/T/go-build3105381437/b845/_pkg_.a -trimpath \\\u0026#34;/var/folders/r6/br06kz3s6lxb_75zz6dkjvvc0000gn/T/go-build3105381437/b845=\\u003e\\\u0026#34; -p github.com/fleetdm/fleet/v4/server/datastore/filesystem -lang=go1.23 -complete -buildid JGOAJdypDJJbwlHvaUPE/JGOAJdypDJJbwlHvaUPE -goversion go1.23.4 -c=4 -shared -nolocalimports -importcfg /var/folders/r6/br06kz3s6lxb_75zz6dkjvvc0000gn/T/go-build3105381437/b845/importcfg -pack /Users/victor/work/fleet/server/datastore/filesystem/software_installer.go\u0026#34; ], \u0026#34;CmdReal\u0026#34;: 17479792, \u0026#34;CmdUser\u0026#34;: 17327000, \u0026#34;CmdSys\u0026#34;: 5692000 }, The CmdReal, CmdUser, and CmdSys fields show the real, user, and system time spent executing the command. The Deps field shows the package\u0026rsquo;s dependencies.\nAlthough we can write our own tool to analyze the actiongraph.json file, we can also use the https://github.com/icio/actiongraph tool. Install the tool with:\ngo install github.com/icio/actiongraph@latest We can find the longest compile steps with:\nactiongraph -f actiongraph.json top 13.786s 16.14% build github.com/mattn/go-sqlite3 1.396s 17.78% build runtime/cgo 1.327s 19.33% build github.com/aws/aws-sdk-go/service/s3 1.295s 20.85% build github.com/aws/aws-sdk-go/aws/endpoints 1.095s 22.13% build github.com/google/go-github/v37/github 1.078s 23.39% build github.com/elastic/go-sysinfo/providers/darwin 0.983s 24.55% build github.com/open-policy-agent/opa/ast 0.975s 25.69% build github.com/klauspost/compress/zstd 0.916s 26.76% build github.com/shoenig/go-m1cpu 0.755s 27.64% build crypto/tls 0.742s 28.51% build github.com/fleetdm/fleet/v4/server/fleet 0.722s 29.36% build github.com/shirou/gopsutil/v3/process 0.664s 30.14% build net 0.626s 30.87% build github.com/open-policy-agent/opa/topdown 0.625s 31.60% build runtime 0.622s 32.33% build google.golang.org/protobuf/internal/impl 0.609s 33.04% build github.com/fleetdm/fleet/v4/server/datastore/mysql 0.605s 33.75% build golang.org/x/net/http2 0.577s 34.43% build github.com/aws/aws-sdk-go/service/lambda 0.576s 35.10% build github.com/spf13/pflag The tool also has a graph subcommand to highlight all import paths from the build target to the package indicated by --why. We can convert the .dot file to an SVG file with the Graphviz dot command.\nactiongraph -f actiongraph.json graph --why github.com/mattn/go-sqlite3 \u0026gt; actiongraph-sqlite3.dot dot -Tsvg \u0026lt; actiongraph-sqlite3.dot \u0026gt; actiongraph-sqlite3.svg Why the go-sqlite3 package is included in the build We can use this knowledge to refactor the codebase or, perhaps, hide the problematic dependency behind a build flag.\nThere is no official documentation for the above debug flags. However, they can be found in the Go source code:\n// Undocumented, unstable debugging flags. cmd.Flag.StringVar(\u0026amp;cfg.DebugActiongraph, \u0026#34;debug-actiongraph\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;) cmd.Flag.StringVar(\u0026amp;cfg.DebugRuntimeTrace, \u0026#34;debug-runtime-trace\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;) cmd.Flag.StringVar(\u0026amp;cfg.DebugTrace, \u0026#34;debug-trace\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;) Further reading Previously, we explained how to accurately measure the execution time of Go tests. We also demonstrated some common code refactorings that can be done with your IDE. Watch how to analyze Go builds Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-02-19T00:00:00Z","image":"https://victoronsoftware.com/posts/analyze-go-build/analyze-go-build-headline_hu_c5924c4c3f29b335.png","permalink":"https://victoronsoftware.com/posts/analyze-go-build/","title":"How to analyze Go build times"},{"content":" Extract method (aka extract function) Inline variable Extract variable Inline method (aka inline function) What is code refactoring? Code refactoring involves modifying existing software code without changing its observable behavior. Refactoring improves the code base\u0026rsquo;s readability and maintainability. See our previous article on why readable code is important.\nWhy are engineers afraid of refactoring? Refactoring is essential to software development and should be done regularly as part of day-to-day work. Unfortunately, many engineers are afraid of refactoring, don\u0026rsquo;t know how to do it, or don\u0026rsquo;t consider it part of their job responsibilities.\nSome engineers fear that refactoring will introduce bugs or break existing functionality. The root cause of this fear is the lack of automated tests. Without automated tests, ensuring that the refactored code behaves as expected is difficult. A code base without automated tests is a ticking time bomb and cannot be maintained by any sane engineer. Before refactoring code in such a code base, you should add automated tests for the targeted code.\nOther engineers fear that refactoring will take too much time. This fear is often unfounded, as refactoring can be done incrementally and in small steps. For example, after refactoring the code for an hour or less, merge your changes to your main branch and, if needed, continue doing the next small refactoring steps. Your organization should never allocate weeks of development for \u0026ldquo;large refactorings.\u0026rdquo;\nEngineers may also fear refactoring because they don\u0026rsquo;t want to make too many changes to the code, making it difficult for reviewers to review the changes. The issue is that many current code review systems don\u0026rsquo;t understand the code changes\u0026rsquo; semantics (i.e., the meaning). These systems only understand line changes and are frequently confused by relocated code. In this case, the coder should explain the changes to the reviewer. Alternatively, the organization can adopt a better code review tool. For a further discussion of issues with GitHub code reviews, see our previous article.\nThis article will show some common refactorings you can safely do with automation from your IDE (Integrated Development Environment).\nExtract method (aka extract function) The extract method refactoring takes a piece of code and moves it into a new method. There are several reasons to do this, all of which improve the readability and maintainability of the code:\nThe code is too long and must be broken into smaller, more manageable pieces. The code is duplicated in multiple places and needs to be consolidated into a single method. We want to separate the code implementation from the code intention. The code implementation is what the code does, and the code intention is why it does it. Move the code implementation into its own method and name the new method based on the code intention. For example, consider the following code:\nfunc (pt PackageTest) expandPackages(pkgs []string) []string { if !slices.ContainsFunc(pkgs, func(p string) bool { return strings.Contains(p, \u0026#34;...\u0026#34;) }) { return pkgs } // lots more code ... When entering the expandPackages method, the reader is immediately confronted with a complex expression. They must stop and think about what the code does. Even though the amount of code is small, it still hampers readability. The code implementation is mixed with the code intention. One way to improve the situation is to add a comment. A better way is to extract the code into its own method and name the new method based on the code intention.\nfunc (pt PackageTest) expandPackages(pkgs []string) []string { if !needExpansion(pkgs) { return pkgs } // lots more code ... } func needExpansion(packages []string) bool { return slices.ContainsFunc(packages, func(p string) bool { return strings.Contains(p, \u0026#34;...\u0026#34;) }) } Most IDEs automatically perform this refactoring. Highlight the code you want to extract, open the refactoring menu, and select the Extract Method option.\nInline variable Every variable should have a purpose and a good explanatory name that describes its intent. As the number of variables grows in a method, it becomes increasingly difficult to understand the code. One way to improve the readability of the code is to inline variables. Inlining a variable is replacing the variable with the right-hand side of the assignment.\nFor example, consider the following code:\nfunc (pd *packageDependency) chain() (string, int) { name := pd.name if pd.parent == nil { return name + \u0026#34;\\n\u0026#34;, 1 } // lots more code ... The variable name does not add any value to the code. It is simply a copy of the pd.name field. We can inline the variable to improve the readability of the code:\nfunc (pd *packageDependency) chain() (string, int) { if pd.parent == nil { return pd.name + \u0026#34;\\n\u0026#34;, 1 } // lots more code ... Many IDEs automatically perform this refactoring. Highlight the variable you want to inline, open the refactoring menu, and select the Inline option.\nExtract variable The extract variable refactoring takes a complex expression and moves it into a new variable. Mechanically, it is the opposite of the above inline variable refactoring. There are several reasons to do this:\nThe expression is complex and must be broken into smaller, more manageable pieces. The meaning of the expression is unclear and needs to be clarified with a descriptive variable name. Sometimes, you have a choice between extracting a method or a variable. In general, you should extract a method to make the code more readable. However, if the method is only used once and the parent function is not complex, it may be better to extract a variable.\nFor example, consider the same code from our extract method example above:\nfunc (pt PackageTest) expandPackages(pkgs []string) []string { if !slices.ContainsFunc(pkgs, func(p string) bool { return strings.Contains(p, \u0026#34;...\u0026#34;) }) { return pkgs } // lots more code ... We can extract the complex expression into a variable to improve the readability of the code:\nfunc (pt PackageTest) expandPackages(pkgs []string) []string { needExpansion := slices.ContainsFunc(pkgs, func(p string) bool { return strings.Contains(p, \u0026#34;...\u0026#34;) }) if !needExpansion { return pkgs } // lots more code ... Many IDEs automatically perform this refactoring. Highlight the expression you want to extract, open the refactoring menu, and select the Extract Variable or Introduce Variable option.\nInline method (aka inline function) The inline method refactoring takes a method and moves its code into the caller. Mechanically, it is the opposite of the extract method refactoring. There are several reasons to do this:\nThe method is too simple, and its body is as clear as its name. We want to simplify code and remove a level of indirection. We must regroup code into a single method before proceeding with a better refactoring. For example, consider the following code:\n// method code ... if dep.has(expandedPackages) { // more coe ... func (pd *packageDependency) has(pkgs []string) bool { return slices.Contains(pkgs, pd.name) } We can inline the has method:\n// method code ... if slices.Contains(expandedPackages, pd.name) { // more coe ... Many IDEs automatically perform this refactoring. Highlight the method call you want to inline, open the refactoring menu, and select the Inline Function/Method option.\nFurther reading The code examples above are from our article on finding package dependencies of a Go package. We also discussed how to scale your codebase with evolutionary architecture. And how to analyze Go build times. Watch examples of top code refactorings Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-02-12T00:00:00Z","image":"https://victoronsoftware.com/posts/common-refactorings/common-refactorings-headline_hu_63da035087f5b0b3.png","permalink":"https://victoronsoftware.com/posts/common-refactorings/","title":"Top code refactorings every software engineer should know"},{"content":" Find package dependencies using go list Find package dependencies using Go code What are package dependencies and module dependencies? A package dependency is another package that your Go package imports. When you import a package in Go, you create a dependency on that package. The Go compiler will not compile your package if it cannot find and compile the package you depend on.\nOn the other hand, a module dependency is a dependency on a module. A module is a collection of related Go packages that are versioned together. You declare your module dependencies in your go.mod file. Your code may use one or more packages from your module dependencies.\nWhy are package dependencies important? Understanding your package dependencies is essential because they:\nindicate the amount of internal coupling in your codebase help you understand the structure of your codebase help you avoid too many dependencies help you avoid circular dependencies help you optimize your build times As your codebase grows, keeping track of package dependencies is vital to ensure that the codebase remains maintainable. Many developers import dependencies without considering the consequences. In modern IDE tools, they quickly click Import in a pop-up to make the squiggly lines go away. In some cases, IDEs add imports without even asking the developer. However, code with many dependencies becomes coupled to other potentially unrelated code. This entanglement makes the codebase harder to understand, test, and maintain. For additional details, see the list of problems with a coupled architecture from our previous article.\nWhat is an architectural test? An architectural test is a test that makes sure your code follows the architectural rules that you have defined. Codebases tend to devolve into a Big Ball of Mud as time passes. Architectural tests are one way to keep your codebase clean.\nIn our example below, we will check to ensure that our Go package is NOT dependent on another package in our codebase. This is a common scenario when you want to refactor your codebase and remove a dependency or add a new package and want to ensure that it is not dependent on other parts of the codebase.\nFind package dependencies using go list go list is a powerful tool that you can use to list information about Go packages. You can use the -deps flag with go list to find package dependencies. Here is an example:\ngo list -deps ./server/android... The result is a list of all the direct and indirect package dependencies of the ./server/android and its subpackages. To filter out standard library packages and sort the list, you can use the following command on macOS:\ngo list -deps ./server/android... | grep -E \u0026#39;^[^\\/]*\\.[^\\/]*\\/\u0026#39; | sort The above regular expression looks for packages with a . before the first / in the package path. This regex filters out standard library packages. The sort command sorts the list alphabetically.\nTo check if a package is dependent on another package, you can use the following command:\n! (go list -deps ./server/android... | grep -q \u0026#39;github.com/fleetdm/fleet/v4/server/mdm/nanomdm/mdm\u0026#39;) The leading ! inverts the command\u0026rsquo;s exit status. If the package is dependent on the specified package, the command will return 1; if it is not, the command will return 0. You can use this command in your CI/CD pipelines to ensure that your package is not dependent on a specific package.\nFind package dependencies using Go code packages is a Go package that allows one to load, parse, type-check, and import Go packages. We will use the Load function to get a list of Package values. In addition, we will use Context.Import method from build package to recursively find dependencies.\nBelow is an example architecture test you can add to your test suite.\nThe above example is based on https://github.com/matthewmcnew/archtest. You can jump to the code example section of the video below for a full explanation.\nA failing run of our architecture test will look like this:\n=== RUN TestPackageDependencies arch_test.go:41: Error: package dependency not allowed. Dependency chain: github.com/fleetdm/fleet/v4/server/android/service github.com/fleetdm/fleet/v4/server/fleet github.com/fleetdm/fleet/v4/server/mdm/nanomdm/mdm --- FAIL: TestPackageDependencies (14.66s) Find how a dependency is included in the build In our article on analyzing Go build times, we show how to use the -debug-actiongraph flag to find why a dependency is included in the build.\nFurther reading In the previous article, we discussed how to scale your codebase with evolutionary architecture. Before that, we explained the difference between Go modules and Go packages. We also covered common code refactorings in Go for readability and maintainability. Watch how to find package dependencies of a Go package Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-02-05T00:00:00Z","image":"https://victoronsoftware.com/posts/go-package-dependencies/go-dependencies-headline_hu_a579fbf09f721b4c.png","permalink":"https://victoronsoftware.com/posts/go-package-dependencies/","title":"How to find package dependencies of a Go package"},{"content":"This article is part of a series on technical debt. Check out the previous article:\nWhy readable code is important and how to refactor code for readability Intro to evolutionary architecture Current architecture Problems with the current architecture Good architecture Evolutionary architecture Evolutionary architecture refers to a software design approach that embraces change as a fundamental aspect of system development. Instead of aiming to create a fixed and perfect architecture upfront, it allows the system to evolve in response to new requirements, technologies, and insights. Evolutionary architecture is a critical tool for reducing technical debt.\nEvolutionary design, or incremental design, is another term for this approach. Generally, evolutionary design refers to changes on a smaller scale, such as refactoring code or adding new features. On the other hand, evolutionary architecture refers to changes on a larger scale, such as reorganizing the codebase or splitting a monolithic application into microservices. That said, there is no strict boundary between the two terms. We will use the term evolutionary architecture.\nIn this article, we provide an example of scaling your codebase to accommodate a growing number of features and developers.\nCurrent architecture We base our example on a theoretical codebase, but real-world experiences inspire it. The problems and solutions we discuss are common in software development, especially in startups and small companies.\nThe initial state of our example codebase is a web application developed in a mono-repository. The application was built from the ground up with a simple architecture, focusing on adding new features and finding product-market fit.\nCurrent design with a few large modules. The current design divides the codebase into a few large modules. We use the term module to mean a logical grouping of code in the same files and directories.\nHowever, after a couple of years, the application has grown significantly in features, complexity, and team size. The organization now has three product teams working on different functional areas of the application. No one has updated the initial architecture, which is insufficient to support the growing codebase and development team.\nProblems with the current architecture A significant problem that the engineering team has been facing is an increase in bugs and a longer time to fix them. The code for each feature is sprinkled throughout the codebase and tightly coupled to other seemingly unrelated features. This complexity makes it difficult to understand, test, and keep existing features working as new ones are added.\nSpeaking of new features, the team has been struggling to add them on time. The codebase has become a tangled web of dependencies, and any change in one part of the codebase can have unintended consequences in other parts. Adding a feature requires modifying many parts of the codebase, which requires understanding the entire codebase, which many developers lack. The lack of knowledge and the changes to many parts of the codebase have led to features taking significantly longer to implement than initially estimated.\nMaintaining feature branches for over a few days and making patch fixes to existing releases has become impossible. The codebase is so intertwined that any changes may cause merge conflicts. The increased likelihood of merge conflicts has discouraged developers from refactoring and cleaning up the code base. This tendency to leave the code as-is has perpetuated the slide in code quality.\nTests have also become a problem. The test suite has been in a frequent state of disrepair. There is no clear ownership of tests, so engineers have been reluctant to fix them. Some engineers have stopped paying attention to failing CI alerts, figuring that the problems are caused by one of the other two teams.\nTests have also become slower and slower, especially the integration tests that test the API and include the service layer, the datastore layer, and an actual database. These tests do not run in parallel; every additional feature slows down the compile and increases test time. Test files have become bloated with tests for multiple features, making them slow to load in the editor, difficult to navigate, and impossible to diff for PR reviews.\nFinally, the onboarding time for new developers has been growing. It takes weeks for new developers to understand the codebase and start contributing.\nGood architecture At this point in the company\u0026rsquo;s life, an exemplary architecture would be separate groups of modules corresponding to the three product teams.\nGood design with dedicated modules for each product team. Each team would be responsible for its own set of modules, which aligns with Agile principles. The modules would be loosely coupled, and the teams would be able to work independently on their features without affecting other teams. The amount of code that each engineer has to understand and change would be drastically reduced.\nThis architecture would have eliminated or significantly reduced the problems that the engineering team has been facing.\nThe reduced complexity and increased understanding of the codebase would lead to fewer and faster to fix bugs Faster feature development due to cleaner code and fewer dependencies Reduced merge conflicts for PRs, especially for database migrations and schema changes Rarely failing test suite due to clear ownership of tests Faster tests due to each team focusing on testing their slice of the product. Limited complete product integration tests would still be present. Faster onboarding time for new developers However, the company does not have this architecture. Building this architecture upfront would have been foolish since it would have consumed critical engineering time. Yes, there was value in creating this structure upfront because it would have saved time in the long run, but this value was insufficient for a young company that may not be around in a few months.\nEvolutionary architecture Many companies and engineers find themselves in this situation. They have a codebase with poor architecture for today\u0026rsquo;s reality, blame the organization for not thinking about these problems earlier, and feel like they can\u0026rsquo;t improve the situation.\nEvolutionary architecture is a way to incrementally improve the architecture of a codebase without having to do a big rewrite. It is a way to make the codebase better today than it was yesterday and better tomorrow than it is today.\nThis situation is not unique to this company. It is the norm. Most companies start with a simple architecture and codebase that is good enough for the first few features. As the company grows, the architecture becomes a bottleneck. Instead of worrying about not making the right decisions in the past, consider where the architecture needs to be a year or two from now and start moving towards that.\nFor example, when adding a new prominent feature to the product, decouple it from the rest of the codebase.\nEvolutionary design with big features going into dedicated modules. Our example shows all the modules decoupled, but it may be OK to decouple one or two.\nDecoupling a feature from the rest of the codebase has many benefits similar to those we listed above for \u0026ldquo;good architecture.\u0026rdquo; Additional benefits include:\nMost of the feature can be tested by itself, reducing test time. The business gets the option to create a new team dedicated to the feature quickly \u0026ndash; the code is already separate/independent Engineering can scale the feature separately from the rest of the product. For example, assign a dedicated database or split the feature into a microservice. Code example of splitting the database schema It is nice to read about a theoretical example, but seeing an actual code example is even better. In this code example, we begin with a monolithic application that has a single database schema. We then split the schema into two separate schemas. It is the starting point and a reference for decoupling a new feature from the rest of the codebase. Since this code example is a bit long and requires some context regarding the current implementation, we will not cover it in this article. Instead, jump to the code example section of the video below.\nLink to the source code example decoupling a new backend feature from the rest of the codebase.\nFurther reading Recently, we covered how to easily track engineering metrics. Previously, we demonstrated the most significant issues with GitHub\u0026rsquo;s code review process. We also showed how to create an architectural test that finds Go package dependencies. We also published an article on the common code refactorings to improve code readability. In addition, we summarized what every software engineer should know about AI. Watch how to scale your codebase with evolutionary architecture Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-01-29T00:00:00Z","image":"https://victoronsoftware.com/posts/scaling-codebase-evolutionary-architecture/evolutionary-architecture-headline_hu_c8a750d616187c20.png","permalink":"https://victoronsoftware.com/posts/scaling-codebase-evolutionary-architecture/","title":"How to scale your codebase with evolutionary architecture"},{"content":" Metrics for unreadable code How to make your codebase more readable Fix poor software structure Refactor local code for understanding Use unit and integration tests Useful comments Readable code is software code that is easy to understand and easy to change.\nUnreadable code is a common complaint among software developers and one of the main contributors to technical debt. Abandoning unreadable code is one of the reasons engineers love to work on greenfield projects—there is no legacy code to deal with.\nYou\u0026rsquo;ve probably heard comments like, \u0026ldquo;It would take me longer to understand this code than to rewrite it from scratch.\u0026rdquo; This sentiment illustrates the problem with unreadable code: it slows down the development process.\nSome engineers refer to readable code as \u0026ldquo;clean code.\u0026rdquo; In our opinion, \u0026ldquo;readable code\u0026rdquo; and \u0026ldquo;clean code\u0026rdquo; are synonymous, and the term \u0026ldquo;readable code\u0026rdquo; is easier to understand and, therefore, more \u0026ldquo;readable.\u0026rdquo;\nWhy is unreadable code a problem? Although the \u0026ldquo;unreadable code\u0026rdquo; claim feels like a subjective opinion, it has a concrete business impact on software projects.\nWe want our code to be readable to speed up adding new features and bug fixes.\nFor example, we recently spent three hours figuring out how a feature worked, only to realize there was a documentation bug. Unfortunately, we made no improvements to make the code more readable, and the next developer will likely have the same problem. This next developer may be ourselves one year from now when we will have forgotten everything we learned about the feature.\nMetrics for unreadable code You can use several metrics to measure your code\u0026rsquo;s readability. The ideal metric would be the time it takes to understand the code, but this isn\u0026rsquo;t easy to measure. Instead, you can use the following proxies:\nTime to fix a bug—Measure the time from when a developer starts working on a bug until the bug fix is ready for code review. Alternatively, measure the time from when a developer starts working on a bug until their first commit. A first commit is a good proxy for understanding the bug and starting to fix it.\nTime to add a new feature—Measure the time from when a developer starts working on a new feature until it is ready for code review.\nTime to onboard a new team member—Measure the time it takes for a new team member to make their first commit.\nCode style violations—Measure the codebase\u0026rsquo;s number of code style violations. Code style violations can be measured using linters or static analysis tools. Some examples of code style violations relevant to readability are:\nLong functions Long files Deeply nested control structures Poorly named variables, such as 1-character variable names Instead of measuring these style code violations, you can also enforce them in your CI pipeline. Most languages have linters that update your code to match a standard style. For example, Go has gofmt.\nHow to make your codebase more readable Readability is not a goal but a process. You can\u0026rsquo;t make your codebase readable overnight, but you can start making incremental improvements. Whenever you touch a piece of code, try to make it more readable.\nFix poor software structure One pattern we see frequently is that the functionality of a core feature is spread across multiple software modules. The first problem this creates is that the software developer trying to understand the feature has to discover all the modules that implement the feature. Often, this requires grepping the codebase for key names \u0026ndash; a tedious and error-prone process. The second problem is that the developer has to jump between files and directories to understand how the feature works, files that often have tons of other unrelated and distracting code.\nHard to understand feature due to poor software design Poor software structure often arises when we hurry to implement a feature and don\u0026rsquo;t consider future developers needing to make changes. This behavior is reactive software design—one developer reacts to the immediate need to implement a feature. Later, when implementing new features becomes almost impossible due to unreadable code, they react again by restructuring the code or rewriting old functionality from scratch. This process makes sense for prototypes or early products looking for product-market fit, but it is not sustainable for mature long-term software projects.\nOften, developers may not be able to create a good software design when they start working on a new feature because they don\u0026rsquo;t understand all its ramifications. However, they should restructure their work before moving on to the next task—the best time to improve code is when you have all the context in your head.\nWe can restructure the above code example to move all the feature\u0026rsquo;s functionality into one or two modules. This reorganization makes it easier to understand the feature because we have to look at a much smaller number of files and are not distracted by unrelated code.\nEasier to understand feature encapsulated in separate modules Refactor local code for understanding When entering a function, you should quickly understand what it does. The function code should be readable. If an engineer who first sees the function can\u0026rsquo;t understand it, it is too complex and should be refactored.\nLong functions are difficult to understand because they require the developer to simultaneously keep a lot of information in their head. Oftentimes, the function presents implementation details to the developer before they can grasp the big picture. This process is cognitively demanding and error-prone.\nInstead, we can refactor extended functions into smaller functions that each do one thing. This refactoring makes the code easier to understand because we can understand each small function in isolation. Hide complex logic in functions with descriptive names.\nIn addition, use descriptive names for variables. Good names make the code self-documenting and reduce the need for comments.\nAs an example of a function before and after refactoring, see this refactoring example gist. For a full explanation, you can jump to the refactoring section of the video below.\nFor more examples of common refactorings, see our article on top refactorings every software developer should know.\nUse unit and integration tests From a readability perspective, tests are a form of documentation. They show how the code is supposed to work. When reading a test, you can see how the code is supposed to behave in different scenarios.\nTests should also be readable. The same restructuring and refactoring principles apply to tests.\nAnother essential benefit of tests is that they allow developers to refactor code with confidence. When you refactor code, you can run the tests to ensure that the code still works as expected. Unfortunately, this means that when you want to make a change in legacy code without tests, you either have to write tests first or do a lot of manual testing to ensure that the code still works.\nUseful comments Comments should explain why the code is written the way it is, not what the code does. The code should be self-explanatory with descriptive variable and function names and encapsulated implementation details.\nSometimes, it is hard to tell the difference between \u0026ldquo;why\u0026rdquo; and \u0026ldquo;what,\u0026rdquo; so feel free to err on the side of commenting.\nYou can remove the comment if you renamed a variable or a function, and now the comment duplicates the code. One problem with comments is that they can get out of date, which is worse than no comments.\nFor example, before refactoring, you had this code:\n// figure out which declarations we should not delete, and put those into keepNames list keepNames := make([]string, 0, len(existingDecls)+len(fleetmdm.ListFleetReservedMacOSDeclarationNames())) for _, p := range existingDecls { if newP := incomingDecls[p.Name]; newP != nil { keepNames = append(keepNames, p.Name) } } keepNames = append(keepNames, fleetmdm.ListFleetReservedMacOSDeclarationNames()...) After refactoring, the comment is a duplicate and no longer needed. It is even worse in this case because we renamed the variable, but the comment still refers to the old name. The comment is not only a duplicate but also misleading:\n// figure out which declarations we should not delete, and put those into keepNames list namesToKeep := namesOfDeclarationsToNotDelete(existingDecls, enrichedDeclarations) Language features that make the code less readable Some language features can make the code less readable. We will give an example from Go because we are familiar with Go, but the same principles apply to other languages.\nGo nested functions Go supports nested functions like this:\nfunc outer(foo Foo, bar Bar) { inner := func(item Item) { // many lines // ... // of implementation details } // many lines // ... // of additional code for _, i := range something { inner(i) } // more code // ... return } Upon entering the function as a reader, the first thing you see is the inner function. The reader is presented with specific implementation details before understanding the big picture. Instead, the reader should know where the nested function is used before reading these implementation details.\nOne way to solve this issue is to forbid nested functions in your style guide. Always extract nested functions to the struct level or file level. However, this approach loses the benefits of closures and increases the number of functions at the struct/file level.\nWe hope that the Go team will improve the readability of nested functions in the future. For example, they could allow nested functions to be defined at the end of the function after the primary implementation:\nfunc outer(foo Foo, bar Bar) { // many lines // ... // of additional code for _, i := range something { inner(i) } // more code // ... return // nested functions func inner(item Item) { // many lines // ... // of implementation details } } Alternatively, IDE vendors can improve readability by entirely hiding nested functions by default.\nAdditional benefits of readable code As you improve the readability of your code, you will notice several side effects:\nMany bugs will be easier to spot Other developers will be less likely to interrupt you with questions about your code If your code is open source, you may get more contributions Make bigger improvements to your codebase with evolutionary architecture In the following article, we discuss how to make bigger improvements to your codebase with evolutionary architecture.\nFurther reading Recently, we showed how to easily track engineering metrics with GitHub Actions and Google APIs. We also wrote about measuring and improving the execution time of Go tests. In addition, we pointed out the issues with GitHub\u0026rsquo;s code review process. We also covered the common use cases of AI for today\u0026rsquo;s software developers. Watch Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-01-22T00:00:00Z","image":"https://victoronsoftware.com/posts/readable-code/readable-code-headline_hu_4105272c635fab02.png","permalink":"https://victoronsoftware.com/posts/readable-code/","title":"What is readable code and why is it important?"},{"content":" Server CPU and memory utilization Server errors Server API latency (response time) Database slow queries Database performance metrics What is software load testing? Software load testing is a type of performance testing that simulates real-world user load on a software application. Load tests usually run in a test environment identical to the production environment.\nThe goals of load testing may include:\nEnsure the application meets the required performance criteria Ensure the application performance did not degrade after changes Test a new feature\u0026rsquo;s performance before releasing it to production Identify bottlenecks in the application to reduce compute costs and/or risks Run chaos engineering performance experiments Load testing can be done manually or automatically. Many open-source and commercial tools are available to help you run load tests. Some features of load testing tools include:\nRecord and replay user interactions, including simulating unique users Simulate different user loads Monitor the application\u0026rsquo;s performance during the test Generate reports with performance metrics This article lists the key metrics you should gather during a software load test of your web application.\nServer CPU and memory utilization CPU utilization is the percentage of time the CPU is busy processing instructions, and memory utilization is the percentage of memory used by the server. Companies deploy multiple instances of the same application web server, and the load balancer distributes the user requests among them. These metrics are averages across all instances.\nHigh CPU or memory utilization can indicate a bottleneck in the application or server. It may also signal that the application needs to be scaled horizontally (add more instances) or vertically (increase the server\u0026rsquo;s resources).\nLow CPU or memory utilization may indicate that the application is over-provisioned, and infrastructure engineers could reduce resources to save costs.\nTypical expectations for CPU and memory utilization are:\nCPU utilization should be below 80% on average Memory utilization should be below 80% on average High CPU utilization during load test Server errors Server errors are error messages in the application logs or 5XX HTTP status codes. They can indicate that the application is not handling the load well, has a bug, or is misconfigured.\nError logs are a key debugging tool for developers. They can help identify the root cause of a functional or performance error and fix it. As such, developers must use error logs to report actual server errors and not just informational messages. For example, a 404 error is typically not a server error but a client error. A website user requesting a resource that does not exist is a common scenario. Client errors should be logged as informational messages or tagged appropriately to be excluded from the server error metric.\nAWS Logs Insights JSON error filter and sample error patterns The ideal number of server errors is zero. However, in practice, some errors are expected. For example, some startup or shutdown-related errors may occur if application servers are scaling up or down due to load. Note the expected errors in the test plan and adjust the error filter accordingly.\nServer API latency (response time) API latency is the time it takes for the server to respond to a request, measured in milliseconds. Typically, the business cares about user-facing API endpoints, such as the login, checkout, or search endpoints.\nAPI latency is a critical metric for user experience. High latency can lead to user frustration and abandonment.\nOne standard metric is the 95th percentile latency. This metric indicates the latency that 95% of the requests are faster than. It is a good indicator of the user experience because it filters out outliers.\nExample spike in latency during a load test experiment Telemetry tools such as OpenTelemetry can help you gather API latency metrics and correlate them with other metrics, such as server errors or CPU utilization.\nDatabase slow queries Query response time is the time it takes for the database to respond to a query. Slow queries can indicate that the query is not optimized or that the table needs an index.\nSlow queries can lead to high API latency and server errors. They can also lead to high CPU and memory utilization on the database server.\nTypically, we want to look at the average query response time multiplied by the number of queries per second for each query signature. This will identify the queries that have the most impact on database performance.\nThe list of slow queries should remain stable during a load test. If it changes, it may indicate a new unoptimized query or a new bug in the application.\nAWS RDS Performance Insights uses Average Active Sessions (AAS) as its slow query metric Database performance metrics Along with slow queries, we always gather the following database performance metrics:\nDatabase CPU utilization Just like the server, we monitor the database\u0026rsquo;s CPU utilization. The typical expectation is that CPU utilization should be below 80% on average.\nMemory utilization may not be as critical for the database as for the server. We expect the database to use as much memory as possible to cache data and speed up queries.\nDatabase threads running (sessions) Database threads running is the number of database connections actively processing queries. High thread counts can indicate that the database is under heavy load.\nThe number of threads should be at or below the number of CPUs on the database server.\nDatabase IO operations per second (IOPS) Database IOPS is the number of disk read and write operations the database performs per second. High IOPS can indicate that the database is not effectively caching data or that too many writes are occurring.\nIOPS should be in line with the database\u0026rsquo;s provisioned IOPS. If IOPS are consistently higher than provisioned, the database may need to be scaled up.\nAdditional metrics The following metrics may also be necessary. However, these additional metrics may be more situational than the above top 5 metrics.\nNetwork traffic Network traffic includes the number of bytes sent and received by the server. Typically, the data received by the server is the user\u0026rsquo;s request, and the data sent by the server is the response.\nHowever, in microservices architectures and servers with 3rd party integrations, our server may also make requests to other web servers.\nUser traffic is typically consistent from load test to load test. Traffic to other servers may change as engineers add new features. If the network traffic changes significantly, it may indicate a new bug in the application, such as application servers making too many requests to a 3rd party service.\nPerformance profile Many performance tools and modern programming languages can generate a performance profile. A performance profile is a breakdown of the time spent in each function of the application. It can help identify bottlenecks in the application code.\nIf code performance is a significant concern, take a performance profile during the load test and compare it to a baseline or the previous load test profile. If the profile changes significantly, it may indicate a new bug in the application or a new performance bottleneck.\nExample performance profile from Go pprof Database replication lag If the database is replicated, the replication lag is the time it takes for changes to be sent from the primary database and applied to the replica database. High replication lag can indicate that the replica is not keeping up with the primary database.\nHigh replication lag can lead to a bad user experience \u0026ndash; for example, if the user saves data, then immediately retrieves it and receives stale data.\nFurther reading Recently, we examined whether OpenTelemetry is useful for the average software engineer. We also looked at benchmarking performance of different Go serializers. Watch us discuss the software load testing performance metrics Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-01-15T00:00:00Z","image":"https://victoronsoftware.com/posts/software-load-testing/loadtest-fail_hu_aa30435f7ee9485d.png","permalink":"https://victoronsoftware.com/posts/software-load-testing/","title":"Top 5 metrics for software load testing performance"},{"content":" Create a CloudFront distribution Create a CloudFront key pair and add it to a key group Associate the key group with the CloudFront distribution Generate a signed URL using AWS SDKs What is CloudFront CDN? Amazon CloudFront is a content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. CloudFront is a popular choice for serving users worldwide with static assets, such as images, videos, and software package files.\nCloudFront uses S3 buckets, EC2 instances, and other AWS resources as origins to cache and serve content. When a user requests a file from a CloudFront distribution, CloudFront checks its cache for the file. If the file is not in the cache, CloudFront retrieves it from the origin and caches it for future requests.\nUsers around the world requesting data from their local Cloudfront CDN cache What are CloudFront signed URLs? CloudFront signed URLs grant access to private content served by CloudFront. By default, CloudFront distributions are public and serve content to anyone who requests it. However, your signed URLs can restrict access according to some of the following rules:\nsource IP address begin access time and/or expiration time Signed URLs are helpful when you want to serve private content to specific users or for a limited time. For example:\nServe paid content to customers who have purchased a subscription Share private documents with a specific group of users Provide temporary access to a file for a limited time Serve content to users without requiring them to log in A signed URL looks like a regular CloudFront URL but contains additional query parameters that specify the access restrictions. Depending on the limits you apply, a signed URL may be quite lengthy.\nhttps://d1nsa5964r3p4i.cloudfront.net/hello-world.txt?Expires=1736178766\u0026Signature=HpcpyniNSBkS695mZhkZRjXo6UQ5JtXQ2sk0poLEMDMeF063IjsBj2O56rruzk3lomYFjqoxc3BdnFqEjrEXQSieSALiCufZ2LjTfWffs7f7qnNVZwlkg-upZd5KBfrCHSIyzMYSPhgWFPOpNRVqOc4NFXx8fxRLagK7NBKFAEfCAwo0~KMCSJiof0zWOdY0a8p0NNAbBn0uLqK7vZLwSttVpoK6ytWRaJlnemofWNvLaa~Et3p5wJJRfYGv73AK-pe4FMb8dc9vqGNSZaDAqw2SOdXrLhrpvSMjNmMO3OvTcGS9hVHMtJvBmgqvCMAWmHBK6v5C9BobSh4TCNLIuA__\u0026Key-Pair-Id=K1HFGXOMBB6TFF\nHow to create CloudFront signed URLs You must have an AWS account and an S3 bucket with private content as a prerequisite.\nCreate a CloudFront distribution Open the CloudFront console. Choose Create Distribution. In the Origin domain section, choose your S3 bucket as the origin. In the Origin access section, select Origin access control settings (recommended) and click Create new OAC. In the Create new OAC modal, click Create. Choose one option in the WebApplication Firewall (WAF) section. Click Create Distribution to create the CloudFront distribution. In the yellow The S3 bucket policy needs to be updated banner, click Copy policy and then click Go to S3 bucket permissions to update policy. Under bucket Permissions \u0026gt; Bucket policy, click Edit and paste the copied policy. Click Save changes. Back in the CloudFront console, wait for the distribution to deploy. When the distribution is done deploying, the Last modified column will change from Deploying to a date and time. At this point, the CloudFront distribution will serve content from the S3 bucket to anyone who requests it. Signed URLs do NOT protect it until we set them up in the following steps. Test the distribution by accessing a file using the CloudFront URL.\nCreate a CloudFront key pair and add it to a key group The recommended method for signing URLs is using trusted key groups. A key group is a collection of public keys that CloudFront uses to verify signed URLs.\nUse OpenSSL to generate a private key and a public key: openssl genrsa -out private_key.pem 2048 openssl rsa -pubout -in private_key.pem -out public_key.pem Open the CloudFront console. In the side menu, choose Key management \u0026gt; Public keys. Click Create public key. Enter a name for the key, paste the contents of the public_key.pem file, and click Create public key. Remember the key ID for a later step. In the CloudFront side menu, choose Key management \u0026gt; Key groups. Click Create key group. Enter a name for the key group, select the public key you created, and click Create key group. Associate the key group with the CloudFront distribution Open the CloudFront console. Click on the CloudFront distribution you created. In the Behaviors tab, select a behavior and click Edit. In the Restrict viewer access section, select Yes, choose the key group you created, and Save changes. Now, the CloudFront URL will only serve content to users using a signed URL with the private key. Accessing content without a signed URL will result in an access denied 403 error.\n\u0026lt;Error\u0026gt; \u0026lt;Code\u0026gt;MissingKey\u0026lt;/Code\u0026gt; \u0026lt;Message\u0026gt;Missing Key-Pair-Id query parameter or cookie value\u0026lt;/Message\u0026gt; \u0026lt;/Error\u0026gt; Generate a signed URL using AWS SDKs You can generate signed URLs using the AWS SDKs for various programming languages. Amazon provides examples for several languages. We will show an example using the Go SDK.\nIn a new directory, create a Go project and add the AWS SDK as a dependency:\ngo mod init cloudfront-signed-urls go get github.com/aws/aws-sdk-go-v2/feature/cloudfront/sign@v1.8.3 Copy the private_key.pem file to the project directory and create a new Go file with the following code:\nRun the Go program to generate a signed URL:\n2025/01/06 08:52:46 Signed URL: https://d1nsa5964r3p4i.cloudfront.net/hello-world.txt?Expires=1736178766\u0026amp;Signature=HpcpyniNSBkS695mZhkZRjXo6UQ5JtXQ2sk0poLEMDMeF063IjsBj2O56rruzk3lomYFjqoxc3BdnFqEjrEXQSieSALiCufZ2LjTfWffs7f7qnNVZwlkg-upZd5KBfrCHSIyzMYSPhgWFPOpNRVqOc4NFXx8fxRLagK7NBKFAEfCAwo0~KMCSJiof0zWOdY0a8p0NNAbBn0uLqK7vZLwSttVpoK6ytWRaJlnemofWNvLaa~Et3p5wJJRfYGv73AK-pe4FMb8dc9vqGNSZaDAqw2SOdXrLhrpvSMjNmMO3OvTcGS9hVHMtJvBmgqvCMAWmHBK6v5C9BobSh4TCNLIuA__\u0026amp;Key-Pair-Id=K1HFGXOMBB6TFF The signed URL will expire in 1 hour.\nFurther reading Recently, we explained launchd agents and daemons on macOS. Previously, we set up a remote development environment for our web app. Watch how to start using CloudFront signed URLs Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-01-08T00:00:00Z","image":"https://victoronsoftware.com/posts/cloudfront-signed-urls/signed-url-headline_hu_f998f4ff0e3724f9.png","permalink":"https://victoronsoftware.com/posts/cloudfront-signed-urls/","title":"Secure private CDN content with CloudFront signed URLs"},{"content":"Introduction In this article, we will create a MySQL replica database. A MySQL replica is a read-only copy of the primary database, which is kept in sync with the main database using MySQL replication threads.\nThe steps we will follow are:\nSpin up MySQL source and replica databases Create a user for replication Obtain source binary log coordinates Configure replica DB and start replication To add a replica to an existing MySQL database, see the copy database from source and start replication manually section.\nTerminology: master-slave vs source-replica In database replication, the terms master-slave and source-replica are used interchangeably. In recent MySQL versions, the term source-replica is preferred over master-slave due to its more neutral connotation. Many keywords and variables in MySQL have recently been renamed to use neutral terms. We will use the terms source and replica in this article.\nWhat is database replication? Database replication is a process that allows data from one database server (the source) to be copied to one or more database servers (replicas). Replication is asynchronous, meaning the replica instance does not need to be connected to the source constantly. The replica can catch up with the source when either becomes available.\nDatabase replicas are used for:\nScaling read operations High availability Disaster recovery MySQL implements replication using the binary log. The source server writes changes to the binary log, and the replica server reads it and applies the changes to its database.\nCreate MySQL source and replica databases We will use Docker to create the MySQL source and replica databases. We will use the official MySQL Docker image. The source database will run on port 3308, and the replica database will run on port 3309. Both servers will have the database named test. We tested these instructions on MySQL 8.0.36, MySQL 8.4.3, and MySQL 9.1.0.\nWe run docker compose up using the following docker-compose.yml file:\nCreate a DB user for replication Replication in MySQL requires a user with the REPLICATION SLAVE privilege. We will create a user named replicator with the password rotacilper.\nConnect to the source database using the MySQL client:\nmysql --host 127.0.0.1 --port 3308 -uroot -ptoor Create the replicator user and grant the REPLICATION SLAVE privilege:\nCREATE USER \u0026#39;replicator\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;rotacilper\u0026#39;; GRANT REPLICATION SLAVE ON *.* TO \u0026#39;replicator\u0026#39;@\u0026#39;%\u0026#39;; FLUSH PRIVILEGES; Retrieve source binary log coordinates For the replica server to start replication, it needs to know the source\u0026rsquo;s binary log file and position. We can obtain this information using the MySQL client we opened in the previous step.\nSHOW BINARY LOG STATUS; In MySQL 8.0, use the SHOW MASTER STATUS command instead of SHOW BINARY LOG STATUS.\nThe output will look like this:\n+------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------+----------+--------------+------------------+-------------------+ | bin.000003 | 862 | | | | +------------+----------+--------------+------------------+-------------------+ 1 row in set (0.01 sec) We must remember the File and Position values for the next step.\nConfigure replica DB and start replication Now, we will connect to the replica database and configure it to replicate from the source database.\nmysql --host 127.0.0.1 --port 3309 -uroot -ptoor Use the CHANGE REPLICATION SOURCE TO command to configure the replica to replicate from the source. Replace SOURCE_LOG_FILE and SOURCE_LOG_POS with the values obtained in the previous step.\nCHANGE REPLICATION SOURCE TO SOURCE_HOST=\u0026#39;mysql_source\u0026#39;, SOURCE_PORT=3306, SOURCE_USER=\u0026#39;replicator\u0026#39;, SOURCE_PASSWORD=\u0026#39;rotacilper\u0026#39;, SOURCE_LOG_FILE=\u0026#39;bin.000003\u0026#39;, SOURCE_LOG_POS=862, GET_SOURCE_PUBLIC_KEY=1; SOURCE_HOST is the primary source\u0026rsquo;s hostname, which matches the docker service name. The GET_SOURCE_PUBLIC_KEY option is needed for caching_sha2_password authentication.\nFinally, start the replica:\nSTART REPLICA; The replica will now start cloning data from the source database. You can check the replication status using the SHOW REPLICA STATUS\\G command. Use this command to check for errors if you suspect something is wrong.\nWe can create a table with data on the source database and check if it is replicated to the replica database:\nUSE test; CREATE TABLE users (id INT PRIMARY KEY, name VARCHAR(255)); INSERT INTO users VALUES (1, \u0026#39;Alice\u0026#39;); Restore replication after an issue If the replica crashes and comes back up, it may be unable to resume replication from where it left off. If the replica stops replicating due to an error, first try to restart replication on the replica:\nSTOP REPLICA; START REPLICA; Check the replication status for errors using the SHOW REPLICA STATUS\\G command.\nIf the replica still does not replicate, we need to copy the database from the source and restart replication manually.\nCopy database from source and start replication manually Reset the replica:\nSTOP REPLICA; RESET REPLICA ALL; Optionally, drop and recreate the database on the replica:\nDROP DATABASE test; CREATE DATABASE test; If the source database still has the binary log files around from the first time we set up replication, we can redo the original steps using the same source log file and position. If not, we need to back up the source database and restore it on the replica.\nBackup the source database (on port 3308 with database name test):\nbash -c \u0026#39;mysqldump --host 127.0.0.1 --port 3308 -uroot -ptoor test | gzip -\u0026#39; \u0026gt; backup.sql.gz Restore the backup on the replica database (on port 3309):\nbash -c \u0026#39;gzip -dc - | mysql --host 127.0.0.1 --port 3308 -uroot -ptoor test\u0026#39; \u0026lt; backup.sql.gz Now, redo the following steps from above:\nObtain source binary log coordinates Configure replica DB and start replication As you can see, restarting replication after an issue can be more involved than just restarting the replica. Regular backups can help in such situations. When backing up the source database, make sure to include the binary log files, along with the corresponding binary log position. You can then use these files to restore or spin up a new replica while the source database is actively running.\nFurther reading on database scaling Recently, we wrote about database gotchas when scaling applications. One of the issues we summarized was optimizing a MySQL INSERT with subqueries. In the past, we encountered a memory issue with MySQL prepared statements when scaling applications. We also wrote about securing your MySQL Docker container for Zero Trust. Follow along with the MySQL source-replica video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-01-06T00:00:00Z","image":"https://victoronsoftware.com/posts/mysql-master-slave-replication/mysql-master-slave-replication_hu_b96bd16c41a6e112.png","permalink":"https://victoronsoftware.com/posts/mysql-master-slave-replication/","title":"Create a MySQL replica database in 4 short steps (2025)"},{"content":" Example of method overriding with embedded structs What is method overriding? Method overriding is a feature of object-oriented programming languages that allows a subclass to provide a specific implementation of a method already defined in its superclass. When a subclass overrides a method, the subclass\u0026rsquo;s method is called instead of the superclass\u0026rsquo;s method when the object is of the subclass type. Method overriding is an example of polymorphism, where the same method name can have different implementations depending on the object\u0026rsquo;s type.\nGo does not have classes, but it has structs and interfaces that can be used to achieve similar functionality.\nEmbedded structs Go favors composition over inheritance. Multiple smaller types can be combined to create a larger type. A clean way to do this is through embedded structs. When a struct embeds another struct, it inherits the embedded struct\u0026rsquo;s fields and methods. For example, consider the following code:\npackage main import \u0026#34;fmt\u0026#34; type MyPrint struct { i int } func (m* MyPrint) Do() { fmt.Printf(\u0026#34;Hello, World! %d\\n\u0026#34;, m.i) m.i++ } type MyComposedPrint struct { MyPrint } func main() { fmt.Println(\u0026#34;Base:\u0026#34;) var m MyPrint m.Do() m.Do() fmt.Println(\u0026#34;Composed:\u0026#34;) var s MyComposedPrint s.Do() s.Do() } This code outputs the following when run:\nBase: Hello, World! 0 Hello, World! 1 Composed: Hello, World! 0 Hello, World! 1 The composed struct forwards the method call to the embedded struct.\nMethod overriding with embedded structs To override a method in Go, you can define a method with the same name and parameters in the composed struct. For example, consider the following code:\npackage main import \u0026#34;fmt\u0026#34; type MyPrint struct { i int } func (m* MyPrint) Do() { fmt.Printf(\u0026#34;Hello, World! %d\\n\u0026#34;, m.i) m.i++ } type MyComposedPrint struct { MyPrint } func (m*MyComposedPrint) Do() { // call the parent method, similar to super.Do() in other languages m.MyPrint.Do() fmt.Printf(\u0026#34;Hello, Composed World! %d\\n\u0026#34;, m.i) m.i++ } func main() { fmt.Println(\u0026#34;Base:\u0026#34;) var m MyPrint m.Do() m.Do() fmt.Println(\u0026#34;Composed:\u0026#34;) var s MyComposedPrint s.Do() s.Do() } This code outputs the following when run:\nBase: Hello, World! 0 Hello, World! 1 Composed: Hello, World! 0 Hello, Composed World! 1 Hello, World! 2 Hello, Composed World! 3 MyComposedPrint.Do overrides MyPrint.Do. When deciding which method to call, Go first looks for the method in the type itself. If the method is not found, Go looks for the method in the embedded types.\nMethod overriding and interfaces Method overriding is often used in conjunction with interfaces. An interface defines a set of methods a type must implement to satisfy the interface. When a type implements an interface, it can be used wherever the interface is expected. An interface is another example of polymorphism in Go.\nOne typical pattern is overriding a method in a 3rd party library. For example, you can embed a type from a library and override a method to add additional functionality. However, you will still pass the new type in your code using the library\u0026rsquo;s original interface.\nFurther reading We recently described the differences between Go modules and packages. Previously, we explained how to properly unmarshal JSON null, set, and missing fields in Go. We also explained the difference between nil slice and empty slice in Go. Watch method overriding in Go video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2025-01-01T00:00:00Z","image":"https://victoronsoftware.com/posts/method-overriding-in-go/method-overriding-headline_hu_6ad0be7b0518e4ee.png","permalink":"https://victoronsoftware.com/posts/method-overriding-in-go/","title":"How to override methods in Go"},{"content":"Engineering metrics are essential for tracking your team\u0026rsquo;s progress and productivity and identifying areas for improvement. However, manually collecting and updating these metrics can be time-consuming and error-prone. In this article, we will show you how to automate the tracking of engineering metrics and visualize them in the Google Office suite.\nSome standard engineering metrics include:\nNumber of bugs Lead time for changes (or bug fixes) Code coverage Build/test success rate Deployment frequency Number of incidents Mean time to recovery Delivered story points and many more Engineering metrics can be further sliced and diced in various ways. For example, you can track bugs by severity or on a per-team basis.\nBuilding an engineering metrics tracker For our example metrics tracker, we will gather the number of GitHub open bugs for a team, update the numbers in a Google Sheet, automate the process with GitHub Actions, and display the data in Google Docs.\nAll the tools for this flow are freely available, and this process does not rely on costly third-party metrics-gathering services. We will use the Go programming language in our example.\nGathering the number of open bugs The GitHub API is a well-documented way to query issues in a repository.\nThere are also many quality client libraries for the API. We will use the go-github client.\nCreate a git repository and set up a new Go module. Here is our code snippet to get the number of open bugs:\nimport ( \u0026#34;github.com/google/go-github/v67/github\u0026#34; ) func getGitHubIssues(ctx context.Context) ([]*github.Issue, error) { githubToken := os.Getenv(\u0026#34;GITHUB_TOKEN\u0026#34;) client := github.NewClient(nil).WithAuthToken(githubToken) // Get issues. var allIssues []*github.Issue opts := \u0026amp;github.IssueListByRepoOptions{ State: \u0026#34;open\u0026#34;, Labels: []string{\u0026#34;#g-mdm\u0026#34;, \u0026#34;:release\u0026#34;, \u0026#34;bug\u0026#34;}, } for { issues, resp, err := client.Issues.ListByRepo(ctx, \u0026#34;fleetdm\u0026#34;, \u0026#34;fleet\u0026#34;, opts) if err != nil { return nil, err } allIssues = append(allIssues, issues...) if resp.NextPage == 0 { break } opts.Page = resp.NextPage } return allIssues, nil } The above code snippet uses a GITHUB_TOKEN environment variable to authenticate with the GitHub API. You can create a personal access token in your GitHub account settings. Later, we will show how to set this token in GitHub Actions automatically. The token is optional for public repositories but required for private repositories.\nThe code snippet queries the open issues in the public fleetdm/fleet repository with the labels #g-mdm, :release, and bug. Fleet\u0026rsquo;s MDM product team currently uses these labels for bugs in progress or ready to be worked on.\nUpdating the Google Sheets spreadsheet To update the Google Sheets spreadsheet, we will use the Google Sheets API with the Google\u0026rsquo;s Go client library.\nFor instructions on getting a Google Sheets API key, sharing the spreadsheet with a service account, and editing the spreadsheet using the API, see our previous article: How to quickly edit Google Sheets spreadsheet using the API.\nSee our integrated function to update the Google Sheets spreadsheet with the number of open bugs on GitHub.\nIn our example, we get the spreadsheet ID and the service account key from environment variables. When running locally, you must set the SPREADSHEET_ID and GOOGLE_SERVICE_ACCOUNT_KEY environment variables.\nspreadsheetId := os.Getenv(\u0026#34;SPREADSHEET_ID\u0026#34;) serviceAccountKey = []byte(os.Getenv(\u0026#34;GOOGLE_SERVICE_ACCOUNT_KEY\u0026#34;)) The glue code combining the above two functions is straightforward.\nfunc main() { ctx := context.Background() allIssues, err := getGitHubIssues(ctx) if err != nil { log.Fatalf(\u0026#34;Unable to get GitHub issues: %v\u0026#34;, err) } fmt.Printf(\u0026#34;Total issues: %d\\n\u0026#34;, len(allIssues)) err = updateSpreadsheet(len(allIssues)) if err != nil { log.Fatalf(\u0026#34;Unable to update spreadsheet: %v\u0026#34;, err) } } We can manually run our script to gather the metrics and update the Google Sheets spreadsheet. However, we want to automate this process so that the metrics are always up to date and we have a consistent historical record.\nAutomating the metric-gathering process with GitHub Actions GitHub Actions allows you to automate, customize, and execute your software development workflows in your GitHub repository. We will use GitHub Actions to run our script on a schedule and update the Google Sheets spreadsheet with the latest metrics.\nCreate a .github/workflows/update-spreadsheet.yml file in your repository with the following content:\nname: Update spreadsheet with latest metrics on: workflow_dispatch: # Manual schedule: - cron: \u0026#39;0 */12 * * *\u0026#39; # At 00:00 and 12:00 UTC env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # automatically generated GOOGLE_SERVICE_ACCOUNT_KEY: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_KEY }} SPREADSHEET_ID: ${{ secrets.SPREADSHEET_ID }} jobs: update-spreadsheet: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2 with: fetch-depth: 0 - name: Setup Go uses: actions/setup-go@3041bf56c941b39c61721a86cd11f3bb1338122a # v5.2.0 with: go-version-file: \u0026#39;go.mod\u0026#39; - name: Run run: go run main.go The above GitHub Actions workflow runs the main.go script every 12 hours. GitHub automatically generates the GITHUB_TOKEN secret. The GOOGLE_SERVICE_ACCOUNT_KEY and SPREADSHEET_ID secrets must be set up manually in the repository settings.\nThe workflow checks out the code, sets up Go, and runs the script. After pushing the workflow file to GitHub, you can manually run the workflow to test it.\nDisplay the metrics in Google Docs To see the metrics in Google Docs or Google Slides, you can copy and paste the relevant cells from the Google Sheets spreadsheet. This operation will create a one-way link from Google Sheets to the document. You can refresh the data by clicking Tools \u0026gt; Linked objects \u0026gt; Update All.\nFurther reading Recently, we explained how to measure unreadable code and turn it into clean code, as well as how to make incremental improvements to your codebase with evolutionary architecture. Previously, we showed how to reuse workflows and steps in GitHub Actions. We also covered measuring the execution time of Go tests. We also described inefficiencies in the GitHub code review process. Code on GitHub For the complete code, see the GitHub repository: github-metrics.\nWatch Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-12-26T00:00:00Z","image":"https://victoronsoftware.com/posts/track-engineering-metrics/engineering-metrics_hu_4b5d14f945936369.png","permalink":"https://victoronsoftware.com/posts/track-engineering-metrics/","title":"How to easily track engineering metrics"},{"content":" Get a Google Sheets API key Share a Google Sheets spreadsheet with the service account Edit a Google Sheets spreadsheet using the API When you need to edit a Google Sheets spreadsheet quickly, you can use the Google Sheets API. The API allows you to programmatically read, write, and update data in a Google Sheets spreadsheet. However, following the Google Sheets API documentation can be overwhelming. In this article, we will show you how to get a Google Sheets API key and edit a Google Sheets spreadsheet using the API.\nUser authentication (OAuth) vs. API key (JWT) The problem is that the Google API documentation focuses on the OAuth 2.0 user authentication flow. This flow is useful when you need to access Google Sheets on behalf of a user. For example, you\u0026rsquo;re creating your web app, and you need to read or write data in a Google Sheets spreadsheet owned by your web app user. The OAuth standard allows Google to authenticate the user and authorize your app to access the user\u0026rsquo;s data without exposing the user\u0026rsquo;s credentials to your web app. The OAuth flow interacts with three parties \u0026ndash; the user, your web app, and Google.\nIn our case, we want to access a specific Google Sheets spreadsheet programmatically without user interaction. We can use the API key (JWT) authentication method. JWT stands for JSON Web Token, a standard for securely transmitting information between two parties. This method allows us to access Google Sheets programmatically without user interaction. The API key (JWT) method interacts with two parties: your app and Google.\nGet a Google Sheets API key To get a Google Sheets API key, follow these steps:\nGo to the Google Cloud Console. Create a new project or select an existing project. In the new project, go to APIs \u0026amp; Services and enable the Google Sheets API. After enabling it, you should see it in the Enabled APIs \u0026amp; services list. Go to APIs \u0026amp; Services \u0026gt; Credentials and create a new Service Account. This account does not need any optional permissions. Create a new JSON key for the service account. After creating the key, the JSON file will be automatically downloaded to your computer. This file contains the credentials for your service account. Keep it secure. For example, we received axial-paratext-444915-f9-8ef1de636587.json with the following content: { \u0026#34;type\u0026#34;: \u0026#34;service_account\u0026#34;, \u0026#34;project_id\u0026#34;: \u0026#34;axial-paratext-444915-f9\u0026#34;, \u0026#34;private_key_id\u0026#34;: \u0026#34;8ef1de636587238a028addaa8be9dbdf1d406420\u0026#34;, \u0026#34;private_key\u0026#34;: \u0026#34;-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDGFXmEc6VK0TO9\\n2E/LDel4gTYl1u8uZGtX16B7Lo4ufM7ics3h9Gyi1lJMcGrHruGEzatDeTRclILd\\nLhLwrckfl3IF9MIsqwaEkHk7YnUXj9zGl+v8LTGJL0ycQ9hVdoD6cCOOAmghLj8F\\n9Sl6KQ5PHGbBUUL4qi8uExKY4tQOrqol1Pi3RPpAOCR6BLC/ZFPp+4e4HRhF+DMD\\nI1QX8QwPit9XdIomnZPUL5sGD+q4cp1gHLBuBp2ehyiFI5MGhgzvCIQzaTExw7GK\\nqrjYMjBKXaFRqGpZJWJMdVmGGHpmZLCL/wQmhujlThrF0FO8BHAGriAvUgDbH7m/\\nMzGRk/KzAgMBAAECggEAE7LwBkWP8xxR9nfMG6fzB3pmHaY93BG9gRtfCNEM77+W\\nvXtoUSfDJACHZ7WoUNpp8BCaDxg/JlPYndFmrcvCnCMuAjygkNujRsytWcQFXAYB\\nETjrjYUbD4cGKeYvXfRuiDldt9Iyc9ZLCzch3FW36BMtft0reVpHXeAksdKg/yKf\\nhM4jw3tQMu5JR3trLHtqwaA8VUav7I2Qn8nxEbB/0+AUatqpDOp/hQNTN7MGZ/i7\\n4V0538U7C9RYDzk9hBBT2/IegGixlL0lX2V6LjYlRcEgC0PLuKF7gM/RRnorNPnv\\nHfHxyZt6/MyI8RLRwwv05ZSITaOj66lXxReVsMXxhQKBgQD+vJPrvKD7mGiXRQto\\nh1LJqPcyknzLzxf2OX5vZyF8asdroU0sRy8pYYHy8JCPlkOJ0fj9kq6R79W+rmn3\\npFkvwRY9dUcJLpoMAMEfO4wQp3QKpxdkjMS8xGcEVOIZacAHCof7uUwrHUUcqRIq\\nwrgZcj5P8ZjwsLmuqLNeXqFYNwKBgQDHEPf4PCyjieF+aGvaOPLfn/lRBbEQGlrg\\n9Z09UXpxcW39RMq7MkS+U9m88Kn9MsEK3umJdP+s5m8ddVdIVgZLj96Ufn52RzRT\\ne8crSjCVC3oQaloScvOBSQA1Z3Bn+QstIko042i2qTNJWMArdCJe9uRbwL1hqEvx\\n+LtNPniDZQKBgQDX4g9maFzx/G8fS+doNc8mkmi01kqnGyJOjNknJnrNi1zoTTIv\\nBUDly/oqXk/VMF6ajXV7yPTjPyOhTwUFV6Yx/2yOtzZ1hKYO6BDDHF8Ouitw37zG\\nfTo6VCSOGjXnnaSdEwK9hYMUwuCQcoSv8oe9IQHIFJMt4EfsypIAtyf7rwKBgFtC\\ntzvRcnGC+6K1AoTnyMimkWkIn/UO8Azj7TM4UFcDtnX+/KY3VHahAFhzSKswgnmW\\nWiBPSAufFN+/dMVP0tD/Yv5Ww2k8GYwQWe3JtF4QBeTSrPp6QpJJwlO5WToBXZNS\\nfgyjGNVs2ntMucTyF/PLYkOCKBBGVJLZAh1Wf29VAoGADf7a1l8kDKgokm6pc4qG\\nzb97GMk1CpE0dGl31dvx2ilckDVP354yfWEwVXWWVfVSq/LQdJVgkdArYbAPdsPb\\nYuUfNwXMSp/OjmEL2QyC2zRm+2ZZZt5bcnPRbYETzb2An8kDYX49vwgBLJXpLOmt\\nlCvxUDyoASHgAMu+OlqHIh4=\\n-----END PRIVATE KEY-----\\n\u0026#34;, \u0026#34;client_email\u0026#34;: \u0026#34;example@axial-paratext-444915-f9.iam.gserviceaccount.com\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;114906617333001451487\u0026#34;, \u0026#34;auth_uri\u0026#34;: \u0026#34;https://accounts.google.com/o/oauth2/auth\u0026#34;, \u0026#34;token_uri\u0026#34;: \u0026#34;https://oauth2.googleapis.com/token\u0026#34;, \u0026#34;auth_provider_x509_cert_url\u0026#34;: \u0026#34;https://www.googleapis.com/oauth2/v1/certs\u0026#34;, \u0026#34;client_x509_cert_url\u0026#34;: \u0026#34;https://www.googleapis.com/robot/v1/metadata/x509/example%40axial-paratext-444915-f9.iam.gserviceaccount.com\u0026#34;, \u0026#34;universe_domain\u0026#34;: \u0026#34;googleapis.com\u0026#34; } Share a Google Sheets spreadsheet with the service account Share the spreadsheet with the service account email address to allow the account to access a Google Sheets spreadsheet. In our case, the service account email is example@axial-paratext-444915-f9.iam.gserviceaccount.com.\nNote the spreadsheet ID from the URL. For example, in the URL https://docs.google.com/spreadsheets/d/1QCtnB6MXfFJLZsBE1E2vq5FxKBgh1Q0s727wRxFkmX4/edit, the spreadsheet ID is 1QCtnB6MXfFJLZsBE1E2vq5FxKBgh1Q0s727wRxFkmX4.\nEdit a Google Sheets spreadsheet using the API Now that we have the Google Sheets API key and editor permissions for the target spreadsheet, we can edit it using the API. For this example, we will use the Go programming language.\nIn an empty directory, create a Go project and get the necessary dependencies:\ngo mod init google-sheets-api go get golang.org/x/oauth2@v0.24.0 go get google.golang.org/api@v0.211.0 Copy the JSON key file to the project directory and rename it to credentials.json.\nCreate a new Go file, main.go, with the following content:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;golang.org/x/oauth2/google\u0026#34; \u0026#34;google.golang.org/api/option\u0026#34; \u0026#34;google.golang.org/api/sheets/v4\u0026#34; ) const spreadsheetId = \u0026#34;1QCtnB6MXfFJLZsBE1E2vq5FxKBgh1Q0s727wRxFkmX4\u0026#34; func main() { ctx := context.Background() serviceAccountKey, err := os.ReadFile(\u0026#34;credentials.json\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Unable to read client secret file: %v\u0026#34;, err) } cfg, err := google.JWTConfigFromJSON(serviceAccountKey, sheets.SpreadsheetsScope) if err != nil { log.Fatalf(\u0026#34;Unable to parse client secret file to config: %v\u0026#34;, err) } client := cfg.Client(ctx) srv, err := sheets.NewService(ctx, option.WithHTTPClient(client)) if err != nil { log.Fatalf(\u0026#34;Unable to retrieve Sheets client: %v\u0026#34;, err) } readRange := \u0026#34;Sheet1!A2:B2\u0026#34; resp, err := srv.Spreadsheets.Values.Get(spreadsheetId, readRange).Do() if err != nil { log.Fatalf(\u0026#34;Unable to retrieve data from sheet: %v\u0026#34;, err) } if len(resp.Values) == 0 { fmt.Println(\u0026#34;No data found.\u0026#34;) } else { fmt.Println(\u0026#34;Date, Value:\u0026#34;) for _, row := range resp.Values { fmt.Printf(\u0026#34;%s, %s\\n\u0026#34;, row[0], row[1]) } } } Replace the spreadsheetId constant with the ID of your target spreadsheet.\nThis code authenticates with the Google Sheets API using the service account key and reads the data from cells A2 and B2 in the Sheet1 sheet of the target spreadsheet.\nUpdate the dependencies and run the program:\ngo mod tidy go run main.go The result should look like:\nDate, Value: 2024-12-16 10:00:00, 10 To write data to a Google Sheets spreadsheet, use the spreadsheets.Values.Update and Spreadsheets.BatchUpdate methods. For example, the following modified code inserts a new row above other rows with the current date and an incremented value:\nWe can review the spreadsheet to verify that our code added the new row.\nNext steps Automate tracking of engineering metrics. Further reading Recently, we covered how to set up a remote development environment. Previously, we showed how to build a webhook flow with Tines. Watch how to edit Google Sheets spreadsheet using the API Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-12-18T00:00:00Z","image":"https://victoronsoftware.com/posts/google-sheets-api/google-sheets-headline_hu_2dc0f36bd926bc9c.png","permalink":"https://victoronsoftware.com/posts/google-sheets-api/","title":"How to quickly edit Google Sheets spreadsheet using the API"},{"content":" How to find the plist file for a running process Create and edit .plist files with PlistBuddy What is launchd? launchd is a macOS system service manager that starts, stops, and manages daemons, agents, and other processes. It is the first process the kernel starts and is responsible for starting all other processes on the system.\nIf you go to Activity Monitor on your Mac and View \u0026gt; All Processes, Hierarchically, you will see that all processes are children of launchd.\nWhat are launchd agents and daemons? launchd can start and manage agents and daemons.\nDaemons Daemons are background processes that run without a user interface. They typically start at boot time and run continuously in the background. One example of a daemon is Apple\u0026rsquo;s timed time synchronization daemon, which maintains system clock accuracy by synchronizing the clock with reference clocks over the network. Another example is a device management daemon, such as Fleet\u0026rsquo;s orbit, which manages the device\u0026rsquo;s configuration and security settings.\nAgents Agents are similar to daemons but run in the context of a user session. They are started when a user logs in and can interact with the user interface. Agents are helpful for tasks that need to run in the background but also need to communicate with the user. For example, a security agent can check the system\u0026rsquo;s state and notify the user if they fail a corporate security policy.\nAgents may or may not have a user interface. Many 3rd party agents run in the background and provide a menu bar icon to configure the agent\u0026rsquo;s behavior.\nHow are agents and daemons configured with plist? launchd uses property list (.plist) files to define the configuration of agents and daemons. These files specify the program to run, the arguments to pass, the environment variables to set, and other settings.\nHere is an example of a .plist file for a launchd daemon:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;EnvironmentVariables\u0026lt;/key\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;ORBIT_ENROLL_SECRET_PATH\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;/opt/orbit/secret.txt\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ORBIT_FLEET_URL\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;https://dogfood.fleetdm.com\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ORBIT_ENABLE_SCRIPTS\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;true\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ORBIT_ORBIT_CHANNEL\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;stable\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ORBIT_OSQUERYD_CHANNEL\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;stable\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ORBIT_UPDATE_URL\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;https://updates.fleetdm.com\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ORBIT_FLEET_DESKTOP\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;true\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ORBIT_DESKTOP_CHANNEL\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;stable\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ORBIT_UPDATE_INTERVAL\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;15m0s\u0026lt;/string\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;key\u0026gt;KeepAlive\u0026lt;/key\u0026gt; \u0026lt;true/\u0026gt; \u0026lt;key\u0026gt;Label\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;com.fleetdm.orbit\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ProgramArguments\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;string\u0026gt;/opt/orbit/bin/orbit/orbit\u0026lt;/string\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;key\u0026gt;RunAtLoad\u0026lt;/key\u0026gt; \u0026lt;true/\u0026gt; \u0026lt;key\u0026gt;StandardErrorPath\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;/var/log/orbit/orbit.stderr.log\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;StandardOutPath\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;/var/log/orbit/orbit.stdout.log\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ThrottleInterval\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;10\u0026lt;/integer\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt; The typical locations for agent and daemon .plist files are:\nType Location User Agents ~/Library/LaunchAgents Global Agents /Library/LaunchAgents System Agents /System/Library/LaunchAgents Global Daemons /Library/LaunchDaemons System Daemons /System/Library/LaunchDaemons Note: In rare cases, the .plist files may be located in other directories or missing entirely.\nHow to view the contents of a .plist file .plist files come in several formats, including binary, XML, and JSON.\nYou can view the contents of a .plist file using the plutil (property list utility) command. For example:\nplutil -p /System/Library/LaunchDaemons/com.apple.analyticsd.plist plutil can also convert between different .plist formats. For example, to convert a binary .plist file to XML, run:\ncp /System/Library/LaunchDaemons/com.apple.analyticsd.plist my.plist plutil -convert xml1 my.plist Can I use a .plist file for cron-like scheduling? Yes, you can use launchd to schedule tasks in a .plist file. launchd is the recommended alternative to cron on macOS. The StartCalendarInterval key specifies when the task should run. For example, to run a task every day at 5 AM, you can add the following to your .plist file:\n\u0026lt;key\u0026gt;StartCalendarInterval\u0026lt;/key\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;Hour\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;5\u0026lt;/integer\u0026gt; \u0026lt;key\u0026gt;Minute\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;0\u0026lt;/integer\u0026gt; \u0026lt;/dict\u0026gt; How to find the plist file for a running process Suppose you identified a process running on your Mac from Activity Monitor and want to find the .plist file that started it. The process should be a child of launchd.\nTo find the identifier of a running process, you can use the launchctl command. The launchctl list command lists all agents and daemons started by the user, while the sudo launchctl list lists all agents and daemons started by the system.\nFor example, to find the identifier of the process with PID 62303, run:\n( /usr/bin/sudo launchctl list; launchctl list ) | grep 62303 The output will show the identifier, such as:\n62303 0 com.fleetdm.orbit You can now look in the standard locations for the com.fleetdm.orbit.plist file. Alternatively, you can use the launchctl dumpstate command to dump the state of all launchd jobs, including the .plist files that started them. For example, in a macOS system with Fleet\u0026rsquo;s orbit running, you can run:\nlaunchctl dumpstate | grep -B 1 -A 4 -E \u0026#34;active count = [1-9]\u0026#34; | grep com.fleetdm.orbit And the output will show the path to the .plist file:\nsystem/com.fleetdm.orbit = { path = /Library/LaunchDaemons/com.fleetdm.orbit.plist You can now view the contents of the .plist file to understand how the process was started.\nplutil -p /Library/LaunchDaemons/com.fleetdm.orbit.plist Create and edit .plist files with PlistBuddy PlistBuddy is a powerful built-in macOS tool for creating and editing .plist files. You can use it to automate the creation and modification of launchd agents and daemons.\nYou can create and edit .plist files using the PlistBuddy. For example, to create a new .plist file with a key-value pair, run:\n/usr/libexec/PlistBuddy -c \u0026#34;Add :Label string com.fleetdm.orbit\u0026#34; com.fleetdm.orbit.plist To edit an existing .plist file, use the -c flag with the Set command. For example, to change the above Label key to com.fleetdm.orbit2, run:\n/usr/libexec/PlistBuddy -c \u0026#34;Set :Label com.fleetdm.orbit2\u0026#34; com.fleetdm.orbit.plist Run /usr/libexec/PlistBuddy --help for more information on using PlistBuddy.\nFurther reading Recently, we showed two ways to turn a script into a macOS install package. Previously, we explained how to configure mTLS using the macOS keychain. We also covered how to create signed URLs with AWS CloudFront. Watch the video on launchd agents and daemons, and how to find the plist file for a running process Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-12-11T00:00:00Z","image":"https://victoronsoftware.com/posts/macos-launchd-agents-and-daemons/macos-agents-and-daemons_hu_b2be6db1d8fc0835.png","permalink":"https://victoronsoftware.com/posts/macos-launchd-agents-and-daemons/","title":"What are launchd agents and daemons on macOS?"},{"content":"What is a Go package? A Go package is a collection of Go source files in the same directory that are compiled together. It can contain functions, types, and variables. Go packages organize code and provide a way to share code between different program parts.\nBy convention, the package name is the same as the last element of the import path. For example, the package name for the fmt package in the fmt directory is fmt. You can name the package differently from the directory name, but it is not recommended.\nWhen a Go application is compiled, the Go compiler compiles all the packages imported by the main package. The main package contains the main function and is the entry point of the program. However, all the imported packages can be compiled separately as well, even if they do not contain the main function. That\u0026rsquo;s what happens when we run unit tests for all packages in a Go project — each package is compiled separately and can be tested independently.\nThe directory structure does not have to match the package dependencies. For example, given the following directory structure:\nservice/ service.go api/ api.go db/ db.go The packages service, api, and db can be completely independent. The service package does not include the api.go and db.go files because they are in a different directory. Go\u0026rsquo;s package dependency graph has no relation to the directory structure.\nWhat is a Go module? A Go module, introduced in Go 1.11, is a collection of Go packages that are versioned together. A Go module is defined by a go.mod file located at the module\u0026rsquo;s root. The go.mod file contains the module name and the versions of the dependencies the module uses.\nA Go module can contain multiple packages, and the packages do not need to be related to each other.\nSample project with two modules A Go module can exist at any level of the directory structure, even nested within another module. The Go toolchain treats all Go modules as independent entities, regardless of their location in the directory structure.\nHow to use multiple modules in one Go workarea 1. Create a new Go workarea: mkdir -p go-modules cd go-modules go mod init example.com/go-modules 2. Create a simple Go file in the root of the workarea: package main import \u0026#34;fmt\u0026#34; func main() { fmt.Printf(\u0026#34;Hello, world.\\n\u0026#34;) } 3. Create a new Go module in a subdirectory: mkdir -p adder cd adder go mod init example.com/go-modules/adder With the following Go file in the adder directory:\npackage adder func Add(a, b int) int { return a + b } 4. Use the adder package in the main package: You cannot simply import the adder package in the main package because they are in different modules.\nOne way to use the adder package is to publish it and use go get to download it as the main dependency. However, this is impractical for local development and doesn\u0026rsquo;t make sense when we explicitly want to use multiple modules in one repository.\nThe proper way is to use a workspace, declared in the go.work file, introduced in Go 1.18.\nIn the top level of the workarea, create a go.work file with all the modules in the subdirectories:\ngo work use -r . The resulting go.work file will look like this:\ngo 1.23.2 use ( . ./adder ) Now, you can import the adder package in the main package:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;example.com/go-modules/adder\u0026#34; ) func main() { fmt.Printf(\u0026#34;Hello, world.\\n\u0026#34;) fmt.Print(myadder.Add(1, 2)) } Should I use multiple Go modules in one repository? Generally, it is not recommended to use multiple Go modules in one repository. However, there are some cases where it makes sense, like:\nTemporarily pull in a third-party module to add a feature before this feature is merged upstream. Work on multiple interdependent modules that can be versioned and released independently. Usually, it is better to use packages within a single module to organize and decouple code.\nFurther reading Read the follow-up article on how to find package dependencies of a Go package. Recently, we covered method overriding in Go. We also wrote about using the staticcheck linter on a large Go project. Previously, we described how to use Go to unmarshal JSON null, set, and missing fields. We also published an article on accurately measuring the execution time of Go tests. Multiple modules in one Go project on GitHub Example Go project with multiple modules\nWatch an explanation of Go modules and packages, along with an example of using multiple modules in one repository Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-12-04T00:00:00Z","image":"https://victoronsoftware.com/posts/go-modules-and-packages/go-modules-and-packages_hu_85866624f192ce69.png","permalink":"https://victoronsoftware.com/posts/go-modules-and-packages/","title":"Go modules and packages — which one to use and when"},{"content":"What is a VLAN? VLAN (Virtual Local Area Network) technology allows you to create multiple isolated networks on a single physical network. For example, a single ethernet wire or a WLAN (wireless LAN) can support multiple VLANs. VLANs improve network security, performance, and scalability.\nHow does VLAN improve network security? VLAN improves network security by isolating devices into separate networks. This isolation prevents devices in one VLAN from communicating with devices in another VLAN. For example, you can create a separate VLAN for your IoT (Internet of Things) devices, such as smart light bulbs and thermostats, to prevent them from accessing your primary network. You can also create a separate VLAN for guest devices to prevent them from accessing your main network and other VLANs.\nFor example, if a hacker gains access to a device in the IoT VLAN, they won\u0026rsquo;t be able to access devices in the office VLAN or the guest VLAN. This isolation limits the damage that a hacker can do to your network.\nHow does VLAN work? VLAN works by adding a VLAN tag to each network packet. The VLAN tag contains the VLAN ID, which identifies the VLAN to which the packet belongs. Network switches use the VLAN tag to forward packets only to devices in the same VLAN. Routers can route packets between different VLANs based on their VLAN tags.\nHow to set up VLANs in your home network Unfortunately, setting up a VLAN in your home network is not as simple as flipping a switch. Multiple parts of your home network need to be configured, and some older or cheaper hardware, such as no-configuration network switches, may not support VLANs.\nSelecting VLAN tags and IP ranges Before configuring VLANs, decide how many VLAN tags you need and what each tag will represent. Also, determine what IP ranges will map to each VLAN. Some people map the VLAN ID to the third octet of the IP address. For example, VLAN 333 may use the IP range 10.0.333.0/24.\nOn the other hand, there is a security argument for using random VLAN IDs. If a hacker gets access to your network, they won\u0026rsquo;t know what each VLAN ID represents and may even have difficulty figuring out which VLAN IDs are active. This security approach is often referred to as security through obscurity.\nSome common VLANs are:\nOffice IoT Guest Media Router interfaces The network router interface is the first place to configure VLANs. A router interface is a physical or virtual router port connecting to a network. So, you must configure the router interface to support your VLAN-selected tags.\nDHCP Dynamic Host Configuration Protocol (DHCP) is a network protocol that automatically assigns IP addresses to devices on a network.\nYou need to configure a DHCP server to assign IP addresses to devices in each VLAN. You can use the same DHCP server for all VLANs but must configure it to assign IP addresses from different ranges for each VLAN.\nFirewall You need to configure firewall rules for each VLAN to control what traffic is allowed in and out of the VLAN. For example, you may not allow devices on the Guest VLAN to access devices on the other VLANs.\nBelow is an example of our firewall rules for the GUEST VLAN.\nThe rules allow access to our local DNS server to block inappropriate content. The rules block all private networking IPs (as defined by RFC 1918) except the VLAN\u0026rsquo;s subnet.\n10.0.0.0/8 (10.0.0.0 - 10.255.255.255) 172.16.0.0/12 (172.16.0.0 - 172.31.255.255) 192.168.0.0/16 (192.168.0.0 - 192.168.255.255) Network switch You need to configure the VLAN tags for each port on the network switch. For example, you may configure port 1 to be part of the Office VLAN and port 2 to be part of the IoT VLAN. A single port can be part of multiple VLANs, often required for a wireless access point.\nBelow is an example of the network switch configuration for our GUEST VLAN.\nThe tagged ports include three wireless access points and the router port.\nWireless Access Point (WAP) You need to configure the SSIDs (Service Set Identifiers) for each VLAN on your wireless access point. In our case, we created a WLAN for each VLAN—Office, IoT, and Guest. Each WLAN is associated with a VLAN tag, which we set in the advanced options of the WLAN configuration, as shown below.\nGuest network Before adding a VLAN for our guest network, we used the \u0026ldquo;Guest Mode\u0026rdquo; feature on our WAP (Wireless Access Point). This feature was secure because it isolated guest devices from our primary network. However, the user experience for our guests was terrible.\nThe guest network directed users to a captive portal before granting them Internet access. Some child guests could not access the captive portal due to parental device restrictions. Guests' devices also had trouble reconnecting to the guest network on a subsequent visit \u0026ndash; they were not automatically reconnected.\nSwitching to a VLAN-based guest network significantly improved the user experience.\nHow to specify VLAN on a wired connection A single wired ethernet connection may be part of multiple VLANs. You can connect your computer to different VLANs for testing or security reasons.\nOn a wired connection, you can specify the VLAN ID in your device\u0026rsquo;s network settings. For example, you can add a virtual interface with a specific VLAN tag on macOS.\nDebugging notes While setting up our VLANs, we encountered the issue of our computer not getting an IP address from the DHCP. After reviewing the settings on our router and switch, we found that our settings did not save for some reason. Make sure to reload your settings after making changes to ensure they stick.\nFurther reading In the past, we discussed how to set up a virtual router. We also covered how to create an IPv6-only linux server. Watch us discuss what is a VLAN and why you need it in your home network Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-11-27T00:00:00Z","image":"https://victoronsoftware.com/posts/why-you-need-vlan/vlan-house_hu_67f10a9ee27478b2.png","permalink":"https://victoronsoftware.com/posts/why-you-need-vlan/","title":"What is a VLAN and why you need it in your home network"},{"content":" Obtain VPN connection details Point your computer to the remote Active Directory DNS server Join your computer to the Active Directory domain Log in with your Active Directory credentials What is Active Directory? Active Directory is a directory service developed by Microsoft for Windows domain networks. It provides authentication and authorization services and a framework for organizing and managing resources in a networked environment. Active Directory stores information about users, computers, and other network objects, making managing and securing your network easier.\nActive Directory runs on Windows Server and is the central component of many Windows-based networks. It is the central piece for a variety of services, including:\nCertificate Services (AD CS) Lightweight Directory Services (AD LDS) Federation Services (AD FS) Rights Management Services (AD RMS) and many others Why connect to a remote Active Directory server? With the rise of remote work and distributed teams, you may need to connect your home computer to a remote Active Directory server. This connection allows you to access network resources, authenticate with your company\u0026rsquo;s domain, and use services that rely on Active Directory.\nThe recommended way to connect to a remote Active Directory server is through a VPN (Virtual Private Network). A VPN creates a secure connection between your computer and the remote network, allowing you to access resources as if you were physically connected.\nSteps to connect to a remote Active Directory server 1. Obtain VPN connection details Contact your IT department or network administrator to obtain the VPN connection details. You will need the following information:\nVPN server address VPN type (e.g., PPTP, L2TP/IPsec, OpenVPN) VPN username and password Any additional settings or requirements, such as a private key or certificate For our example, we are using WireGuard, a modern VPN protocol known for its simplicity and security.\nNote: The Allowed IPs field above specifies which IP addresses will be routed through the VPN. Make sure to include the IP addresses of the remote Active Directory server. Also, ensure the IP addresses do not conflict with your local network.\nInstall the VPN client on your computer and activate the VPN connection using the provided details.\nTest the VPN connection by pinging the remote Active Directory server.\nPS C:\\Users\\victor\u0026gt; ping 10.98.1.1 Pinging 10.98.1.1 with 32 bytes of data: Reply from 10.98.1.1: bytes=32 time=155ms TTL=127 Reply from 10.98.1.1: bytes=32 time=156ms TTL=127 Reply from 10.98.1.1: bytes=32 time=156ms TTL=127 Reply from 10.98.1.1: bytes=32 time=156ms TTL=127 Ping statistics for 10.98.1.1: Packets: Sent = 4, Received = 4, Lost = 0 (0% loss), Approximate round trip times in milli-seconds: Minimum = 155ms, Maximum = 156ms, Average = 155ms 2. Point your computer to the remote Active Directory DNS server To use Active Directory, your computer must know where to find the domain controller. This information is stored in the DNS (Domain Name System) settings.\nSince we are using a VPN to connect to the remote Active Directory server, we need to update the DNS settings of our VPN connection.\nIn the VPN settings above, we specified the DNS server as part of the WireGuard configuration.\nDNS = 10.98.1.1 Alternatively, we can manually set the DNS server for the VPN connection in Control Panel \u0026gt; Network and Sharing Center \u0026gt; your VPN connection \u0026gt; Properties \u0026gt; Networking \u0026gt; Internet Protocol Version 4 (TCP/IPv4) \u0026gt; Properties.\n3. Join your computer to the Active Directory domain Open Control Panel \u0026gt; System and Security \u0026gt; System \u0026gt; Domain or workgroup \u0026gt; Change\u0026hellip;.\nEnter the domain name provided by your IT department. You can also change your computer name if necessary.\nClick OK and enter your domain credentials when prompted. You must have permission from Active Directory to join a computer to the domain.\nAfter a few seconds, you should see a message indicating that your computer has successfully joined the domain.\nYou must restart your computer for the changes to take effect.\n4. Log in with your Active Directory credentials Your VPN connection must be active to log in with your Active Directory credentials after joining the domain. Some VPN clients allow you to connect before logging in to Windows. This feature ensures that your computer can reach the domain controller during the login process.\nIf your VPN client does not support connecting before logging in, you may need to log in with a local account first and then connect to the VPN. Then, you can switch users and login with your Active Directory credentials.\nAdditional information The local computer caches credentials for Active Directory users. You can log in to your computer with your Active Directory credentials on subsequent logins, even when you are not connected to the domain. For example, you can log in and then connect to the VPN.\nAfter joining the domain, we found that our local computer refused SSH connections from other local computers. We resolved this issue by allowing SSH access in Active Directory settings.\nFurther reading Recently, we covered how to test a Windows NDES SCEP server. Previously, we explained how to code sign a Windows application. Watch how to connect to remote Active Directory server Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-11-20T00:00:00Z","image":"https://victoronsoftware.com/posts/connect-to-remote-active-directory/connect-to-ad-headline_hu_34208139348c5ed8.png","permalink":"https://victoronsoftware.com/posts/connect-to-remote-active-directory/","title":"How to connect to remote Active Directory server in 4 steps"},{"content":" Create a script-only install package using the pkgbuild command Create a script-only install package using the Packages app What is a macOS install package? A macOS install package is a file that contains the files and scripts needed to install an application on a macOS system. It is commonly used to distribute software to macOS users and can contain multiple files, scripts, and metadata.\nWhy create a macOS install package that only runs a script? Sometimes, you must distribute a script that performs a specific task on a macOS system, such as fixing a known issue. You can create a macOS install package that contains the script and any other files needed to run that script. This workflow allows you to distribute the script as an install package that users can easily install on their macOS systems.\nAnother reason to create a macOS install package that only runs a script is to use a third-party installer instead of the built-in macOS installer. This custom installer can provide additional features and customization options.\nCreate a script-only install package using the pkgbuild command The pkgbuild command is a command-line tool included with macOS. It allows you to create macOS install packages from the command line.\nFirst, create a directory for your script files:\nmkdir Scripts Create a script file in the above directory called postinstall that contains the script you want to run. Below is an example script for testing:\n#!/bin/bash # installer script variables: # $0 = path to the script # $1 = path to the package # $2 = target location, i.e., /Applications # $3 = target volume, i.e., /Volumes/Macintosh HD # $4 = \u0026#34;/\u0026#34; if this is the startup disk mkdir -p /opt/hello target=\u0026#34;/opt/hello/hello.txt\u0026#34; echo \u0026#34;\\$0=$0\u0026#34; \u0026gt; $target echo \u0026#34;\\$1=$1\u0026#34; \u0026gt;\u0026gt; $target echo \u0026#34;\\$2=$2\u0026#34; \u0026gt;\u0026gt; $target echo \u0026#34;\\$3=$3\u0026#34; \u0026gt;\u0026gt; $target echo \u0026#34;\\$4=$4\u0026#34; \u0026gt;\u0026gt; $target echo \u0026#34;\\$INSTALL_PKG_SESSION_ID=$INSTALL_PKG_SESSION_ID\u0026#34; \u0026gt;\u0026gt; $target echo \u0026#34;\\$USER=$USER\u0026#34; \u0026gt;\u0026gt; $target echo \u0026#34;\\$HOME=$HOME\u0026#34; \u0026gt;\u0026gt; $target # Always succeed exit 0 The above script creates a directory /opt/hello and writes various script variables to a file /opt/hello/hello.txt.\nMake sure the script is executable:\nchmod +x Scripts/postinstall Create the installation package using the pkgbuild command:\npkgbuild --nopayload --scripts Scripts --identifier com.victoronsoftware.pkgbuild-demo --version 1.0 PkgbuildDemo.pkg The above command creates an install package PkgbuildDemo.pkg.\nThe --nopayload flag tells pkgbuild that there are no application files to include in the package. The --scripts flag specifies the directory containing the scripts to run during the installation. The scripts directory may also contain additional files needed by the script.\nAt this point, you can try installing the package on a test macOS system:\nsudo installer -pkg PkgbuildDemo.pkg -target / Create a script-only install package using the Packages app One popular GUI tool for creating macOS installer packages is the Packages app.\nDownload and install the Packages app.\nCreate a new project in the Packages app using the Raw Package template.\nChoose the name and location of your project.\nIn the Scripts tab, choose a Post-installation script. Add additional script resource files if needed for the script.\nSave your project with File \u0026gt; Save and build the package with Build \u0026gt; Build.\nThe tool will save the new PKG file to your project directory.\nAnalyze the install package To analyze the install package, you can use a tool like Suspicious Package\nSign and notarize the install package Before distributing the package to users, you may need to sign and notarize it.\nTo sign the package, you need a Developer ID Installer certificate. The Apple Developer Program currently costs 99 USD per membership year. To sign your package, place the certificate and corresponding private key (together called an \u0026ldquo;identity\u0026rdquo;) into your keychain. Then, you can sign the package using the productsign command-line utility:\nproductsign --sign \u0026#34;Developer ID Installer: ********\u0026#34; ~/PkgbuildDemo.pkg ~/PkgbuildDemo-signed.pkg You can notarize your package with Apple using the notarytool command-line utility. For more information, see Notarizing macOS software before distribution.\nDistribute the install package You can distribute the package by posting a download link on your website, through a package manager, or using your MDM tool.\nIf you\u0026rsquo;re using a macOS MDM platform such as Fleet, you can upload the package to the MDM and deploy it to your managed devices.\nFurther reading Recently, we explained launchd agents and daemons on macOS. In the past, we showed how to create an EXE installer for Windows and code sign a Windows application. We also covered using Mutual TLS (mTLS) with macOS keychain. Watch how to create a script-only macOS install package Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-11-13T00:00:00Z","image":"https://victoronsoftware.com/posts/script-only-macos-install-package/script-package-headline_hu_25e92af6247d7164.png","permalink":"https://victoronsoftware.com/posts/script-only-macos-install-package/","title":"2 ways to turn a script into a macOS install package"},{"content":"What is staticcheck? Staticcheck is a Go linter that checks your Go code for bugs and performance issues. It is a powerful tool that can help you find issues in your code before they become problematic. Staticcheck is one of the default linters in the golangci-lint tool.\nRun staticcheck on your Go project In this example, we will enable staticcheck via the golangci-lint tool in a large Go project. The golangci-lint tool is a lint runner that runs many linters in parallel. It is a great tool to use in your CI/CD pipeline to catch issues early.\nInstall golangci-lint To install the golangci-lint tool, you can use one of the options in golangci-lint install documentation. We install it using the following command:\ngo install github.com/golangci/golangci-lint/cmd/golangci-lint@v1.61.0 Although the documentation does not recommend this way of installing from source, we use it to ensure that our version of golangci-lint is compiled using the same Go version as our project. We previously encountered issues with golangci-lint compiled with a different Go version.\nCheck the version of golangci-lint:\ngolangci-lint --version Sample output:\ngolangci-lint has version v1.61.0 built with go1.23.1 from (unknown, modified: ?, mod sum: \u0026#34;h1:VvbOLaRVWmyxCnUIMTbf1kDsaJbTzH20FAMXTAlQGu8=\u0026#34;) on (unknown) Run golangci-lint with staticcheck You can run staticcheck using the golangci-lint tool. In the root of your Go project, run the following command:\ngolangci-lint run --disable-all --enable staticcheck This command turns off all default linters and enables only the staticcheck linter. You can view the complete list of run options with:\ngolangci-lint run --help For our project, we add a few more flags to the golangci-lint run command:\ngolangci-lint run --disable-all --enable staticcheck --timeout 10m --max-same-issues 0 --max-issues-per-linter 0 --exclude-dirs ./node_modules Analyze and fix staticcheck issues SA1019 - Using a deprecated function, variable, constant or field After running the linter, the first thing we notice is a considerable number of SA1019 fails flagging deprecations, such as:\ncmd/osquery-perf/agent.go:2574:2: SA1019: rand.Seed has been deprecated since Go 1.20 and an alternative has been available since Go 1.0: As of Go 1.20 there is no reason to call Seed with a random value. Programs that call Seed with a known value to get a specific sequence of results should use New(NewSource(seed)) to obtain a local random generator. (staticcheck) rand.Seed(*randSeed) ^ or\nserver/service/appconfig.go:970:5: SA1019: customSettings[i].Labels is deprecated: the Labels field is now deprecated, it is superseded by LabelsIncludeAll, so any value set via this field will be transferred to LabelsIncludeAll. (staticcheck) customSettings[i].Labels = nil ^ The first fail flags a Go library depreciation issue. Although we could fix it, we are not worried because of Go\u0026rsquo;s commitment to backward compatibility.\nThe second SA1019 deprecation fail flags an internal depreciation within our app. However, we must maintain many deprecated functions within our app for backward compatibility until they can be removed with the next major release. So, many of these failures cannot be fixed. We could waive each one, but that would be a lot of busy work.\nEnabling SA1019 as a default staticcheck rule is a mistake. We suspect many potential users of staticcheck will be turned off by the sheer number of these fails and will simply turn off staticcheck in their projects.\nWe decide to suppress them for now by creating a custom configuration file:\nlinters-settings: staticcheck: checks: [\u0026#34;all\u0026#34;, \u0026#34;-ST1000\u0026#34;, \u0026#34;-ST1003\u0026#34;, \u0026#34;-ST1016\u0026#34;, \u0026#34;-ST1020\u0026#34;, \u0026#34;-ST1021\u0026#34;, \u0026#34;-ST1022\u0026#34;, \u0026#34;-SA1019\u0026#34;] We use the default staticcheck checks and turn off the SA1019 check.\nWe then run golangci-lint with the custom configuration file:\ngolangci-lint run --disable-all --enable staticcheck --config staticcheck.yml SA1032 - Wrong order of arguments to errors.Is After rerunning the linter, we saw a SA1032 fail:\nserver/datastore/mysql/vpp.go:1090:6: SA1032: arguments have the wrong order (staticcheck) if errors.Is(sql.ErrNoRows, err) { ^ This failure is a good catch and a potential bug. We fix it by swapping the arguments:\nif errors.Is(err, sql.ErrNoRows) { SA4005 - Field assignment that will never be observed. Did you mean to use a pointer receiver? Another fail we saw was SA4005:\nserver/mail/users.go:44:2: SA4005: ineffective assignment to field PasswordResetMailer.CurrentYear (staticcheck) r.CurrentYear = time.Now().Year() ^ The relevant Go code is:\nr.CurrentYear = time.Now().Year() t, err := server.GetTemplate(\u0026#34;server/mail/templates/password_reset.html\u0026#34;, \u0026#34;email_template\u0026#34;) if err != nil { return nil, err } var msg bytes.Buffer if err = t.Execute(\u0026amp;msg, r); err != nil { return nil, err } In this case, the CurrentYear field was used in our template, but the linter could not detect it. We spent a few minutes testing the template to ensure that the CurrentYear field was being populated correctly. To waive this failure, we add a comment:\nr.CurrentYear = time.Now().Year() // nolint:staticcheck // SA4005 false positive for Go templates SA4006 - A value assigned to a variable is never read before being overwritten. Forgotten error check or dead code? We saw a lot of SA4006 fails in our codebase. It was the most common staticcheck fail we encountered. Here is an example:\nee/fleetctl/updates_test.go:455:2: SA4006: this value of `repo` is never used (staticcheck) repo, err = openRepo(tmpDir) ^ This is a bug or a potential bug. The developer assigned a value to repo but never used it. We fix it by removing the assignment:\n_, err = openRepo(tmpDir) SA4009 - A function argument is overwritten before its first use Another fail we saw was SA4009. Here is an example:\norbit/pkg/installer/installer.go:288:37: SA4009: argument ctx is overwritten before first use (staticcheck) func (r *Runner) runInstallerScript(ctx context.Context, scriptContents string, installerPath string, fileName string) (string, int, error) { ^ This is another bug or potential bug. A function argument is passed in but then immediately overwritten and never used. This issue could be challenging to fix because it requires specific code knowledge.\nOther fails We found a few other fails that were not as critical as the ones mentioned above. We fixed them as we went along. See the video below for more details.\nOverall impressions Overall, we like the staticcheck linter. It found many bugs or potential bugs and provided a lot of value.\nWe did have to ignore the SA1019 check and encountered an SA4005 false positive.\nWe will enable it in our CI/CD pipeline and continue to use it in our project.\nFurther reading Recently, we wrote about finding performance issues with OpenTelemetry and Jaeger in your Go project. We also wrote about optimizing the performance of your Go code. We also published an article on Go modules and packages. Example code on GitHub Fleet repo we used when enabling staticcheck (as of this writing)\nWatch us enable staticcheck in our Go project Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-11-06T00:00:00Z","image":"https://victoronsoftware.com/posts/staticcheck-go-linter/staticcheck-go-linter-headline_hu_2be66890d9a88d57.png","permalink":"https://victoronsoftware.com/posts/staticcheck-go-linter/","title":"Is staticcheck linter useful for my Go project?"},{"content":"This article discusses our first impressions of using OpenTelemetry with Jaeger.\nUse cases for OpenTelemetry and Jaeger Problems with OpenTelemetry and Jaeger What is OpenTelemetry? OpenTelemetry is a set of APIs, libraries, agents, and instrumentation for collecting distributed traces and metrics from your applications. It provides a standardized way to instrument your code and collect telemetry data. OpenTelemetry supports programming languages like Java, Python, Go, JavaScript, etc.\nTracing is a method of monitoring and profiling your application to understand how requests flow through your system. For example, you can view the associated database calls and requests to other services for a single API request. Tracing allows you to identify bottlenecks, latency issues, and other performance problems.\nWhat is Jaeger? Jaeger is an open-source, end-to-end distributed tracing system. Jaeger is popular for tracing applications because of its scalability, ease of use, and integration with other tools. Jaeger provides a web-based UI for viewing traces and analyzing performance data.\nAdd OpenTelemetry instrumentation to your application To start with OpenTelemetry and Jaeger, you must instrument your application with OpenTelemetry libraries.\nIn our case, we used the OpenTelemetry Go SDK to instrument our Go application. We added the necessary dependencies to our project.\ngo get go.opentelemetry.io/otel@v1.31.0 go get go.opentelemetry.io/otel/exporters/otlp/otlptrace@v1.31.0 go get go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc@v1.31.0 go get go.opentelemetry.io/otel/sdk@v1.31.0 go get go.opentelemetry.io/contrib/instrumentation/github.com/gorilla/mux/otelmux@v0.56.0 go get github.com/XSAM/otelsql@v0.35.0 The go.opentelemetry.io/contrib/instrumentation/github.com/gorilla/mux/otelmux package is needed to instrument our gorilla/mux HTTP router.\nr := mux.NewRouter() r.Use(otelmux.Middleware(\u0026#34;fleet\u0026#34;)) The github.com/XSAM/otelsql package is needed to instrument our SQL database queries.\n// ... import \u0026#34;github.com/XSAM/otelsql\u0026#34; import semconv \u0026#34;go.opentelemetry.io/otel/semconv/v1.26.0\u0026#34; // ... var otelTracedDriverName string func init() { var err error otelTracedDriverName, err = otelsql.Register(\u0026#34;mysql\u0026#34;, otelsql.WithAttributes(semconv.DBSystemMySQL), otelsql.WithSpanOptions(otelsql.SpanOptions{ // DisableErrSkip ignores driver.ErrSkip errors, which are frequently returned by the MySQL // driver when certain optional methods or paths are not implemented/taken. // For example, interpolateParams=false (the secure default) will not do a parametrized // sql.conn.query directly without preparing it first, causing driver.ErrSkip DisableErrSkip: true, // Omitting span for sql.conn.reset_session since it takes ~1us and doesn\u0026#39;t provide useful // information OmitConnResetSession: true, // Omitting span for sql.rows since it is very quick and typically doesn\u0026#39;t provide useful // information beyond what\u0026#39;s already reported by prepare/exec/query OmitRows: true, }), // WithSpanNameFormatter allows us to customize the span name, which is especially useful for SQL // queries run outside an HTTPS transaction, which do not belong to a parent span, show up as their // own trace, and would otherwise be named \u0026#34;sql.conn.query\u0026#34; or \u0026#34;sql.conn.exec\u0026#34;. otelsql.WithSpanNameFormatter(func(ctx context.Context, method otelsql.Method, query string) string { if query == \u0026#34;\u0026#34; { return string(method) } // Append query with extra whitespaces removed query = strings.Join(strings.Fields(query), \u0026#34; \u0026#34;) if len(query) \u0026gt; 100 { query = query[:100] + \u0026#34;...\u0026#34; } return string(method) + \u0026#34;: \u0026#34; + query }), ) if err != nil { panic(err) } } Then, use otelTracedDriverName to open a connection to your database.\ndb, err := sql.Open(otelTracedDriverName, \u0026#34;user:password@tcp(localhost:3306)/database\u0026#34;) When starting your application, you must create an OpenTelemetry exporter and a trace provider.\nctx := context.Background() client := otlptracegrpc.NewClient() otlpTraceExporter, err := otlptrace.New(ctx, client) if err != nil { panic(\u0026#34;Failed to initialize tracing\u0026#34;) } batchSpanProcessor := trace.NewBatchSpanProcessor(otlpTraceExporter) tracerProvider := trace.NewTracerProvider(trace.WithSpanProcessor(batchSpanProcessor)) otel.SetTracerProvider(tracerProvider) Launch Jaeger To view traces, you need to launch Jaeger. You can run Jaeger locally using Docker. Based on the Jaeger 1.62 Getting Started guide, you can run the following command:\ndocker run --rm --name jaeger \\ -p 16686:16686 \\ -p 4317:4317 \\ jaegertracing/all-in-one:1.62.0 In our example, we are only exposing two ports:\n4317 for the Jaeger collector, which receives trace data using OpenTelemetry Protocol (OTLP) over gRPC 16686 for the Jaeger UI Launch your application Before starting your application, you must set the OpenTelemetry endpoint to send traces to Jaeger. For example:\nexport OTEL_SERVICE_NAME=fleet export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317 Now, you can start your application.\nView traces in Jaeger Open your browser and navigate to http://localhost:16686 to view traces in the Jaeger UI. Select your Service name and click Find Traces.\nYou can click into a trace to view the details of each span. You can see the duration, logs, and tags for each span. The example below shows the HTTP request details and multiple SQL queries.\nUse cases for OpenTelemetry and Jaeger In a local software development environment, OpenTelemetry and Jaeger can be used to:\nFix bottlenecks and latency issues Understand how requests flow through your system If a bottleneck is known or suspected, Jaeger can help you identify the root cause. For example, you can see which database queries are taking the most time and optimize them.\nWhen developing new features, Jaeger can help you understand how requests flow through your system. This telemetry data provides a quick check to ensure your new feature works as expected.\nIn a production environment, OpenTelemetry and Jaeger can be used to:\nMonitor and profile your applications Troubleshoot performance issues Optimize your applications and improve user experience Ensure your applications meet service level objectives (SLOs) Problems with OpenTelemetry and Jaeger OpenTelemetry and Jaeger are powerful tools, yet their development use seems limited to fixing performance bottlenecks. They cannot be used for general debugging out of the box since they don\u0026rsquo;t provide enough detail for each specific request, such as the request body.\nIn addition, missing spans can be a problem. If your application is not instrumented correctly, you may not see all the spans you expect or know about in Jaeger. Our application lacks spans for some API endpoints, Redis transactions, outbound HTTP requests, and asynchronous processes. Adding all of these spans requires additional development and QA efforts.\nThe Jaeger UI itself is basic and lacks some features. For example, regex search is missing out of the box, unless Elasticsearch/OpenSearch storage is added.\nOur chosen SQL instrumentation library, github.com/XSAM/otelsql, could be better. It does not provide a way to trace the transaction lifecycle, and it creates many spans at the root level, clogging the Jaeger UI.\nFurther reading Recently, we listed the key metrics to gather during software load testing. Previously, we wrote about benchmarking the performance of your Go code. Example code on GitHub Fleet Device Management repo with OpenTelemetry instrumentation (as of this writing)\nWatch OpenTelemetry with Jaeger video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-10-30T00:00:00Z","image":"https://victoronsoftware.com/posts/opentelemetry-with-jaeger/opentelemetry-with-jaeger-headline_hu_d4aa9472c6a8d09e.png","permalink":"https://victoronsoftware.com/posts/opentelemetry-with-jaeger/","title":"Is OpenTelemetry useful for the average software developer?"},{"content":"In this article, we\u0026rsquo;ll create a custom reusable GitHub Action using TypeScript. As covered in our article on reusing GitHub workflows and steps, GitHub Actions allow you to automate your software development workflows. By creating a custom GitHub Action, you can extend the functionality of GitHub Actions to suit your specific needs.\nWe will create a simple GitHub Action to replace GitHub\u0026rsquo;s broken Pull Request review process. This custom GitHub Action will automatically approve Pull Requests that meet specific criteria.\nStart with GitHub\u0026rsquo;s Action template GitHub provides a template for creating a new GitHub Action using TypeScript. You can use this template to get started quickly. The template includes the necessary files and structure to create a new GitHub Action. Anything unnecessary can be removed or modified to suit your requirements.\nFollow the instructions in the template\u0026rsquo;s README to set up your action.\nClick the Use this template button at the top of the repository Select Create a new repository Select an owner and name for your new repository Click Create repository Clone your new repository Check out your new repository, go to the cloned repo directory, and make sure your node version matches the one in the .node-version file. If you\u0026rsquo;re using nvm (Node Version Manager), you can run:\ncp .node-version .nvmrc nvm install node --version Note: Our example is based on this commit of the template repository.\nImplement your custom GitHub Action Install the template\u0026rsquo;s dependencies with npm install. In addition, install the @actions/github package with npm install @actions/github.\nThe template provides a basic structure for your GitHub Action. Update the action.yml file with your action\u0026rsquo;s name, description, author, and updated inputs/outputs.\nname: \u0026#39;Code review\u0026#39; description: \u0026#39;Improved code review process\u0026#39; author: \u0026#39;Victor on Software\u0026#39; # Add your action\u0026#39;s branding here. This will appear on the GitHub Marketplace. branding: icon: \u0026#39;heart\u0026#39; color: \u0026#39;red\u0026#39; # Define your inputs here. inputs: github-token: description: \u0026#39;GitHub secret token\u0026#39; required: true runs: using: node20 main: dist/index.js In this example, our new action will have a single input, github-token, the GitHub secret token to use the GitHub API.\nNext, we can modify the code in the src/main.ts file to implement our custom logic.\nimport * as core from \u0026#39;@actions/core\u0026#39; import * as github from \u0026#39;@actions/github\u0026#39; import { readFileSync } from \u0026#39;fs\u0026#39; /** * The main function of the action. * @returns {Promise\u0026lt;void\u0026gt;} Resolves when the action is complete. */ export async function run(): Promise\u0026lt;void\u0026gt; { try { // Get the PR number from the payload. This action is only intended for PRs. const prNumber = github.context.payload.pull_request?.number if (!prNumber) { return } // Get the required reviewer from the REVIEWERS file. // This simplified example assumes that the REVIEWERS file contains a single reviewer. // In a real-world scenario, we would need to parse the REVIEWERS file at the top directory // of our changed files to get the reviewers. const reviewer = readFileSync(\u0026#39;REVIEWERS\u0026#39;, \u0026#39;utf8\u0026#39;).trim() if (!reviewer) { core.setFailed(\u0026#39;No reviewer found in REVIEWERS file\u0026#39;) return } // Get all the reviews for this PR. const githubToken = core.getInput(\u0026#39;github-token\u0026#39;) const octokit = github.getOctokit(githubToken) const reviews = await octokit.rest.pulls.listReviews({ owner: github.context.repo.owner, repo: github.context.repo.repo, pull_number: prNumber }) // Check if the required reviewer has approved the PR. // This action does not require the reviewer to re-approve the PR if new changes are pushed. let approved = false reviews.data.forEach(review =\u0026gt; { if (review.user?.login === reviewer \u0026amp;\u0026amp; review.state === \u0026#39;APPROVED\u0026#39;) { approved = true } }) // Fail the workflow run if the required reviewer has not approved the PR. if (!approved) { core.setFailed(`Reviewer ${reviewer} needs to approve the PR`) return } } catch (error) { // Fail the workflow run if an error occurs if (error instanceof Error) core.setFailed(error.message) } } We read the REVIEWERS file to get the required reviewer for the PR. We then retrieve all the reviews for the pull request and check if the needed reviewer has approved the PR. If the reviewer has not approved the PR, we fail the run.\nBuild your custom GitHub Action To build your action, run npm run bundle. This will compile the TypeScript code in the src/ directory and output the JavaScript code in the dist/ directory. The exact command for bundle is defined in the package.json file.\nThe bundle step is required before you can test your action locally or use it in a workflow because the action.yml file points at the dist/index.js bundled version of your code.\nThis step can easily be forgotten, and you may wonder why your changes are not reflected in the action. You can configure our IDE to do npm run bundle automatically on save, create a check as a git pre-commit hook to ensure the dist/ directory is up-to-date, or rely on the Check Transpiled Javascript workflow in the .github/workflows/ directory.\nNow, commit your changes and push them to your repository.\nNote: We will not cover testing in this article, but you should add tests to the __tests__/ directory for your source code. For this example, we must refactor the code to make it more testable.\nTest your custom GitHub Action in another repository Add workflows and REVIEWERS file You can use the uses keyword in a workflow file to try your action in another repository. Create a new workflow file in the repository\u0026rsquo;s .github/workflows/ directory where you want to test your action. For example:\nname: Code review on: workflow_dispatch: # Manual (for debug) pull_request: jobs: code-review: name: Code review runs-on: ubuntu-latest steps: - name: Checkout id: checkout uses: actions/checkout@v4 - name: Code review action id: test-action uses: getvictor/code-review-demo@main # Your action goes here with: github-token: ${{ secrets.GITHUB_TOKEN }} We are using @main as the version of the action to test the latest code on the main branch. You can replace this with a specific version tag or branch name.\nIn addition, create a REVIEWERS file at the repository\u0026rsquo;s root with the required reviewer\u0026rsquo;s GitHub username.\nNotice that the above workflow runs on pull_request events but not on pull_request_review events. This is because GitHub Actions treats the pull_request workflow runs and the pull_request_review workflow runs as distinct. Instead, we want the same workflow run to be triggered by both events. This is a common pitfall when working with GitHub Actions. We need to add another workflow file that listens for pull_request_review events and triggers the pull_request workflow run to fix this.\nname: Rerun checks after review on: pull_request_review: types: - submitted - dismissed jobs: rerun_checks: name: Rerun specified checks permissions: actions: write runs-on: ubuntu-latest steps: - name: Rerun Checks uses: shqear93/rerun-checks@v3 with: github-token: ${{ secrets.GITHUB_TOKEN }} check-names: \u0026#39;Code review\u0026#39; Configure a GitHub rule to require code review Create a branch protection rule in the repository settings that requires the above Code review workflow to pass in a PR before merging to your default branch.\nCreate a pull request Commit your changes to a new branch and create a pull request. The Code review workflow should run automatically and show the Failed status.\nOnce the required reviewer approves the PR, the Rerun checks after review workflow will run and trigger the Code review workflow. The rerun should pass, and you should be able to merge the PR.\nClean up your custom GitHub Action repo As an optional step, you can clean up your custom GitHub Action repository:\nUpdate README.md with instructions on how to use your new action Remove the src/wait.ts file and associated tests Update workflows in the .github/workflows/ directory Further reading In a previous article, we described what happens in a GitHub pull request after a git merge.\nIn another article, we covered how to use GitHub Actions for general-purpose tasks.\nExample code on GitHub The code for our simple GitHub Action is available on GitHub: https://github.com/getvictor/code-review-demo\nWatch how to create a custom GitHub Action using TypeScript Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-10-23T00:00:00Z","image":"https://victoronsoftware.com/posts/typescript-github-action/typescript-github-action-headline_hu_8d56de5f8631c5e1.png","permalink":"https://victoronsoftware.com/posts/typescript-github-action/","title":"How to create a custom GitHub Action using TypeScript"},{"content":"JSON unmarshalling use cases When passing a JSON payload to a Go application, you may encounter situations where you must tell the difference between set, missing, or null fields.\nFor example, consider the following JSON payload:\n{ \u0026#34;name\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;age\u0026#34;: 30, \u0026#34;address\u0026#34;: { \u0026#34;street\u0026#34;: \u0026#34;123 Main St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Springfield\u0026#34; } } We can unmarshal this JSON payload using JSON tags and the following Go structs:\ntype Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` Address Address `json:\u0026#34;address\u0026#34;` } type Address struct { Street string `json:\u0026#34;street\u0026#34;` City string `json:\u0026#34;city\u0026#34;` } However, we will not be able to tell the difference between these two JSON payloads:\n{ \u0026quot;name\u0026quot;: null } { \u0026quot;name\u0026quot;: \u0026quot;\u0026quot; } Go\u0026rsquo;s zero values are not distinguishable from missing fields when unmarshalling JSON.\nWe can change the above struct to use pointers to identify null fields:\ntype Person struct { Name *string `json:\u0026#34;name\u0026#34;` Age *int `json:\u0026#34;age\u0026#34;` Address *Address `json:\u0026#34;address\u0026#34;` } However, we will still not be able to tell the difference between these two JSON payloads:\n{ \u0026quot;name\u0026quot;: null } { } Both of these payloads will unmarshal into a Person struct with all fields set to nil, and we cannot distinguish between a missing field and a field set to null.\nOne reason to distinguish between missing and null fields is to avoid overwriting existing values with null values. For example, when name is not specified in the JSON payload, we may want to keep the existing name value in the Person struct. But we may want to clear the name when name is defined as null.\nDetecting null, set, and missing JSON fields with Go We can use custom unmarshalling logic by implementing the Unmarshaler interface to detect the difference between null and missing fields. The UnmarshalJSON method allows us to inspect the JSON token stream and decide how to unmarshal the JSON payload. The critical insight is that UnmarshalJSON is only called when the field is present in the JSON payload. So, we can mark a Set flag as true when the field is present and false when it is not.\nHere is an example implementation:\ntype Any[T any] struct { Set bool Valid bool Value T } // MarshalJSON implements the json.Marshaler interface. // Only Value is marshaled, and only if Valid is true. func (s Any[T]) MarshalJSON() ([]byte, error) { if !s.Valid { return []byte(\u0026#34;null\u0026#34;), nil } return json.Marshal(s.Value) } // UnmarshalJSON implements the json.Unmarshaler interface. // Set is always set to true, even if the JSON data was set to null. // Valid is set if the JSON data is not set to null. func (s *Any[T]) UnmarshalJSON(data []byte) error { s.Set = true s.Valid = false if bytes.Equal(data, []byte(\u0026#34;null\u0026#34;)) { // The key was set to null, set value to zero/default value var zero T s.Value = zero return nil } // The key isn\u0026#39;t set to null var v T if err := json.Unmarshal(data, \u0026amp;v); err != nil { return err } s.Value = v s.Valid = true return nil } We used a generic type T to allow Any to work with any type. The Valid flag distinguishes between nil and non-nil values. The Set flag is set to true only when the field is present in the JSON payload.\nHere is how we can use the Any type in a Person struct:\ntype Person struct { Name Any[string] `json:\u0026#34;name\u0026#34;` Age Any[int] `json:\u0026#34;age\u0026#34;` Address Any[Address] `json:\u0026#34;address\u0026#34;` } Testing the custom unmarshalling logic The following example demonstrates how the Any type works:\ntype Form struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` } type Config struct { Form Any[Form] `json:\u0026#34;form\u0026#34;` Member Any[bool] `json:\u0026#34;member\u0026#34;` } func main() { tests := []struct { description string JSON string }{ { \u0026#34;Nothing set\u0026#34;, `{}`, }, { \u0026#34;Set all fields\u0026#34;, `{\u0026#34;form\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;age\u0026#34;: 30}, \u0026#34;member\u0026#34;: false}`, }, { \u0026#34;Set only member field, and leave form fields unchanged\u0026#34;, `{\u0026#34;member\u0026#34;: true}`, }, { \u0026#34;Set only the form field, and leave the member field unchanged\u0026#34;, `{\u0026#34;form\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;Jane\u0026#34;, \u0026#34;age\u0026#34;: 25}}`, }, { \u0026#34;Leave all fields unchanged\u0026#34;, `{}`, }, { \u0026#34;Clear all fields\u0026#34;, `{\u0026#34;form\u0026#34;: null, \u0026#34;member\u0026#34;: null}`, }, { \u0026#34;Set only form field\u0026#34;, `{\u0026#34;form\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;Chris\u0026#34;, \u0026#34;age\u0026#34;: 35}}`, }, } c := Config{} for _, test := range tests { fmt.Printf(\u0026#34;\\nTest: %s\\n\u0026#34;, test.description) _ = json.Unmarshal([]byte(test.JSON), \u0026amp;c) fmt.Printf(\u0026#34;Input: %s\\n\u0026#34;, test.JSON) fmt.Printf(\u0026#34;%+v\\n\u0026#34;, c) data, _ := json.Marshal(c) fmt.Printf(\u0026#34;Output: %s\\n\u0026#34;, data) } } The test output will show how the Any type behaves when unmarshalling JSON payloads with different fields set, missing, or set to null.\nTest: Nothing set Input: {} {Form:{Set:false Valid:false Value:{Name: Age:0}} Member:{Set:false Valid:false Value:false}} Output: {\u0026#34;form\u0026#34;:null,\u0026#34;member\u0026#34;:null} Test: Set all fields Input: {\u0026#34;form\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;age\u0026#34;: 30}, \u0026#34;member\u0026#34;: false} {Form:{Set:true Valid:true Value:{Name:John Age:30}} Member:{Set:true Valid:true Value:false}} Output: {\u0026#34;form\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;John\u0026#34;,\u0026#34;age\u0026#34;:30},\u0026#34;member\u0026#34;:false} Test: Set only member field, and leave form fields unchanged Input: {\u0026#34;member\u0026#34;: true} {Form:{Set:true Valid:true Value:{Name:John Age:30}} Member:{Set:true Valid:true Value:true}} Output: {\u0026#34;form\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;John\u0026#34;,\u0026#34;age\u0026#34;:30},\u0026#34;member\u0026#34;:true} Test: Set only the form field, and leave the member field unchanged Input: {\u0026#34;form\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;Jane\u0026#34;, \u0026#34;age\u0026#34;: 25}} {Form:{Set:true Valid:true Value:{Name:Jane Age:25}} Member:{Set:true Valid:true Value:true}} Output: {\u0026#34;form\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Jane\u0026#34;,\u0026#34;age\u0026#34;:25},\u0026#34;member\u0026#34;:true} Test: Leave all fields unchanged Input: {} {Form:{Set:true Valid:true Value:{Name:Jane Age:25}} Member:{Set:true Valid:true Value:true}} Output: {\u0026#34;form\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Jane\u0026#34;,\u0026#34;age\u0026#34;:25},\u0026#34;member\u0026#34;:true} Test: Clear all fields Input: {\u0026#34;form\u0026#34;: null, \u0026#34;member\u0026#34;: null} {Form:{Set:true Valid:false Value:{Name: Age:0}} Member:{Set:true Valid:false Value:false}} Output: {\u0026#34;form\u0026#34;:null,\u0026#34;member\u0026#34;:null} Test: Set only form field Input: {\u0026#34;form\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;Chris\u0026#34;, \u0026#34;age\u0026#34;: 35}} {Form:{Set:true Valid:true Value:{Name:Chris Age:35}} Member:{Set:true Valid:false Value:false}} Output: {\u0026#34;form\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Chris\u0026#34;,\u0026#34;age\u0026#34;:35},\u0026#34;member\u0026#34;:null} Complete code on Go Playground The complete Go code for unmarshalling JSON null, set, and missing fields is available on the Go Playground.\nFurther reading Recently, we published an article on how to optimize the performance of a Go application. We benchmarked JSON decoding vs gob decoding in that article. In addition, we wrote about how to read program arguments from STDIN with Go, which is more secure than using environment variables or command-line arguments. Also, we explained the difference between Go modules and packages, which is essential for organizing and managing Go code. In addition, we explained method overriding in Go. Watch how to use Go to unmarshal JSON null, set, and missing fields accurately Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-10-09T00:00:00Z","image":"https://victoronsoftware.com/posts/go-json-unmarshal/go-json-unmarshal-headline_hu_c883e153a4331d6.png","permalink":"https://victoronsoftware.com/posts/go-json-unmarshal/","title":"Use Go to unmarshal JSON null, set, and missing fields"},{"content":" Test NDES using PowerShell Test NDES using a SCEP client Test NDES using Apple MDM profile What is a Windows NDES SCEP server? SCEP (Simple Certificate Enrollment Protocol) is a protocol used to issue certificates with a Certificate Authority (CA) in a Public Key Infrastructure (PKI). It allows devices to request and receive certificates over a secure channel without user interaction. IT admins use SCEP for network devices, mobile devices, and other endpoints that need to authenticate themselves. The issued certificates can be used for various purposes, such as Wi-Fi authentication, VPN access, email encryption, etc. For example, a new mobile device can request a certificate from the SCEP server to authenticate on the corporate Wi-Fi network.\nNDES (Network Device Enrollment Service) is a Microsoft implementation of the SCEP protocol. NDES is part of the Active Directory Certificate Services (AD CS) role in Windows Server.\nSetting up a Windows NDES SCEP server Before testing your Windows NDES SCEP server, you must set it up. Numerous articles and guides cover the installation and configuration of NDES. This article will focus on testing the NDES SCEP server to ensure the correct setup. We wrote this article because we could not find a comprehensive guide on how to test the NDES SCEP server.\nHere are the high-level steps to configure a Windows NDES SCEP server:\nCreate or use an existing Windows AD (Active Directory) server and domain. Install the Active Directory Certificate Services (AD CS) role on a Windows Server that is part of the AD domain. Configure the Enterprise NDES role service within AD CS. (Optional) Configure the certificate templates for NDES. We used Windows Server 2022 for our tests, and we will update this article once we test with Windows Server 2025.\nTest NDES using a web browser First, we must make sure the NDES server is accessible via a web browser. If the server should be accessible outside the corporate network, test it using the public URL or IP address.\nThe NDES server has an admin web interface for retrieving the SCEP challenge. The URL typically looks like http://ndes-server/certsrv/mscep_admin/ and requires authentication. The username must use the Windows name format, like username@example.domain.com. Accessing this URL should prompt you to log in and display the SCEP challenge.\nNote: The above admin page is encoded as UTF-16, as opposed to the more popular UTF-8 encoding. This encoding must considered when parsing this page with a script.\nThe other URL to test is the actual SCEP enrollment URL, typically http://ndes-server/certsrv/mscep/mscep.dll. It returns the following.\nTest NDES using PowerShell For our first test, we will use PowerShell to request a certificate from another Windows machine in the same AD domain.\nBelow is a sample PowerShell script that requests a certificate from the NDES server. Update the URL and the challenge password.\nAfter running the script, check that NDES issued a certificate.\nTest NDES using a SCEP client For our next test, we will use an SCEP client to request a certificate from the NDES server. Several SCEP clients are available, but many have been abandoned and do not work with NDES.\nWe will use micromdm/scep, a Go-based open-source SCEP server and client. We will use the latest code from the main branch, with the following commit hash: 781f8042a79cabcf61a5e6c01affdbadcb785932.\nFollow the instructions from the above URL to install the scep client. We built it for macOS M1 using the following command:\nmake scepclient-darwin-arm64 After building the client, obtain a new enrollment challenge password and run the following command to request a certificate from the NDES server:\nmkdir test cd test ../scepclient-darwin-arm64 -key-encipherment-selector -cn \u0026#34;ScepClient\u0026#34; -challenge \u0026#34;ABBFE34CF11C2C04\u0026#34; -server-url \u0026#34;https://victor-ndes.ngrok.app/certsrv/mscep/mscep.dll\u0026#34; -debug -private-key ./ndes-pk Note: We recommend running the above command in a separate directory because the SCEP client generates several intermediate files during the certificate request process. If you don\u0026rsquo;t clean them up, the client may reuse them instead of generating new ones from the command line flags.\nThe above command will generate a new certificate request and send it to the NDES server. The server will respond with a signed certificate, which the client will save to the current directory as a client.pem file.\nAs a final step, verify that the certificate and the private key match by building a PKCS#12 file:\n/usr/bin/openssl pkcs12 -export -inkey ndes-pk -in client.pem -out client.p12 Test NDES using Apple MDM profile For our final test, we will use an Apple MDM profile to request a certificate from the NDES server. We will use a macOS VM enrolled in Fleet Device Management\u0026rsquo;s MDM server. However, adding the MDM profile manually via System Settings -\u0026gt; Profiles should also work.\nFirst, create a new Device Management SCEP payload with the NDES server\u0026rsquo;s URL and the challenge password. Then, assign the SCEP payload to your device. Here\u0026rsquo;s an example payload:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;PayloadContent\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;PayloadContent\u0026lt;/key\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;Challenge\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;8E6D19CAEC9411CC\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;Key Type\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;RSA\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;Key Usage\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;5\u0026lt;/integer\u0026gt; \u0026lt;key\u0026gt;Keysize\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;2048\u0026lt;/integer\u0026gt; \u0026lt;key\u0026gt;Retries\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;3\u0026lt;/integer\u0026gt; \u0026lt;key\u0026gt;RetryDelay\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;10\u0026lt;/integer\u0026gt; \u0026lt;key\u0026gt;Subject\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;array\u0026gt; \u0026lt;array\u0026gt; \u0026lt;string\u0026gt;CN\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;MDM TEST VM\u0026lt;/string\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;array\u0026gt; \u0026lt;array\u0026gt; \u0026lt;string\u0026gt;OU\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;FLEET DEVICE MANAGEMENT\u0026lt;/string\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;key\u0026gt;URL\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;https://victor-ndes.ngrok.app/certsrv/mscep/mscep.dll\u0026lt;/string\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;key\u0026gt;PayloadDisplayName\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;SCEP #1\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;PayloadIdentifier\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;com.apple.security.scep.9DCC35A5-72F9-42B7-9A98-7AD9A9CCA3AA\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;PayloadType\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;com.apple.security.scep\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;PayloadUUID\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;9DCC35A5-72F9-42B7-9A98-7AD9A9CCA3AA\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;PayloadVersion\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;1\u0026lt;/integer\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;key\u0026gt;PayloadDisplayName\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;SCEP cert\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;PayloadIdentifier\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;Victors-Fleet-MBP.4CD1BD65-1D2C-4E9E-9E18-9BCD400CDEDB\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;PayloadType\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;Configuration\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;PayloadUUID\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;4CD1BD65-1D2C-4E9E-9E18-9BCD400CDEDB\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;PayloadVersion\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;1\u0026lt;/integer\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt; Once the device receives the payload, it immediately requests a certificate from the NDES server. The server responds with a signed certificate, which the device saves to the keychain.\nFurther reading Recently, we covered how to connect to a remote Active Directory server.\nWe also wrote a series of articles on building a mutual TLS client which uses a system keystore, such as a Windows certificate store.\nIn addition, we presented an example of code signing a Windows application.\nWatch how to test a Windows NDES SCEP server Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-10-02T00:00:00Z","image":"https://victoronsoftware.com/posts/test-ndes-scep-server/windows-security-headline_hu_c084f5c3e01ef57e.png","permalink":"https://victoronsoftware.com/posts/test-ndes-scep-server/","title":"How to test a Windows NDES SCEP server"},{"content":"Why set up a remote development environment? A remote development environment can be beneficial for several reasons:\nOffload processing power: Your local machine may not have enough processing power to run resource-intensive tasks. By using a remote development environment, you can use more powerful hardware. Consistent environment: A remote development environment ensures all team members can work in the same environment, reducing configuration issues and ensuring consistent behavior across different machines. For example, developers may be using a mix of macOS, Windows, and Linux machines, which can lead to differences in behavior due to operating system-specific issues. Multiple environments: You can set up multiple environments for different projects or tasks without cluttering your local machine. Access from anywhere: A remote development environment allows you to access your work from any device with an internet connection. Collaboration: You can easily collaborate with team members by sharing the same development environment. For example, after coding a feature, the developer can hand off the environment to another engineer for review or QA. Security: Keeping your code and development environment on a remote server reduces the risk of data loss in case of local hardware failure or theft. Scalability: You can quickly scale your development environment up or down based on your needs without affecting your local machine. Cost-effective: A remote development environment can be more cost-effective than purchasing and maintaining high-end hardware for your local machine. Setting up a remote development environment For our development example, we will use a standalone server application connected to a database and a Redis cache. The application uses a monolith repo with a frontend and a backend codebase.\nChoose a cloud provider We used a Digital Ocean VM with 8GB of RAM, 4 CPUs, and a 160 GB disk, running Ubuntu 24.04 LTS for our remote development environment. We found that Digital Ocean provides VMs that are generally cheaper than other cloud providers.\nAny other cloud provider, such as AWS, Google Cloud, or Azure, can also be used. Your choice of provider depends on your specific requirements, budget, and familiarity with the platform.\nAfter spinning up the VM, we SSH\u0026rsquo;ed into the server, installed the necessary software for our application, and launched the server.\nSince our server required multiple running processes, we used tmux to manage multiple terminal sessions. Tmux allowed us to create numerous panes and windows within a single terminal session, making it easier to manage the server processes. We could disconnect from the server, and the tmux processes ran in the background. When reconnected to the server, we could easily reattach to the tmux session and resume work. Additionally, we used iTerm2 tmux integration to enhance our terminal experience.\ntmux running on remote dev server Connect your IDE to the remote development environment We used JetBrains IDE for development work and JetBrains Gateway to connect to our remote development server using SSH. JetBrains Gateway automatically installed the IDE backend on the remote server and brought up a local client of the IDE.\nIn our case, we wanted to use one IDE for backend development (GoLand) and another IDE for frontend development (WebStorm).\nWe had trouble starting them up and could not run both IDEs simultaneously. Either one or both of them would disconnect from the remote development server without an obvious way to fix the issue. We suspect the issue was due to insufficient memory on the machine \u0026ndash; try to plan for around 4 GB of memory per IDE.\nHowever, we could use one of the IDEs at a time, which was sufficient for most of our needs.\nUsing a remote development environment After setting up the remote development environment, we reviewed common development use cases to ensure that everything was working as expected.\nMake a code change and restart the application server We made a simple code change in the backend service, saved the file, and restarted the application server. We verified that the change was reflected in the application.\nThe compile time was slower than on our local machine, likely due to the remote server\u0026rsquo;s lower CPU count and total RAM compared to our local machine.\nRun unit tests We ran the unit tests for the backend service. The tests passed successfully.\nConnect to the database and Redis cache From our local development machine, we connected to the development server\u0026rsquo;s database and Redis cache to verify that the services were running correctly.\nReconnecting to remote development environment After opening up our local computer the next day, we found that the JetBrains Gateway and the IDE has disconnected from the remote server. Refreshing the Gateway re-established the connection, and the IDE also showed as connected within 60 seconds or so.\nSecurity considerations When setting up a remote development environment, consider the following security best practices:\nSSH key authentication: For secure access to the remote server, use SSH key authentication instead of passwords. Firewall rules: Configure firewall rules to restrict access to the server to only necessary IP addresses. Secure connections: Use HTTPS for web applications and encrypted connections for database access. Data encryption: Encrypt sensitive data at rest and in transit. Always encrypt sensitive data in the database. Docker firewall rules Docker containers use iptables rules to open ports for incoming traffic. We can restrict external connections to containers by adding rules to the DOCKER-USER chain, such as:\niptables -I DOCKER-USER -i eth0 ! -s \u0026lt;your local IP\u0026gt; -j DROP Where eth0 is the network interface connected to the internet and \u0026lt;your local IP\u0026gt; is the IP address of your local machine. This rule blocks all incoming traffic to Docker from the internet except for your local IP.\nAfter setting up and testing your rules, you can persist them across restarts with the iptables-persistent package or other methods.\nOverall impressions After using the remote development environment for a few days, we found it usable but not as smooth as working on a local machine. For our use case, it is an excellent option for a secondary development environment or for working on a resource-intensive feature.\nSome issues we encountered included:\nLatency: Occasionally, clicking on an element or using a keyboard shortcut had a noticeable delay. Missing features: Some features, such as only searching inside text strings, were not available in the remote development environment. Issues with plugins: GitHub Copilot did not work out of the box; it did not provide suggestions in the editor. We did not drill down to the issue, but a potential workaround is to use JetBrains\u0026rsquo;s code assistant plugin. Further reading We recently explained how to secure a MySQL Docker container for Zero Trust. We also discussed the issues with GitHub\u0026rsquo;s code review process. We wrote about quickly editing a Google Sheets spreadsheet via the API. We also covered creating secure signed URLs with AWS CloudFront. Watch how to set up a remote development environment Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-09-22T00:00:00Z","image":"https://victoronsoftware.com/posts/remote-development-environment/remote-dev-environment-headline_hu_ac22e8baf05be8aa.png","permalink":"https://victoronsoftware.com/posts/remote-development-environment/","title":"How to set up a remote development environment"},{"content":" CODEOWNERS is not scalable Re-approvals for every push Impractical for protected feature branches Our team has been using GitHub to review the code for our open-source product. We have encountered several issues with GitHub code reviews. The default GitHub code review process is not scalable and provides a poor developer experience.\nHow to set up a GitHub code review process GitHub admins can create a branch protection rule that requires a code review before merging the code to the main branch.\nHere\u0026rsquo;s a representative branch protection rule:\nPR branch protection rule When a developer creates a pull request, GitHub requires code reviews from all relevant owners specified by the CODEOWNERS file in the repository. If someone makes a new push to the PR, all the owners need to re-approve the PR.\nIn a previous article, we covered how to find the required code owners for a PR. This is another issue, but we will not discuss it in this article.\nIssue 1: CODEOWNERS is not scalable GitHub uses a CODEOWNERS file to define individuals or teams responsible for each file in the repository.\nThe CODEOWNERS file format favors fine-grained code ownership, where the last matching pattern takes precedence over previous patterns. Here is an example from the GitHub documentation:\n# In this example, @octocat owns any file in the `/apps` # directory in the root of your repository except for the `/apps/github` # subdirectory, as this subdirectory has its own owner @doctocat /apps/ @octocat /apps/github @doctocat The CODEOWNERS file is not scalable for medium-to-large and even small organizations. As the number of code owners grows, each pull request is likely to require approval from more code owners. Each code owner may request changes, potentially leading to cycles and cycles of re-approvals.\nTracking down multiple people to approve and re-approve a PR can be time-consuming and frustrating for developers. This results in longer PR turnaround times, slower development velocity, and missed commitments.\nFrom a developer experience perspective, we want to make the code review process as smooth and efficient as possible, which means one reviewer for one PR. This approach is feasible by manually inverting the last matching pattern takes precedence rule in the CODEOWNERS file by always including the owner(s) from the previous pattern. For example, we would rewrite the above owners as:\n/apps/ @octocat /apps/github @octocat @doctocat Keeping the CODEOWNERS file in this format may be cumbersome to do manually, but it can be done with a script.\nIssue 2: Re-approvals for every push When a developer makes a new push to a PR, all the code owners need to re-approve it. This is a poor developer experience, as it requires the code owners to review potentially the same code changes multiple times.\nThe issue stems from the lack of fine-grained control over the following option:\nWith multiple code owners, every code owner must re-approve every change.\nA code owner should not need to re-review code that didn\u0026rsquo;t change \u0026ndash; this is a waste of time and effort.\nWith a single code owner, the reviewer must re-approve trivial or irrelevant changes, such as:\nfixing a typo in a comment fully accepting a suggestion from the reviewer re-generating an auto-generated file, such as documentation The required re-approvals can be frustrating and time-consuming for developers and code owners. They make developers feel untrusted and inefficient.\nThe main argument for requiring re-approvals is security—we don\u0026rsquo;t want to merge potentially malicious code. If that\u0026rsquo;s the case, we should have a security review process in place, not a code review process. A security review can be done by a separate individual and improved by automated tools.\nIn addition, we should be able to completely exclude some files/directories from the code review process. For example, generated files, such as documentation based on code changes, should not require code review. Other generated files, such as testing mocks, may have CI/CD checks that ensure they are generated correctly, and they should not require code review either.\nIssue 3: Impractical to maintain a protected feature branch A protected feature branch requires code reviews before merging. Since all the commits on the feature branch have already been reviewed and approved, it is considered safe to merge into the main branch.\nThe main issue is that the developer cannot simply update this feature branch with the latest changes on the main branch. They need PR approval from all the code owners who have already approved the same changes on the main branch. This busy work is another example of a waste of time and effort.\nIn addition, a feature branch may be long-lived and introduce changes across multiple areas of the code base. This means that it may require approval from many code owners, which can be time-consuming and frustrating.\nSolution: Custom GitHub Action to manage code reviews Instead of relying on the default GitHub code review process, we can create a custom GitHub Action to manage code reviews. The custom GitHub Action can:\nautomatically identify a single reviewer for a PR (or identify a small group of reviewers, each of whom can approve the PR) automatically exclude specific files/directories from the code review process automatically maintain the approval state of the PR when new commits meeting explicit criteria are pushed enable a usable and practical protected feature branch Here is an example GitHub Action to replace GitHub\u0026rsquo;s pull request review process.\nFurther reading How to track engineering metrics with GitHub Actions How git merge works with PRs How to reuse GitHub workflows and steps What is clean, readable code and why it matters? How to scale your codebase with evolutionary architecture Set up a remote dev environment Watch the top 3 issues with GitHub code reviews Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-09-15T00:00:00Z","image":"https://victoronsoftware.com/posts/github-code-review-issues/developer-on-tightrope-headline_hu_8292ddb59e762702.png","permalink":"https://victoronsoftware.com/posts/github-code-review-issues/","title":"Top 3 issues with GitHub code review process"},{"content":"MSI versus EXE installers When distributing software for Windows, you have two main options for installers: MSI and EXE. An MSI installer is a Windows Installer package that contains installation information and files. It uses the Windows Installer service. On the other hand, an EXE installer is a self-extracting executable file containing the installation files and an installation program. EXE installers are more customizable and do not depend on the built-in Windows Installer technology.\nThis article will show how to create an EXE installer for a program using the Inno Setup tool.\nBuild your program We will create a simple Hello World program using the Go programming language for this example.\nWith Go installed, we can build our program using the go build command. For example, given the source code in main.go:\npackage main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;hello world\u0026#34;) } We can build the program for Windows using:\nGOOS=windows GOARCH=amd64 go build -o hello-world.exe main.go Download and install Inno Setup We will need to use a Windows machine to create an EXE installer.\nInno Setup is a free installer for Windows programs. You can download it from the official website. Once you have downloaded the installer, run it and follow the installation instructions.\nCreate an EXE installer Launch the Inno Setup Compiler application. The main window will appear, with a toolbar and a script editor.\nOn the Welcome modal, choose Create a new script file using the Script Wizard and click OK.\nFollow the instructions on several subsequent screens.\nOn the Application Files screen, add your program executable file and any other files, such as a README.\nContinue following the instructions.\nOn the Compiler Settings screen, select the file name for your installer.\nFinally, click\u0026rsquo; Finish\u0026rsquo; after a couple more screens to generate the script.\nClick Yes to compile the script.\nClick No to save the script before compiling. If needed, it can be saved later.\nThe Inno Setup Compiler will create an EXE installer for your program and put it in the Documents/Output folder.\nYou can try running the installer to make sure it works as expected.\nFurther reading In the past, we demonstrated how to code sign a Windows application Recently, we explained how to create a script-only macOS install package Related Go articles How to measure Go test execution time and derive actionable insights Go benchmarks Watch how to create an EXE installer Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-09-11T00:00:00Z","image":"https://victoronsoftware.com/posts/exe-installer/exe-installer-headline_hu_a2497754ae7527ce.png","permalink":"https://victoronsoftware.com/posts/exe-installer/","title":"How to create an EXE installer for your program"},{"content":" Accurately measuring test execution time Why measure test execution time? By speeding up your test suite, you\u0026rsquo;re improving developer experience and productivity. Faster tests mean faster feedback, which leads to quicker iterations and better code quality.\nWhen you run tests, you want to know how long they take to execute. This information can help you optimize your test suite and make it run faster. By measuring the execution time of your tests, you can identify slow tests and improve their performance.\nProblems with current measurement tools We have yet to find a tool that provides detailed, actionable insights into the performance of Go tests.\nFor example, running the gotestsum tool slowest command from the gotestsum tool gave us the following output for our test suite:\ngithub.com/fleetdm/fleet/v4/server/datastore/mysql TestMDMApple 6m9.65s github.com/fleetdm/fleet/v4/server/datastore/mysql TestSoftware 4m8.9s github.com/fleetdm/fleet/v4/server/datastore/mysql TestPolicies 3m31s github.com/fleetdm/fleet/v4/server/datastore/mysql TestActivity 2m16.67s github.com/fleetdm/fleet/v4/server/datastore/mysql TestMDMWindows 2m14.85s github.com/fleetdm/fleet/v4/server/datastore/mysql TestMDMShared 2m10.27s github.com/fleetdm/fleet/v4/server/datastore/mysql TestVulnerabilities 2m7.98s github.com/fleetdm/fleet/v4/server/datastore/mysql TestPacks 1m59.2s github.com/fleetdm/fleet/v4/server/worker TestAppleMDM 1m55.11s github.com/fleetdm/fleet/v4/server/datastore/mysql TestTeams 1m47.82s github.com/fleetdm/fleet/v4/server/datastore/mysql TestAppConfig 1m42.81s github.com/fleetdm/fleet/v4/server/datastore/mysql TestHosts 1m41.79s github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240709183940 1m36.43s github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240709132642 1m36.34s github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240725182118 1m35.95s github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240730171504 1m35.73s github.com/fleetdm/fleet/v4/server/vulnerabilities/nvd TestTranslateCPEToCVE/recent_vulns 1m34.87s ... The first thing to notice is that the numbers don\u0026rsquo;t add up. Our test suite takes around 14 minutes to run, but the times in the report add up to more than 14 minutes. This discrepancy makes it hard to identify the slowest tests.\nThe second thing to notice is that our tests contain many subtests. The TestMDMApple test contains over 40 subtests. We want to know the execution time of each subtest, not just the total time for the test.\nThe third thing to notice is that the output does not provide any information regarding parallelism. We want to know if our tests run in parallel and how many run concurrently. We want to run tests in parallel when possible to speed up the test suite.\nUnderstanding parallelism in Go tests Before measuring the execution time of our tests, we need to understand how Go tests run in parallel.\nWhen you run go test, Go compiles each package in your test suite in a separate binary. It then runs each binary in parallel. The tests in different packages run concurrently. This behavior is controlled by the -p flag, which defaults to GOMAXPROCS, the number of CPUs on your machine.\nWithin a package, tests run sequentially by default \u0026ndash; the tests in the same package run one after the other. However, you can run tests in parallel within a package by calling t.Parallel() in your test functions. This behavior is controlled by the -parallel flag, which also defaults to GOMAXPROCS. So, in a system with 8 CPUs, running a test suite with many packages and parallel tests will run 8 packages concurrently and 8 tests within each package concurrently, for a total of 64 tests running concurrently.\nEach test function may have multiple subtests, which may have their own subtests, and so on. Subtests run sequentially by default. However, you can also run subtests in parallel by calling t.Parallel() in your subtest functions.\nAccurately measuring test execution time To measure the execution time of your tests, we must use the -json flag with the go test command. This flag outputs test results in JSON format, which we can parse and analyze.\nThe Action field in the JSON output shows the start and end times of each test and subtest.\n{ \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:51.969606869Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;run\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB\u0026#34; } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:51.96984165Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;run\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB/test1\u0026#34; } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:51.969928132Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;pause\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB/test1\u0026#34; } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:51.969983777Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;run\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB/test2\u0026#34; } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:51.970052987Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;pause\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB/test2\u0026#34; } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:51.970090377Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;cont\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB/test1\u0026#34; } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:51.973464469Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;cont\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB/test2\u0026#34; } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:52.015505184Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;pass\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB/test1\u0026#34;, \u0026#34;Elapsed\u0026#34;: 0.04 } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:52.015523238Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;pass\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB/test2\u0026#34;, \u0026#34;Elapsed\u0026#34;: 0.04 } { \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:22:52.015527907Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;pass\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/cpe\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestCPEDB\u0026#34;, \u0026#34;Elapsed\u0026#34;: 0 } While parsing the JSON output, we can track how many tests are running in parallel. We can then adjust the execution time of each test by dividing the total time by the number of tests running concurrently. Since we don\u0026rsquo;t have access to the actual CPU time each test used, this is the best approximation we can get.\nWhen tests run in parallel, we typically see the pause and cont actions. If we see these actions, we know that the test or subtest is running in parallel.\nWe created a parser called goteststats that does these calculations.\nAccurate test execution time measurement in practice By running our goteststats parser on the JSON output of our test suite, we gained actionable insights into our tests\u0026rsquo; performance.\nWARNING: Stopped test not found in running tests: TestGenerateMDMApple/successful_run github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240709132642: 8.142s (total: 2m12.806s parallel: 16) github.com/fleetdm/fleet/v4/server/cron TestCalendarEvents1KHosts: 7.853s (total: 36.158s parallel: 4) github.com/fleetdm/fleet/v4/server/cron TestEventForDifferentHost: 7.853s (total: 36.158s parallel: 4) github.com/fleetdm/fleet/v4/cmd/fleet TestCronVulnerabilitiesCreatesDatabasesPath: 6.878s (total: 30.232s parallel: 4) github.com/fleetdm/fleet/v4/server/vulnerabilities/nvd TestTranslateCPEToCVE/find_vulns_on_cpes: 6.849s (total: 1m34.89s parallel: 13) github.com/fleetdm/fleet/v4/server/vulnerabilities/nvd TestTranslateCPEToCVE/recent_vulns: 6.849s (total: 1m34.89s parallel: 13) github.com/fleetdm/fleet/v4/server/vulnerabilities/oval TestOvalAnalyzer/#load/invalid_vuln_path: 5.844s (total: 1m25.152s parallel: 14) github.com/fleetdm/fleet/v4/server/vulnerabilities/oval TestOvalAnalyzer/analyzing_RHEL_software: 5.844s (total: 1m25.152s parallel: 14) github.com/fleetdm/fleet/v4/server/vulnerabilities/oval TestOvalAnalyzer/analyzing_Ubuntu_software: 5.844s (total: 1m25.151s parallel: 14) github.com/fleetdm/fleet/v4/cmd/fleet TestAutomationsSchedule: 5.699s (total: 14.213s parallel: 2) github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240725182118: 5.623s (total: 1m37.577s parallel: 17) github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240709183940: 5.588s (total: 1m36.771s parallel: 17) github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240709124958: 5.52s (total: 1m35.622s parallel: 17) github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240730171504: 5.517s (total: 1m35.74s parallel: 17) github.com/fleetdm/fleet/v4/server/datastore/mysql/migrations/tables TestUp_20240726100517: 5.418s (total: 1m33.987s parallel: 17) ... For a given test, we provide the adjusted time, the total time, and the average number of tests running concurrently with this test. The adjusted time is the time the test took to execute, which is also the time saved if we removed this test from the suite.\nThe first thing to notice is that the numbers add up. The total time for the test suite is around 14 minutes, and the times in the report add up to around 14 minutes.\nThe second thing to notice is that we now have the execution time of each subtest. This information is crucial for identifying slow tests and improving their performance.\nThe third thing to notice is that we now have information about parallelism. We can see how many tests are running concurrently and how many tests are running in parallel. If we see a test with a low parallelism number, we know that this test is a bottleneck and should parallelized.\nThe WARNING message indicates that the JSON output did not contain the start time of the test. This issue can happen if the console output of the code under test does not include a new line and gets mixed with the output of Go\u0026rsquo;s testing package. For example:\n{ \u0026#34;Time\u0026#34;: \u0026#34;2024-08-26T19:23:17.8084601Z\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;output\u0026#34;, \u0026#34;Package\u0026#34;: \u0026#34;github.com/fleetdm/fleet/v4/cmd/fleetctl\u0026#34;, \u0026#34;Test\u0026#34;: \u0026#34;TestGenerateMDMApple/CSR_API_call_fails\u0026#34;, \u0026#34;Output\u0026#34;: \u0026#34;requesting APNs CSR: GET /api/latest/fleet/mdm/apple/request_csr received status 502 Bad Gateway: FleetDM CSR request failed: bad request=== RUN TestGenerateMDMApple/successful_run\\n\u0026#34; } goteststats on GitHub goteststats is available on GitHub. You can use it to get detailed performance data for your Go test suite.\nFurther reading Recently, we wrote about optimizing the performance of Go applications and analyzing Go build times. And how to measure and fix unreadable code. We also explored fuzz testing with Go. In addition, we showed how to create an EXE installer for a Go program. We also published an article on using Go modules and packages. And we wrote about automatically tracking engineering metrics with Go. Watch how to measure the execution time of Go tests accurately Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-09-04T00:00:00Z","image":"https://victoronsoftware.com/posts/go-test-execution-time/crash-test-dummy-headline_hu_ceb34e7ab7732ad1.png","permalink":"https://victoronsoftware.com/posts/go-test-execution-time/","title":"How to measure the execution time of Go tests accurately"},{"content":" Creating a Go benchmark Running Go benchmarks What is benchmarking? Performance optimization is a critical part of software development. Once your application has been released and is being used by real users, you may need to optimize its performance. One way to do this is to benchmark your code to identify bottlenecks and improve its performance. Benchmarking provides you with data to make informed decisions about what parts of your code can be sped up and by how much.\nBenchmarking is the process of measuring your code\u0026rsquo;s performance. It involves running your code multiple times and measuring how long it takes to execute. By running your code multiple times, you can get an average execution time, which is more reliable than a one-off report.\nIdentifying the bottlenecks In our application, we deserialize and process large amounts of JSON data once every hour. We noticed that this process was taking a long time for some of our users. First, we used Go pprof to enable profiling and generated a flame graph to identify the bottlenecks in our code.\nGo pprof flame graph The flame graph showed us that the JSON decoding process took the most time. We benchmarked different serialization libraries to find the fastest one for our use case.\nCreating a Go benchmark In Go, you can write benchmarks using the built-in testing package. Benchmarks are written similarly to unit tests but with the Benchmark prefix instead of the Test prefix.\nBefore creating and running the benchmark, we generated 1000 test JSON files in the testdata directory.\nTo benchmark JSON decoding, we created the following benchmark.\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;testing\u0026#34; ) const files = 1000 const itemsPerFile = 1000 func BenchmarkJSONImport(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { // Read in the file (do not time) b.StopTimer() fileNumber := i % files data, err := os.ReadFile(fmt.Sprintf(\u0026#34;testdata/sample_%d.json\u0026#34;, fileNumber)) if err != nil { b.Fatal(err) } b.StartTimer() var samples []Sample dec := json.NewDecoder(bytes.NewReader(data)) if err := dec.Decode(\u0026amp;samples); err != nil { b.Fatal(err) } if len(samples) != itemsPerFile { b.Fatalf(\u0026#34;expected %d samples, got %d\u0026#34;, itemsPerFile, len(samples)) } } } Starting the function name with Benchmark indicates to go test that this is a benchmark.\nThe testing package adjusts the number of iterations through the for i := 0; i \u0026lt; b.N; i++ loop until the function lasts long enough to be timed reliably.\nThe b.StopTimer() and b.StartTimer() calls exclude part of the code from the benchmark.\nRunning Go benchmarks To run all benchmarks, add -bench=. flag to go test:\ngo test -bench=. The result will look like this:\ngoos: darwin goarch: arm64 pkg: serializer cpu: Apple M2 Pro BenchmarkJSONImport-12 357 3324997 ns/op PASS ok serializer 1.868s It tells us that unmarshalling a single file with json.Decode takes an average of 3.3 milliseconds. The benchmark ran the loop 357 times.\nBenchmarking encoding/gob Next, we will benchmark the built-in encoding/gob library.\nfunc BenchmarkGobImport(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { // Read in the file (do not time) b.StopTimer() fileNumber := i % files data, err := os.ReadFile(fmt.Sprintf(\u0026#34;testdata/sample_%d.bin\u0026#34;, fileNumber)) if err != nil { b.Fatal(err) } b.StartTimer() // decode gob var samples []Sample dec := gob.NewDecoder(bytes.NewReader(data)) if err := dec.Decode(\u0026amp;samples); err != nil { b.Fatal(err) } if len(samples) != itemsPerFile { b.Fatalf(\u0026#34;expected %d samples, got %d\u0026#34;, itemsPerFile, len(samples)) } } } Running the two benchmarks gives us:\nBenchmarkJSONImport-12 360 3279579 ns/op BenchmarkGobImport-12 2262 475469 ns/op The benchmark data shows that decoding with encoding/gob takes almost 7 times faster than using encoding/json. This gives sufficient data to present to our management and argue for switching from JSON. In addition, we can benchmark other serialization libraries to see if any of them are even faster.\nFor additional data, we included reading the file in our benchmark numbers for a complete picture of the expected speedup:\nBenchmarkJSONImport-12 360 3374064 ns/op BenchmarkGobImport-12 2710 481935 ns/op BenchmarkJSONImportFile-12 306 3746758 ns/op BenchmarkGobImportFile-12 2176 554254 ns/op Go benchmark code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/go-benchmark-serializers\nFurther reading Beyond benchmarking, you can step up your performance game with OpenTelemetry and Jaeger.\nIn addition:\nRecently, we listed the top metrics to gather during software load testing. We also wrote about accurately measuring Go test execution time. We discussed how to use Go to accurately unmashal JSON payloads with null, set, and missing fields. Also, see our previous article on creating fuzz tests in Go. Watch how to benchmark Go serializers Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-08-28T00:00:00Z","image":"https://victoronsoftware.com/posts/optimizing-performance-of-go-app/race-cars-headline_hu_a076cfe676e1bed4.png","permalink":"https://victoronsoftware.com/posts/optimizing-performance-of-go-app/","title":"How to benchmark performance of Go serializers"},{"content":" Securing database secrets with Docker secrets Securing database secrets with SQL commands What is a Zero Trust development environment? Zero Trust is a security model that assumes no trust, even inside the network. Every request is authenticated, authorized, and encrypted in a Zero Trust environment. This approach helps protect against data breaches and insider threats.\nIn our example use case, we create a development environment in a cloud instance, which includes a MySQL database running in a Docker container. We need to be able to access the MySQL database from our local machine for development purposes. However, the database may contain sensitive data, such as API keys or user passwords. We want to secure the MySQL database to prevent unauthorized access.\nWe want to make sure that the MySQL database is not easily accessible from the internet. In addition, we want to limit the exposure of database credentials.\nLaunching MySQL Docker container We can run a MySQL database in a Docker container using the official MySQL Docker image. We create docker-compose.yml like:\nservices: mysql: image: mysql:8.0 command: [ \u0026#34;mysqld\u0026#34;, \u0026#34;--datadir=/tmp/mysqldata\u0026#34;, ] environment: MYSQL_ROOT_PASSWORD: toor MYSQL_DATABASE: fleet MYSQL_USER: fleet MYSQL_PASSWORD: insecure ports: - \u0026#34;3306:3306\u0026#34; And we run docker-compose up to start the MySQL database.\nWe can access the MySQL database by using the MySQL client:\nmysql -h 127.0.0.1 -P 3306 -uroot -ptoor As we can see, the passwords are stored in plain text in the docker-compose.yml file. We want to avoid storing sensitive data in plain text.\nSecuring database secrets with Docker secrets Docker secrets allow us to store sensitive data, such as passwords, securely. We can create secrets and use them in the docker-compose.yml file.\nsecrets: mysql_root_password: file: ./mysql_root_password.txt mysql_password: environment: MYSQL_PASSWORD services: mysql: image: mysql:8.0 command: [ \u0026#34;mysqld\u0026#34;, \u0026#34;--datadir=/tmp/mysqldata\u0026#34;, ] secrets: - mysql_root_password - mysql_password environment: MYSQL_ROOT_PASSWORD_FILE: /run/secrets/mysql_root_password MYSQL_DATABASE: fleet MYSQL_USER: fleet MYSQL_PASSWORD_FILE: /run/secrets/mysql_password ports: - \u0026#34;3306:3306\u0026#34; We create a mysql_root_password.txt file and run MYSQL_PASSWORD=insecure docker-compose up to start the MySQL database.\nThe above example shows that the MySQL root password is stored in a file, and the MySQL password is passed as an environment variable. Although this approach may be an improvement, it is not secure for a Zero Trust environment. A user with access to the file system can read the secrets, and environment variables can be read by anyone who can run the ps command, like: ps eww \u0026lt;docker compose process ID\u0026gt;.\nIn addition, a user can dump the secrets from the Docker container by running:\ndocker exec \u0026lt;container ID\u0026gt; cat /run/secrets/mysql_root_password Securing database secrets with SQL commands To secure the MySQL database without exposing the secrets on the server, we can use MySQL commands to set the passwords. We spin up MySQL with the following docker-compose.yml:\nservices: mysql: image: mysql:8.0 command: [ \u0026#34;mysqld\u0026#34;, \u0026#34;--datadir=/tmp/mysqldata\u0026#34;, ] environment: MYSQL_ROOT_PASSWORD: toor MYSQL_ONETIME_PASSWORD: true ports: - \u0026#34;3306:3306\u0026#34; We set the root password and marked the root user as expired with MYSQL_ONETIME_PASSWORD: true.\nNow, as the second step, we can run the following commands to set the passwords:\necho \\ \u0026#34;ALTER USER root IDENTIFIED BY \u0026#39;$(op read op://employee/DEMO_SERVER/MYSQL_ROOT_PASSWORD)\u0026#39;;\u0026#34; \\ \u0026#34;CREATE DATABASE fleet;\u0026#34; \\ \u0026#34;CREATE USER \u0026#39;fleet\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;$(op read op://employee/DEMO_SERVER/MYSQL_PASSWORD)\u0026#39;;\u0026#34; \\ \u0026#34;GRANT ALL PRIVILEGES ON fleet.* TO \u0026#39;fleet\u0026#39;@\u0026#39;%\u0026#39;;\u0026#34; \\ \u0026#34;FLUSH PRIVILEGES;\u0026#34; \\ | mysql -h 127.0.0.1 -P 3306 -uroot -ptoor --connect-expired-password In the above command, we use 1Password as our secrets manager. We read the secrets from 1Password and pass them to the MySQL client to set the passwords.\nAdditional security considerations This article focused on securing the MySQL passwords. However, there are additional security considerations when running MySQL in a Zero Trust environment:\nEncrypting sensitive data \u0026ndash; all sensitive data should be encrypted when stored in the database Limiting access to specific IPs \u0026ndash; we can add a server firewall to restrict access to the MySQL port Further reading Recently, we wrote about setting up a remote development environment.\nWe also explained how to use STDIN to read your program arguments.\nWe\u0026rsquo;ve previously written about MySQL master-slave replication. You can use MySQL replication to create a high-availability setup for your MySQL databases.\nWatch how to secure a MySQL Docker container for Zero Trust Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-08-20T00:00:00Z","image":"https://victoronsoftware.com/posts/secure-mysql-docker/mysql-docker-headline_hu_e55bbd55daceafb0.png","permalink":"https://victoronsoftware.com/posts/secure-mysql-docker/","title":"How to secure MySQL Docker container for Zero Trust"},{"content":"STDIN is more secure than environment variables or command-line arguments When you pass command-line arguments to a program, they are visible to anyone who can run the ps command. Allowing others to read arguments is a security risk if the arguments contain sensitive information like passwords or API keys.\nEnvironment variables are also visible to anyone who can run the ps command. They are also globally visible to the program, so any arbitrary code in your application can extract the environment variables.\nTo get the environment variables of a process, run ps eww \u0026lt;PID\u0026gt;. For example:\n$ ps eww 1710 PID TTY STAT TIME COMMAND 1710 pts/0 Ss+ 0:00 bash SYSTEMD_EXEC_PID=1209 SSH_AUTH_SOCK=/run/user/1000/keyring/ssh SESSION_MANAGER=local/victor-ubuntu:@/tmp/.ICE-unix/1176,unix/victor-ubuntu:/tmp/.ICE-unix/1176 GNOME_TERMINAL_SCREEN=/org/gnome/Terminal/screen/ab0b9d6a_a699_4bc5_bb53_628be016afa5 LANG=en_US.UTF-8 XDG_CURRENT_DESKTOP=ubuntu:GNOME PWD=/home/victor WAYLAND_DISPLAY=wayland-0 DISPLAY=:0 QT_IM_MODULE=ibus USER=victor DESKTOP_SESSION=ubuntu XDG_MENU_PREFIX=gnome- HOME=/home/victor DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus SSH_AGENT_LAUNCHER=gnome-keyring _=/usr/bin/gnome-session XDG_CONFIG_DIRS=/etc/xdg/xdg-ubuntu:/etc/xdg VTE_VERSION=6800 XDG_SESSION_DESKTOP=ubuntu QT_ACCESSIBILITY=1 GNOME_DESKTOP_SESSION_ID=this-is-deprecated GNOME_SETUP_DISPLAY=:1 GTK_MODULES=gail:atk-bridge LOGNAME=victor GNOME_TERMINAL_SERVICE=:1.83 GNOME_SHELL_SESSION_MODE=ubuntu XDG_RUNTIME_DIR=/run/user/1000 XMODIFIERS=@im=ibus SHELL=/bin/bash XDG_SESSION_TYPE=wayland PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin USERNAME=victor COLORTERM=truecolor XAUTHORITY=/run/user/1000/.mutter-Xwaylandauth.R816R2 XDG_DATA_DIRS=/usr/share/ubuntu:/usr/local/share/:/usr/share/:/var/lib/snapd/desktop IM_CONFIG_PHASE=1 TERM=xterm-256color GDMSESSION=ubuntu XDG_SESSION_CLASS=user STDIN is more secure because it is not visible to the ps command and is not globally visible to the program. Thus, only the parts of the program that explicitly read from STDIN can access this data.\nHow to read program arguments from STDIN with Go In the code example below, we check if any data is being piped in from STDIN with os.ModeNamedPipe. Then, we wait to read all the data from STDIN with ioutil.ReadAll. Finally, we parse the STDIN data just like a shell would using the github.com/kballard/go-shellquote library and append it to any existing command-line arguments.\nHow to integrate with a secret manager One way to securely pass sensitive information to a program is to store it in a secret manager like 1Password. Then, you can read the secret from the secret manager and pass it to the program via STDIN. For example:\necho --secret $(op read op://employee/example_server/secret) | go run read-args-from-stdin.go Further reading Recently, we discussed how to unmarshal JSON payloads with null, set, and missing keys using Go.\nPreviously, we wrote how we catch missed authorization checks in our Go application.\nWatch how to read program arguments from STDIN Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-08-12T00:00:00Z","image":"https://victoronsoftware.com/posts/get-args-from-stdin/stdin-headline_hu_d34d818ae2ec114f.png","permalink":"https://victoronsoftware.com/posts/get-args-from-stdin/","title":"Why you should use STDIN to read your program arguments"},{"content":" Find code owners from the command line What are GitHub code owners? GitHub CODEOWNERS is a file that defines the individuals or teams responsible for code in a repository. When a user creates a pull request, GitHub uses the CODEOWNERS file to suggest the appropriate reviewers for the pull request. This process helps ensure that the right people review the code changes.\nRepository owners can enable branch protection rules that require the code owner of each changed file to approve the pull request.\nThe problem: too many files and too many code owners It can be challenging to determine who needs to review a pull request in a large repository with many files and many code owners. This challenge is especially true when the pull request touches many files.\nMany times, I\u0026rsquo;ve asked another engineer to approve my PR, and they approved it, but GitHub said that the PR still needed approval from another code owner. I needed another approval because my PR changed another file with another code owner, and I didn\u0026rsquo;t know about it.\nSteps to find the minimum required code owners To find the minimum required code owners for a pull request, we can use these steps:\nGet the list of changed files in the pull request. For each changed file, get the list of code owners. Find the minimum set of code owners that covers all the above lists. Finding the code owners manually The above steps can be done manually by opening the pull request in GitHub and hovering over the blue CODEOWNERS icon for each changed file to see the code owners. However, this can be time-consuming and error-prone.\nSee the code owners on hover Finding the code owners from the command line To automate the above steps, we will need the following prerequisites:\nGitHub CLI installed and logged in Command-line JSON processor jq installed A CODEOWNERS parser installed, such as https://github.com/hmarr/codeowners From the top directory containing your git repository, run the following:\ngh pr view $MY_PR_NUMBER --json files | jq -r \u0026#39;.files[] .path\u0026#39; \\ | xargs codeowners | tr -s \u0026#39; \u0026#39; | cut -f2- -d \u0026#39; \u0026#39; | sort -u Where $MY_PR_NUMBER is the number of your pull request.\nThe first part of the command, gh pr view $MY_PR_NUMBER --json files, gets the list of changed files in the pull request in JSON format. The second part, jq -r '.files[] .path', extracts the file paths from the JSON. The third part, xargs codeowners, runs the codeowners command for each file. The fourth optional part, tr -s ' ', removes extra spaces. The fifth part, cut -f2- -d ' ', removes the first column. The last part, sort -u, sorts and removes duplicates.\nThe output will be the list of code owners for the changed files in the pull request. For example:\n(unowned) @fleetdm/go @getvictor @lucasmrod @roperzh @mostlikelee @lucasmrod @getvictor @jacobshandling @roperzh @gillespi314 @lucasmrod @getvictor At this point, we can do additional processing, such as excluding some code owners.\nBy visually inspecting the output, we can determine the minimum set of code owners that need to review the pull request.\nFurther reading Recently, we covered top 3 issues with GitHub code review process.\nWe also wrote about how merges work with GitHub pull requests.\nPreviously, we explained how to create reusable workflows and steps in GitHub Actions.\nWatch how to find the minimum required code owners for a pull request Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-08-06T00:00:00Z","image":"https://victoronsoftware.com/posts/find-code-owners-for-pull-request/codeowners-headline_hu_f73c50cc04e718c6.png","permalink":"https://victoronsoftware.com/posts/find-code-owners-for-pull-request/","title":"Find required code owner approvers for a PR in 3 steps"},{"content":" Add Jest testing framework Write a unit test Review unit test coverage This article is part of our series on building a Chrome extension.\nWhy add unit tests? Unit tests help us catch bugs early, ensure our extension continues to work as expected in different scenarios, and make it easier to refactor our code. In this article, we will add unit tests to our Chrome extension.\nAdd Jest to the project Jest is a popular JavaScript testing framework. We will use Jest to write and run unit tests for our Chrome extension.\nTo install Jest, run:\nnpm install --save-dev jest jest-environment-jsdom ts-jest @types/jest jest is the testing framework jest-environment-jsdom simulates a browser environment for Jest tests ts-jest allows Jest to work with TypeScript @types/jest provides TypeScript definitions for Jest Configure Jest Create a jest.config.ts file in the root of the project with the following content:\nimport type { JestConfigWithTsJest } from \u0026#34;ts-jest\u0026#34; const config: JestConfigWithTsJest = { setupFiles: [\u0026#34;./__mocks__/chrome.ts\u0026#34;], testEnvironment: \u0026#34;jsdom\u0026#34;, transform: { \u0026#34;^.+.ts$\u0026#34;: [\u0026#34;ts-jest\u0026#34;, {}], }, } export default config The setupFiles option loads a mock for the Chrome API. In the next step, we will create this mock.\nThe testEnvironment option sets a browser testing environment by default. We can override the environment at the top of each test file:\n/** * @jest-environment jsdom */ The transform option specifies to process TypeScript test files with ts-jest.\nCreate a mock for the Chrome API Our extension code relies on the Chrome API, which is unavailable in our unit test environment. We will create a mock for the Chrome API to simulate its behavior in our tests.\nA mock is a fake implementation of a function or object that allows us to test our code in isolation. Mocks are helpful for testing code that depends on external services or APIs.\nCreate a __mocks__ folder in the root of the project. The __mocks__ name is a Jest convention for mock files. In that folder, add a chrome.ts file with the following content:\n// eslint-disable-next-line @typescript-eslint/ban-ts-comment -- disable ESLint check for the next line // @ts-nocheck -- this TS comment turns off TypeScript type checking for this file because we do not // mock the entire Chrome API, but only the parts we need global.chrome = { runtime: { onInstalled: { addListener: jest.fn(), }, onMessage: { addListener: jest.fn(), }, onStartup: { addListener: jest.fn(), }, sendMessage: jest.fn(), }, storage: { sync: { get: jest.fn(), set: jest.fn(), }, }, } The empty jest.fn() implementations can be replaced during testing with custom behavior using Jest\u0026rsquo;s mocking functions using jest.spyOn.\nWrite a unit test We will test the content.ts file in our first unit test. This file contains the logic for the content script that runs on web pages when the extension is active. The content script blurs a page element that contains a user-defined keyword.\nCreate a content.test.ts file in the src folder with the following content:\nimport { blurFilter, observe, config } from \u0026#34;./content\u0026#34; describe(\u0026#34;blur\u0026#34;, () =\u0026gt; { test(\u0026#34;blur a secret\u0026#34;, () =\u0026gt; { // Define the document (web page) that we will test against document.body.innerHTML = ` \u0026lt;div id=\u0026#34;testDiv\u0026#34;\u0026gt; \u0026#34;My secret\u0026#34; \u0026lt;/div\u0026gt;` // Set value to blur config.item = \u0026#34;secret\u0026#34; // Start observing the document. observe() // Make sure the element is blurred as expected const testDiv = document.getElementById(\u0026#34;testDiv\u0026#34;) as HTMLInputElement expect(testDiv).toBeDefined() expect(testDiv.style.filter).toBe(blurFilter) }) }) In the above test, the Jest functions describe and test define a test suite and a test case, respectively. The expect function checks whether the test results match the expected values.\nRun the unit tests The Jest unit test can be run using the following command:\nnpx jest The result of the test should look like:\nconsole.debug blurred id:testDiv class: tag:DIV text: \u0026#34;My secret\u0026#34; at blurElement (src/content.ts:36:11) at Array.forEach (\u0026lt;anonymous\u0026gt;) at Array.forEach (\u0026lt;anonymous\u0026gt;) at Array.forEach (\u0026lt;anonymous\u0026gt;) at Array.forEach (\u0026lt;anonymous\u0026gt;) PASS src/content.test.ts blur ✓ blur a secret (15 ms) Test Suites: 1 passed, 1 total Tests: 1 passed, 1 total Snapshots: 0 total Time: 1.149 s Ran all test suites. Add the following script to the package.json file:\n\u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;jest\u0026#34;, } Now you can run the tests using the npm test or npm run test.\nReview unit test coverage Code coverage measures how much of the code is tested by the unit tests. A high percentage indicates that most of the code is tested and less likely to contain bugs. Code coverage is an important metric for assessing the quality of the code. A common target for code coverage is 80% or higher.\nJest can generate a code coverage report to show which parts of the code are covered by the unit tests. To create a coverage report, add the --coverage flag to the Jest command:\nnpx jest --coverage The terminal output will include the code coverage summary:\n------------|---------|----------|---------|---------|-------------------------- File | % Stmts | % Branch | % Funcs | % Lines | Uncovered Line #s ------------|---------|----------|---------|---------|-------------------------- All files | 45.23 | 48.57 | 42.85 | 45.23 | content.ts | 45.23 | 48.57 | 42.85 | 45.23 | 22,26,50-54,62-69,88-117 ------------|---------|----------|---------|---------|-------------------------- The full report is available in the coverage folder. Open the coverage/lcov-report/index.html file in a browser to view the detailed coverage report.\nNote that the code coverage report only includes the files in the test run. If you want to include all files in the coverage report, we can add the collectCoverageFrom option to the jest.config.ts Jest configuration file:\ncollectCoverageFrom: [\u0026#34;src/**/*.ts\u0026#34;], Now, the report shows a complete picture:\n---------------|---------|----------|---------|---------|-------------------------- File | % Stmts | % Branch | % Funcs | % Lines | Uncovered Line #s ---------------|---------|----------|---------|---------|-------------------------- All files | 16.96 | 30.9 | 10.71 | 16.96 | background.ts | 0 | 0 | 0 | 0 | 1-21 common.ts | 0 | 0 | 0 | 0 | 17-19 content.ts | 45.23 | 48.57 | 42.85 | 45.23 | 22,26,50-54,62-69,88-117 options.ts | 0 | 0 | 0 | 0 | 2-31 popup.ts | 0 | 0 | 0 | 0 | 1-86 ---------------|---------|----------|---------|---------|-------------------------- HTML coverage report Adding unit tests to GitHub Actions To make sure that our unit tests are run automatically on every push to the repository, we can add them to a GitHub Actions workflow. In the Linting and formatting TypeScript article, we added ESLint to GitHub Actions. We can add a step to run the Jest tests in the same workflow.\n- name: Test run: | npm run test Unit test code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/7-unit-tests\nOther articles on Unit Testing Explore fuzz testing with Go Watch how we set up unit testing for our Chrome extension Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-07-26T00:00:00Z","image":"https://victoronsoftware.com/posts/add-unit-tests-to-chrome-extension/chrome-jest-headline_hu_acca9c60d21bac2c.png","permalink":"https://victoronsoftware.com/posts/add-unit-tests-to-chrome-extension/","title":"Add unit tests to Chrome extension (2024)"},{"content":"This article will present a problem we encountered in our production distributed system and how we solved it using a distributed lock.\nThe problem \u0026ndash; data inconsistency Recently, we started using the Google Calendar API to monitor calendar changes. However, we noticed that it is possible to receive a second callback while processing the first one. This second callback can lead to data inconsistency, race conditions, and deadlocks.\nData inconsistency In the above diagram, two servers, A and B, are processing calendar events. Server B receives a callback from Google Calendar API stating that something has changed in the calendar. Google does not provide information about what event changed, so server B must fetch the event of interest from the calendar. While server B is fetching the event, server A also receives a callback. Server A also fetches the event from the calendar. Both servers now have the same event but are unaware of each other\u0026rsquo;s actions. Server B updates the event with new information. Server A also updates the event with different details, potentially overwriting or duplicating Server B\u0026rsquo;s changes. The calendar event is now in an inconsistent state, as is the data in our database.\nWhat is a distributed lock? A distributed lock is a mechanism that allows multiple servers to coordinate access to a shared resource. This mechanism is widely used across the software industry to ensure data consistency in distributed systems.\nIn our case, we need to make sure that only one server is processing a calendar event at a time. The distributed lock will prevent the second server from processing the event until the first server completes.\nImplementation of distributed lock We implemented a distributed lock using Redis. Redis is an in-memory data structure store that can be used as a database, cache, and message broker. To acquire the lock, our server sets a key in Redis with a unique value using the Redis SET command.\nSET mykey \u0026#34;myvalue\u0026#34; NX PX 60000 The NX option only sets the key if it does not exist. The PX 60000 option sets the key\u0026rsquo;s expiration time to 60 seconds. This ensures that the lock is released if the server crashes or does not release it in a timely manner.\nTo release the lock, we EVAL a Lua script that checks if the key\u0026rsquo;s value matches the unique value set by the server. If the values match, the script deletes the key using the Redis DEL command.\nif redis.call(\u0026#34;get\u0026#34;, KEYS[1]) == ARGV[1] then return redis.call(\u0026#34;del\u0026#34;, KEYS[1]) else return 0 end We only release the lock if the value matches, ensuring that the server that acquired the lock releases it.\nDistributed lock solution With the distributed lock in place, we can make sure that only one server is processing a calendar event at a time.\nUsing distributed lock with a processing queue In the above diagram, server B receives a calendar callback and acquires a lock from Redis. Server A also gets a callback but cannot acquire the lock since it has already been taken by Server B. Instead of waiting, Server A puts the event in a processing queue. Once Server B finishes processing the event, it releases the lock. Server B then checks the queue. Finding an event in the queue, the server starts a new worker process to process the events. The worker processes all outstanding events in the queue and exits on completion.\nWaiting to acquire the lock One issue we encountered was that another system process needed to acquire the lock. The process could keep trying to obtain the lock, but there was no guarantee that it would be successful in a reasonable amount of time because it could compete with other servers.\nFairness in acquiring the lock We implemented a fairness mechanism to ensure a priority process could acquire the lock.\nCron job is guaranteed to acquire the lock In the above diagram, the worker process has acquired the lock. Another process, a cron job, also needs to acquire the lock. The cron job is a priority process that needs to run at a specific time. The cron job tries to acquire the lock but fails because the worker process has it. The cron job sets a key in Redis that indicates that it wants to acquire the lock next. This action tells the other servers not to acquire the lock. The cron job then retries acquiring the lock until it is successful.\nDistributed lock code on GitHub We implemented the distributed lock logic in Go. The crucial part of the code is in the redis_lock.go file.\nDistributed lock video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-07-18T00:00:00Z","image":"https://victoronsoftware.com/posts/distributed-lock/distributed-lock-headline_hu_17089af628d186e7.png","permalink":"https://victoronsoftware.com/posts/distributed-lock/","title":"Using a distributed lock in production distributed systems"},{"content":" Add CSS to the webpack bundle Add Tailwind CSS Use Tailwind CSS utility classes This article is part of our complete guide to building a Chrome extension.\nIn the previous articles on creating a Chrome extension and adding an options page to a Chrome extension, we built two user interface pages \u0026ndash; the popup and the options page. In this article, we will improve the look and maintainability of our extension by adding a CSS framework.\nWhy add a CSS framework? Using a CSS framework like Tailwind CSS or Bootstrap can help you:\nQuickly style your extension Make your extension look professional Save time on writing custom CSS Improve the maintainability of your code Use pre-built and optimized components Add CSS to the webpack bundle Before we can use a CSS framework, we need to add CSS integration to our webpack bundle. In adding webpack to a Chrome extension, we set up webpack with a static CSS file. We will include the CSS file as part of our JavaScript bundle. Although the CSS files can be kept separate, we will include them in the JavaScript bundle since this is currently the best practice in web development.\nWe will use style-loader to inject CSS into our JavaScript and css-loader to convert our CSS file to a string. First, install the packages:\nnpm install --save-dev style-loader css-loader Then, update the webpack.common.ts to include the rule for CSS files:\n{ test: /\\.css$/, use: [ \u0026#34;style-loader\u0026#34;, \u0026#34;css-loader\u0026#34;, ], }, Move the CSS files from the static folder to the src folder, and update the popup.ts and options.ts to import the CSS files:\nimport \u0026#34;./popup.css\u0026#34;; import \u0026#34;./options.css\u0026#34;; And remove the link CSS references from the static/popup.html and static/options.html HTML files.\nWhen you run npm run build, the CSS file content will be included in the JavaScript bundle. You can verify this by inspecting the dist folder and looking at the popup.js and options.js files.\nAdd Tailwind CSS We will use Tailwind CSS for our extension. It is a popular CSS framework focused on providing CSS utility classes. First, install these packages and generate the TypeScript config file:\nnpm install --save-dev tailwindcss postcss-loader npx tailwindcss init --ts postcss-loader is a webpack loader that processes CSS with PostCSS. PostCSS is a plugin-based CSS transformer recommended for integrating Tailwind CSS with the webpack build flow.\nUpdate the webpack.common.ts CSS rule to include the loader for PostCSS with the tailwindcss plugin:\n{ test: /\\.css$/, use: [ \u0026#34;style-loader\u0026#34;, \u0026#34;css-loader\u0026#34;, { loader: \u0026#34;postcss-loader\u0026#34;, options: { postcssOptions: { plugins: [\u0026#34;postcss-import\u0026#34;, \u0026#34;tailwindcss\u0026#34;], }, }, }, ], }, The postcss-import plugin processes @import statements in CSS files, which we will use in the next step.\nUpdate the generated tailwind.config.ts file to point to our HTML files:\nimport type { Config } from \u0026#39;tailwindcss\u0026#39; export default { content: [\u0026#34;./static/*.html\u0026#34;], theme: { extend: {}, }, plugins: [], } satisfies Config Use Tailwind CSS utility classes in the HTML and CSS files This section will update our CSS to use Tailwind CSS utility classes. Using Tailwind CSS utility classes is a slightly different approach to writing CSS. Instead of writing custom CSS classes, we use pre-built utility classes to style our elements. It is a small step above writing raw CSS, but it is more maintainable and easier to read. Utility classes are documented in the Tailwind CSS documentation.\nFirst, we create a new CSS file, src/common.css to contain styles shared by both pages:\n@tailwind base; @tailwind components; @tailwind utilities; .my-input { @apply block m-1.5 p-2 bg-gray-50 border border-gray-300 text-gray-900 text-sm rounded-lg focus:ring-blue-500 focus:border-blue-500; } The @apply directive is a Tailwind CSS feature that allows us to create a custom CSS class using Tailwind CSS utility classes.\nThen, update the popup.css and options.css files to use the Tailwind CSS classes. popup.css will use Tailwind CSS utility classes:\n@import \u0026#34;./common.css\u0026#34;; .my-center { @apply w-full inline-flex items-center justify-center; } .my-button-link { @apply text-indigo-400 m-1.5 px-3 py-1 text-xs outline-none focus:outline-none ease-linear transition-all duration-150; } options.css will only use our common CSS file:\n@import \u0026#34;./common.css\u0026#34;; Next, we update static/popup.html to use our new CSS classes as well as other Tailwind CSS classes:\n\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;My popup\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;label class=\u0026#34;my-center cursor-pointer\u0026#34;\u0026gt; \u0026lt;input id=\u0026#34;enabled\u0026#34; type=\u0026#34;checkbox\u0026#34; class=\u0026#34;sr-only peer\u0026#34; /\u0026gt; \u0026lt;span class=\u0026#34;m-1.5 relative w-16 h-8 bg-gray-200 rounded-full after:content-[\u0026#39;\u0026#39;] after:absolute after:top-0.5 after:start-[4px] after:bg-white after:border-gray-300 after:border after:rounded-full after:h-7 after:w-7 after:transition-all peer-focus:outline-none peer-focus:ring-4 peer-focus:ring-blue-300 peer-checked:after:translate-x-full rtl:peer-checked:after:-translate-x-full peer-checked:after:border-white peer-checked:bg-blue-600\u0026#34; \u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;label for=\u0026#34;item\u0026#34; class=\u0026#34;invisible\u0026#34;\u0026gt;\u0026lt;/label\u0026gt; \u0026lt;input class=\u0026#34;my-input w-52\u0026#34; id=\u0026#34;item\u0026#34; type=\u0026#34;text\u0026#34; /\u0026gt; \u0026lt;div class=\u0026#34;my-center\u0026#34;\u0026gt; \u0026lt;button id=\u0026#34;go-to-options\u0026#34; class=\u0026#34;my-button-link\u0026#34;\u0026gt;Advanced options\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;popup.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; We built a custom checkbox using Tailwind CSS utility classes. The peer class styles the checkbox and the label together.\nAnd static/options.html:\n\u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Advanced options\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body class=\u0026#34;m-1.5\u0026#34;\u0026gt; \u0026lt;h2 class=\u0026#34;text-3xl m-1.5 font-extrabold\u0026#34;\u0026gt;Advanced options\u0026lt;/h2\u0026gt; \u0026lt;br /\u0026gt; \u0026lt;h3 class=\u0026#34;text-xl m-1.5 font-bold\u0026#34;\u0026gt;Web host to exclude\u0026lt;/h3\u0026gt; \u0026lt;label for=\u0026#34;exclude_host\u0026#34; class=\u0026#34;invisible\u0026#34;\u0026gt;\u0026lt;/label\u0026gt; \u0026lt;input id=\u0026#34;exclude_host\u0026#34; class=\u0026#34;my-input w-96\u0026#34; type=\u0026#34;text\u0026#34; placeholder=\u0026#34;example.com\u0026#34;/\u0026gt; \u0026lt;script src=\u0026#34;options.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Testing the extension, we should see a nicer-looking popup page:\nPopup with Tailwind CSS classes And a nicer-looking options page:\nOptions with Tailwind CSS classes Next steps In the next part of this series, we will add unit tests to our Chrome extension. Unit tests help us catch bugs early, ensure our extension continues to work as expected in different scenarios and make it easier to refactor our code.\nCSS framework code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/6-css-framework\nCSS framework video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-07-10T00:00:00Z","image":"https://victoronsoftware.com/posts/add-css-framework-to-chrome-extension/chrome-tailwind-headline_hu_693c0d39d71548c2.png","permalink":"https://victoronsoftware.com/posts/add-css-framework-to-chrome-extension/","title":"Add CSS framework to Chrome extension (2024)"},{"content":"This article is part of a series on building a complete production-ready Chrome extension.\nIn the first article of the series, we introduced the main parts of a Chrome extension \u0026ndash; the service worker (background script), content script, and popup. This article will add a fourth part to our Chrome extension \u0026ndash; an options page. This page will allow users to configure the extension\u0026rsquo;s behavior and settings.\nWhy add an options page? An options page is a user-friendly way for users to customize the extension to their needs. It can be as simple as a few checkboxes or as complex as a full settings page with multiple tabs. Users can access the options page from the Chrome extension\u0026rsquo;s popup or the Chrome extension\u0026rsquo;s context menu.\nAdding an options page To add an options page to our Chrome extension, we need to create a new HTML file and add it to the extension\u0026rsquo;s manifest. Our options page will be a simple HTML file with some JavaScript to handle user interactions.\nOur example options page will allow a user to exclude a web host, like victoronsoftware.com, from the extension\u0026rsquo;s functionality. The extension will store the excluded host in the extension\u0026rsquo;s local storage.\nWe will add several new files, update the extension\u0026rsquo;s configuration, and update the existing files.\nAdd a new file options.html to the static folder:\n\u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Advanced options\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;options.css\u0026#34;/\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h2\u0026gt;Advanced options\u0026lt;/h2\u0026gt; \u0026lt;h3\u0026gt;Web host to exclude\u0026lt;/h3\u0026gt; \u0026lt;input id=\u0026#34;exclude_host\u0026#34; type=\u0026#34;text\u0026#34;/\u0026gt; \u0026lt;script src=\u0026#34;options.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Add a new file options.css to the static folder:\n#exclude_host { margin: 5px; width: 500px; } Add a new file options.ts to the src folder, which will watch for changes on the options page:\nimport {Message, StoredConfig} from \u0026#34;./common\u0026#34; chrome.storage.sync.get(null, (data) =\u0026gt; { const config = data as StoredConfig const excludeHost = config.excludeHost ?? \u0026#34;\u0026#34; const input = document.getElementById( `exclude_host`, ) as HTMLInputElement input.value = excludeHost input.addEventListener(\u0026#34;change\u0026#34;, (event) =\u0026gt; { if (event.target instanceof HTMLInputElement) { const updatedExcludeWebsite = event.target.value const updatedConfig: StoredConfig = {excludeHost: updatedExcludeWebsite} void chrome.storage.sync.set(updatedConfig) // Send message to content script in all tabs void chrome.tabs .query({}) .then((tabs) =\u0026gt; { const message: Message = {excludeHost: updatedExcludeWebsite} for (const tab of tabs) { if (tab.id !== undefined) { chrome.tabs .sendMessage(tab.id, message) .catch(() =\u0026gt; { // We ignore tabs without a proper URL, like chrome://extensions/ // Do nothing }) } } }) .catch((error: unknown) =\u0026gt; { console.error(\u0026#34;Could not query tabs\u0026#34;, error) }) } }) }) The types in src/common.ts need to be updated to include the new excludeHost field:\nexport interface Message { enabled?: boolean excludeHost?: string } export interface StoredConfig { enabled?: boolean item?: string excludeHost?: string } The content script src/content.ts needs to be updated to handle the new excludeHost setting. See the updated file here.\nWe need to update the extension\u0026rsquo;s manifest to include the new options page. Add the following to the manifest.json file:\n{ \u0026#34;options_page\u0026#34;: \u0026#34;static/options.html\u0026#34; } In addition, we need to tell webpack to compile the new options.ts file. Update the webpack.common.ts file to include the new entry point:\nentry: { background: \u0026#34;./src/background.ts\u0026#34;, content: \u0026#34;./src/content.ts\u0026#34;, popup: \u0026#34;./src/popup.ts\u0026#34;, options: \u0026#34;./src/options.ts\u0026#34;, }, Testing the options page To test the options page, load the extension in Chrome and right-click on its icon. You should see a new Options item. Clicking on this item will open the options page.\nNew Options selection Adding a link to the options page We can add a link to the popup to make it easier for users to access the options page. Update the popup.html file to include a link to the options page:\n\u0026lt;button id=\u0026#34;go-to-options\u0026#34; class=\u0026#34;button-link\u0026#34;\u0026gt;Advanced options\u0026lt;/button\u0026gt; Add the CSS for the link:\n.button-link { margin: 10px; background: none !important; border: none; padding: 0 !important; font-family: arial, sans-serif; color: #069; text-decoration: underline; cursor: pointer; } And add the TypeScript in popup.ts to handle the link click:\n// Options page const optionsElement = document.querySelector(\u0026#34;#go-to-options\u0026#34;) if (!optionsElement) { console.error(\u0026#34;Could not find options element\u0026#34;) } else { optionsElement.addEventListener(\u0026#34;click\u0026#34;, function () { // This code is based on Chrome for Developers documentation // eslint-disable-next-line @typescript-eslint/no-unnecessary-condition if (chrome.runtime.openOptionsPage) { chrome.runtime.openOptionsPage().catch((error: unknown) =\u0026gt; { console.error(\u0026#34;Could not open options page\u0026#34;, error) }) } else { window.open(chrome.runtime.getURL(\u0026#34;options.html\u0026#34;)) } }) } We will now see the Advanced options link in the popup. Clicking on the link will take the user to the options page.\nPopup with Advanced options link Embedded options page Instead of a full options page, Chrome extensions can use an embedded options page. However, this approach was confusing and not user-friendly because Chrome takes the user to the extension details page. We recommend using a dedicated options page. To try an embedded options page, add the following to the manifest.json file:\n{ \u0026#34;options_ui\u0026#34;: { \u0026#34;page\u0026#34;: \u0026#34;options.html\u0026#34;, \u0026#34;open_in_tab\u0026#34;: false } } Next steps In the next part of this series, we will focus on the look of our popup and options page. We will add CSS to make Chrome extension pages visually appealing and user-friendly.\nOptions page code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/5-options-page\nOptions page video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-07-03T00:00:00Z","image":"https://victoronsoftware.com/posts/add-options-to-chrome-extension/chrome-extension-options-headline_hu_ea69e055dfdcc0a4.png","permalink":"https://victoronsoftware.com/posts/add-options-to-chrome-extension/","title":"Adding options page to Chrome extension (2024)"},{"content":"This article covers how git merge works with GitHub pull requests. We will focus on the use case where developers want to keep their feature branches updated with the main branch. After completing the feature work, developers create a pull request to merge their feature branch into the main branch.\ngit merge Pull request after a merge Updating a protected feature branch with a pull request What is a merge in version control? Git is a distributed version control system that allows multiple developers to work on the same codebase. When developers work on different branches, they must merge their changes into the main branch. A merge is the process of combining changes from one branch into another branch, resulting in a single branch that contains the changes from both branches.\ngit merge The standard git merge command takes each commit from one branch and applies it to another. The final commit has two parent commits: one from the current branch and one from the merged branch.\nIn the following example, we have a branch that we want to merge into the main branch:\ngit merge of two branches before merge The git log of the main branch shows the commit history:\ncommit e493ac8fea4e0efe125a561b9014313bec41a489 (HEAD -\u0026gt; main, origin/main) Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:31:12 2024 -0500 m3 commit b79e810cb86405061dc979ce4fc05fe36a724256 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:29:41 2024 -0500 m2 commit eaccad9476b472dbfb3cdfbd17088425be75b7b1 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:26:55 2024 -0500 m1 commit 4265c0a30b7b6f03d93331bc112261393c97ee1d Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:25:40 2024 -0500 first commit And the git log of the branch shows the commit history:\ncommit 2afb078875a84095327ab2ef7c83711534c5eef8 (HEAD -\u0026gt; branch, origin/branch) Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:30:20 2024 -0500 b2 commit 9fc53c2ca9a58637a5d433de4c6150b832d4d275 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:28:25 2024 -0500 b1 commit eaccad9476b472dbfb3cdfbd17088425be75b7b1 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:26:55 2024 -0500 m1 commit 4265c0a30b7b6f03d93331bc112261393c97ee1d Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:25:40 2024 -0500 first commit We merge the branch into the main branch:\ngit checkout main git merge branch The resulting commit history shows all the commits from both branched as well as the final empty merge commit pointing to the two parent commits: e493ac8 2afb078:\ncommit 09d569bd079162643462dde112246f4167f14889 (HEAD -\u0026gt; main) Merge: e493ac8 2afb078 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 21:03:01 2024 -0500 Merge branch \u0026#39;branch\u0026#39; commit e493ac8fea4e0efe125a561b9014313bec41a489 (origin/main) Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:31:12 2024 -0500 m3 commit 2afb078875a84095327ab2ef7c83711534c5eef8 (origin/branch, branch) Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:30:20 2024 -0500 b2 commit b79e810cb86405061dc979ce4fc05fe36a724256 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:29:41 2024 -0500 m2 commit 9fc53c2ca9a58637a5d433de4c6150b832d4d275 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:28:25 2024 -0500 b1 commit eaccad9476b472dbfb3cdfbd17088425be75b7b1 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:26:55 2024 -0500 m1 commit 4265c0a30b7b6f03d93331bc112261393c97ee1d Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:25:40 2024 -0500 first commit git merge of two branches after merge The result would be the same if instead we merged the main branch into the branch branch:\ngit checkout branch git merge main except the final merge commit would be slightly different:\ncommit 06713a38ed38c12f599c6e810ee50d4cacfe2de7 (HEAD -\u0026gt; branch) Merge: 2afb078 e493ac8 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 17:32:01 2024 -0500 Merge branch \u0026#39;main\u0026#39; into branch The merged changes on branch can be pushed to the remote repository without issues because the remote branch can be fast-forwarded to the new commit.\ngit push origin branch What is a fast-forward merge? A fast-forward merge is a merge where the base branch (target branch) has no new commits. In this case, git moves the target branch to the commit of the source branch. It is a fast-forward merge because the target branch is moved forward to the new commit.\nA fast-forward merge does not lose any history \u0026ndash; it is always possible to undo a fast-forward merge.\ngit push does not, by default, allow a merge that is not a fast-forward. Use the\u0026rsquo;- force\u0026rsquo; option to enable a merge that is not a fast-forward.\nUndo a git merge The above merge can be undone by resetting the branch to the commit before the merge, which is one of the parent commits of the merge commit.\nThis command resets the branch to the commit before the merge:\ngit reset --hard 2afb078875a84095327ab2ef7c83711534c5eef8 git rebase Another way to combine changes from one branch into another is to use git rebase. This command applies the changes from the source branch to the target branch by reapplying the commits from the source branch to the target branch.\nThe git rebase command will modify the commit history of the source branch. In our pull request examples, we will use git merge instead of git rebase to preserve all the commit histories.\nPull request after a merge When working on a feature branch, developers often want to update their branch with the latest changes from the main to make sure their feature works with the newest code. We start this process with the above-described git merge command, where we merge the main branch into the branch branch.\nAfter the merge, the developer can create a GitHub pull request to merge the branch into the main branch.\nGitHub pull request after merge Note that the commit history only shows the commits from the branch and the merge commit. The main commits are not shown in the pull request.\nGitHub shows a few options for merging the pull request:\nMerge pull request Create a merge commit: This option creates a new merge commit combining the changes from the branch and the main branches. This is the default option. Squash and merge: This option combines all the commits from the branch into a single commit and merges that commit into the main branch. Rebase and merge: This option applies the changes from the branch onto the main branch by rebasing the commits from the branch onto the main branch. Selecting Create a merge commit results in the following commit history:\nCommit history after pull request The last two commits are both merge commits.\ncommit 1eb0af8c0e9ad16a0267d8abd1ce667f125ab7e8 (HEAD -\u0026gt; main, origin/main) Merge: e493ac8 22d2107 Author: Victor Lyuboslavsky \u0026lt;******\u0026gt; Date: Sun Jun 23 07:35:57 2024 -0500 Merge pull request #1 from getvictor/branch My pull request commit 22d2107b49bca56e67b7d4e800d93f93378a0956 (origin/branch, branch) Merge: 2afb078 e493ac8 Author: Victor on Software \u0026lt;\u0026gt; Date: Sat Jun 22 21:25:39 2024 -0500 Merge branch \u0026#39;main\u0026#39; into branch The PR merge commit points to the previous merge commit and the last commit on main.\nDiagram of commit history after pull request Updating a protected feature branch We have two branches in this example: main and feature. Both branches are protected, meaning that changes to them must be made through a pull request. We want to update the feature branch with the latest changes from the main branch.\nWe can do this by merging the main branch into the feature branch, creating a new branch, and creating a pull request.\ngit checkout feature git merge main git checkout -b feature-update git push origin feature-update And create a pull request to merge feature-update into feature.\nCreate a PR to merge into feature branch This pull request shows all the commits from the main branch and the merge commit. This commit history is problematic because the PR may trigger a code review from the code owners of the files that were already reviewed in previous pull requests to the main branch.\nCommits from feature-update branch After the merge, the feature branch commit history looks like:\nCommit history of feature branch after PR Now, we create a pull request to merge the update feature branch into the main branch.\nPR to merge feature branch into main After the merge, the main branch commit history looks like:\nCommit history of main after PR from feature branch The last three commits are merge commits.\ncommit 59caaf1cc5103099f850c32f1729c5ffe3525404 (HEAD -\u0026gt; main, origin/main) Merge: e493ac8 dcbc117 Author: Victor Lyuboslavsky \u0026lt;******\u0026gt; Date: Sun Jun 23 08:56:53 2024 -0500 Merge pull request #3 from getvictor/feature Feature commit dcbc117e5811e683f1074d947bc25da21b5fa5f6 (origin/feature) Merge: 2afb078 373dd82 Author: Victor Lyuboslavsky \u0026lt;******\u0026gt; Date: Sun Jun 23 08:45:44 2024 -0500 Merge pull request #2 from getvictor/feature-update Feature update commit 373dd82672a4879bfcf3b29c4feb97004359adfe (origin/feature-update, feature-update, feature) Merge: 2afb078 e493ac8 Author: Victor on Software \u0026lt;\u0026gt; Date: Sun Jun 23 08:03:22 2024 -0500 Merge branch \u0026#39;main\u0026#39; into feature Diagram of commit history after two pull requests Merging a pull request with Squash and merge If the final pull request is merged with Squash and merge, the commit history will look like:\nCommit history of main after squash and merge The last commit is a single commit that combines all the changes from the feature branch. The merge commits and all other commits are eliminated.\nThe downside of Squash and merge is that the commit history is lost. The commit history is useful for debugging, understanding the changes made, and keeping ownership of the changes when multiple developers work on the same feature branch.\nFurther reading Recently, we covered why GitHub code review process is broken for our organization.\nPreviously, we explained how to find the minimum required code owner approvers for a pull request.\nWatch how git merge works with GitHub pull requests Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-06-26T00:00:00Z","image":"https://victoronsoftware.com/posts/git-merges-and-pull-requests/git-merges-and-pull-requests-feature_hu_aeafd121b1f10dc.png","permalink":"https://victoronsoftware.com/posts/git-merges-and-pull-requests/","title":"How git merge works with GitHub pull requests"},{"content":" Setting up linting with ESLint and typescript-eslint Setting up formatting with Prettier Adding linting and formatting to CI This article is part of a series on building a maintainable Chrome extension.\nIn the previous article, we added TypeScript code for communicating between parts of a Chrome extension. This check-in will be our starting point for this article. This article will add linting and formatting to our TypeScript code, which will help us catch errors and enforce a consistent code style for larger teams.\nWhat is linting? Linting is the process of running a static code analysis program to analyze code for potential errors. Linters can catch syntax errors, typos, and other common mistakes that can lead to bugs. They can also enforce coding standards, such as indentation, variable naming, and other style rules.\nWhat is formatting? Formatting automatically changes the code\u0026rsquo;s appearance to match a specific style guide. Formatting tools can automatically add or remove whitespace, change indentation, and reformat code to make it more readable. Formatting tools can enforce a consistent code style across a project.\nWhy use linting and formatting tools? Linters and formatters work together to help developers write better code and accelerate the development process \u0026ndash; linters flag errors, while formatters automatically enforce a consistent code style.\nTogether, they can help prevent bugs, improve code quality, and make it easier for developers to read and understand the code. The result is cleaner, more maintainable code that uses many coding best practices and is easier to work with.\nLinting can also teach developers about best practices and help them avoid common pitfalls. For example, a linter can flag misused promises, such as missing await or uncaught errors.\nSetting up linting with ESLint and typescript-eslint To set up linting for TypeScript code, we will use ESLint with the typescript-eslint plugin. ESLint is a popular linter that can analyze JavaScript and TypeScript code. The typescript-eslint plugin adds TypeScript-specific rules to ESLint.\nTo set up ESLint with typescript-eslint, we need to install the following packages:\nnpm install --save-dev eslint @eslint/js @types/eslint__js typescript-eslint Next, we need to create an ESLint configuration file. We will create an eslint.config.mjs file at the root of our project:\n// @ts-check import eslint from \u0026#39;@eslint/js\u0026#39; import tseslint from \u0026#34;typescript-eslint\u0026#34; const config = tseslint.config( eslint.configs.recommended, ...tseslint.configs.recommendedTypeChecked, ...tseslint.configs.stylisticTypeChecked, { ignores: [\u0026#34;dist/**/*\u0026#34;, \u0026#34;eslint.config.mjs\u0026#34;], }, { languageOptions: { parserOptions: { project: true, tsconfigRootDir: import.meta.dirname, }, }, }, ) export default config This configuration file sets up ESLint with the recommended TypeScript type-checked rules and ignores our dist directory containing the webpack-generated bundles. We also ignore the config file because we do not want to apply TypeScript linting to it.\nWhy use the .mjs extension instead of .js for the configuration file? We are using .mjs extension for the configuration file to take advantage of ECMAScript modules. Using ES modules allows us to import and export modules using the import and export keywords. There are other ways to enable ECMAScript modules in JavaScript for our project, but this is the simplest way for just one JavaScript file. Our TypeScript files already use ECMAScript modules via these included recommended tsconfig.json settings:\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;module\u0026#34;: \u0026#34;commonjs\u0026#34;, \u0026#34;esModuleInterop\u0026#34;: true, If we used .js extension for the configuration file, we would need to use require and module.exports syntax. Otherwise, we would get an error like this:\n/Users/victor/work/create-chrome-extension/4-linting-and-formatting/eslint.config.js:3 import eslint from \u0026#39;@eslint/js\u0026#39; ^^^^^^ SyntaxError: Cannot use import statement outside a module at internalCompileFunction (node:internal/vm:77:18) at wrapSafe (node:internal/modules/cjs/loader:1288:20) at Module._compile (node:internal/modules/cjs/loader:1340:27) at Module._extensions..js (node:internal/modules/cjs/loader:1435:10) at Module.load (node:internal/modules/cjs/loader:1207:32) at Module._load (node:internal/modules/cjs/loader:1023:12) at cjsLoader (node:internal/modules/esm/translators:356:17) at ModuleWrap.\u0026lt;anonymous\u0026gt; (node:internal/modules/esm/translators:305:7) at ModuleJob.run (node:internal/modules/esm/module_job:218:25) at async ModuleLoader.import (node:internal/modules/esm/loader:329:24) Running ESLint We can run ESLint from the command line using the following command:\n./node_modules/.bin/eslint . Alternatively, we can use npx, which is a package runner tool that comes with npm:\nnpx eslint . This command will run ESLint on all TypeScript files in the current directory and subdirectories. ESLint will output any errors or warnings it finds in the code, such as:\n/Users/victor/work/create-chrome-extension/4-linting-and-formatting/src/background.ts 14:17 error Unsafe member access .enabled on an `any` value @typescript-eslint/no-unsafe-member-access /Users/victor/work/create-chrome-extension/4-linting-and-formatting/src/content.ts 51:9 error Unsafe assignment of an `any` value @typescript-eslint/no-unsafe-assignment 74:17 error Unsafe member access .enabled on an `any` value @typescript-eslint/no-unsafe-member-access 76:9 error Unsafe assignment of an `any` value @typescript-eslint/no-unsafe-assignment 76:27 error Unsafe member access .enabled on an `any` value @typescript-eslint/no-unsafe-member-access /Users/victor/work/create-chrome-extension/4-linting-and-formatting/src/popup.ts 9:23 error Unsafe argument of type `any` assigned to a parameter of type `boolean` @typescript-eslint/no-unsafe-argument 11:37 error Promise returned in function argument where a void return was expected @typescript-eslint/no-misused-promises 23:34 error Unsafe member access .title on an `any` value @typescript-eslint/no-unsafe-member-access 23:50 error Unsafe member access .url on an `any` value @typescript-eslint/no-unsafe-member-access 43:5 error Unsafe assignment of an `any` value @typescript-eslint/no-unsafe-assignment ✖ 10 problems (10 errors, 0 warnings) At this point, we should fix the errors and warnings that ESLint has found in our code.\nWe can also update the scripts section of our package.json file to run ESLint with npm run:\n\u0026#34;scripts\u0026#34;: { \u0026#34;lint\u0026#34;: \u0026#34;eslint .\u0026#34;, Now we can run ESLint with the following command:\nnpm run lint Setting up formatting with Prettier To format TypeScript code, we will use Prettier. Prettier is a popular code formatter that automatically formats code to match a specific style guide.\nTo set up Prettier, we need to install the following package:\nnpm install --save-dev --save-exact prettier Next, create a .prettierignore file in the root of our project to ignore the dist directory:\n/dist By default, Prettier ignores the node_modules directory.\nNext, create a .prettierrc file in the root of our project to configure Prettier:\n{ \u0026#34;semi\u0026#34;: false } We will use the default Prettier settings but turn off the semi rule to remove semicolons from the end of TypeScript lines. Removing semicolons is a common style choice in modern JavaScript and TypeScript code.\nRunning Prettier We can run Prettier from the command line using the following command:\nnpx prettier --write . This command will format all eligible files in the current directory and subdirectories.\nWe can also update the scripts section of our package.json file to run Prettier with the following command:\n\u0026#34;scripts\u0026#34;: { \u0026#34;format\u0026#34;: \u0026#34;prettier --write .\u0026#34;, \u0026#34;format-check\u0026#34;: \u0026#34;prettier --check .\u0026#34;, npm run format will format all eligible files, while npm run format-check will check if the files are formatted.\nAdding linting and formatting to continuous integration (CI) We will use GitHub Actions to automate linting and formatting checks on every pull request and commit to our main branch. This will make sure all code changes are linted and formatted correctly on the main branch.\nThis automatic check means that all contributors can expect that the code they are working on uses a consistent style and meets a quality standard. Consistency is beneficial for open-source projects where contributors may not be familiar with the codebase.\nTo set up GitHub Actions, create a .github/workflows/lint-and-format.yml file in the root of our git repository:\nname: Lint check, format check, and build on: push: branches: - main paths: # We only run the workflow if the code in these files/directories changes - \u0026#39;.github/workflows/lint-and-format.yml\u0026#39; # This file - \u0026#39;4-linting-and-formatting/**\u0026#39; # The working directory for this article pull_request: paths: - \u0026#39;.github/workflows/lint-and-format.yml\u0026#39; - \u0026#39;4-linting-and-formatting/**\u0026#39; # This allows a subsequently queued workflow run to interrupt previous runs concurrency: group: ${{ github.workflow }}-${{ github.head_ref || github.run_id}} cancel-in-progress: true defaults: run: shell: bash working-directory: ./4-linting-and-formatting permissions: contents: read jobs: lint-format-build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v4 - name: Install dependencies run: | npm install --no-save - name: Format check and lint run: | npm run format-check npm run lint - name: Build run: | npm run build Since our git repository is shared by multiple projects (from various articles), we use the paths key to only run the workflow when the code in the 4-linting-and-formatting directory changes.\nAfter pushing our code to GitHub and waiting for the GitHub Actions workflow to run, we can see the results in the Actions tab of our repository. We can see the linting and formatting checks, as well as the build step:\nGitHub Actions workflow results For more details on GitHub Actions workflows, see our article on reusing GitHub Actions workflows and steps.\nAdding stricter linting rules to typescript-eslint The recommended ruleset is a good starting point for linting TypeScript code. However, we can add stricter rules to catch even more potential issues in our code. It is easiest to start with strict rules early in the project when fixing issues is relatively painless. Otherwise, it is a good idea to gradually add stricter rules to avoid overwhelming developers with too many errors and warnings.\nTo switch to a stricter, more opinionated ruleset, replace tseslint.configs.recommendedTypeChecked with tseslint.configs.strictTypeChecked in the eslint.config.mjs file.\nESLint rules can be configured or disabled using configuration comments in the code or the ESLint configuration file. For more details, see the ESLint configure rules.\nNext steps In the next part of this series, we will add an options page to our Chrome extension. This page will allow users to configure the extension\u0026rsquo;s behavior and settings.\nFurther reading We recently wrote about enabling staticcheck linter in a large Go project Linting and formatting TypeScript code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/4-linting-and-formatting\nLinting and formatting TypeScript video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-06-19T00:00:00Z","image":"https://victoronsoftware.com/posts/linting-and-formatting-typescript/linting-and-formatting-headline_hu_e9db304e6b3e549.png","permalink":"https://victoronsoftware.com/posts/linting-and-formatting-typescript/","title":"Linting and formatting TypeScript in Chrome extension (2024)"},{"content":"This article is part of a series on building a production-ready Chrome extension.\nIn the previous article, we set up our Chrome extension with TypeScript support and the webpack bundler. This article will build on that code, dive into the new APIs, and cover message-passing communication between different parts of our Chrome extension.\nCommunication between parts of a Chrome extension As we covered in the first article, a Chrome extension consists of three main parts:\nservice worker (background script) content script popup These parts need to communicate with each other. For example, a popup needs to send a message to a content script to change the appearance of a webpage. Or a background script needs to send a message to a popup to update the user interface based on the page that\u0026rsquo;s being visited.\nCommunication in a Chrome extension One way to communicate between these parts is to use the local storage via the chrome.storage APIs. We do not recommend this method because it is slow and can cause performance issues. This method is slow because it is not synchronous \u0026ndash; the scripts need to check the storage for changes periodically. A better way to communicate between extension parts is to use message passing.\nWhat is message passing? In computer science, message passing is a method for communicating between different processes or threads. A process or thread sends a message to another process or thread, which receives the message and acts on it. This method is often used in distributed systems, where processes run on different machines and need to communicate with each other. The sender sends a message, and the receiver decodes it and executes the appropriate code.\nMessage passing in a Chrome extension Message passing is a way to communicate between different parts of a Chrome extension. Its main advantage is that it\u0026rsquo;s fast and efficient. When a message is sent, the receiver gets it immediately and can respond to it right away.\nMessage passing is done in Chrome extensions using the chrome.runtime.sendMessage, chrome.tabs.sendMessage and chrome.runtime.onMessage functions. Here\u0026rsquo;s how it works:\nThe sender calls chrome.runtime.sendMessage or chrome.tabs.sendMessage with the message to send. The receiver listens for messages using chrome.runtime.onMessage.addListener The receiver processes the incoming message and, optionally, responds to the message. Message passing from a popup to a content script Let\u0026rsquo;s see how we can use message passing to communicate between a popup and a content script. We will send a message when the user toggles the enable slider in the popup, which will enable or disable the content script\u0026rsquo;s processing. We will use the chrome.tabs.sendMessage function to send a message to a specific tab ID.\nIn the popup script (popup.ts), we send a message to all the tabs when we detect a change in the top slider:\n// Send message to content script in all tabs const tabs = await chrome.tabs.query({}) for (const tab of tabs) { // Note: sensitive tab properties such as tab.title or tab.url can only be accessed for // URLs in the host_permissions section of manifest.json chrome.tabs.sendMessage(tab.id!, {enabled: event.target.checked}) .then((response) =\u0026gt; { console.info(\u0026#34;Popup received response from tab with title \u0026#39;%s\u0026#39; and url %s\u0026#34;, response.title, response.url) }) .catch((error) =\u0026gt; { console.warn(\u0026#34;Popup could not send message to tab %d\u0026#34;, tab.id, error) }) } In the content script (content.ts), we listen for the message and process it:\n// Listen for messages from popup. chrome.runtime.onMessage.addListener((request, sender, sendResponse) =\u0026gt; { if (request.enabled !== undefined) { console.log(\u0026#34;Received message from sender %s\u0026#34;, sender.id, request) enabled = request.enabled if (enabled) { observe() } else { observer.disconnect() } sendResponse({title: document.title, url: window.location.href}) } }) When the user toggles the slider in the popup, the popup sends a message to all tabs. The receiving tab will print this message to the Chrome Developer Tools console.\nContent script received message Then, the popup will receive a response from the content script with the tab\u0026rsquo;s title and URL. This response prints to the Inspect Popup console. System tabs like chrome://extensions/ will not respond to messages.\nPopup received response Message passing from a popup to the service worker (background script) To send a message to the service worker, we must use the chrome.runtime.sendMessage function instead of chrome.tabs.sendMessage. The service worker does not have a tab ID, so we cannot use chrome.tabs.sendMessage.\nchrome.runtime.sendMessage({enabled: event.target.checked}) .then((response) =\u0026gt; { console.info(\u0026#34;Popup received response\u0026#34;, response) }) .catch((error) =\u0026gt; { console.warn(\u0026#34;Popup could not send message\u0026#34;, error) }) In the service worker script (background.ts), we listen for the message and process it:\nchrome.runtime.onMessage.addListener((request, sender, sendResponse) =\u0026gt; { if (request.enabled !== undefined) { console.log(\u0026#34;Service worker received message from sender %s\u0026#34;, sender.id, request) sendResponse({message: \u0026#34;Service worker processed the message\u0026#34;}) } }) Message passing from a content script to the popup and service worker To send a message from the content script, use the chrome.runtime.sendMessage function. The popup and service worker can listen and receive this message.\nMessage passing from the service worker (background script) to a content script and the popup Use the chrome.tabs.sendMessage function to send a message to the content script. Use the chrome.runtime.sendMessage function to send a message to the popup.\nThe code for sending a message from the service worker is the same as the code for sending a message from the popup. The receiving code in the content and popup scripts is also the same.\nNext steps In the next part of this series, we will add linting and formatting tools to our Chrome extension. These tools increase the quality of our code and increase engineering velocity for projects with multiple developers.\nChrome extension with webpack and TypeScript code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/3-message-passing\nMessage passing in a Chrome extension video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-06-12T00:00:00Z","image":"https://victoronsoftware.com/posts/message-passing-in-chrome-extension/message-passing-headline_hu_f787375be03f5119.png","permalink":"https://victoronsoftware.com/posts/message-passing-in-chrome-extension/","title":"Message passing in Chrome extension (2024)"},{"content":"We are creating a series of articles on building a production-ready Chrome extension. In this series, we cover the basics of building a Chrome extension, how to set up industry leading development tooling, and how to test and deploy your extension. Our goal is to show you how to build a Chrome extension that is easy to maintain, test, and deploy for a software development team.\n1. Create a Chrome extension from scratch Create a basic Chrome extension without any development tools. We cover the basics such as the major parts of the extension, manifest.json, and manually testing the extension in the Chrome browser.\n2. Add webpack and TypeScript to a Chrome extension We add support for TypeScript (which replaces JavaScript) and webpack (which bundles the extension) to our Chrome extension.\n3. Message passing in a Chrome extension We cover message passing communication between different parts of a Chrome extension. We dive into the code and show how to communicate between the service worker (background script), content scripts, and the popup.\n4. Linting and formatting TypeScript in a Chrome extension We set up ESLint and Prettier to lint and format our TypeScript code. This ensures our code is consistent and follows best practices.\n5. Add options page to Chrome extension We add an advanced options page to our Chrome extension. This page allows users to configure the extension\u0026rsquo;s behavior and settings.\n6. Add CSS framework to Chrome extension We improve the look and maintainability of our extension by adding a CSS framework. We use Tailwind CSS to quickly style our extension and make it look professional.\n7. Add unit tests to Chrome extension We write and run unit tests for our Chrome extension using the Jest testing framework. Unit tests help us catch bugs early, ensure our extension continues to work as expected, and make improve our code quality.\nBuild a production-ready Chrome extension video playlist ","date":"2024-06-11T00:00:00Z","image":"https://victoronsoftware.com/posts/chrome-extension/chrome-extension-headline_hu_a0b5db252f228762.png","permalink":"https://victoronsoftware.com/posts/chrome-extension/","title":"Build a production-ready Chrome extension in 7 steps"},{"content":"What is a webhook? A webhook is a way for one application to send data to another application in real time. It is a simple way to trigger an action based on an event. In other words, a webhook is a custom HTTP callback.\nWhat is Tines? Tines is a no-code automation platform that allows you to automate repetitive tasks. It is a powerful tool that can be used to automate workflows, such as sending emails, creating tickets, and updating databases.\nWhat is Fleet? Fleet is an open-source platform for managing and gathering telemetry from devices such as laptops, desktops, VMs, etc. Osquery agents run on these devices and report to the Fleet server.\nOur example IT workflow In this article, we will build a webhook flow with Tines. When a device has an outdated OS version, Tines will receive a webhook callback from Fleet. Tines will then send an MDM (Mobile Device Management) command to the device to update the device\u0026rsquo;s OS version.\nFleet will send a callback via its calendar integration feature. Fleet can put a \u0026ldquo;System Maintenance\u0026rdquo; event on the device user\u0026rsquo;s calendar. This event warns the device owner that their computer will be restarted to remediate one or more failing policies. During the calendar event time, Fleet sends a webhook. The IT admin must set up a flow to remediate the failing policy. This article is an example of one such flow.\nGetting started \u0026ndash; webhook action First, we create a new Tines story. A story is a sequence of actions that are executed in order. Next, we add a webhook action to the story. The webhook action listens for incoming webhooks. The webhook will contain a JSON body.\nTines webhook action Handling errors Often, webhooks may contain error messages if there is an issue with the configuration, flow, etc. In this example, we add a trigger action that checks whether the webhook body contains an error. Specifically, our action checks whether the webhook body contains a non-empty \u0026ldquo;error\u0026rdquo; field.\nTines trigger action checking for an error We leave this error-handling portion of the story as a stub. In the future, we can expand it by sending an email or triggering other actions.\nChecking whether webhook indicates an outdated OS At the same time, we also check whether the webhook was triggered by a policy indicating an outdated OS. From previous testing, we know that the webhook payload will look like this:\n{ \u0026#34;timestamp\u0026#34;: \u0026#34;2024-03-28T13:57:31.668954-05:00\u0026#34;, \u0026#34;host_id\u0026#34;: 11058, \u0026#34;host_display_name\u0026#34;: \u0026#34;Victor\u0026#39;s Virtual Machine (2)\u0026#34;, \u0026#34;host_serial_number\u0026#34;: \u0026#34;Z5C4L7GKY0\u0026#34;, \u0026#34;failing_policies\u0026#34;: [ { \u0026#34;id\u0026#34;: 479, \u0026#34;name\u0026#34;: \u0026#34;macOS - OS version up to date\u0026#34; } ] } The payload contains:\nThe device\u0026rsquo;s ID (host ID). Display name. Serial number. A list of failing policies. We are interested in the failing policies. When one of the failing policies contains a policy named \u0026ldquo;macOS - OS version up to date,\u0026rdquo; we know that the device\u0026rsquo;s OS is outdated. Hence, we create a trigger that looks for this policy.\nTines trigger action checking for an outdated OS We use the following formula, which loops over all policies and will only allow the workflow to proceed if true:\nIF(FIND(calendar_webhook.body.failing_policies, LAMBDA(item, item.name = \u0026#34;macOS - OS version up to date\u0026#34;)).id \u0026gt; 0, TRUE) Getting device details from Fleet Next, we need to get more details about the device from Fleet. Devices are called hosts in Fleet. We add an \u0026ldquo;HTTP Request\u0026rdquo; action to the story. The action makes a GET request to the Fleet API to get the device details. We use the host ID from the webhook payload. We are looking for the device\u0026rsquo;s UUID, which we need to send the OS update MDM command.\nTines HTTP Request action to get Fleet device details To access Fleet\u0026rsquo;s API, we need to provide an API key. We store the API key as a CREDENTIAL in the current story. The API key should belong to an API-only user in Fleet so that the key does not reset when the user logs out.\nAdd credential to Tines story Creating MDM command payload to update OS version We can create the MDM payload now that we have the device\u0026rsquo;s UUID. The payload contains the command to update the OS version. We use the ScheduleOSUpdate command from Apple\u0026rsquo;s MDM protocol.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;Command\u0026lt;/key\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;RequestType\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;ScheduleOSUpdate\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;Updates\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;InstallAction\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;InstallASAP\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ProductVersion\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;14.4.1\u0026lt;/string\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;key\u0026gt;CommandUUID\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;\u0026lt;\u0026lt;UUID()\u0026gt;\u0026gt;\u0026lt;/string\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt; This command will download macOS 14.4.1, install it, and pop up a 60-second countdown dialog box before restarting the device. Note that the \u0026lt;\u0026lt;UUID()\u0026gt;\u0026gt; Tines function creates a unique UUID for this MDM command.\nTines event to create ScheduleOSUpdate MDM command The Fleet API requires the command to be sent as a base64-encoded string. We add a \u0026ldquo;Base64 Encode\u0026rdquo; action to the story to encode the XML payload. It uses the Tines BASE64_ENCODE function.\nTines Base64 Encode event Run MDM command on device Finally, we send the MDM command to the device. We add another \u0026ldquo;HTTP Request\u0026rdquo; action to the story. The action makes a POST request to the Fleet API to send the MDM command to the device.\nTines HTTP Request action to run MDM command on device The MDM command will run on the device, downloading and installing the OS update.\nmacOS restart notification after OS update Conclusion In this article we built a webhook flow with Tines. We received a webhook callback from Fleet when a device had an outdated OS version. We then sent an MDM command to the device to update the OS version. This example demonstrates how Tines can automate workflows and tasks in IT environments.\nFurther reading Recently, we explained how to quickly get started with Google Sheets API for your development scripts. Building a webhook flow with Tines video Note: If you want to comment on this article, please do so on the YouTube video.\nThis article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2024-06-05T00:00:00Z","image":"https://victoronsoftware.com/posts/webhook-flow-with-tines/tines-fleet-webhook-workflow_hu_7c6c1d70ffb9ce23.png","permalink":"https://victoronsoftware.com/posts/webhook-flow-with-tines/","title":"Building a webhook flow with Tines"},{"content":" Excessive database locks Read-after-write consistency Index limitations When building an application, the database is often an afterthought. The database used in a development environment often contains limited data with little traffic. However, when the application is deployed to production, real-world traffic can expose issues that were not caught in development or testing.\nIn this article, we cover issues we ran into with our customers. We assume the production application is deployed with one master and one or more read replicas. See this article on creating a MySQL slave replica in dev environment.\nExcessive database locks One write query can bring your database to its knees if it locks too many rows.\nConsider this simplified INSERT with a subquery transaction:\nINSERT INTO software_counts (host_id, count) SELECT host_id, COUNT(*) as count FROM host_software GROUP BY host_software.host_id; Simplified INSERT with a subquery The above query scans the entire host_software table index to create a count. While the database is doing the scan and the INSERT, it locks the host_software table, preventing other transactions from writing to that table. If the table and insert are large, the query can hold the lock for a long time. In production, we saw a lock time of over 30 seconds, creating a bottleneck and spiking DB resource usage.\nPay special attention to the following queries, as they can cause performance issues:\nCOUNT(*) Using a non-indexed column, like WHERE non_indexed_column = value Returning a large number of rows, like SELECT * FROM table One way to solve the above performance issue is to separate the SELECT and INSERT queries. First, run the SELECT query on the replica to get the data, then run the INSERT query on the master to insert the data. We completely eliminate the lock since the read is done on the replica. This article goes through a specific example of optimizing an INSERT with subqueries.\nAs general advice, avoid running SELECT queries and subqueries on the master, especially if they scan the entire table.\nRead-after-write consistency When you write to the master and read from the replica, you might not see the data you wrote. The replica is not in sync with the master in real time. In our production, the replica is usually less than 30 milliseconds behind the master.\nRead-after-write database issue These issues are typically not caught in development since dev environments usually have one database instance. Unit or integration tests might not even see these issues if they run on a single database instance. Even in testing or small production environments, you might only see these issues if the replica sync time is high. Customers with large deployments may be experiencing these consistency issues without the development team knowing about it.\nOne way to solve this issue is to read from the master after writing to it. This way, you are guaranteed to see the data you just wrote. In our Go backend, forcing reads from the master can be done by updating the Context:\nctxUsePrimary := ctxdb.RequirePrimary(ctx, true) However, additional master reads increase the load on the master, defeating the purpose of having a replica for read scaling.\nIn addition, what about expensive read queries, like COUNT(*) and calculations, which we don\u0026rsquo;t want to run on the master? In this case, we can wait for the replica to catch up with the master.\nOne generic approach to waiting for the replica is to read the last written data from the replica and retry the read if the data is not found. The app could check the updated_at column to see if the data is recent. If the data is not found, the app can sleep for a few milliseconds and retry the read. This approach is imperfect but a good compromise between read consistency and performance.\nNote: The default precision of MySQL date and time data types is 1 second (0 fractional seconds).\nIndex limitations What are SQL indexes? Indexes are a way to optimize read queries. They are a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure. Indexes are created using one or more database columns and are stored and sorted using a B-tree or a similar data structure. The goal is to reduce the number of data comparisons needed to find the data.\nDatabase index Indexes are generally beneficial. They speed up read queries but slightly slow down write queries. Indexes can also be large and take up a lot of disk space.\nIndex size is limited As the product grows with more features, the number of columns in a specific table can also increase. Sometimes, the new columns need to be part of a unique index. However, the maximum index size in MySQL is 3072 bytes. This limit can be quickly reached if columns are of type VARCHAR or TEXT.\nCREATE TABLE `activities` ( `user_name` VARCHAR(255) NOT NULL, One way to solve the issue of hitting the index size limit is to create a new column that makes the hash of the other relevant column(s), and use that as the unique index. For example, in our backend we use a checksum column in the software table to create a unique index for a software item.\nForeign keys may cause performance issues If a table has a foreign key, any insert, update, or delete with a constraint on the foreign key column will lock the corresponding row in the parent table. This locking can lead to performance issues when\nthe parent table is large the parent has many foreign key constraints the parent table or child tables are frequently updated The performance issue manifests as excessive lock wait times for queries. One way to solve this issue is to remove the foreign key constraint. Instead, the application code can handle the data integrity checks that the foreign key constraint provides. In our application, we run a regular clean-up job to remove orphaned child rows.\nBonus database gotchas Additional database gotchas that we have seen in production include:\nPrepared statements consuming too much memory Deadlocks caused by using an UPDATE/INSERT upsert pattern Also, we recently solved a problem in production with distributed lock.\n3 database gotchas video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-29T00:00:00Z","image":"https://victoronsoftware.com/posts/database-gotchas-when-scaling-apps/database-thumbnail_hu_86c34d7127c945c8.png","permalink":"https://victoronsoftware.com/posts/database-gotchas-when-scaling-apps/","title":"3 database gotchas when building apps for scale"},{"content":"This article is part of a series on creating a production-ready Chrome extension. The previous article covered creating a basic Chrome extension without any tooling. For a list of all articles in the series, see the Chrome extension series overview.\nAdd webpack bundler Add TypeScript Convert webpack configuration from JavaScript to TypeScript Introduction This article will add the webpack module bundler and TypeScript support to the Chrome extension we created in the previous article. This software tooling will allow us to use modern JavaScript features and development tools.\nA module bundler and TypeScript are essential tools for modern web development. They improve the development experience for large or long-running projects.\nPrerequisites - Node.js and npm Before we start, make sure you have Node.js and npm installed. Node.js is a JavaScript runtime. We will use it to run webpack and future development tools. npm is a JavaScript package manager.\nYou can check if you have them installed by running the following commands:\nnode -v npm -v package.json First, we will create a package.json file containing project and dependency info. We can use the npm init command to create the file. Or manually create one containing something like:\n{ \u0026#34;name\u0026#34;: \u0026#34;my-chrome-extension\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/create-chrome-extension\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34; } What is webpack? Webpack is a module bundler for JavaScript applications. It takes modules with dependencies and generates static assets representing those modules. We will use webpack to bundle multiple JavaScript files into a single file.\nA module bundler allows you to write modular code and bundle it into a single file. TypeScript is a superset of JavaScript that adds static typing and other features to the language.\nWe will install webpack with npm:\nnpm install --save-dev webpack webpack-cli webpack-merge copy-webpack-plugin webpack is the core module bundler webpack-cli is the command-line interface for webpack webpack-merge is a utility to merge multiple webpack configurations, which we will use to differentiate development and production configs copy-webpack-plugin is a plugin to copy files and directories in webpack The above npm command will install the packages, create a package-lock.json file, and add them to the devDependencies section of the package.json file. The updated package.json should look like this:\n{ \u0026#34;name\u0026#34;: \u0026#34;my-chrome-extension\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/create-chrome-extension\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;copy-webpack-plugin\u0026#34;: \u0026#34;^12.0.2\u0026#34;, \u0026#34;webpack\u0026#34;: \u0026#34;^5.91.0\u0026#34;, \u0026#34;webpack-cli\u0026#34;: \u0026#34;^5.1.4\u0026#34;, \u0026#34;webpack-merge\u0026#34;: \u0026#34;^5.10.0\u0026#34; } } webpack configuration Next, we will create webpack configuration files. Webpack uses a configuration file to define how to bundle the project. We will create two configurations: one for development and one for production. Initially, we will use JavaScript for the configuration files, but we will convert them to TypeScript later.\nCreate a webpack.common.js file with the shared configuration:\nconst path = require(\u0026#39;path\u0026#39;) const CopyWebpackPlugin = require(\u0026#39;copy-webpack-plugin\u0026#39;); module.exports = { entry: { background: \u0026#39;./src/background.js\u0026#39;, content: \u0026#39;./src/content.js\u0026#39;, popup: \u0026#39;./src/popup.js\u0026#39;, }, output: { filename: \u0026#39;[name].js\u0026#39;, path: path.resolve(__dirname, \u0026#39;dist\u0026#39;), clean: true, // Clean the output directory before emit. }, plugins: [ new CopyWebpackPlugin({ patterns: [{ from: \u0026#39;static\u0026#39; }], }), ] } Create a webpack.dev.js file with the development configuration:\nconst { merge } = require(\u0026#39;webpack-merge\u0026#39;) const common = require(\u0026#39;./webpack.common.js\u0026#39;) module.exports = merge(common, { mode: \u0026#39;development\u0026#39;, devtool: \u0026#39;inline-source-map\u0026#39;, }) Create a webpack.prod.js file with the production configuration:\nconst { merge } = require(\u0026#39;webpack-merge\u0026#39;) const common = require(\u0026#39;./webpack.common.js\u0026#39;) module.exports = merge(common, { mode: \u0026#39;production\u0026#39;, devtool: \u0026#39;source-map\u0026#39;, }) Refactoring directory structure We will refactor the directory structure to separate the source code from the static files. Create a src directory and move the JavaScript files (background.js, content.js, popup.js) into it. Create a static directory and move the manifest.json, popup.html, and popup.css file into it.\nThe directory structure should look like this (running tree . -I node_modules):\n. ├── package-lock.json ├── package.json ├── src │ ├── background.js │ ├── content.js │ └── popup.js ├── static │ ├── manifest.json │ ├── popup.css │ └── popup.html ├── webpack.common.js ├── webpack.dev.js └── webpack.prod.js Running webpack Now, we can run the webpack bundler using the following command:\n./node_modules/.bin/webpack --watch --config webpack.dev.js This command creates a dist directory with the bundled files. The --watch flag tells webpack to continue running, watch for changes, and recompile the files when changes occur. This recompilation is crucial for development, as it allows us to see our code changes in real time.\nWe can run the production build with:\n./node_modules/.bin/webpack --config webpack.prod.js Now, we can add scripts to the package.json file to simplify how we run webpack:\n... \u0026#34;scripts\u0026#34;: { \u0026#34;build\u0026#34;: \u0026#34;webpack --config webpack.prod.js\u0026#34;, \u0026#34;start\u0026#34;: \u0026#34;webpack --watch --config webpack.dev.js\u0026#34; } These scripts allow us to run npm run build to build the production version and npm start (or npm run start) to start the development version.\nAt this point, we can test the browser extension to ensure it is still working as before. Open the Chrome browser, go to chrome://extensions, enable Developer mode, click on Load unpacked, and select the dist directory.\nWhat is TypeScript? TypeScript is a superset of JavaScript that adds static typing and other features to the language. It compiles to plain JavaScript and can be used in any browser or JavaScript engine. Although TypeScript is not required for writing Chrome extensions, it is highly recommended as it can help catch errors early and improve code quality.\nWe install TypeScript with:\nnpm install --save-dev typescript @tsconfig/recommended ts-node ts-loader @types/chrome typescript is the core TypeScript compiler @tsconfig/recommended is a recommended TypeScript configuration, which we will use ts-node is a TypeScript execution environment for Node.js, which is needed for converting the webpack configuration to TypeScript ts-loader is a TypeScript loader for webpack, which is needed for webpack to understand TypeScript source files @types/chrome is the TypeScript type definitions for the Chrome extension API What are TypeScript type definitions? We loaded the @types/chrome packages to provide TypeScript type definitions for the Chrome extension API.\nTypeScript type definitions are files that describe the shape of a JavaScript library. They provide type information for JavaScript libraries that were not written in TypeScript. This information allows TypeScript to understand the library\u0026rsquo;s API and provide type checking. With this information, TypeScript can check our code.\n@types/chrome provides a global chrome object representing the Chrome extension API. No additional code is needed to use it from the command line, as TypeScript automatically loads it. However, IDEs may need to be configured to recognize this global type definition.\ntsconfig.json Next, we will create a tsconfig.json file to configure TypeScript. This file tells the TypeScript compiler how to compile the project. Create a tsconfig.json file with the recommended config:\n{ \u0026#34;extends\u0026#34;: \u0026#34;@tsconfig/recommended/tsconfig.json\u0026#34;, \u0026#34;compilerOptions\u0026#34;: { \u0026#34;sourceMap\u0026#34;: true } } We added the sourceMap option to generate source maps, which help debug TypeScript code in the browser.\nConvert webpack configuration from JavaScript to TypeScript First, rename the webpack configuration files to TypeScript files by changing the extension from .js to .ts. For example, webpack.common.js becomes webpack.common.ts. Then, update the contents of the files to TypeScript syntax.\nwebpack.common.ts:\nimport path from \u0026#39;path\u0026#39; import webpack from \u0026#39;webpack\u0026#39; import CopyWebpackPlugin from \u0026#39;copy-webpack-plugin\u0026#39; const config: webpack.Configuration = { entry: { background: \u0026#39;./src/background.ts\u0026#39;, content: \u0026#39;./src/content.ts\u0026#39;, popup: \u0026#39;./src/popup.ts\u0026#39;, }, output: { filename: \u0026#39;[name].js\u0026#39;, path: path.resolve(__dirname, \u0026#39;dist\u0026#39;), clean: true, // Clean the output directory before emit. }, plugins: [ new CopyWebpackPlugin({ patterns: [{from: \u0026#39;static\u0026#39;}], }), ] } export default config We made the following changes to the shared config:\nWe changed the require statements to import statements We changed the module.exports to export default We added the webpack.Configuration type from the webpack package webpack.dev.ts:\nimport {Configuration} from \u0026#39;webpack\u0026#39; import {merge} from \u0026#39;webpack-merge\u0026#39; import config from \u0026#39;./webpack.common\u0026#39; const merged = merge\u0026lt;Configuration\u0026gt;(config,{ mode: \u0026#39;development\u0026#39;, devtool: \u0026#39;inline-source-map\u0026#39;, }) export default merged webpack.prod.ts:\nimport {Configuration} from \u0026#39;webpack\u0026#39; import {merge} from \u0026#39;webpack-merge\u0026#39; import config from \u0026#39;./webpack.common\u0026#39; const merged = merge\u0026lt;Configuration\u0026gt;(config,{ mode: \u0026#39;production\u0026#39;, devtool: \u0026#39;source-map\u0026#39;, }) export default merged And update the package.json scripts to use the TypeScript configuration files:\n... \u0026#34;scripts\u0026#34;: { \u0026#34;build\u0026#34;: \u0026#34;webpack --config webpack.prod.ts\u0026#34;, \u0026#34;start\u0026#34;: \u0026#34;webpack --watch --config webpack.dev.ts\u0026#34; } We can test npm run start and npm run build to ensure the new webpack Typescript configurations are working correctly.\nConvert JavaScript source files to TypeScript Finally, we will convert the JavaScript source files to TypeScript. Rename the .js files to .ts files. For example, background.js becomes background.ts. Update the contents of the files to TypeScript syntax.\nAlso, we will refactor the common setBadgeText function to a shared common.ts file:\nexport function setBadgeText(enabled: boolean) { const text = enabled ? \u0026#34;ON\u0026#34; : \u0026#34;OFF\u0026#34; void chrome.action.setBadgeText({text: text}) } Updated background.ts:\nimport {setBadgeText} from \u0026#34;./common\u0026#34; function startUp() { chrome.storage.sync.get(\u0026#34;enabled\u0026#34;, (data) =\u0026gt; { setBadgeText(!!data.enabled) }) } // Ensure the background script always runs. chrome.runtime.onStartup.addListener(startUp) chrome.runtime.onInstalled.addListener(startUp) Updated content.ts:\nconst blurFilter = \u0026#34;blur(6px)\u0026#34; let textToBlur = \u0026#34;\u0026#34; // Search this DOM node for text to blur and blur the parent element if found. function processNode(node: Node) { if (node.childNodes.length \u0026gt; 0) { Array.from(node.childNodes).forEach(processNode) } if (node.nodeType === Node.TEXT_NODE \u0026amp;\u0026amp; node.textContent !== null \u0026amp;\u0026amp; node.textContent.trim().length \u0026gt; 0) { const parent = node.parentElement if (parent == null) { return } if (parent.tagName === \u0026#39;SCRIPT\u0026#39; || parent.style.filter === blurFilter) { // Already blurred return } if (node.textContent.includes(textToBlur)) { blurElement(parent) } } } function blurElement(elem: HTMLElement) { elem.style.filter = blurFilter console.debug(\u0026#34;blurred id:\u0026#34; + elem.id + \u0026#34; class:\u0026#34; + elem.className + \u0026#34; tag:\u0026#34; + elem.tagName + \u0026#34; text:\u0026#34; + elem.textContent) } // Create a MutationObserver to watch for changes to the DOM. const observer = new MutationObserver((mutations) =\u0026gt; { mutations.forEach((mutation) =\u0026gt; { if (mutation.addedNodes.length \u0026gt; 0) { mutation.addedNodes.forEach(processNode) } else { processNode(mutation.target) } }) }) // Enable the content script by default. let enabled = true const keys = [\u0026#34;enabled\u0026#34;, \u0026#34;item\u0026#34;] chrome.storage.sync.get(keys, (data) =\u0026gt; { if (data.enabled === false) { enabled = false } if (data.item) { textToBlur = data.item } // Only start observing the DOM if the extension is enabled and there is text to blur. if (enabled \u0026amp;\u0026amp; textToBlur.trim().length \u0026gt; 0) { observer.observe(document, { attributes: false, characterData: true, childList: true, subtree: true, }) // Loop through all elements on the page for initial processing. processNode(document) } }) Updated popup.ts:\nimport {setBadgeText} from \u0026#34;./common\u0026#34; console.log(\u0026#34;Hello, world from popup!\u0026#34;) // Handle the ON/OFF switch const checkbox = document.getElementById(\u0026#34;enabled\u0026#34;) as HTMLInputElement chrome.storage.sync.get(\u0026#34;enabled\u0026#34;, (data) =\u0026gt; { checkbox.checked = !!data.enabled void setBadgeText(data.enabled) }) checkbox.addEventListener(\u0026#34;change\u0026#34;, (event) =\u0026gt; { if (event.target instanceof HTMLInputElement) { void chrome.storage.sync.set({\u0026#34;enabled\u0026#34;: event.target.checked}) void setBadgeText(event.target.checked) } }) // Handle the input field const input = document.getElementById(\u0026#34;item\u0026#34;) as HTMLInputElement chrome.storage.sync.get(\u0026#34;item\u0026#34;, (data) =\u0026gt; { input.value = data.item || \u0026#34;\u0026#34; }); input.addEventListener(\u0026#34;change\u0026#34;, (event) =\u0026gt; { if (event.target instanceof HTMLInputElement) { void chrome.storage.sync.set({\u0026#34;item\u0026#34;: event.target.value}) } }) Update webpack configuration to handle TypeScript source files Update webpack.common.ts to use the new TypeScript source files and add the ts-loader to the webpack configuration:\nimport path from \u0026#39;path\u0026#39; import webpack from \u0026#39;webpack\u0026#39; import CopyWebpackPlugin from \u0026#39;copy-webpack-plugin\u0026#39; const config: webpack.Configuration = { entry: { background: \u0026#39;./src/background.ts\u0026#39;, content: \u0026#39;./src/content.ts\u0026#39;, popup: \u0026#39;./src/popup.ts\u0026#39;, }, resolve: { extensions: [\u0026#34;.ts\u0026#34;], }, module: { rules: [ { test: /\\.ts$/, loader: \u0026#34;ts-loader\u0026#34;, exclude: /node_modules/, }, ], }, output: { filename: \u0026#39;[name].js\u0026#39;, path: path.resolve(__dirname, \u0026#39;dist\u0026#39;), clean: true, // Clean the output directory before emit. }, plugins: [ new CopyWebpackPlugin({ patterns: [{from: \u0026#39;static\u0026#39;}], }), ] } export default config Debug our TypeScript extension in Chrome Build the extension with npm run start and load it in Chrome.\nRight-click the extension icon (M) and select Inspect popup to open the Chrome Developer Tools. By default, you can see the console logs from the popup.ts file.\nGo to the Sources tab in the Chrome Developer Tools and open the top/my-chrome-extension/src/popup.ts file. You can set breakpoints and debug the popup script.\nDebugging Chrome extension popup The popup.ts file should exactly match the TypeScript code we wrote. You can set breakpoints, inspect variables, and step through the code.\nNext steps In the next part of this series, we will add message passing between the content script, the background script, and the popup script. This communication will allow us to make real-time changes across all parts of our Chrome extension.\nChrome extension with webpack and TypeScript code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/2-webpack-typescript\nAdd webpack and TypeScript to a Chrome extension video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-23T00:00:00Z","image":"https://victoronsoftware.com/posts/add-webpack-and-typescript-to-chrome-extension/chrome-typescript-webpack_hu_b7e6ec8db32c6378.png","permalink":"https://victoronsoftware.com/posts/add-webpack-and-typescript-to-chrome-extension/","title":"Add webpack and TypeScript to a Chrome extension (2024)"},{"content":"In this series, we will be building a production-ready Chrome extension. We will start with a basic extension and then add more features.\nWhat is a Chrome extension? A Chrome extension is a small software program that customizes the browsing experience. It can modify and enhance the functionality of the Chrome browser. Extensions are written using web technologies such as HTML, CSS, and JavaScript.\nWhy build a Chrome extension? Users can utilize Chrome extensions to:\nModify web pages Automate tasks Integrate with third-party services Add new features to the browser And much more Prerequisites For this tutorial, no additional tools are required. We will create the extension using a text editor and the Chrome browser.\nThree parts of a Chrome extension The three main parts of a Chrome extension are the background script, content script(s), and popup. All these parts are optional.\nParts of a Chrome extension background script: Also known as a service worker, this is a long-running script that runs in the background. It can listen for events and perform tasks. content script(s): This script runs in the context of a web page. It can interact with the DOM and modify the page, including adding UI elements. The extension can statically inject this script or dynamically inject it by the background script or the popup. popup: This small HTML page appears when a user clicks the extension icon. It can contain buttons, forms, and other UI elements. This is the extension\u0026rsquo;s user interface. These three parts of the extension run independently but can communicate with each other using message passing, events, and storage.\nOur first extension will have a popup with a turn-on/off switch and an input field. The extension will blur the page elements containing the text in the input field.\nmanifest.json configuration file Create a src directory for the extension. This directory will contain all the extension files.\nThe manifest.json file is the configuration file of a Chrome extension. It contains metadata about the extension, such as its name, version, permissions, and scripts.\nCreating the popup Add a manifest.json file with the following content:\n{ \u0026#34;manifest_version\u0026#34;: 3, \u0026#34;name\u0026#34;: \u0026#34;My Chrome Extension\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;My first Chrome extension.\u0026#34;, \u0026#34;action\u0026#34;: { \u0026#34;default_popup\u0026#34;: \u0026#34;popup.html\u0026#34; }, \u0026#34;permissions\u0026#34;: [ \u0026#34;storage\u0026#34; ] } The permissions specify the permissions required by the extension. In this case, we need the storage permission to store data in the Chrome storage so that the extension can remember the state of its configuration.\nCreate popup.html with the content below.\n\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;My popup\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;popup.css\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;label class=\u0026#34;switch\u0026#34;\u0026gt; \u0026lt;input id=\u0026#34;enabled\u0026#34; type=\u0026#34;checkbox\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;slider round\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;input class=\u0026#34;secret\u0026#34; id=\u0026#34;item\u0026#34; type=\u0026#34;text\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;popup.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Our popup.html includes a CSS file and a script. Create popup.js with the following content:\n\u0026#34;use strict\u0026#34;; console.log(\u0026#34;Hello, world from popup!\u0026#34;) function setBadgeText(enabled) { const text = enabled ? \u0026#34;ON\u0026#34; : \u0026#34;OFF\u0026#34; void chrome.action.setBadgeText({text: text}) } // Handle the ON/OFF switch const checkbox = document.getElementById(\u0026#34;enabled\u0026#34;) chrome.storage.sync.get(\u0026#34;enabled\u0026#34;, (data) =\u0026gt; { checkbox.checked = !!data.enabled void setBadgeText(data.enabled) }) checkbox.addEventListener(\u0026#34;change\u0026#34;, (event) =\u0026gt; { if (event.target instanceof HTMLInputElement) { void chrome.storage.sync.set({\u0026#34;enabled\u0026#34;: event.target.checked}) void setBadgeText(event.target.checked) } }) // Handle the input field const input = document.getElementById(\u0026#34;item\u0026#34;) chrome.storage.sync.get(\u0026#34;item\u0026#34;, (data) =\u0026gt; { input.value = data.item || \u0026#34;\u0026#34; }); input.addEventListener(\u0026#34;change\u0026#34;, (event) =\u0026gt; { if (event.target instanceof HTMLInputElement) { void chrome.storage.sync.set({\u0026#34;item\u0026#34;: event.target.value}) } }) The script listens for changes in the switch and the input field. It saves the switch\u0026rsquo;s state and the input field\u0026rsquo;s value in Chrome storage.\nCreate popup.css with the following content to style the switch and the input field:\n/* The switch - the box around the slider */ .switch { margin-left: 30%; /* Center the switch */ position: relative; display: inline-block; width: 60px; height: 34px; } /* Hide default HTML checkbox */ .switch input { opacity: 0; width: 0; height: 0; } /* The slider */ .slider { position: absolute; cursor: pointer; top: 0; left: 0; right: 0; bottom: 0; background-color: #ccc; } .slider::before { position: absolute; content: \u0026#34;\u0026#34;; height: 26px; width: 26px; left: 4px; bottom: 4px; background-color: white; } input:checked + .slider { background-color: #2196F3; } input:checked + .slider:before { transform: translateX(26px); /* Move the slider to the right when checked */ } /* Rounded sliders */ .slider.round { border-radius: 34px; } .slider.round::before { border-radius: 50%; } .secret { margin: 5px; } Loading and testing the extension in Chrome Even though we have not added the background script and content script, we can load the extension in Chrome.\nOpen the Chrome browser. Go to chrome://extensions/. Enable the Developer mode. Click on Load unpacked. Select the src directory containing the extension files. Click Select Folder. The extension will be loaded. Pin the extension to the toolbar by clicking the pin button in the extension dropdown. This pin will make it easier to test the extension. The popup page will appear when you click the M extension icon. Chrome extension popup We can now do some basic testing:\nTest the switch and the input field. The state of the switch and the value of the input field should be saved in the Chrome storage. The values should persist even after restarting the browser. The badge text of the extension icon should change to \u0026ldquo;ON\u0026rdquo; or \u0026ldquo;OFF\u0026rdquo; based on the state of the switch. To inspect the extension, right-click the extension icon and select Inspect popup. You should see a \u0026ldquo;Hello, world\u0026rdquo; message in the Console tab. Creating the content script Update the manifest.json file to include the content_scripts section. The entire file should look like this:\n{ \u0026#34;manifest_version\u0026#34;: 3, \u0026#34;name\u0026#34;: \u0026#34;My Chrome Extension\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;My first Chrome extension.\u0026#34;, \u0026#34;action\u0026#34;: { \u0026#34;default_popup\u0026#34;: \u0026#34;popup.html\u0026#34; }, \u0026#34;permissions\u0026#34;: [ \u0026#34;storage\u0026#34; ], \u0026#34;content_scripts\u0026#34;: [ { \u0026#34;matches\u0026#34;: [\u0026#34;\u0026lt;all_urls\u0026gt;\u0026#34;], \u0026#34;js\u0026#34;: [\u0026#34;content.js\u0026#34;] } ] } Create the new file content.js with the following content:\n\u0026#34;use strict\u0026#34; const blurFilter = \u0026#34;blur(6px)\u0026#34; let textToBlur = \u0026#34;\u0026#34; // Search this DOM node for text to blur and blur the parent element if found. function processNode(node) { if (node.childNodes.length \u0026gt; 0) { Array.from(node.childNodes).forEach(processNode) } if (node.nodeType === Node.TEXT_NODE \u0026amp;\u0026amp; node.textContent !== null \u0026amp;\u0026amp; node.textContent.trim().length \u0026gt; 0) { const parent = node.parentElement if (parent !== null \u0026amp;\u0026amp; (parent.tagName === \u0026#39;SCRIPT\u0026#39; || parent.style.filter === blurFilter)) { // Already blurred return } if (node.textContent.includes(textToBlur)) { blurElement(parent) } } } function blurElement(elem) { elem.style.filter = blurFilter console.debug(\u0026#34;blurred id:\u0026#34; + elem.id + \u0026#34; class:\u0026#34; + elem.className + \u0026#34; tag:\u0026#34; + elem.tagName + \u0026#34; text:\u0026#34; + elem.textContent) } // Create a MutationObserver to watch for changes to the DOM. const observer = new MutationObserver((mutations) =\u0026gt; { mutations.forEach((mutation) =\u0026gt; { if (mutation.addedNodes.length \u0026gt; 0) { mutation.addedNodes.forEach(processNode) } else { processNode(mutation.target) } }) }) // Enable the content script by default. let enabled = true const keys = [\u0026#34;enabled\u0026#34;, \u0026#34;item\u0026#34;] chrome.storage.sync.get(keys, (data) =\u0026gt; { if (data.enabled === false) { enabled = false } if (data.item) { textToBlur = data.item } // Only start observing the DOM if the extension is enabled and there is text to blur. if (enabled \u0026amp;\u0026amp; textToBlur.trim().length \u0026gt; 0) { observer.observe(document, { attributes: false, characterData: true, childList: true, subtree: true, }) // Loop through all elements on the page for initial processing. processNode(document) } }) The script listens for changes in the DOM and blurs elements that contain the text specified in the input field of the popup.\nAt this point, we can test the extension by entering text in the input field and enabling it. After reloading the page, the extension should blur elements that contain the text.\nCreating the background script Our background script will update the badge text of the extension icon on startup.\nUpdate the manifest.json file to include the background section. The complete file should look like this:\n{ \u0026#34;manifest_version\u0026#34;: 3, \u0026#34;name\u0026#34;: \u0026#34;My Chrome Extension\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;My first Chrome extension.\u0026#34;, \u0026#34;action\u0026#34;: { \u0026#34;default_popup\u0026#34;: \u0026#34;popup.html\u0026#34; }, \u0026#34;permissions\u0026#34;: [ \u0026#34;storage\u0026#34; ], \u0026#34;content_scripts\u0026#34;: [ { \u0026#34;matches\u0026#34;: [\u0026#34;\u0026lt;all_urls\u0026gt;\u0026#34;], \u0026#34;js\u0026#34;: [\u0026#34;content.js\u0026#34;] } ], \u0026#34;background\u0026#34;: { \u0026#34;service_worker\u0026#34;: \u0026#34;background.js\u0026#34; } } Create a new file background.js with the following content:\n\u0026#34;use strict\u0026#34; function setBadgeText(enabled) { const text = enabled ? \u0026#34;ON\u0026#34; : \u0026#34;OFF\u0026#34; void chrome.action.setBadgeText({text: text}) } function startUp() { chrome.storage.sync.get(\u0026#34;enabled\u0026#34;, (data) =\u0026gt; { setBadgeText(!!data.enabled) }) } // Ensure the background script always runs. chrome.runtime.onStartup.addListener(startUp) chrome.runtime.onInstalled.addListener(startUp) The script listens for the startup and installation events and sets the badge text based on the extension\u0026rsquo;s saved state.\nAt this point, our basic extension is complete. We can test the extension.\nNext steps In the next part of this series, we will add development tooling to the Chrome extension, such as TypeScript support, a bundling tool called webpack, and a development mode that will reload the extension automatically when changes are made.\nFor a list of all articles in the series, see the production-ready Chrome extension series overview.\nOther getting started guides Recently, we wrote about creating a React application from scratch while minimizing the amount of tools used. We also have a guide on getting started with CGO in Go. Basic extension code on GitHub The complete code is available on GitHub at: https://github.com/getvictor/create-chrome-extension/tree/main/1-basic-extension\nCreate a Chrome extension from scratch step-by-step video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-15T00:00:00Z","image":"https://victoronsoftware.com/posts/create-chrome-extension/chrome-extension-headline_hu_a0b5db252f228762.png","permalink":"https://victoronsoftware.com/posts/create-chrome-extension/","title":"Create a Chrome extension from scratch step-by-step (2024)"},{"content":"Introduction IPv6 is the latest version of the Internet Protocol. It provides a larger address space than IPv4, which is running out of addresses. IPv6 is essential for the future of the Internet, and many cloud providers support it.\nIn addition, IPv6 is more secure than IPv4. It has built-in security features like IPsec, which is optional in IPv4. IPv6 also has a simplified header, which makes it faster than IPv4.\nMany corporations use IPv6 internally, and some have even disabled IPv4. This tutorial will create a Linux VM using IPv6, with IPv4 disabled.\nThe steps are:\nCreate droplets with IPv6 enabled SSH from IPv4 client to IPv6-only server Disable IPv4 on the Linux server Prerequisites We will use Digital Ocean as our cloud provider. Their IPv6 documentation is available at https://docs.digitalocean.com/products/networking/ipv6/.\nDroplets are Digital Ocean\u0026rsquo;s virtual private servers. They run on virtualized hardware and are available in various sizes. We will create a new droplet with IPv6.\nStep 1: Create droplets with IPv6 enabled We will create two Digital Ocean droplets. The first droplet will have only IPv6 enabled, and the second droplet will have both IPv4 and IPv6 enabled. We only need the second droplet to SSH into the first droplet because our client machine uses IPv4 only.\nBoth droplets will use Ubuntu 24.04 (LTS), although any Linux distribution should work. Both droplets should have IPv6 enabled in Advanced Options.\nThe first droplet will use the Password authentication method.\nThe second droplet can have either Password or SSH authentication.\nStep 2: SSH from IPv4 client to IPv6-only server You can find the Droplet IPv4 and IPv6 addresses in the Droplet details.\nNow, we connect to the second droplet using SSH.\nssh root@143.198.235.211 From there, we can SSH into the first droplet using its IPv6 address.\nssh root@2604:a880:4:1d0::4d3:3000 Install the net-tools package to use the ifconfig command.\nsudo apt update sudo apt install net-tools Step 3: Disable IPv4 on the Linux server To disable IPv4 on the first droplet, edit the /etc/netplan/50-cloud-init.yaml network configuration file by removing all the IPv4 addresses and routes, and adding the IPv6 nameservers, as shown below.\nnetwork: version: 2 ethernets: eth0: accept-ra: false addresses: - 2604:a880:4:1d0::4d3:3000/64 match: macaddress: da:a1:07:89:d9:a1 mtu: 1500 nameservers: addresses: - 2001:4860:4860::8844 - 2001:4860:4860::8888 search: [] routes: - to: ::/0 via: 2604:a880:4:1d0::1 set-name: eth0 Apply the changes.\nsudo netplan apply --debug Now, you can view the network configuration using the ifconfig command. It should look like:\neth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet6 fe80::d8a1:7ff:fe89:d9a1 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; inet6 2604:a880:4:1d0::4d3:3000 prefixlen 64 scopeid 0x0\u0026lt;global\u0026gt; ether da:a1:07:89:d9:a1 txqueuelen 1000 (Ethernet) RX packets 5179 bytes 3832240 (3.8 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 5099 bytes 696019 (696.0 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eth1: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet6 fe80::e826:4cff:feb7:6659 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether ea:26:4c:b7:66:59 txqueuelen 1000 (Ethernet) RX packets 12 bytes 916 (916.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 43 bytes 2266 (2.2 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 233 bytes 22136 (22.1 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 233 bytes 22136 (22.1 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 You can see that the eth0 interface has an IPv6 address but no IPv4 address. The eth1 interface also has an IPv6 address. The lo interface is the loopback interface and still uses the IPv4 127.0.0.1 address. We will not disable IPv4 on the loopback interface at this point since many tools may break.\nTransfer files between IPv4 and IPv6-only servers To transfer files between the IPv4 and IPv6-only servers, you can use the scp command. First, transfer to the droplet that supports both IPv4 and IPv6, like:\nscp fleet-osquery_1.24.0_amd64.deb root@143.198.235.211:~ Then, SSH into that droplet and transfer the file to the IPv6-only droplet:\nscp fleet-osquery_1.24.0_amd64.deb root@\\[2604:a880:4:1d0::4d3:3000\\]:~ Conclusion In this tutorial, we created a Linux VM using IPv6, with IPv4 disabled. We also transferred files between an IPv4 and an IPv6-only server. IPv6 is the future of the Internet, and learning how to use it is essential. You can now create your own IPv6-only servers and experiment with them.\nFurther reading Recently, we discussed why you need VLANs in your home network.\nCreate an IPv6-only Linux server video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-08T00:00:00Z","image":"https://victoronsoftware.com/posts/create-ipv6-only-linux-server/ipv6-only_hu_356df42add42edc9.png","permalink":"https://victoronsoftware.com/posts/create-ipv6-only-linux-server/","title":"Create an IPv6-only Linux server in 3 easy steps"},{"content":"Introduction We recently encountered a performance issue in production. Once an hour, we saw a spike in average DB lock time, along with occasional deadlocks and server errors. We identified the problematic query using Amazon RDS logs. It was an INSERT statement with subqueries.\nINSERT INTO policy_stats (policy_id, inherited_team_id, passing_host_count, failing_host_count) SELECT p.id, t.id AS inherited_team_id, ( SELECT COUNT(*) FROM policy_membership pm INNER JOIN hosts h ON pm.host_id = h.id WHERE pm.policy_id = p.id AND pm.passes = true AND h.team_id = t.id ) AS passing_host_count, ( SELECT COUNT(*) FROM policy_membership pm INNER JOIN hosts h ON pm.host_id = h.id WHERE pm.policy_id = p.id AND pm.passes = false AND h.team_id = t.id ) AS failing_host_count FROM policies p CROSS JOIN teams t WHERE p.team_id IS NULL GROUP BY p.id, t.id ON DUPLICATE KEY UPDATE updated_at = NOW(), passing_host_count = VALUES(passing_host_count), failing_host_count = VALUES(failing_host_count); This statement calculated passing/failing results and inserted them into a policy_stats summary table. Unfortunately, this query took over 30 seconds to execute. During this time, it locked the important policy_membership table, preventing other threads from writing to it.\nReproducing slow SQL queries Since we saw the issue in production, we needed to reproduce it in a test environment. We created a similar schema and loaded it with data. We used a Go script to populate the tables with dummy data: https://github.com/getvictor/mysql/blob/main/insert-with-subqueries-perf/main.go.\nInitially, we used ten policies and ten teams with 10,000 hosts each, resulting in 100 inserted rows with the above query. However, the performance was only three to six seconds. Then, we increased the number of policies to 50, resulting in 500 inserted rows. The performance dropped to 30 to 60 seconds.\nThe above data made it clear that this query needed to be more scalable. As the GROUP BY p.id, t.id clause demonstrates, performance exponentially degrades with the number of policies and teams.\nDebugging slow SQL queries MySQL has powerful tools called EXPLAIN and EXPLAIN ANALYSE. These tools show how MySQL executes a query and help identify performance bottlenecks. We ran EXPLAIN ANALYSE on the problematic query and viewed the results as a tree and a diagram.\nMySQL EXPLAIN result in TREE format MySQL EXPLAIN result as a diagram Although the EXPLAIN output was complex, it was clear that the SELECT subqueries were executing too many times.\nFixing INSERT with subqueries performance The first step was to separate the INSERT from the SELECT. The top SELECT subquery took most of the time. But, more importantly, the SELECT does not block other threads from updating the policy_membership table.\nHowever, the single standalone SELECT subquery was still slow. In addition, memory usage could be high for many teams and policies.\nWe decided to process one policy row at a time. This reduced the time to complete an individual SELECT query to less than two seconds and limited the memory usage. We did not use a transaction to minimize locks. Not utilizing a transaction meant that the INSERT could fail if a parallel process deleted the policy. Also, the INSERT could overwrite a clearing of the policy_stats row. These drawbacks were acceptable, as they were rare cases.\nSELECT p.id as policy_id, t.id AS inherited_team_id, ( SELECT COUNT(*) FROM policy_membership pm INNER JOIN hosts h ON pm.host_id = h.id WHERE pm.policy_id = p.id AND pm.passes = true AND h.team_id = t.id ) AS passing_host_count, ( SELECT COUNT(*) FROM policy_membership pm INNER JOIN hosts h ON pm.host_id = h.id WHERE pm.policy_id = p.id AND pm.passes = false AND h.team_id = t.id ) AS failing_host_count FROM policies p CROSS JOIN teams t WHERE p.team_id IS NULL AND p.id = ? GROUP BY t.id, p.id; After each SELECT, we inserted the results into the policy_stats table.\nINSERT INTO policy_stats (policy_id, inherited_team_id, passing_host_count, failing_host_count) VALUES (?, ?, ?, ?), ... ON DUPLICATE KEY UPDATE updated_at = NOW(), passing_host_count = VALUES(passing_host_count), failing_host_count = VALUES(failing_host_count); Further reading about MySQL MySQL deadlock on UPDATE/INSERT upsert pattern Scaling DB performance using master slave replication Fully supporting Unicode and emojis in your app SQL prepared statements are broken when scaling applications MySQL code to populate DB on GitHub The code to populate our test DB is available on GitHub at: https://github.com/getvictor/mysql/tree/main/insert-with-subqueries-perf\nMySQL query performance: INSERT with subqueries video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-06T00:00:00Z","image":"https://victoronsoftware.com/posts/mysql-query-performance-insert-subqueries/INSERT%20with%20subqueries_hu_e678b21b78faffd9.png","permalink":"https://victoronsoftware.com/posts/mysql-query-performance-insert-subqueries/","title":"Optimize MySQL query performance: INSERT with subqueries"},{"content":" GitHub reusable workflows GitHub reusable steps (composite action) Introduction GitHub Actions is a way to automate your software development workflows. The approach is similar to CI/CD tools like Jenkins, CircleCI, and TravisCI. However, GitHub Actions are built into GitHub.\nHigh level diagram of GitHub Actions The entry point for GitHub Actions is the .github/workflows directory in your repository. This directory contains one or more YAML files that define your workflows. A workflow is an automated process made up of one or more jobs. Each job runs on a separate runner. A runner is a server that runs the job. A job contains one or more steps. Each step runs a separate command.\nWhy reuse? Code reuse is a fundamental principle of software development. Reusing GitHub Actions code allows you to:\nImprove maintainability by keeping common code in one place and reducing the amount of code Increase consistency since multiple workflows can use the same code Promote best practices Increase productivity Reduce errors Examples of reusable GitHub Actions code include:\nCode signing Uploading artifacts to cloud services Security checks Notifications and reports Data processing and many others Reusable workflows A reusable workflow replaces a job in the main workflow.\nGitHub Actions reusable workflow A reusable workflow may be shared across repositories and run on a different platform than the main workflow.\nFor file sharing, \u0026lsquo;build artifacts\u0026rsquo; must be used to share files with the main workflow. The reusable workflow does not inherit environment variables. However, it accepts inputs and secrets from the calling workflow and may use outputs to pass data back to the main workflow.\nHere is an example of a reusable workflow. It uses the same schema as a regular workflow.\nname: Reusable workflow on: workflow_call: inputs: reusable_input: description: \u0026#39;Input to the reusable workflow\u0026#39; required: true type: string filename: required: true type: string secrets: HELLO_WORLD_SECRET: required: true outputs: # Map the workflow output(s) to job output(s) reusable_output: description: \u0026#39;Output from the reusable workflow\u0026#39; value: ${{ jobs.reusable-workflow-job.outputs.job_output }} defaults: run: shell: bash jobs: reusable-workflow-job: runs-on: ubuntu-20.04 # Map the job output(s) to step output(s) outputs: job_output: ${{ steps.process-step.outputs.step_output }} steps: - name: Process reusable input id: process-step env: HELLO_WORLD_SECRET: ${{ secrets.HELLO_WORLD_SECRET }} run: | echo \u0026#34;reusable_input=${{ inputs.reusable_input }}\u0026#34; echo \u0026#34;HELLO_WORLD_SECRET=${HELLO_WORLD_SECRET}\u0026#34; echo \u0026#34;step_output=${{ inputs.reusable_input }}_processed\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT - uses: actions/download-artifact@v4 with: name: input_file - name: Process file run: | echo \u0026#34;Processing file: ${{ inputs.filename }}\u0026#34; echo \u0026#34;file processed\u0026#34; \u0026gt;\u0026gt; ${{ inputs.filename }} - uses: actions/upload-artifact@v4 with: name: output_file path: ${{ inputs.filename }} The reusable workflow is triggered on: workflow_call. It accepts an input called reusable_input and generates an output called reusable_output. It also downloads an artifact called input_file, processes a file, and uploads an artifact called output_file.\nThe main workflow calls the reusable workflow using the uses keyword.\njob-2: needs: job-1 # We do not need to check out the repository to use the reusable workflow uses: ./.github/workflows/reusable-workflow.yml with: reusable_input: \u0026#34;job-2-input\u0026#34; filename: \u0026#34;input.txt\u0026#34; secrets: # Can also implicitly pass the secrets with: secrets: inherit HELLO_WORLD_SECRET: TERCES_DLROW_OLLEH A successful run of the main workflow looks like this on GitHub:\nGitHub Actions reusable workflow success Reusable steps (composite action) Reusable steps replace a regular step in a job. We will use a composite action for reusable steps in our example.\nGitHub Actions reusable steps (composite action) Like a reusable workflow, a composite action may be shared across repositories, it accepts inputs, and it may use outputs to pass data back to the main workflow.\nUnlike a reusable workflow, a composite action inherits environment variables. However, it does not inherit secrets. Secrets must be passed explicitly as inputs or environment variables. Also, there is no need to use \u0026lsquo;build artifacts\u0026rsquo; to share files since the reusable steps run on the same runner and in the same work area as the main job.\nHere is an example of a composite action. It uses a different schema than a workflow. Also, the file must be named action.yml or similar.\nname: Reusable steps (AKA composite action) description: Demonstrate how to use reusable steps in a workflow # Schema: https://json.schemastore.org/github-action.json inputs: reusable_input: description: \u0026#39;Input to the reusable workflow\u0026#39; required: true filename: required: true outputs: # Map the action output(s) to step output(s) reusable_output: description: \u0026#39;Output from the reusable workflow\u0026#39; value: ${{ steps.process-step.outputs.step_output }} runs: using: \u0026#39;composite\u0026#39; steps: - name: Process reusable input id: process-step # Shell must explicitly specify the shell for each step. https://github.com/orgs/community/discussions/18597 shell: bash run: | echo \u0026#34;reusable_input=${{ inputs.reusable_input }}\u0026#34; echo \u0026#34;HELLO_WORLD_SECRET=${HELLO_WORLD_SECRET}\u0026#34; echo \u0026#34;step_output=${{ inputs.reusable_input }}_processed\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT - name: Process file shell: bash run: | echo \u0026#34;Processing file: ${{ inputs.filename }}\u0026#34; echo \u0026#34;file processed\u0026#34; \u0026gt;\u0026gt; ${{ inputs.filename }} The composite action is called via the uses setting on a step. Our action accepts an input called reusable_input and generates an output called reusable_output. It also processes a file called filename.\nThe following code snippet shows how to use the composite action in a job.\n- name: Use reusable steps id: reusable-steps uses: ./.github/reusable-steps # To use this syntax, we must have the repository checked out with: reusable_input: \u0026#34;job-2-input\u0026#34; filename: \u0026#34;input.txt\u0026#34; env: HELLO_WORLD_SECRET: TERCES_DLROW_OLLEH A successful run of the main workflow with reusable steps looks like this on GitHub:\nGitHub Actions composite action success For a reusable TypeScript action example, see the How to create a custom GitHub Action using TypeScript article.\nConclusion Reusable workflows and steps are powerful tools for improving the maintainability, consistency, and productivity of your GitHub Actions. They allow you to reuse code across repositories and workflows and promote best practices. They are a great way to reduce errors and increase productivity.\nFor larger units of work, a reusable workflow should be used. A composite action should be used for smaller units of work that may run on the same runner and share the same work area.\nExample code on GitHub The example code is available on GitHub at: https://github.com/getvictor/github-reusable-workflows-and-steps\nOther articles related to GitHub Automate tracking of engineering metrics with GitHub Actions Is GitHub code review process broken in your company? git merge and GitHub pull requests explained Finding the minimum required code owner approvers for pull request Use GitHub actions for general-purpose tasks GitHub Actions reusable workflows and steps video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-05-01T00:00:00Z","image":"https://victoronsoftware.com/posts/github-reusable-workflows-and-steps/GitHub%20Actions%20thumbnail_hu_976f041deee6d2a9.png","permalink":"https://victoronsoftware.com/posts/github-reusable-workflows-and-steps/","title":"How to reuse workflows and steps in GitHub Actions (2024)"},{"content":"What is an SQL deadlock? A deadlock occurs when two or more SQL transactions are waiting for each other to release locks. This can occur when two transactions have locks on separate resources and each is waiting for the other to release its lock.\nWhat is an upsert? An upsert combines the words \u0026ldquo;update\u0026rdquo; and \u0026ldquo;insert.\u0026rdquo; It is a database operation that inserts a new row into a table if the row does not exist or updates the row if it already exists.\nINSERT/UPDATE upsert pattern One common way to implement an upsert operation in MySQL is to use the following pattern:\nUPDATE table_name SET column1 = value1, column2 = value2 WHERE id = ?; -- If the UPDATE statement does not affect any rows, insert a new row: INSERT INTO table_name (id, column1, column2) VALUES (?, value1, value2); UPDATE returns the number of rows that were actually changed.\nThis UPDATE/INSERT pattern is optimized for frequent updates and rare inserts. However, it can lead to deadlocks when multiple transactions try to insert simultaneously.\nMySQL deadlock example We assume the default transaction isolation level of REPEATABLE READ. Given the following table with one row:\nCREATE TABLE my_table ( id int(10) unsigned NOT NULL, amount int(10) unsigned NOT NULL, PRIMARY KEY (id) ); INSERT INTO my_table (id, amount) VALUES (1, 1); One transaction executes the following SQL:\nUPDATE my_table SET amount = 2 WHERE id = 2; Another transaction executes the following SQL:\nUPDATE my_table SET amount = 3 WHERE id = 3; INSERT INTO my_table (id, amount) VALUES (3, 3); At this point, the second transaction is waiting for the first transaction to release the lock.\nThe first transaction then executes the following SQL:\nINSERT INTO my_table (id, amount) VALUES (2, 2); Causing a deadlock:\n[40001][1213] Deadlock found when trying to get lock; try restarting transaction Why does the deadlock occur? The deadlock occurs because both transactions set next-key locks on the rows they attempted to update. Since the rows they attempted to update did not exist, the lock is set on the \u0026ldquo;supremum\u0026rdquo; pseudo-record. This pseudo-record has a value higher than any value actually in the index. This lock prevents the other transaction from inserting the row it needs.\nDebugging MySQL deadlocks To view the last deadlock detected by MySQL, you can use:\nSHOW ENGINE INNODB STATUS; The output will contain a section like this:\n------------------------ LATEST DETECTED DEADLOCK ------------------------ 2024-04-28 12:29:17 281472351068032 *** (1) TRANSACTION: TRANSACTION 1638819, ACTIVE 7 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 3 lock struct(s), heap size 1128, 2 row lock(s) MySQL thread id 97926, OS thread handle 281471580295040, query id 24192112 192.168.65.1 root update /* ApplicationName=DataGrip 2024.1 */ INSERT INTO my_table (id, amount) VALUES (3, 3) *** (1) HOLDS THE LOCK(S): RECORD LOCKS space id 158 page no 4 n bits 72 index PRIMARY of table `test`.`my_table` trx id 1638819 lock_mode X Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 158 page no 4 n bits 72 index PRIMARY of table `test`.`my_table` trx id 1638819 lock_mode X insert intention waiting Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (2) TRANSACTION: TRANSACTION 1638812, ACTIVE 13 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 3 lock struct(s), heap size 1128, 2 row lock(s) MySQL thread id 97875, OS thread handle 281471585578880, query id 24192285 192.168.65.1 root update /* ApplicationName=DataGrip 2024.1 */ INSERT INTO my_table (id, amount) VALUES (2, 2) *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 158 page no 4 n bits 72 index PRIMARY of table `test`.`my_table` trx id 1638812 lock_mode X Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 158 page no 4 n bits 72 index PRIMARY of table `test`.`my_table` trx id 1638812 lock_mode X insert intention waiting Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; We can see the supremum locks held by both transactions: 0: len 8; hex 73757072656d756d; asc supremum;;.\nAnother way to debug MySQL deadlocks is to enable the innodb_print_all_deadlocks option. This option prints all deadlocks to the error log.\nPreventing the UPDATE/INSERT deadlock One way to prevent the deadlock is to use the INSERT \u0026hellip; ON DUPLICATE KEY UPDATE pattern. This syntax allows you to insert a new row or update an existing row in a single statement. However, it is slower than an UPDATE and always increments the auto-increment value if present.\nAnother way is to roll back the transaction once we know that the UPDATE did not affect any rows. This avoids the deadlock by not holding the lock while we insert the new row. After the rollback, we can retry the transaction using the above INSERT ... ON DUPLICATE KEY UPDATE pattern.\nA third way is not to use transactions. In this case, the locks are released immediately after the statement is executed. However, this approach may not be suitable for all use cases.\nConclusion The UPDATE/INSERT upsert pattern can lead to MySQL deadlocks when multiple transactions try to insert simultaneously. To prevent deadlocks, consider using the INSERT ... ON DUPLICATE KEY UPDATE pattern, rolling back the transaction, or not using transactions.\nMySQL deadlock on UPDATE/INSERT upsert pattern video Other articles related to MySQL Optimize MySQL query performance: INSERT with subqueries Master slave replication in MySQL Fully supporting Unicode and emojis in your app SQL prepared statements are broken when scaling applications Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-04-28T00:00:00Z","image":"https://victoronsoftware.com/posts/mysql-upsert-deadlock/MySQL%20deadlock_hu_76d5d11012123a5d.png","permalink":"https://victoronsoftware.com/posts/mysql-upsert-deadlock/","title":"MySQL deadlock on UPDATE/INSERT upsert pattern"},{"content":"Introduction In this article, we will create a simple React app from scratch. We will not use any templates or helper scripts. We aim to reduce tool usage and fully understand each step of the process.\nWhat is React? React is a popular JavaScript library for building user interfaces. It was created by Meta (Facebook) and is maintained by Meta and a community of developers. React is used to build single-page applications (SPAs) and dynamic web applications.\nPrerequisites \u0026ndash; Node.js and npm Node.js and npm are the most popular tools for working with React. Node.js is a JavaScript runtime. npm is a package manager for Node.js. These two tools are essential for modern web development.\npackage.json We will start by creating a package.json file. This file contains metadata about the project and its dependencies. You can use the npm init command to create the package.json file. Or create one yourself containing something like:\n{ \u0026#34;name\u0026#34;: \u0026#34;react-hello-world\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Hello world app using React\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/react\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34; } TypeScript Next, we will add TypeScript to our project. TypeScript is a superset of JavaScript that adds static types to the language. It helps catch errors early in the development process and improves code quality.\nAlthough TypeScript is not required to build a React app, it is strongly recommended. TypeScript is widely used in the React community and provides many benefits. Modern IDEs, such as Visual Studio Code and WebStorm, support TypeScript, making development and learning easier.\nnpm install --save-dev typescript This command updates the package.json file with the TypeScript dependency.\n{ \u0026#34;name\u0026#34;: \u0026#34;react-hello-world\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Hello world app using React\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/react\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;typescript\u0026#34;: \u0026#34;^5.4.5\u0026#34; } } It also creates a node_modules directory. This directory contains the packages installed by npm.\nFinally, the command creates a package-lock.json file. This file locks the dependencies to specific versions, ensuring that the project is built with the same versions of the dependencies across different machines.\nThe --save-dev flag tells npm to save the package as a development dependency. Development dependencies are not required for the production build of the app.\ntsconfig.json We need to create a tsconfig.json file to configure TypeScript. This file specifies the root files and compiler options for the TypeScript compiler. We will extend the recommended base configuration.\nInstall the recommended configuration with the following:\nnpm install --save-dev @tsconfig/recommended Then, create a tsconfig.json file with the following content:\n{ \u0026#34;extends\u0026#34;: \u0026#34;@tsconfig/recommended/tsconfig.json\u0026#34;, \u0026#34;compilerOptions\u0026#34;: { \u0026#34;jsx\u0026#34;: \u0026#34;react-jsx\u0026#34; } } What is JSX? In our tsconfig.json file, we set the jsx option to react-jsx. This option tells TypeScript to treat JSX as React JSX.\nJSX is a syntax extension for JavaScript. It allows you to write HTML-like code in JavaScript. JSX is used in React. It is syntactic sugar that is generally transpiled into JavaScript by the build tool.\nReact and ReactDOM Next, we will add React and ReactDOM to our project. React is the base library. ReactDOM is the package that provides DOM-specific methods for React.\nnpm install react react-dom Since we are using TypeScript, we must also install type definitions for React and ReactDOM. The TypeScript compiler uses these definitions for type checking.\nnpm install --save-dev @types/react @types/react-dom What is Webpack? Webpack is a module bundler for JavaScript. It takes modules with dependencies and generates static assets representing those modules. We will use Webpack as the build tool for our React app.\nWe will install the Webpack packages:\nnpm install --save-dev webpack webpack-cli webpack-dev-server html-webpack-plugin ts-loader webpack is the core package webpack-cli provides the command-line interface, which we will use to run Webpack commands webpack-dev-server is a development server that serves the app html-webpack-plugin will generate the index.html file to serve our app ts-loader is a TypeScript loader for Webpack. It allows Webpack to compile TypeScript files. webpack.config.ts By default, Webpack does not need a configuration file. However, since we use TypeScript, we must create a webpack.config.ts file to configure Webpack.\nNote that we use the .ts extension for the configuration file. The TypeScript compiler will compile this file. Using a .js file is also possible, but we prefer TypeScript for type safety.\nNo additional type definitions are required for our Webpack configuration at this time.\nimport HtmlWebpackPlugin from \u0026#39;html-webpack-plugin\u0026#39;; module.exports = { entry: \u0026#39;./src/index.tsx\u0026#39;, module: { rules: [ { test: /\\.(ts|tsx)$/, loader: \u0026#34;ts-loader\u0026#34;, exclude: /node_modules/, }, ], }, plugins: [new HtmlWebpackPlugin()], } We specify src/index.tsx as our app\u0026rsquo;s top-level file. By default, the build\u0026rsquo;s output will go to the dist directory.\nWe configure the TypeScript loader to compile .ts and .tsx files.\nWe also use the html-webpack-plugin to generate an index.html file. This file will load the Webpack bundle.\nWe need to add a TypeScript execution engine to the Node.js runtime so that it can understand the above TypeScript configuration file. We will use ts-node for this purpose.\nnpm install --save-dev ts-node Final package.json After all the installations, our package.json file should look similar to this:\n{ \u0026#34;name\u0026#34;: \u0026#34;react-hello-world\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Hello world app using React\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;https://github.com/getvictor/react\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;getvictor\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;@tsconfig/recommended\u0026#34;: \u0026#34;^1.0.6\u0026#34;, \u0026#34;@types/react\u0026#34;: \u0026#34;^18.2.79\u0026#34;, \u0026#34;@types/react-dom\u0026#34;: \u0026#34;^18.2.25\u0026#34;, \u0026#34;html-webpack-plugin\u0026#34;: \u0026#34;^5.6.0\u0026#34;, \u0026#34;ts-loader\u0026#34;: \u0026#34;^9.5.1\u0026#34;, \u0026#34;ts-node\u0026#34;: \u0026#34;^10.9.2\u0026#34;, \u0026#34;typescript\u0026#34;: \u0026#34;^5.4.5\u0026#34;, \u0026#34;webpack\u0026#34;: \u0026#34;^5.91.0\u0026#34;, \u0026#34;webpack-cli\u0026#34;: \u0026#34;^5.1.4\u0026#34;, \u0026#34;webpack-dev-server\u0026#34;: \u0026#34;^5.0.4\u0026#34; }, \u0026#34;dependencies\u0026#34;: { \u0026#34;react\u0026#34;: \u0026#34;^18.2.0\u0026#34;, \u0026#34;react-dom\u0026#34;: \u0026#34;^18.2.0\u0026#34; } } src/index.tsx We are finally ready to write some React code. TSX files are TypeScript files that contain JSX.\nWe will create the src/index.tsx file. It will render a simple React component. React components are the reusable building blocks of React apps.\nimport React from \u0026#34;react\u0026#34;; import {createRoot} from \u0026#34;react-dom/client\u0026#34; // A simple Class component class HelloWorld extends React.Component { render() { return \u0026lt;h1\u0026gt;Hello world!\u0026lt;/h1\u0026gt; } } // Use traditional DOM manipulation to create a root element for React document.body.innerHTML = \u0026#39;\u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt;\u0026lt;/div\u0026gt;\u0026#39; // Create a root element for React const app = createRoot(document.getElementById(\u0026#34;app\u0026#34;)!) // Render our HelloWorld component app.render(\u0026lt;HelloWorld/\u0026gt;) Running the app on the Webpack development server Now, we can run the app on the Webpack development server. This server will serve the app and automatically reload the page when the code changes.\nnode_modules/.bin/webpack serve --mode development --open The --mode development flag tells Webpack to build the app in development mode. The --open flag tells Webpack to open the app in the default browser.\nThe browser should show the following:\nReact app served by Webpack dev server package.json scripts Instead of remembering the above webpack command, we can add a script to the package.json file to run the Webpack development server.\n\u0026#34;scripts\u0026#34;: { \u0026#34;start\u0026#34;: \u0026#34;webpack serve --mode development --open\u0026#34; } start is a special script name that maps to the npm start command. Now, we can run the development server with:\nnpm start or\nnpm run start Building the app for production To build the app for production, we can run:\nnode_modules/.bin/webpack --mode production This command will create a dist directory with the app\u0026rsquo;s production build. The directory will contain the index.html file and the main.js JavaScript bundle. The production files are optimized for performance, and they are minified and compressed to reduce their size.\nIt is possible to host these production files on a local HTTP server like Apache or Nginx, or deploy the app to cloud providers such as AWS, Cloudflare Pages, Netlify, Render, or Vercel.\nOther getting started guides Recently, we wrote about creating a Chrome extension from scratch without any additional tooling. As part of that series, we covered adding linting and formatting tooling for TypeScript. We also have a guide on using CGO in Go programming language. Example code on GitHub The example code is available on GitHub at https://github.com/getvictor/react/tree/main/1-hello-world\nReact Hello World video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-04-24T00:00:00Z","image":"https://victoronsoftware.com/posts/react-hello-world/react-hello-world_hu_f7ec4c1ee10d39e7.png","permalink":"https://victoronsoftware.com/posts/react-hello-world/","title":"Build a React app from scratch: getting started (2024)"},{"content":"Why fix security vulnerabilities? Security vulnerabilities are a common issue in software development. They can lead to data breaches, unauthorized access, and other security incidents. It is important to fix security vulnerabilities as soon as possible to protect your data and users.\nFinding vulnerabilities Nowadays, it is possible to integrate various vulnerability scanning tools into your CI/CD pipeline. These tools can help you identify security vulnerabilities in your code and dependencies. One such tool is OpenSSF Scorecard, which combines multiple other tools into a single GitHub action. It uses the OSV service to find vulnerabilities affecting your project\u0026rsquo;s dependencies. OSV (Open Source Vulnerabilities) is a Google-based vulnerability database providing information about open-source projects' vulnerabilities.\nIn this article, we will focus on fixing a few recent real-world security vulnerabilities in our yarn.lock dependencies.\nscore is 3: 6 existing vulnerabilities detected: Warn: Project is vulnerable to: GHSA-crh6-fp67-6883 Warn: Project is vulnerable to: GHSA-wf5p-g6vw-rhxx Warn: Project is vulnerable to: GHSA-p6mc-m468-83gw Warn: Project is vulnerable to: GHSA-566m-qj78-rww5 Warn: Project is vulnerable to: GHSA-7fh5-64p2-3v2j Warn: Project is vulnerable to: GHSA-4wf5-vphf-c2xc Click Remediation section below to solve this issue Using local tools to find vulnerabilities In a local environment, we can use OSV-Scanner to find vulnerabilities in our dependencies. Running:\nosv-scanner scan --lockfile yarn.lock It will output the same vulnerabilities mentioned above but with additional details.\n╭─────────────────────────────────────┬──────┬───────────┬────────────────┬─────────┬───────────╮ │ OSV URL │ CVSS │ ECOSYSTEM │ PACKAGE │ VERSION │ SOURCE │ ├─────────────────────────────────────┼──────┼───────────┼────────────────┼─────────┼───────────┤ │ https://osv.dev/GHSA-crh6-fp67-6883 │ 9.8 │ npm │ @xmldom/xmldom │ 0.8.3 │ yarn.lock │ │ https://osv.dev/GHSA-wf5p-g6vw-rhxx │ 6.5 │ npm │ axios │ 0.21.4 │ yarn.lock │ │ https://osv.dev/GHSA-p6mc-m468-83gw │ 7.4 │ npm │ lodash.set │ 4.3.2 │ yarn.lock │ │ https://osv.dev/GHSA-566m-qj78-rww5 │ 5.3 │ npm │ postcss │ 6.0.23 │ yarn.lock │ │ https://osv.dev/GHSA-7fh5-64p2-3v2j │ 5.3 │ npm │ postcss │ 6.0.23 │ yarn.lock │ │ https://osv.dev/GHSA-7fh5-64p2-3v2j │ 5.3 │ npm │ postcss │ 7.0.39 │ yarn.lock │ │ https://osv.dev/GHSA-7fh5-64p2-3v2j │ 5.3 │ npm │ postcss │ 8.4.21 │ yarn.lock │ │ https://osv.dev/GHSA-4wf5-vphf-c2xc │ 7.5 │ npm │ terser │ 5.12.1 │ yarn.lock │ ╰─────────────────────────────────────┴──────┴───────────┴────────────────┴─────────┴───────────╯ Another way to find these vulnerabilities is by using the built-in yarn audit command.\nWaiving vulnerabilities In some cases, you may decide to waive a vulnerability. This approach means that you examine the vulnerability documentation and acknowledge it but decide not to fix it.\nTo waive a vulnerability for the OSV flow, you can create an osv-scanner.toml file in the root of your project. For example, to waive the GHSA-crh6-fp67-6883 vulnerability, you can add the following to the osv-scanner.toml file:\n[[IgnoredVulns]] id = \u0026#34;GHSA-crh6-fp67-6883\u0026#34; reason = \u0026#34;We examined this vulnerability and concluded that it does not affect our project for a very good reason.\u0026#34; In our example, we will not waive any vulnerabilities, but we will fix them by updating the dependencies.\nUpdating an inner dependency In our example, we have a vulnerability in the @xmldom/xmldom package. From the vulnerability URL, we know we must update this package to 0.8.4 or later.\nRunning yarn why @xmldom/xmldom will show that it is an inner dependency of another package:\n=\u0026gt; Found \u0026#34;@xmldom/xmldom@0.8.3\u0026#34; info Reasons this module exists - \u0026#34;msw#@mswjs#interceptors\u0026#34; depends on it - Hoisted from \u0026#34;msw#@mswjs#interceptors#@xmldom#xmldom\u0026#34; Looking at yarn.lock shows:\n\u0026#34;@xmldom/xmldom@^0.8.3\u0026#34;: version \u0026#34;0.8.3\u0026#34; resolved \u0026#34;https://registry.yarnpkg.com/@xmldom/xmldom/-/xmldom-0.8.3.tgz#beaf980612532aa9a3004aff7e428943aeaa0711\u0026#34; integrity sha512-Lv2vySXypg4nfa51LY1nU8yDAGo/5YwF+EY/rUZgIbfvwVARcd67ttCM8SMsTeJy51YhHYavEq+FS6R0hW9PFQ== We see that 0.8.4 will satisfy the dependency requirement of ^0.8.3. We can update the package by deleting the above section from yarn.lock and running yarn install\nWe will then see the update:\n\u0026#34;@xmldom/xmldom@^0.8.3\u0026#34;: version \u0026#34;0.8.10\u0026#34; resolved \u0026#34;https://registry.yarnpkg.com/@xmldom/xmldom/-/xmldom-0.8.10.tgz#a1337ca426aa61cef9fe15b5b28e340a72f6fa99\u0026#34; integrity sha512-2WALfTl4xo2SkGCYRt6rDTFfk9R1czmBvUQy12gK2KuRKIpWEhcbbzy8EZXtz/jkRqHX8bFEc6FC1HjX4TUWYw== Upgrading an inner dependency by overriding the version Our following vulnerability is in the axios package. We need to update it to 0.28.0 or later. By running yarn why axios we see that this package is part of a deep dependency chain:\n=\u0026gt; Found \u0026#34;wait-on#axios@0.21.4\u0026#34; info This module exists because \u0026#34;@storybook#test-runner#jest-playwright-preset#jest-process-manager#wait-on\u0026#34; depends on it. The needed version 0.28.0 does not satisfy the ^0.21.4 requirement. We can override the version by adding the following to the package.json file:\n\u0026#34;resolutions\u0026#34;: { \u0026#34;**/wait-on/axios\u0026#34;: \u0026#34;^0.28.0\u0026#34; }, Upgrading the parent dependency The following vulnerability is in the lodash.set package. The vulnerability URL shows that there is no fix for this vulnerability. We also see at npmjs.com that this package was last updated eight years ago.\nWe need to update the parent package that uses lodash.set. Running yarn why lodash.set shows:\ninfo Reasons this module exists - \u0026#34;nock\u0026#34; depends on it - Hoisted from \u0026#34;nock#lodash.set\u0026#34; We update the parent by running yarn upgrade nock@latest. Luckily, the latest version of nock does not depend on lodash.set, and lodash.set is removed from yarn.lock.\nRemoving a dependency Sometimes the best way to fix a vulnerability is to remove the vulnerable dependency. This can be done with the yarn remove \u0026lt;dependency\u0026gt; command. However, this requires code changes. You must find a different library or implement the removed functionality yourself.\nConclusion We use the above strategies to fix the vulnerabilities in our project.\nUpdating an inner dependency Upgrading an inner dependency by overriding the version Upgrading the parent dependency Removing a dependency We can now rerun the vulnerability scanner to verify that we fixed the vulnerabilities.\nIn addition, we must run our unit test and integration test suite to ensure that the updates do not break our application.\nFix security vulnerabilities in Yarn video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-04-10T00:00:00Z","image":"https://victoronsoftware.com/posts/fix-security-vulnerabilities-yarn/cover_hu_c56039400bd50d01.png","permalink":"https://victoronsoftware.com/posts/fix-security-vulnerabilities-yarn/","title":"Fix security vulnerabilities in Yarn"},{"content":"We recently completed a series of articles on mutual TLS (mTLS). In this series, we covered the basics of mTLS, how to use macOS keychain and Windows certificate store, and how to build an mTLS Go client. Our goal was to show you how to use mTLS in your applications and securely store your mTLS certificates and keys without exposing them to the filesystem.\nHere is a summary of the articles in the series:\nMutual TLS intro and hands-on example An introduction to mTLS and a hands-on example of using an mTLS client to connect to an mTLS server.\nUsing mTLS with the macOS keychain A guide on how to use the macOS system keystore to store your mTLS certificates and keys. We connect to an mTLS server with applications that use the macOS system keychain to find the mTLS certificates.\nCreate an mTLS Go client We create a standard mTLS client in Go using the crypto/tls library. This client loads the client certificate and private key from the filesystem.\nAdd a custom certificate signer to the mTLS Go client We implement a custom crypto.Signer to sign a client certificate during the mTLS handshake. Thus, we are a step closer to removing our client certificate and private key from the filesystem.\nA complete mTLS Go client using the macOS keychain In this article, we continue the previous article by connecting our custom signer to the macOS keychain using CGO and Apple APIs.\nUsing mTLS with the Windows certificate store Switching to Windows, we learn how to use the Windows system keystore to store your mTLS certificates and keys. We connect to an mTLS server with applications that use the Windows certificate store to find the mTLS certificates.\nCreate an mTLS Go client using the Windows certificate store Using the software pattern from the previous articles on the macOS keychain, we build an mTLS client in Go integrated with the Windows certificate store to store the mTLS certificates and keys.\nMutual TLS (mTLS): building a client using the system keystore video playlist ","date":"2024-04-01T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls/mtls-handshake_hu_87e12984511b4ef4.png","permalink":"https://victoronsoftware.com/posts/mtls/","title":"Mutual TLS (mTLS): building a client using the system keystore"},{"content":"What is code signing? Code signing is the process of digitally signing executables and scripts to confirm the software author and guarantee that the code has not been altered or corrupted since it was signed. The method employs a cryptographic hash to validate the authenticity and integrity of the code.\nThe benefits of code signing Code signing provides several benefits:\nUser trust: Users are likelier to trust signed software because they can verify its origin. Security: Code signing helps prevent tampering and makes sure that bad actors have not altered the software. Malware protection: Code signing helps protect users from malware by verifying the software\u0026rsquo;s authenticity. Software updates: Code signing helps users verify that software updates are legitimate and not malicious. Windows Defender: Code signing helps prevent Windows Defender warnings. Code signing process for Windows The code signing process for Windows involves the following steps:\nObtain a code signing certificate: Purchase a code signing certificate from a trusted certificate authority (CA) or use a self-signed certificate. Sign the code: Use a code signing tool to sign the code with the code signing certificate. Timestamp the signature: Timestamp the signature to make sure that the signature remains valid even after the certificate expires. Distribute the signed code: Distribute the signed code to users. Verify the signature: Users can verify the signature to confirm the software\u0026rsquo;s authenticity. Obtaining a code signing certificate In our example, we will use a self-signed certificate. This approach is suitable for internal business applications. For public applications, you should obtain a code signing certificate from a trusted CA.\nWe will use the OpenSSL command line tool to generate the certificates. OpenSSL is a popular open-source library for TLS and SSL protocols.\nThe following script generates the certificate and key needed for code signing. It also generates a certificate authority (CA) and signs the code signing certificate with the CA.\n#!/usr/bin/env bash # -e: Immediately exit if any command has a non-zero exit status. # -x: Print all executed commands to the terminal. # -u: Exit if an undefined variable is used. # -o pipefail: Exit if any command in a pipeline fails. set -exuo pipefail # This script generates certificates and keys needed for code signing. mkdir -p certs # Certificate authority (CA) openssl genrsa -out certs/ca.key 2048 openssl req -new -x509 -nodes -days 1000 -key certs/ca.key -out certs/ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testCodeSignCA\u0026#34; # Generate a certificate for code signing, signed by the CA openssl req -newkey rsa:2048 -nodes -keyout certs/sign.key -out certs/sign.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testCodeSignCert\u0026#34; openssl x509 -req -in certs/sign.req -days 398 -CA certs/ca.crt -CAkey certs/ca.key -set_serial 01 -out certs/sign.crt # Clean up rm certs/sign.req Building the application We will build a simple \u0026ldquo;Hello World\u0026rdquo; Windows application using the Go programming language for this example. We compile the application with:\nexport GOOS=windows export GOARCH=amd64 go build ./hello-world.go The Go build process generates the hello-world.exe Windows executable.\nSigning and timestamping the code To sign the code, we will use osslsigncode, an open-source code signing tool that uses OpenSSL to sign Windows executables. Unlike Microsoft\u0026rsquo;s signtool, osslsigncode is cross-platform and can be used on Linux and macOS.\nTo sign the code, we use the following script:\n#!/usr/bin/env bash # -e: Immediately exit if any command has a non-zero exit status. # -x: Print all executed commands to the terminal. # -u: Exit if an undefined variable is used. # -o pipefail: Exit if any command in a pipeline fails. set -exuo pipefail input_file=$1 if [ ! -f \u0026#34;$input_file\u0026#34; ] then echo \u0026#39;First argument must be path to binary\u0026#39; exit 1 fi # Check that input file is a windows PE (Portable Executable) if ! ( file \u0026#34;$input_file\u0026#34; | grep -q PE ) then echo \u0026#39;File must be a Portable Executable (PE) file.\u0026#39; exit 0 fi # Check that osslsigncode is installed if ! command -v osslsigncode \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 ; then echo \u0026#34;osslsigncode utility is not present or missing from PATH. Binary cannot be signed.\u0026#34; exit 1 fi orig_file=\u0026#34;${input_file}_unsigned\u0026#34; mv \u0026#34;$input_file\u0026#34; \u0026#34;$orig_file\u0026#34; osslsigncode sign -certs \u0026#34;./certs/sign.crt\u0026#34; -key \u0026#34;./certs/sign.key\u0026#34; -n \u0026#34;Hello Windows code signing\u0026#34; -i \u0026#34;https://victoronsoftware.com/\u0026#34; -t \u0026#34;http://timestamp.comodoca.com/authenticode\u0026#34; -in \u0026#34;$orig_file\u0026#34; -out \u0026#34;$input_file\u0026#34; rm \u0026#34;$orig_file\u0026#34; In addition to signing the code, we timestamp the signature using the Comodo server. Timestamping makes sure the signature remains valid even after the certificate expires or is invalidated.\nWe can use osslsigncode to verify the signature:\n#!/usr/bin/env bash input_file=$1 osslsigncode verify -CAfile ./certs/ca.crt \u0026#34;$input_file\u0026#34; Distributing and manually verifying the signed code After signing the code, we can distribute the signed executable to users. Users can manually verify the signature by right-clicking the executable, selecting \u0026ldquo;Properties,\u0026rdquo; and navigating to the \u0026ldquo;Digital Signatures\u0026rdquo; tab. The user can then view the certificate details and verify that the signature is valid.\nHowever, since we are using the self-signed certificate, users will see a warning that the certificate is not trusted. Our self-signed certificate is not trusted because the certificate authority is not part of the Windows trusted root certificate store.\nCertificate in code signature cannot be verified We can add the certificate authority to the Windows trusted root certificate store with the following Powershell command:\nImport-Certificate -FilePath \u0026#34;certs\\ca.crt\u0026#34; -CertStoreLocation Cert:\\LocalMachine\\Root After adding the certificate authority to the trusted root certificate store, users will see that the certificate is trusted and the signature is valid.\nCertificate in code signature is be verified Code signing using a certificate from a public CA To sign public applications, we must obtain a code signing certificate from a trusted CA. The latest industry standards require private keys for code signing certificates to be stored in hardware security modules (HSMs) to prevent unauthorized access. This security requirement means certificates for code signing in CI/CD pipelines must use a cloud HSM vendor or a private pipeline runner with an HSM.\nIn a future article, we will explore signing a Windows application using a cloud HSM vendor.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/code-sign-windows\nFurther reading Recently, we explained how to create an EXE installer.\nWe also discussed connecting your local machine to remote Active Directory and covered how to test a Windows NDES SCEP server.\nCode signing a Windows application video ","date":"2024-03-27T00:00:00Z","image":"https://victoronsoftware.com/posts/code-signing-windows/digital-signature-ok_hu_f6994734badb4536.png","permalink":"https://victoronsoftware.com/posts/code-signing-windows/","title":"Code signing a Windows application"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain mTLS Go client mTLS Go client with custom certificate signer mTLS Go client using macOS keychain mTLS with Windows certificate store Why use the Windows certificate store? Keeping the mTLS client private key on the filesystem is insecure and not recommended. In the mTLS Go client using macOS keychain, we demonstrated achieving greater mTLS security with macOS keychain. In this article, we reach a similar level of protection with the Windows certificate store.\nThe Windows certificate store is a secure location where certificates and keys can be stored. Many applications, such as Edge and Powershell, use it. The Windows certificate store is an excellent place to store mTLS client certificates and keys.\nBuilding a custom tls.Certificate for the Windows certificate store This work builds on the mTLS Go client with custom certificate signer article. We will use the CustomSigner from that article to build a custom tls.Certificate that uses the Windows certificate store.\nHowever, before the application uses the Public and Sign methods of the CustomSigner, we must retrieve the client certificate using Windows APIs.\nRetrieving mTLS client certificate from Windows certificate store using Go We will use the golang.org/x/sys/windows package to access the Windows APIs. We use the windows package to call the CertOpenStore, CertFindCertificateInStore, and CryptAcquireCertificatePrivateKey functions from the crypt32 DLL (dynamic link library).\nFirst, we open the MY store, which is the personal store for the current user. This store contains our client mTLS certificate.\n// Open the certificate store storePtr, err := windows.UTF16PtrFromString(windowsStoreName) if err != nil { return nil, err } store, err := windows.CertOpenStore( windows.CERT_STORE_PROV_SYSTEM, 0, uintptr(0), windows.CERT_SYSTEM_STORE_CURRENT_USER, uintptr(unsafe.Pointer(storePtr)), ) if err != nil { return nil, err } Next, we find the certificate by the common name.\n// Find the certificate var pPrevCertContext *windows.CertContext var certContext *windows.CertContext commonNamePtr, err := windows.UTF16PtrFromString(commonName) for { certContext, err = windows.CertFindCertificateInStore( store, windows.X509_ASN_ENCODING, 0, windows.CERT_FIND_SUBJECT_STR, unsafe.Pointer(commonNamePtr), pPrevCertContext, ) if err != nil { return nil, err } // We can extract the certificate chain and further filter the certificate // we want here. break } Converting the Windows certificate to a Go x509.Certificate After retrieving the certificate from the Windows certificate store, we convert it to a Go x509.Certificate.\n// Copy the certificate data so that we have our own copy outside the windows context encodedCert := unsafe.Slice(certContext.EncodedCert, certContext.Length) buf := bytes.Clone(encodedCert) foundCert, err := x509.ParseCertificate(buf) if err != nil { return nil, err } Building the custom tls.Certificate Finally, we put together the custom tls.Certificate using the x509.Certificate. We hold on to the certContext pointer to get the private key later.\ncustomSigner := \u0026amp;CustomSigner{ store: store, windowsCertContext: certContext, } customSigner.x509Cert = foundCert certificate := tls.Certificate{ Certificate: [][]byte{foundCert.Raw}, PrivateKey: customSigner, SupportedSignatureAlgorithms: []tls.SignatureScheme{supportedAlgorithm}, } Our example only supports the tls.PSSWithSHA256 signature algorithm to keep the code simple.\nSigning the mTLS digest with the Windows certificate store As discussed in the previous mTLS Go client with custom certificate signer article, we must sign the CertificateVerify message during the TLS handshake. We will use the CustomSigner to sign the digest, which implements the crypto.Signer interface as defined in the Go standard library\u0026rsquo;s crypto package.\n// CustomSigner is a crypto.Signer that uses the client certificate and key to sign type CustomSigner struct { store windows.Handle windowsCertContext *windows.CertContext x509Cert *x509.Certificate } func (k *CustomSigner) Public() crypto.PublicKey { fmt.Printf(\u0026#34;crypto.Signer.Public\\n\u0026#34;) return k.x509Cert.PublicKey } func (k *CustomSigner) Sign(_ io.Reader, digest []byte, opts crypto.SignerOpts ) (signature []byte, err error) { ... Retrieve the private key reference from the Windows certificate store We retrieve the private key reference from the Windows certificate store using the CryptAcquireCertificatePrivateKey function.\n// Get private key var ( privateKey windows.Handle pdwKeySpec uint32 pfCallerFreeProvOrNCryptKey bool ) err = windows.CryptAcquireCertificatePrivateKey( k.windowsCertContext, windows.CRYPT_ACQUIRE_CACHE_FLAG|windows.CRYPT_ACQUIRE_SILENT_FLAG| windows.CRYPT_ACQUIRE_ONLY_NCRYPT_KEY_FLAG, nil, \u0026amp;privateKey, \u0026amp;pdwKeySpec, \u0026amp;pfCallerFreeProvOrNCryptKey, ) if err != nil { return nil, err } Signing the mTLS digest We will use the NCryptSignHash function from ncrypt.dll to sign the digest.\nvar ( nCrypt = windows.MustLoadDLL(\u0026#34;ncrypt.dll\u0026#34;) nCryptSignHash = nCrypt.MustFindProc(\u0026#34;NCryptSignHash\u0026#34;) ) But before we do that, we must create a BCRYPT_PSS_PADDING_INFO structure for our supported RSA-PSS algorithm.\nflags := nCryptSilentFlag | bCryptPadPss pPaddingInfo, err := getRsaPssPadding(opts) if err != nil { return nil, err } Where getRsaPssPadding is a helper function:\nfunc getRsaPssPadding(opts crypto.SignerOpts) (unsafe.Pointer, error) { pssOpts, ok := opts.(*rsa.PSSOptions) if !ok || pssOpts.Hash != crypto.SHA256 { return nil, fmt.Errorf(\u0026#34;unsupported hash function %s\u0026#34;, opts.HashFunc().String()) } if pssOpts.SaltLength != rsa.PSSSaltLengthEqualsHash { return nil, fmt.Errorf(\u0026#34;unsupported salt length %d\u0026#34;, pssOpts.SaltLength) } sha256, _ := windows.UTF16PtrFromString(\u0026#34;SHA256\u0026#34;) // Create BCRYPT_PSS_PADDING_INFO structure: // typedef struct _BCRYPT_PSS_PADDING_INFO { // LPCWSTR pszAlgId; // ULONG cbSalt; // } BCRYPT_PSS_PADDING_INFO; return unsafe.Pointer( \u0026amp;struct { pszAlgId *uint16 cbSalt uint32 }{ pszAlgId: sha256, cbSalt: uint32(pssOpts.HashFunc().Size()), }, ), nil } Finally, we sign the digest using the NCryptSignHash function.\n// Sign the digest // The first call to NCryptSignHash retrieves the size of the signature var size uint32 success, _, _ := nCryptSignHash.Call( uintptr(privateKey), uintptr(pPaddingInfo), uintptr(unsafe.Pointer(\u0026amp;digest[0])), uintptr(len(digest)), uintptr(0), uintptr(0), uintptr(unsafe.Pointer(\u0026amp;size)), uintptr(flags), ) if success != 0 { return nil, fmt.Errorf(\u0026#34;NCryptSignHash: failed to get signature length: %#x\u0026#34;, success) } // The second call to NCryptSignHash retrieves the signature signature = make([]byte, size) success, _, _ = nCryptSignHash.Call( uintptr(privateKey), uintptr(pPaddingInfo), uintptr(unsafe.Pointer(\u0026amp;digest[0])), uintptr(len(digest)), uintptr(unsafe.Pointer(\u0026amp;signature[0])), uintptr(size), uintptr(unsafe.Pointer(\u0026amp;size)), uintptr(flags), ) if success != 0 { return nil, fmt.Errorf(\u0026#34;NCryptSignHash: failed to generate signature: %#x\u0026#34;, success) } return signature, nil Putting it all together With the above code, we can create our new Go mTLS client that uses the Windows certificate store.\nfunc main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } client := http.Client{ Transport: \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ GetClientCertificate: signer.GetClientCertificate, MinVersion: tls.VersionTLS13, MaxVersion: tls.VersionTLS13, }, }, } // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } We limit the scope of this example to TLS 1.3\nSetting up the environment The next step is to use the Windows certificate store to store the client certificate and private key. We will use the certificates and keys scripts from the previous mTLS with Windows certificate store article. If you still need to generate the certificates and keys, please follow the instructions in that article.\nFinally, as in the mTLS with Windows certificate store article, we start two nginx servers:\nhttps://\u0026lt;your_host\u0026gt;:8888 for TLS https://\u0026lt;your_host\u0026gt;:8889 for mTLS Running the Go mTLS client using the Windows certificate store We can run our mTLS client without pointing to certificate/key files and retrieving everything from the Windows certificate store. Hitting the ordinary TLS server:\ngo run .\\client-signer.go --url https://myhost:8888/hello-world.txt Returns the expected:\nTLS Hello World! While hitting the mTLS server:\ngo run .\\client-signer.go --url https://myhost:8889/hello-world.txt Returns a more detailed message, including the print statements in our custom code:\nServer requested certificate Found certificate with common name testClientTLS crypto.Signer.Public crypto.Signer.Public crypto.Signer.Sign with key type *rsa.PublicKey, opts type *rsa.PSSOptions, hash SHA-256 mTLS Hello World! Example code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-go-windows\nmTLS Go client using Windows certificate store video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-03-20T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-go-client-windows-certificate-store/mtls-go-windows_hu_fe4de4fef5202383.png","permalink":"https://victoronsoftware.com/posts/mtls-go-client-windows-certificate-store/","title":"Mutual TLS (mTLS) Go client using Windows certificate store"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain mTLS Go client mTLS Go client with custom certificate signer mTLS Go client using macOS keychain Why use Windows certificate store? In our previous articles, we introduced mTLS and demonstrated how to use mTLS client certificates and keys. Keeping the mTLS client private key on the filesystem is insecure and not recommended. In the mTLS Go client using macOS keychain, we demonstrated achieving greater mTLS security with macOS keychain. In this article, we start exploring how to achieve the same level of protection with Windows certificate store.\nThe Windows certificate store is a secure location to store certificates and keys. Many applications, such as Edge and Powershell use it. The Windows certificate store is an excellent place to store mTLS client certificates and keys.\nThe Windows certificate stores have two types:\nUser certificate store: Certificates and keys are stored for the current user, local to a user account. Local machine certificate store: Certificates and keys are stored for all users on the computer. We will store our client mTLS certificate in the user certificate store and the other certificates in the local machine certificate store.\nGenerating mTLS certificates and keys We will use the following Powershell script to generate the mTLS certificates and keys. OpenSSL must be installed on your computer.\nNew-Item -ItemType Directory -Force certs # Private keys for CAs openssl genrsa -out certs/server-ca.key 2048 openssl genrsa -out certs/client-ca.key 2048 # Generate CA certificates openssl req -new -x509 -nodes -days 1000 -key certs/server-ca.key -out certs/server-ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testServerCA\u0026#34; openssl req -new -x509 -nodes -days 1000 -key certs/client-ca.key -out certs/client-ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testClientCA\u0026#34; # Generate a certificate signing request openssl req -newkey rsa:2048 -nodes -keyout certs/server.key -out certs/server.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testServerTLS\u0026#34; openssl req -newkey rsa:2048 -nodes -keyout certs/client.key -out certs/client.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testClientTLS\u0026#34; # Have the CA sign the certificate requests and output the certificates. openssl x509 -req -in certs/server.req -days 398 -CA certs/server-ca.crt -CAkey certs/server-ca.key -set_serial 01 -out certs/server.crt -extfile localhost.ext openssl x509 -req -in certs/client.req -days 398 -CA certs/client-ca.crt -CAkey certs/client-ca.key -set_serial 01 -out certs/client.crt # Create PFX file for importing to certificate store openssl pkcs12 -export -out certs\\client.pfx -inkey certs\\client.key -in certs\\client.crt -passout pass: # Clean up Remove-Item certs/server.req Remove-Item certs/client.req The maximum validity period for a TLS certificate is 398 days.\nThe localhost.ext file is used to specify the subject alternative name (SAN) for the server certificate. The localhost.ext file contains the following:\n[alt_names] DNS.1 = localhost DNS.2 = myhost We can access the server using either localhost or myhost names.\nThe above script generates the following files:\ncerts/server-ca.crt: Server CA certificate certs/server-ca.key: Server CA private key certs/client-ca.crt: Client CA certificate certs/client-ca.key: Client CA private key certs/server.crt: Server certificate certs/server.key: Server private key certs/client.crt: Client certificate certs/client.key: Client private key certs/client.pfx: Client certificate and private key in PFX format, needed for importing into the Windows certificate store Importing the client certificate and key into the Windows certificate store We will import the client certificate and key into the user certificate store using the following powershell script.\n# Import the server CA Import-Certificate -FilePath \u0026#34;certs\\server-ca.crt\u0026#34; -CertStoreLocation Cert:\\LocalMachine\\Root # Import the client CA so that client TLS certificates can be verified Import-Certificate -FilePath \u0026#34;certs\\client-ca.crt\u0026#34; -CertStoreLocation Cert:\\LocalMachine\\Root # Import the client TLS certificate and key Import-PfxCertificate -FilePath \u0026#34;certs\\client.pfx\u0026#34; -CertStoreLocation Cert:\\CurrentUser\\My The command result should be similar to the following:\nPSParentPath: Microsoft.PowerShell.Security\\Certificate::LocalMachine\\Root Thumbprint Subject ---------- ------- 0A31BF3C48A3D98A91A2F63B5BD286818311A707 CN=testServerCA, OU=Your Unit, O=Your Organization, L=Austin, S=Texas, C=US 7F7E5612F3A90B9EB246762358251F98911A9D1A CN=testClientCA, OU=Your Unit, O=Your Organization, L=Austin, S=Texas, C=US PSParentPath: Microsoft.PowerShell.Security\\Certificate::CurrentUser\\My Thumbprint Subject ---------- ------- E2EBB991E3849E32E934D8465FAE42787D34C9ED CN=testClientTLS, OU=Your Unit, O=Your Organization, L=Austin, S=Texas, C=US By default, the private key is marked as non-exportable. A user or an application cannot export the private key from the certificate store. They can only access the private key via Windows APIs. Using a non-exportable private key is the recommended security approach. You can use the -Exportable parameter if you need to export the private key.\nVerifying imported certificates and keys As an extra step, we can verify that the certificates and keys exist in the Windows certificate store. We can use the certlm Local Machine Certificate Manager GUI, certmgr User Certificate Manager GUI, or the Get-ChildItem powershell command.\nGet-ChildItem -Path Cert:\\LocalMachine\\Root | Where-Object{$_.Subject -match \u0026#39;testServerCA\u0026#39;} | Test-Certificate -Policy SSL Get-ChildItem -Path Cert:\\CurrentUser\\My | Where-Object{$_.Subject -match \u0026#39;testClientTLS\u0026#39;} Running the mTLS server We will use the same docker-compose.yml file from the mTLS Hello World article. The docker-compose.yml file starts two nginx servers:\nhttps://\u0026lt;your_host\u0026gt;:8888 for TLS https://\u0026lt;your_host\u0026gt;:8889 for mTLS We can run Docker on WSL (Windows Subsystem for Linux) or another machine. We will run it on a different machine, so we need to copy the certs directory to the machine running Docker. When running the server on a different machine, we must update the C:\\Windows\\System32\\drivers\\etc\\hosts file to point to the other machine.\n10.0.0.5 myhost Connecting to the TLS and mTLS servers with clients Because we added the server CA to the root certificate store, we can now access the TLS server without any additional flags:\nInvoke-WebRequest -Uri https://myhost:8888/hello-world.txt Result:\nStatusCode : 200 StatusDescription : OK Content : TLS Hello World! RawContent : HTTP/1.1 200 OK Connection: keep-alive Accept-Ranges: bytes Content-Length: 17 Content-Type: text/plain Date: Sun, 03 Mar 2024 17:28:29 GMT ETag: \u0026#34;65b29c19-11\u0026#34; Last-Modified: Thu, 25 Jan 2024 1... Forms : {} Headers : {[Connection, keep-alive], [Accept-Ranges, bytes], [Content-Length, 17], [Content-Type, text/plain]...} Images : {} InputFields : {} Links : {} ParsedHtml : System.__ComObject RawContentLength : 17 However, we cannot access the mTLS server directly.\nInvoke-WebRequest -Uri https://myhost:8889/hello-world.txt The client attempted the TLS handshake, but the server rejected the connection because the client did not provide a certificate. Result:\nInvoke-WebRequest : 400 Bad Request No required SSL certificate was sent nginx/1.25.3 At line:1 char:1 + Invoke-WebRequest -Uri https://myhost:8889/hello-world.txt + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : InvalidOperation: (System.Net.HttpWebRequest:HttpWebRequest) [Invoke-WebRequest], WebException + FullyQualifiedErrorId : WebCmdletWebResponseException,Microsoft.PowerShell.Commands.InvokeWebRequestCommand We can, however, provide the client certificate thumbprint to access the mTLS server. We saw the thumbprint of the client certificate earlier when we imported it into the Windows certificate store.\nInvoke-WebRequest -Uri https://myhost:8889/hello-world.txt -CertificateThumbprint E2EBB991E3849E32E934D8465FAE42787D34C9ED Result:\nStatusCode : 200 StatusDescription : OK Content : mTLS Hello World! RawContent : HTTP/1.1 200 OK Connection: keep-alive Accept-Ranges: bytes Content-Length: 18 Content-Type: text/plain Date: Sun, 03 Mar 2024 17:31:55 GMT ETag: \u0026#34;65b29c19-12\u0026#34; Last-Modified: Thu, 25 Jan 2024 1... Forms : {} Headers : {[Connection, keep-alive], [Accept-Ranges, bytes], [Content-Length, 18], [Content-Type, text/plain]...} Images : {} InputFields : {} Links : {} ParsedHtml : System.__ComObject RawContentLength : 18 Edge browser can access the mTLS server. We can verify this by opening the following URL:\nhttps://myhost:8889/hello-world.txt We see the following popup:\nEdge mTLS popup\nWe can click OK to connect to the mTLS server. Future connections will not show the popup and will automatically use the client certificate.\nNote: Here is a helpful link that may resolve issues trying to use mTLS client certificates on Windows 10: https://superuser.com/questions/1181163/unable-to-use-client-certificates-in-chrome-or-ie-on-windows-10\nExample code on Github The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-with-windows\nCreating our own Windows mTLS client In the following article, we will create a custom Windows mTLS client using the Windows certificate store.\nFurther reading Recently, we wrote an article on testing a Windows NDES SCEP server.\nmTLS with Windows certificate store video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-03-06T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-with-windows/mtls-edge_hu_e5f37290d112f7f1.png","permalink":"https://victoronsoftware.com/posts/mtls-with-windows/","title":"Mutual TLS (mTLS) with Windows certificate store"},{"content":"Introduction Any app aiming to reach an international audience must support Unicode. Emojis, which are based on Unicode, are everywhere. They are used in text messages, social media, and programming languages. Supporting Unicode and emojis in your app can be tricky. This article will cover common Unicode and emoji support issues and how to fix them.\nWhat is Unicode? Unicode is a standard for encoding, representing, and handling text. It is a character set that assigns a unique number to every character. The most common encoding for Unicode is UTF-8, which stands for Unicode Transformation Format 8-bit. UTF-8 is a variable-width encoding that can represent every character in the Unicode character set.\nUTF-8 format can take one to four bytes to represent a code point. Multiple code points can be combined to form a single character. For example, the emoji \u0026ldquo;👍\u0026rdquo; is represented by the code point U+1F44D. In UTF-8, it is represented by the bytes F0 9F 91 8D. The same emoji with skin tone \u0026ldquo;👍🏽\u0026rdquo; is represented by the code point U+1F44D U+1F3FD. In UTF-8, that emoji is represented by the bytes F0 9F 91 8D F0 9F 8F BD. Generally, emojis take up at least four bytes in UTF-8.\nUnicode equivalence Our first gotcha is unicode equivalence.\nUnicode equivalence is the concept that two different sequences of code points can represent the same character. For example, the character é can be represented by the code point U+00E9 or by the sequence of code points U+0065 U+0301. The first representation is the composed form, and the second is the decomposed form. Unicode equivalence is essential when comparing strings or searching for a string character.\nDatabases typically do not support Unicode equivalence out of the box. For example, given this table using MySQL 5.7:\nCREATE TABLE test ( id INT UNSIGNED NOT NULL AUTO_INCREMENT, name VARCHAR(255) NOT NULL, PRIMARY KEY (id)) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; INSERT INTO test (name) VALUES (\u0026#39;가\u0026#39;), (CONCAT(\u0026#39;ᄀ\u0026#39;, \u0026#39;ᅡ\u0026#39;)); SELECT * from test WHERE name = \u0026#39;가\u0026#39;; The query will return a single row, even though the Korean character 가 and character sequence ᄀ + ᅡ are equivalent. The incorrect result is because the utf8mb4_unicode_ci collation does not support Unicode equivalence. One way to fix this is to use the utf8mb4_0900_ai_ci collation, which supports Unicode equivalence. However, this requires updating the database to MySQL 8.0 or later, which may not be possible in some cases.\nEmoji equivalence Our second gotcha is emoji equivalence.\nSome databases may not support emoji equivalence out of the box. For example, given this table using MySQL 5.7:\nCREATE TABLE test ( id INT UNSIGNED NOT NULL AUTO_INCREMENT, name VARCHAR(255) NOT NULL, PRIMARY KEY (id)) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; INSERT INTO test (name) VALUES (\u0026#39;🔥\u0026#39;), (\u0026#39;🔥🔥\u0026#39;), (\u0026#39;👍\u0026#39;), (\u0026#39;👍🏽\u0026#39;); SELECT * from test WHERE name = \u0026#39;🔥\u0026#39;; The query will return:\n1,🔥 3,👍 And the following query:\nSELECT * from test WHERE name LIKE \u0026#39;%🔥%\u0026#39;; Will return:\n1,🔥 2,🔥🔥 The utf8mb4_unicode_ci collation does not support emoji equivalence, and the behavior of = differs from LIKE.\nOne way to fix the problem of emoji equivalence is to use a different collation during the = comparison. For example:\nSELECT * from test WHERE name COLLATE utf8mb4_bin = \u0026#39;🔥\u0026#39;; Will return the single correct result:\n1,🔥 However, this solution is not ideal because it requires the developer to remember to use the utf8mb4_bin collation for emoji equivalence. There is also a slight performance impact when using a different collation.\nCase-insensitive sorting Our third gotcha is sorting.\nTypically, app users want to see case-insensitive sorting of strings. For example, the strings \u0026ldquo;apple\u0026rdquo;, \u0026ldquo;Banana\u0026rdquo;, and \u0026ldquo;cherry\u0026rdquo; should be sorted as \u0026ldquo;apple\u0026rdquo;, \u0026ldquo;Banana\u0026rdquo;, and \u0026ldquo;cherry\u0026rdquo;. The utf8mb4_unicode_ci collation used above supports case-insensitive sorting. However, switching to another collation, such as utf8mb4_bin, to support emoji equivalence will break case-insensitive sorting. Hence, whatever solution you develop for full Unicode support should also support case-insensitive sorting.\nSolving our gotchas with normalization A partial solution to the above gotchas is to use normalization. Normalization is the process of transforming text into a standard form. Unicode defines four normalization forms: NFC, NFD, NFKC, and NFKD. The most common normalization form is NFC, which is the composed form. NFC is the standard form for most text processing.\nFor example, in the following Go code:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;golang.org/x/text/unicode/norm\u0026#34; \u0026#34;strconv\u0026#34; ) func main() { str1, _ := strconv.Unquote(`\u0026#34;\\uAC00\u0026#34;`) // 가 str2, _ := strconv.Unquote(`\u0026#34;\\u1100\\u1161\u0026#34;`) // ᄀ + ᅡ fmt.Println(str1) fmt.Println(str2) if str1 == str2 { fmt.Println(\u0026#34;raw equal\u0026#34;) } else { fmt.Println(\u0026#34;raw not equal\u0026#34;) } strNorm1 := norm.NFC.String(str1) strNorm2 := norm.NFC.String(str2) if strNorm1 == strNorm2 { fmt.Println(\u0026#34;normalized equal\u0026#34;) } else { fmt.Println(\u0026#34;normalized not equal\u0026#34;) } } The two strings are not equal in their raw form but equal after normalization. Normalizing before inserting, updating, and searching in the database can solve the Unicode equivalence issue while allowing the user to keep the case-insensitive sorting.\nTo solve emoji equivalence, we can use the utf8mb4_bin collation for the = comparison. However, if our column is indexed, we may need to use the utf8mb4_bin collation for the index. We cannot have a different collation for the column and the index, but we could use a second generated column with the utf8mb4_bin collation and index that column.\nConclusion Unicode and emoji support is essential for any app aiming to reach an international audience. Unicode equivalence, emoji equivalence, and case-insensitive sorting are common issues with Unicode and emoji support. Normalization can solve the Unicode equivalence issue while allowing the user to keep the case-insensitive sorting. Using the utf8mb4_bin collation for the = comparison can solve the emoji equivalence issue.\nFully supporting Unicode and emojis in your app video Other articles related to MySQL Optimize MySQL query performance: INSERT with subqueries MySQL deadlock on UPDATE/INSERT upsert pattern SQL prepared statements are broken when scaling applications Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-02-29T00:00:00Z","image":"https://victoronsoftware.com/posts/unicode-and-emoji-gotchas/unicode-emoji_hu_1a354683464348c.png","permalink":"https://victoronsoftware.com/posts/unicode-and-emoji-gotchas/","title":"Fully supporting Unicode and emojis in your app"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain mTLS Go client mTLS Go client with custom certificate signer Why use macOS keychain? In the mTLS Go client article, we built a simple Go client that uses mTLS. Our client used Go standard library methods and loaded the client certificate and private key from the filesystem. However, keeping the private key on the filesystem is insecure and not recommended. We aim to build an mTLS client fully integrated with the operating system\u0026rsquo;s keystore.\nThe macOS keychain is a secure storage system for passwords and other confidential information. It is used by many Apple applications, such as Safari, Mail, and iCloud, to store the user\u0026rsquo;s passwords and additional sensitive information.\nBuilding a custom tls.Certificate for macOS keychain This work builds on the mTLS Go client with custom certificate signer article. We will use the CustomSigner from that article to build a custom tls.Certificate that uses the macOS keychain.\nHowever, before the application uses the Public and Sign methods of the CustomSigner, we need to retrieve the certificate from the keychain using Apple\u0026rsquo;s API.\nRetrieving certificate from macOS keychain with CGO We will use CGO to call the macOS keychain API to retrieve the client certificate. To set up CGO, we include the following code above our imports:\n/* #cgo LDFLAGS: -framework CoreFoundation -framework Security #include \u0026lt;CoreFoundation/CoreFoundation.h\u0026gt; #include \u0026lt;Security/Security.h\u0026gt; */ import \u0026#34;C\u0026#34; To find the identities from the keychain, we use SecItemCopyMatching. An identity is a certificate and its associated private key.\nidentitySearch := C.CFDictionaryCreateMutable( C.kCFAllocatorDefault, maxCertificatesNum, \u0026amp;C.kCFTypeDictionaryKeyCallBacks, \u0026amp;C.kCFTypeDictionaryValueCallBacks, ) defer C.CFRelease(C.CFTypeRef(unsafe.Pointer(identitySearch))) const commonName = \u0026#34;testClientTLS\u0026#34; var commonNameCFString = stringToCFString(commonName) defer C.CFRelease(C.CFTypeRef(commonNameCFString)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecClass), unsafe.Pointer(C.kSecClassIdentity)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecAttrCanSign), unsafe.Pointer(C.kCFBooleanTrue)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecMatchSubjectWholeString), unsafe.Pointer(commonNameCFString)) // To filter by issuers, we must provide a CFDataRef array of DER-encoded ASN.1 items. // C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecMatchIssuers), unsafe.Pointer(issuerCFArray)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecReturnRef), unsafe.Pointer(C.kCFBooleanTrue)) C.CFDictionaryAddValue(identitySearch, unsafe.Pointer(C.kSecMatchLimit), unsafe.Pointer(C.kSecMatchLimitAll)) var identityMatches C.CFTypeRef if status := C.SecItemCopyMatching(C.CFDictionaryRef(identitySearch), \u0026amp;identityMatches); status != C.errSecSuccess { return nil, fmt.Errorf(\u0026#34;failed to find client certificate: %v\u0026#34;, status) } defer C.CFRelease(identityMatches) In our example, we find the identities by a common name, which we hardcode for demonstration purposes. We can filter by the certificate issuer, as shown in the commented-out code. Filtering by issuer requires an array of DER-encoded ASN.1 items, which can be created from the tls.CertificateRequestInfo object. Another approach to finding the proper certificate is to retrieve all the keychain certificates and filter them in Go code.\nConverting the Apple identity to a Go x509.Certificate After we retrieve the array of identities from the keychain, we convert them to Go x509.Certificate objects and pick the first one that is not expired.\nvar foundCert *x509.Certificate var foundIdentity C.SecIdentityRef identityMatchesArrayRef := C.CFArrayRef(identityMatches) numIdentities := int(C.CFArrayGetCount(identityMatchesArrayRef)) fmt.Printf(\u0026#34;Found %d identities\\n\u0026#34;, numIdentities) for i := 0; i \u0026lt; numIdentities; i++ { identityMatch := C.CFArrayGetValueAtIndex(identityMatchesArrayRef, C.CFIndex(i)) x509Cert, err := identityRefToCert(C.SecIdentityRef(identityMatch)) if err != nil { continue } // Make sure certificate is not expired if x509Cert.NotAfter.After(time.Now()) { foundCert = x509Cert foundIdentity = C.SecIdentityRef(identityMatch) fmt.Printf(\u0026#34;Found certificate from issuer %s with public key type %T\\n\u0026#34;, x509Cert.Issuer.String(), x509Cert.PublicKey) break } } The identityRefToCert function converts the SecIdentityRef to a Go x509.Certificate object. It exports the certificate to PEM format using SecItemExport and then parses the PEM to get the x509.Certificate object.\nfunc identityRefToCert(identityRef C.SecIdentityRef) (*x509.Certificate, error) { // Convert the identity to a certificate var certificateRef C.SecCertificateRef if status := C.SecIdentityCopyCertificate(identityRef, \u0026amp;certificateRef); status != 0 { return nil, fmt.Errorf(\u0026#34;failed to get certificate from identity: %v\u0026#34;, status) } defer C.CFRelease(C.CFTypeRef(certificateRef)) // Export the certificate to PEM // SecItemExport: https://developer.apple.com/documentation/security/1394828-secitemexport var pemDataRef C.CFDataRef if status := C.SecItemExport( C.CFTypeRef(certificateRef), C.kSecFormatPEMSequence, C.kSecItemPemArmour, nil, \u0026amp;pemDataRef, ); status != 0 { return nil, fmt.Errorf(\u0026#34;failed to export certificate to PEM: %v\u0026#34;, status) } defer C.CFRelease(C.CFTypeRef(pemDataRef)) certPEM := C.GoBytes(unsafe.Pointer(C.CFDataGetBytePtr(pemDataRef)), C.int(C.CFDataGetLength(pemDataRef))) var x509Cert *x509.Certificate for block, rest := pem.Decode(certPEM); block != nil; block, rest = pem.Decode(rest) { if block.Type == \u0026#34;CERTIFICATE\u0026#34; { var err error x509Cert, err = x509.ParseCertificate(block.Bytes) if err != nil { return nil, fmt.Errorf(\u0026#34;error parsing client certificate: %v\u0026#34;, err) } } } return x509Cert, nil } Retrieve the private key reference from the keychain At this point, we also retrieve the private key reference from the keychain. We will use the private key reference to sign the CertificateVerify message during the TLS handshake. The reference does not contain the private key. When importing private keys to the keychain, they should be marked as non-exportable so that no one can retrieve the private key cleartext from the keychain.\nvar privateKey C.SecKeyRef if status := C.SecIdentityCopyPrivateKey(C.SecIdentityRef(foundIdentity), \u0026amp;privateKey); status != 0 { return nil, fmt.Errorf(\u0026#34;failed to copy private key ref from identity: %v\u0026#34;, status) } Building the custom tls.Certificate Finally, we put together the custom tls.Certificate using the x509.Certificate and the private key reference.\ncustomSigner := \u0026amp;CustomSigner{ x509Cert: foundCert, privateKey: privateKey, } certificate := tls.Certificate{ Certificate: [][]byte{foundCert.Raw}, PrivateKey: customSigner, SupportedSignatureAlgorithms: []tls.SignatureScheme{supportedAlgorithm}, } Our example only supports the tls.PSSWithSHA256 signature algorithm to keep the code simple. Adding additional algorithm support is easy since it only requires passing the right parameter to the SecKeyCreateSignature function, which we will review next.\nSigning the mTLS digest with Apple\u0026rsquo;s keychain As discussed in the previous mTLS Go client with custom certificate signer article, we need to sign the CertificateVerify message during the TLS handshake. We will use the CustomSigner to sign the digest, which implements the crypto.Signer interface as defined in the Go standard library\u0026rsquo;s crypto package.\ntype CustomSigner struct { x509Cert *x509.Certificate privateKey C.SecKeyRef } func (k *CustomSigner) Public() crypto.PublicKey { fmt.Printf(\u0026#34;crypto.Signer.Public\\n\u0026#34;) return k.x509Cert.PublicKey } func (k *CustomSigner) Sign(_ io.Reader, digest []byte, opts crypto.SignerOpts) ( signature []byte, err error) { fmt.Printf(\u0026#34;crypto.Signer.Sign with key type %T, opts type %T, hash %s\\n\u0026#34;, k.Public(), opts, opts.HashFunc().String()) // Convert the digest to a CFDataRef digestCFData := C.CFDataCreate(C.kCFAllocatorDefault, (*C.UInt8)(unsafe.Pointer(\u0026amp;digest[0])), C.CFIndex(len(digest))) defer C.CFRelease(C.CFTypeRef(digestCFData)) // SecKeyAlgorithm: https://developer.apple.com/documentation/security/seckeyalgorithm // SecKeyCreateSignature: https://developer.apple.com/documentation/security/1643916-seckeycreatesignature var cfErrorRef C.CFErrorRef signCFData := C.SecKeyCreateSignature( k.privateKey, C.kSecKeyAlgorithmRSASignatureDigestPSSSHA256, C.CFDataRef(digestCFData), \u0026amp;cfErrorRef, ) if cfErrorRef != 0 { return nil, fmt.Errorf(\u0026#34;failed to sign data: %v\u0026#34;, cfErrorRef) } defer C.CFRelease(C.CFTypeRef(signCFData)) // Convert CFDataRef to Go byte slice return C.GoBytes(unsafe.Pointer(C.CFDataGetBytePtr(signCFData)), C.int(C.CFDataGetLength(signCFData))), nil } We use the SecKeyCreateSignature function to sign the digest. The function takes the private key reference, the algorithm, the digest, and a pointer to a CFErrorRef. The function returns a CFDataRef, which we convert to a Go byte slice. Additional algorithms can be supported by passing the proper parameter to the SecKeyCreateSignature function.\nPutting it all together With the above code, we can create our new Go mTLS client that uses the macOS keychain.\nfunc main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } client := http.Client{ Transport: \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ GetClientCertificate: signer.GetClientCertificate, MinVersion: tls.VersionTLS13, MaxVersion: tls.VersionTLS13, }, }, } // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } We limit the scope of this example to TLS 1.3\nBuild the mTLS client With go build client-signer.go, we generate the client-signer executable.\nSetting up the environment The next step is to use the macOS keychain to store the client certificate and private key. We will use the same certificates and keys script from the mTLS with macOS keychain article. If you still need to generate the certificates and keys, please follow the instructions in that article.\nWe must also import the generated certificates and keys into the macOS keychain.\n# Import the server CA security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain certs/server-ca.crt # Import the client CA so that client TLS certificates can be verified security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain certs/client-ca.crt # Import the client TLS certificate and key security import certs/client.crt -k /Library/Keychains/System.keychain security import certs/client.key -k /Library/Keychains/System.keychain -x -T $PWD/client-signer -T /usr/bin/curl -T /Applications/Safari.app -T \u0026#39;/Applications/Google Chrome.app\u0026#39; We specify our application $PWD/client-signer as one of the trusted applications that can access the private key. If we do not select the trusted application, we will get a security pop-up whenever our app tries to access the private key.\nFinally, as in the mTLS Hello World article, we will use docker compose up to start two nginx servers:\nhttps://localhost:8888 for TLS https://localhost:8889 for mTLS Running the Go mTLS client using the macOS keychain We can now run our mTLS client without pointing to certificate and key files. Hitting the ordinary TLS server:\n./client-signer --url https://localhost:8888/hello-world.txt Returns the expected:\nTLS Hello World! While hitting the mTLS server:\n./client-signer --url https://localhost:8889/hello-world.txt Returns a more detailed message, including the print statements in our custom code:\nServer requested certificate Found 1 identities Found certificate from issuer CN=testClientCA,OU=Your Unit,O=Your Organization,L=Austin,ST=Texas,C=US with public key type *rsa.PublicKey crypto.Signer.Public crypto.Signer.Public crypto.Signer.Sign with key type *rsa.PublicKey, opts type *rsa.PSSOptions, hash SHA-256 mTLS Hello World! Using certificate and key from the Windows certificate store The following article will explore using the Windows certificate store to hold the mTLS client certificate and private key.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-go-apple-keychain\nmTLS Go client using macOS keychain video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-02-22T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-go-client-using-apple-keychain/mtls-go-apple-keychain_hu_f384428683025.png","permalink":"https://victoronsoftware.com/posts/mtls-go-client-using-apple-keychain/","title":"Mutual TLS (mTLS) Go client using macOS keychain"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain mTLS Go client Why a custom certificate signer? In the mTLS Go client article, we built a simple Go client that uses mTLS. Our client used Go standard library methods and loaded the client certificate and private key from the filesystem. However, keeping the private key on the filesystem is insecure and not recommended. We aim to build an mTLS client fully integrated with the operating system\u0026rsquo;s keystore.\nThe first step toward that goal is to extract the functionality of the mTLS handshake that requires the private key. Luckily, the client\u0026rsquo;s private key is only needed to sign the CertificateVerify message. The CertificateVerify message is the last in the mTLS handshake. It proves to the server that the client has the private key associated with the client certificate.\nFrom Wikipedia entry on TLS:\nThe client sends a CertificateVerify message, which is a signature over the previous handshake messages using the client\u0026rsquo;s certificate\u0026rsquo;s private key. This signature can be verified by using the client\u0026rsquo;s certificate\u0026rsquo;s public key. This lets the server know that the client has access to the private key of the certificate and thus owns the certificate.\nSetting up the environment We will use the same certificates and keys script from the mTLS with macOS keychain article. If you still need to generate the certificates and keys, please follow the instructions in that article.\nIn addition, we will import the generated certificates and keys into the macOS keychain. (In a future article, we will use the Windows Certificate Store instead.)\nFinally, as in the mTLS Hello World article, we will use docker compose up to start two nginx servers:\nhttps://localhost:8888 for TLS https://localhost:8889 for mTLS Building our crypto.Signer We will build a custom crypto.Signer that signs the CertificateVerify message. The crypto.Signer interface is defined in the Go standard library\u0026rsquo;s crypto package. It is used to sign messages with a private key.\n// CustomSigner is a crypto.Signer that uses the client certificate and key to sign type CustomSigner struct { x509Cert *x509.Certificate clientCertPath string clientKeyPath string } func (k *CustomSigner) Public() crypto.PublicKey { fmt.Printf(\u0026#34;crypto.Signer.Public\\n\u0026#34;) return k.x509Cert.PublicKey } func (k *CustomSigner) Sign(rand io.Reader, digest []byte, opts crypto.SignerOpts) ( signature []byte, err error) { fmt.Printf(\u0026#34;crypto.Signer.Sign\\n\u0026#34;) tlsCert, err := tls.LoadX509KeyPair(k.clientCertPath, k.clientKeyPath) if err != nil { log.Fatalf(\u0026#34;error loading client certificate: %v\u0026#34;, err) } fmt.Printf(\u0026#34;Sign using %T\\n\u0026#34;, tlsCert.PrivateKey) return tlsCert.PrivateKey.(crypto.Signer).Sign(rand, digest, opts) } Although we still use the filesystem to load the client certificate and private key, we now use the crypto.Signer interface to sign the CertificateVerify message. In the future, we will replace this code by calls to the operating system\u0026rsquo;s keystore. The vital thing to note is that we only load the private key when we need to sign the digest and do not load the key during the client configuration.\nGetting the client certificate Besides building a custom crypto.Signer, we will implement a custom GetClientCertificate function. This function will be called during the TLS handshake when the server requests a certificate from the client. The function will load the client certificate and create a CustomSigner instance. It will not load the private key at this time. Once again, the client certificate is only loaded when needed and not during the client\u0026rsquo;s configuration.\nWe set Certificate: [][]byte{cert.Raw}, because the Go implementation of the TLS handshake requires the client certificate here to validate it against the server\u0026rsquo;s CA.\nfunc GetClientCertificate(clientCertPath string, clientKeyPath string) (*tls.Certificate, error) { fmt.Printf(\u0026#34;Server requested certificate\\n\u0026#34;) if clientCertPath == \u0026#34;\u0026#34; || clientKeyPath == \u0026#34;\u0026#34; { return nil, errors.New(\u0026#34;client certificate and key are required\u0026#34;) } clientBytes, err := os.ReadFile(clientCertPath) if err != nil { return nil, fmt.Errorf(\u0026#34;error reading client certificate: %w\u0026#34;, err) } var cert *x509.Certificate for block, rest := pem.Decode(clientBytes); block != nil; block, rest = pem.Decode(rest) { if block.Type == \u0026#34;CERTIFICATE\u0026#34; { cert, err = x509.ParseCertificate(block.Bytes) if err != nil { return nil, fmt.Errorf(\u0026#34;error parsing client certificate: %v\u0026#34;, err) } } } certificate := tls.Certificate{ Certificate: [][]byte{cert.Raw}, PrivateKey: \u0026amp;CustomSigner{ x509Cert: cert, clientCertPath: clientCertPath, clientKeyPath: clientKeyPath, }, } return \u0026amp;certificate, nil } Putting it all together With the above customizations, we create our new Go mTLS client:\npackage main import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/getvictor/mtls/signer\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) clientCert := flag.String(\u0026#34;cert\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Client certificate file\u0026#34;) clientKey := flag.String(\u0026#34;key\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Client key file\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } client := http.Client{ Transport: \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ GetClientCertificate: func(info *tls.CertificateRequestInfo) ( *tls.Certificate, error) { return signer.GetClientCertificate(*clientCert, *clientKey) }, }, }, } // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } Trying to hit the mTLS server with:\ngo run client-signer.go --url https://localhost:8889/hello-world.txt --cert certs/client.crt --key certs/client.key Returns the expected result:\nmTLS Hello World! Using certificate and key from the macOS keychain In the following article, we will use the macOS keychain to load the client certificate and generate the CertificateVerify message without extracting the private key.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-go-custom-signer\nmTLS Go client with custom certificate signer video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-02-14T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-go-custom-signer/signer_hu_c0014d6dcb3e7237.png","permalink":"https://victoronsoftware.com/posts/mtls-go-custom-signer/","title":"Mutual TLS (mTLS) Go client with custom certificate signer"},{"content":"This article is part of a series on mTLS. Check out the previous articles:\nmTLS Hello World mTLS with macOS keychain What is Go? Go is a statically typed, compiled programming language designed at Google. It is known for its simplicity, efficiency, and ease of use. Go is often used for building web servers, APIs, and command-line tools. We will use Go to make a client that uses mTLS.\nSetting up the environment We will use the same certificates and keys script from the mTLS with macOS keychain article. If you still need to generate the certificates and keys, please follow the instructions in that article.\nIn addition, we will import the generated certificates and keys into the macOS keychain. (In a future article, we will use the Windows Certificate Store instead.) Keeping private keys on the filesystem is insecure and not recommended. We aim to build an mTLS client fully integrated with the operating system\u0026rsquo;s keystore.\nFinally, as in the mTLS Hello World article, we will use docker compose up to start two nginx servers:\nhttps://localhost:8888 for TLS https://localhost:8889 for mTLS Building the TLS Go client Below is a simple Go HTTP client.\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } client := http.Client{} // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } Trying the ordinary TLS server with:\ngo run client.go --url https://localhost:8888/hello-world.txt Gives the expected result:\nTLS Hello World! The Go client is integrated with the system keystore out of the box.\nHowever, when trying the mTLS server with the following:\ngo run client.go --url https://localhost:8889/hello-world.txt We get the error:\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;400 No required SSL certificate was sent\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;400 Bad Request\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;center\u0026gt;No required SSL certificate was sent\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx/1.25.3\u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; The Go libraries are not integrated with the system keystore for using the mTLS client certificate and key.\nModifying the Go client for mTLS We will use the crypto/tls package to build the mTLS client.\npackage main import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { urlPath := flag.String(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;URL to make request to\u0026#34;) clientCert := flag.String(\u0026#34;cert\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Client certificate file\u0026#34;) clientKey := flag.String(\u0026#34;key\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Client key file\u0026#34;) flag.Parse() if *urlPath == \u0026#34;\u0026#34; { log.Fatalf(\u0026#34;URL to make request to is required\u0026#34;) } var certificate tls.Certificate if *clientCert != \u0026#34;\u0026#34; \u0026amp;\u0026amp; *clientKey != \u0026#34;\u0026#34; { var err error certificate, err = tls.LoadX509KeyPair(*clientCert, *clientKey) if err != nil { log.Fatalf(\u0026#34;error loading client certificate: %v\u0026#34;, err) } } client := http.Client{ Transport: \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ Certificates: []tls.Certificate{certificate}, }, }, } // Make a GET request to the URL rsp, err := client.Get(*urlPath) if err != nil { log.Fatalf(\u0026#34;error making get request: %v\u0026#34;, err) } defer func() { _ = rsp.Body.Close() }() // Read the response body rspBytes, err := io.ReadAll(rsp.Body) if err != nil { log.Fatalf(\u0026#34;error reading response: %v\u0026#34;, err) } // Print the response body fmt.Printf(\u0026#34;%s\\n\u0026#34;, string(rspBytes)) } Now, trying the mTLS server with:\ngo run client-mtls.go --url https://localhost:8889/hello-world.txt --cert certs/client.crt --key certs/client.key Returns the expected result:\nmTLS Hello World! However, we pass the client certificate and key as command-line arguments. In a real-world scenario, we want to use the system keystore to manage the client certificate and key.\nUsing a custom signer for the mTLS client certificate The following article will cover creating a custom Go signer for the mTLS client certificate. This work will pave the way for us to use the system keystore to manage the client certificate and key.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-with-go\nmTLS Go client video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-02-07T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-go-client/go-client_hu_5110c87ffac65136.png","permalink":"https://victoronsoftware.com/posts/mtls-go-client/","title":"Mutual TLS (mTLS) Go client"},{"content":"This article is part of a series on mTLS. Check out the previous article: mTLS Hello World.\nSecuring mTLS certificates and keys In the mTLS Hello World article, we generated mTLS certificates and keys for the client and the server. We also created two certificate authorities (CAs) and signed the client and server certificates with their respective CAs. We ended up with the following files:\nserver CA: certs/server-ca.crt server CA private key: certs/server-ca.key TLS certificate for localhost server: certs/server.crt server TLS certificate private key: certs/server.key client CA: certs/client-ca.crt client CA private key: certs/client-ca.key TLS certificate for client: certs/client.crt client TLS certificate private key: certs/client.key In a real-world scenario, we would need to secure these files. The server CA private key and the client CA private key are the most important files to secure. If an attacker gets access to these files, they can create new certificates and impersonate the server or the client. These two files should be secured in a dedicated secure storage.\nThe server will need access to the client CA, the server TLS certificate, and the server TLS certificate private key. The server TLS certificate private key is the most important to secure out of these three files.\nThe client will need access to the server CA, the client TLS certificate, and the client TLS certificate private key. We can use the macOS keychain to secure these files. In a future article, we will show how to secure these on Windows with certificate stores.\nApple\u0026rsquo;s macOS keychain As I\u0026rsquo;ve written in inspecting keychain files on macOS, keychains are the macOS\u0026rsquo;s method to track and protect secure information such as passwords, private keys, and certificates.\nThe system keychain is located at /Library/Keychains/System.keychain. It contains the root certificates and other certificates. The login keychain is located at /Users/\u0026lt;username\u0026gt;/Library/Keychains/login.keychain-db. It contains the user\u0026rsquo;s certificates and private keys. In this example, we will use the system keychain, which all users on the system can access.\nGenerating mTLS certificates and keys We will use the following script to generate the mTLS certificates and keys. It resembles the script from the mTLS Hello World article.\n#!/bin/bash # This script generates certificates and keys needed for mTLS. mkdir -p certs # Private keys for CAs openssl genrsa -out certs/server-ca.key 2048 openssl genrsa -out certs/client-ca.key 2048 # Generate CA certificates openssl req -new -x509 -nodes -days 1000 -key certs/server-ca.key -out certs/server-ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testServerCA\u0026#34; openssl req -new -x509 -nodes -days 1000 -key certs/client-ca.key -out certs/client-ca.crt -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testClientCA\u0026#34; # Generate a certificate signing request openssl req -newkey rsa:2048 -nodes -keyout certs/server.key -out certs/server.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testServerTLS\u0026#34; openssl req -newkey rsa:2048 -nodes -keyout certs/client.key -out certs/client.req -subj \u0026#34;/C=US/ST=Texas/L=Austin/O=Your Organization/OU=Your Unit/CN=testClientTLS\u0026#34; # Have the CA sign the certificate requests and output the certificates. openssl x509 -req -in certs/server.req -days 398 -CA certs/server-ca.crt -CAkey certs/server-ca.key -set_serial 01 -out certs/server.crt -extfile localhost.ext openssl x509 -req -in certs/client.req -days 398 -CA certs/client-ca.crt -CAkey certs/client-ca.key -set_serial 01 -out certs/client.crt # Clean up rm certs/server.req rm certs/client.req The maximum validity period for a TLS certificate is 398 days. Apple will reject certificates with a more extended validity period.\nImporting client mTLS certificates and keys into the macOS keychain We will import the client mTLS certificates and keys into the macOS keychain using the following script. The script uses the security command line tool. Accessing the system keychain must be run as root (sudo).\n#!/bin/bash # This script imports mTLS certificates and keys into the Apple Keychain. # Import the server CA security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain certs/server-ca.crt # Import the client CA so that client TLS certificates can be verified security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain certs/client-ca.crt # Import the client TLS certificate and key security import certs/client.crt -k /Library/Keychains/System.keychain security import certs/client.key -k /Library/Keychains/System.keychain -x -T /usr/bin/curl -T /Applications/Safari.app -T \u0026#39;/Applications/Google Chrome.app\u0026#39; The -x option marks the imported key as non-extractable. No application or user can view the private key once it is imported. The private key can only be used indirectly via Apple\u0026rsquo;s APIs.\nThe -T option specifies the applications that can access the key. Additional applications may be added later to the access control list.\nVerifying imported certificates and keys As an extra step, we can verify the client and server certificates before using them in an application.\nWe can verify the server certificate by running the following command:\nsecurity verify-cert -c certs/server.crt -p ssl -s localhost -k /Library/Keychains/System.keychain The output should include:\n...certificate verification successful. The Apple keychain automatically combines the certificate and the private key into an identity. We can verify the client identity by running the following command:\nsecurity find-identity -p ssl-client /Library/Keychains/System.keychain The list of identities should include:\nPolicy: SSL (client) Matching identities 1) B307B90CCD374080E74F1B15AF602B35A75D8401 \u0026#34;testClientTLS\u0026#34; 1 identities found Valid identities only 1) B307B90CCD374080E74F1B15AF602B35A75D8401 \u0026#34;testClientTLS\u0026#34; 1 valid identities found macOS can validate the identity because we also imported the client CA into the system keychain.\nRunning the mTLS server As in the mTLS Hello World article, we will use docker compose up to start two nginx servers:\nhttps://localhost:8888 for TLS https://localhost:8889 for mTLS Connecting to the TLS and mTLS servers with clients Because the server CA was added to the system keychain, curl can now access the TLS server without any additional flags:\ncurl https://localhost:8888/hello-world.txt However, the built-in curl client cannot access the mTLS server. We use the -v option for additional information:\ncurl -v https://localhost:8889/hello-world.txt The output:\n* Trying [::1]:8889... * Connected to localhost (::1) port 8889 * ALPN: curl offers h2,http/1.1 * (304) (OUT), TLS handshake, Client hello (1): * CAfile: /etc/ssl/cert.pem * CApath: none * (304) (IN), TLS handshake, Server hello (2): * (304) (IN), TLS handshake, Unknown (8): * (304) (IN), TLS handshake, Request CERT (13): * (304) (IN), TLS handshake, Certificate (11): * (304) (IN), TLS handshake, CERT verify (15): * (304) (IN), TLS handshake, Finished (20): * (304) (OUT), TLS handshake, Certificate (11): * (304) (OUT), TLS handshake, Finished (20): * SSL connection using TLSv1.3 / AEAD-CHACHA20-POLY1305-SHA256 * ALPN: server accepted http/1.1 * Server certificate: * subject: C=US; ST=Texas; L=Austin; O=Your Organization; OU=Your Unit; CN=testServerTLS * start date: Jan 28 17:08:10 2024 GMT * expire date: Mar 1 17:08:10 2025 GMT * subjectAltName: host \u0026#34;localhost\u0026#34; matched cert\u0026#39;s \u0026#34;localhost\u0026#34; * issuer: C=US; ST=Texas; L=Austin; O=Your Organization; OU=Your Unit; CN=testServerCA * SSL certificate verify ok. * using HTTP/1.1 \u0026gt; GET /hello-world.txt HTTP/1.1 \u0026gt; Host: localhost:8889 \u0026gt; User-Agent: curl/8.4.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 400 Bad Request \u0026lt; Server: nginx/1.25.3 \u0026lt; Date: Sun, 28 Jan 2024 18:28:20 GMT \u0026lt; Content-Type: text/html \u0026lt; Content-Length: 237 \u0026lt; Connection: close \u0026lt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;400 No required SSL certificate was sent\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;400 Bad Request\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;center\u0026gt;No required SSL certificate was sent\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx/1.25.3\u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; * Closing connection The client attempted the TLS handshake, but the server rejected the connection because the client did not provide a certificate. Our built-in curl client does not currently support mTLS using the macOS keychain. The client used for this example is:\ncurl 8.4.0 (x86_64-apple-darwin23.0) libcurl/8.4.0 (SecureTransport) LibreSSL/3.3.6 zlib/1.2.12 nghttp2/1.55.1 Release-Date: 2023-10-11 Protocols: dict file ftp ftps gopher gophers http https imap imaps ldap ldaps mqtt pop3 pop3s rtsp smb smbs smtp smtps telnet tftp Features: alt-svc AsynchDNS GSS-API HSTS HTTP2 HTTPS-proxy IPv6 Kerberos Largefile libz MultiSSL NTLM NTLM_WB SPNEGO SSL threadsafe UnixSockets On the other hand, Safari can access the mTLS server. We can verify this by opening the following URL in Safari:\nhttps://localhost:8889/hello-world.txt We see the following popup:\nSafari mTLS popup\nWe can click Continue to connect to the mTLS server. Future connections will not show the popup and will automatically use the client certificate.\nGoogle Chrome\u0026rsquo;s behavior is similar.\nNote: If we did not add Safari as an application that can access the client key, Safari would ask for a username and password to connect to the system keychain.\nCreating our own mTLS client In the following article, we will create our own mTLS client with the Go programming language. This is the first step toward creating an mTLS client integrated with the macOS keychain.\nLater, we will use mTLS with the Windows certificate store and create an mTLS client integrated with the Windows certificate store.\nFurther reading Recently, we explained agents and daemons and plists on macOS. We also showed how to convert a script into a macOS install package. Example code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/mtls-with-apple-keychain\nmTLS with macOS keychain video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-01-31T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-with-apple-keychain/mtls-safari_hu_939e70d1d9c590e6.png","permalink":"https://victoronsoftware.com/posts/mtls-with-apple-keychain/","title":"Mutual TLS (mTLS) with macOS keychain"},{"content":"What is mTLS (mutual TLS)? TLS stands for Transport Layer Security. It is a cryptographic protocol that provides privacy and data integrity between two communicating applications. It is the successor to SSL (Secure Sockets Layer).\nIn ordinary (non-mutual) TLS, the client authenticates the server, but the server does not authenticate the client. Most websites use regular TLS. The client (web browser) knows it is talking to the correct server (website), but the server knows very little about the client. Instead, web applications use other client authentication methods, such as passwords, cookies, and session tokens.\nMutual TLS (mTLS) is a way to authenticate both the client and the server in a TLS connection. It is also known as client certificate authentication. In addition to the server authenticating itself to the client, the client also authenticates itself to the server.\nmTLS is helpful as an additional layer of security. It is used in many applications, including:\nVPNs Microservices Service mesh IoT (Internet of Things) Mobile apps How does Fleet Device Management use mTLS? Many of Fleet\u0026rsquo;s customers use mTLS as an additional layer of security to authenticate the Fleet server to the Fleet agent. The Fleet agent is a small program that runs on each host device, such as a corporate laptop. It collects information about the host and sends it to the Fleet server.\nHow does mTLS work? TLS is a complex protocol with multiple versions (1.2, 1.3, etc.). We will only go over the basics to understand how mTLS works.\nTLS uses a handshake protocol to establish a secure connection. The handshake protocol is a series of messages between the client and the server.\nThe client sends a \u0026ldquo;Client Hello\u0026rdquo; message to the server. The server responds with a \u0026ldquo;Server Hello\u0026rdquo; message and sends its certificate to the client. As an additional step for mTLS, the server requests a certificate from the client.\nThe client verifies the server\u0026rsquo;s certificate by checking the certificate\u0026rsquo;s signature and verifying that the certificate is valid and has not expired. The client also checks that the server\u0026rsquo;s hostname matches the hostname in the certificate.\nThe client uses the server\u0026rsquo;s public key to encrypt the messages sent to the server, including the session key and its certificate. The server decrypts these messages with its private key.\nThe client also sends a digital signature, encrypted with its private key, to the server. The server verifies the signature by decrypting it with the client\u0026rsquo;s public key.\nAt this point, both the client and the server have verified each other\u0026rsquo;s identity. They complete the TLS handshake and can exchange encrypted messages using a symmetric session key.\nGenerate certificates and keys We will use the OpenSSL command line tool to generate the certificates. OpenSSL is a popular open-source library for TLS and SSL protocols.\nThe following script generates the certificates and keys for the client and the server. It also creates two certificate authorities (CAs) and signs the client and server certificates with their respective CA. The same CA may sign the certificates, but we will use separate CAs for this example.\n#!/bin/bash # This script generates files needed for mTLS. mkdir -p certs # Private keys for CAs openssl genrsa -out certs/server-ca.key 2048 openssl genrsa -out certs/client-ca.key 2048 # Generate CA certificates openssl req -new -x509 -nodes -days 1000 -key certs/server-ca.key -out certs/server-ca.crt openssl req -new -x509 -nodes -days 1000 -key certs/client-ca.key -out certs/client-ca.crt # Generate a certificate signing request openssl req -newkey rsa:2048 -nodes -keyout certs/server.key -out certs/server.req openssl req -newkey rsa:2048 -nodes -keyout certs/client.key -out certs/client.req # Have the CA sign the certificate requests and output the certificates. openssl x509 -req -in certs/server.req -days 1000 -CA certs/server-ca.crt -CAkey certs/server-ca.key -set_serial 01 -out certs/server.crt -extfile localhost.ext openssl x509 -req -in certs/client.req -days 1000 -CA certs/client-ca.crt -CAkey certs/client-ca.key -set_serial 01 -out certs/client.crt # Clean up rm certs/server.req rm certs/client.req The localhost.ext file is used to specify the hostname for the server certificate. In our example, we will use localhost. The file contains the following:\nauthorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment subjectAltName = @alt_names [alt_names] DNS.1 = localhost Run the mTLS server We will use nginx as our mTLS server. nginx is a popular open-source web server.\nUsing docker compose, we can run two nginx servers. One server will use ordinary TLS, and one will use mutual TLS. We will use the following docker-compose.yml file:\n--- version: \u0026#34;2\u0026#34; services: nginx-tls: image: nginx volumes: - ./certs/server.crt:/etc/nginx/certificates/server.crt - ./certs/server.key:/etc/nginx/certificates/server.key - ./nginx-tls/nginx.conf:/etc/nginx/conf.d/default.conf - ./nginx-tls/hello-world.txt:/www/data/hello-world.txt ports: - \u0026#34;8888:8888\u0026#34; nginx-mtls: image: nginx volumes: - ./certs/server.crt:/etc/nginx/certificates/server.crt - ./certs/server.key:/etc/nginx/certificates/server.key - ./certs/client-ca.crt:/etc/nginx/certificates/client-ca.crt - ./nginx-mtls/nginx.conf:/etc/nginx/conf.d/default.conf - ./nginx-mtls/hello-world.txt:/www/data/hello-world.txt ports: - \u0026#34;8889:8889\u0026#34; The nginx-tls service uses the nginx-tls/nginx.conf file, which contains the following:\nserver { listen 8888 ssl; server_name tls-hello-world; # Server TLS certificate (client must have the CA cert to connect) ssl_certificate /etc/nginx/certificates/server.crt; ssl_certificate_key /etc/nginx/certificates/server.key; location / { root /www/data; } } The nginx-mtls service uses the nginx-mtls/nginx.conf file, which contains the following:\nserver { listen 8889 ssl; server_name mtls-hello-world; # Server TLS certificate (client must have the CA cert to connect) ssl_certificate /etc/nginx/certificates/server.crt; ssl_certificate_key /etc/nginx/certificates/server.key; # Enable mTLS ssl_client_certificate /etc/nginx/certificates/client-ca.crt; ssl_verify_client on; location / { root /www/data; } } The hello-world.txt files contain a simple text message.\nConnect to the mTLS server with curl client We can connect to the mTLS server with the curl command line tool. We will use the following command:\ncurl https://localhost:8889/hello-world.txt --cacert ./certs/server-ca.crt --cert ./certs/client.crt --key ./certs/client.key The --cacert option specifies the CA certificate that signed the server certificate. The --cert and --key options select the client certificate and key.\nTo connect to the ordinary TLS server, we do not need to specify the client certificate and key:\ncurl https://localhost:8888/hello-world.txt --cacert ./certs/server-ca.crt Curl can use --insecure to ignore the server certificate:\ncurl --insecure https://localhost:8888/hello-world.txt However, it is impossible to ignore the client certificate for mTLS. The server will reject the connection if the client does not provide a valid certificate.\nExample code on GitHub The example code is available on GitHub at https://github.com/getvictor/mtls/tree/master/hello-world\nSecuring mTLS certificates and keys In the next article, we will secure the mTLS certificates and keys with the macOS keychain.\nIn a later article, we also secure the mTLS certificates and keys with the Windows certificate store.\nThis article is part of a series on mTLS.\nmTLS Hello World video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-01-24T00:00:00Z","image":"https://victoronsoftware.com/posts/mtls-hello-world/mtls-handshake_hu_87e12984511b4ef4.png","permalink":"https://victoronsoftware.com/posts/mtls-hello-world/","title":"Mutual TLS intro and hands-on example"},{"content":"Simple CGO examples CGO is a way to call C code from Go. It helps call existing C libraries or for performance reasons. CGO is enabled by default but can be disabled with the -cgo build flag.\nBelow is a simple example of calling a C function from Go.\npackage main /* double add(double a, double b) { return a + b; } */ import \u0026#34;C\u0026#34; import \u0026#34;fmt\u0026#34; func main() { fmt.Println(C.add(1, 2)) } The C code is embedded in the Go code as a comment above import \u0026quot;C\u0026quot;. The comment must start with /* and end with */. The C code must be valid. The Go compiler compiles the C code and links the resulting object file with the Go code.\nHere is an example of using an existing C library.\npackage main /* #include \u0026#34;math.h\u0026#34; double add(double a, double b) { return a + b; } */ import \u0026#34;C\u0026#34; import \u0026#34;fmt\u0026#34; func main() { fmt.Println(C.floor(C.add(1, 2.1))) } We call the floor function from the math.h library. The math.h library is included with the C compiler, so we don\u0026rsquo;t need to do anything special to use it.\nCGO Hello World fail Here is another example where we print \u0026ldquo;Hello World\u0026rdquo; from C.\npackage main /* #include \u0026#34;stdio.h\u0026#34; */ import \u0026#34;C\u0026#34; func main() { C.printf(C.CString(\u0026#34;Hello World\\n\u0026#34;)) } However, the above seemingly straightforward example will fail to compile with the following enigmatic error:\ncgo: ./exmaple.go:9:2: unexpected type: ... The problem is that printf is a variadic function that can take a variable number of arguments. CGO does not support variadic functions. Even using Go variadic syntax will not work:\nargs := []interface{}{} C.printf(C.CString(\u0026#34;Hello World\\n\u0026#34;), args...) The workaround for this is to use another non-variadic function, such as vprintf, or to wrap the variadic C function in a non-variadic C function.\npackage main /* #include \u0026#34;stdio.h\u0026#34; void wrapPrintf(const char *s) { printf(\u0026#34;%s\u0026#34;, s); } */ import \u0026#34;C\u0026#34; func main() { C.wrapPrintf(C.CString(\u0026#34;Hello, World\\n\u0026#34;)) } C++ Hello World fail Another issue with CGO is only C code can be called from Go. C++ code cannot be called from Go. The following code will fail to compile:\npackage main /* #include \u0026lt;iostream\u0026gt; void helloWorld() { std::cout \u0026lt;\u0026lt; \u0026#34;Hello, World\u0026#34; \u0026lt;\u0026lt; std::endl; } */ import \u0026#34;C\u0026#34; func main() { C.helloWorld() } However, C++ code can be called from C, so we can write a C wrapper for the C++ code.\nCGO real-world example The following is an example of real-world usage of CGO, which uses Apple\u0026rsquo;s APIs to add a secret to the keychain.\npackage keystore /* #cgo LDFLAGS: -framework CoreFoundation -framework Security #include \u0026lt;CoreFoundation/CoreFoundation.h\u0026gt; #include \u0026lt;Security/Security.h\u0026gt; */ import \u0026#34;C\u0026#34; import ( \u0026#34;fmt\u0026#34; \u0026#34;unsafe\u0026#34; ) const service = \u0026#34;com.fleetdm.fleetd.enroll.secret\u0026#34; var serviceStringRef = stringToCFString(service) // AddSecret will add a secret to the keychain. This application can retrieve this // secret without any user authorization. func AddSecret(secret string) error { query := C.CFDictionaryCreateMutable( C.kCFAllocatorDefault, 0, \u0026amp;C.kCFTypeDictionaryKeyCallBacks, \u0026amp;C.kCFTypeDictionaryValueCallBacks, ) defer C.CFRelease(C.CFTypeRef(query)) data := C.CFDataCreate(C.kCFAllocatorDefault, (*C.UInt8)(unsafe.Pointer(C.CString(secret))), C.CFIndex(len(secret))) defer C.CFRelease(C.CFTypeRef(data)) C.CFDictionaryAddValue(query, unsafe.Pointer(C.kSecClass), unsafe.Pointer(C.kSecClassGenericPassword)) C.CFDictionaryAddValue(query, unsafe.Pointer(C.kSecAttrService), unsafe.Pointer(serviceStringRef)) C.CFDictionaryAddValue(query, unsafe.Pointer(C.kSecValueData), unsafe.Pointer(data)) status := C.SecItemAdd(C.CFDictionaryRef(query), nil) if status != C.errSecSuccess { return fmt.Errorf(\u0026#34;failed to add %v to keychain: %v\u0026#34;, service, status) } return nil } // stringToCFString will return a CFStringRef func stringToCFString(s string) C.CFStringRef { bytes := []byte(s) ptr := (*C.UInt8)(\u0026amp;bytes[0]) return C.CFStringCreateWithBytes(C.kCFAllocatorDefault, ptr, C.CFIndex(len(bytes)), C.kCFStringEncodingUTF8, C.false) } The C linker flags are specified with the #cgo LDFLAGS directive.\nThe CGO code uses a lot of casting and data conversion. Let\u0026rsquo;s break down the following segment:\n(*C.UInt8)(unsafe.Pointer(C.CString(secret))) C.CString converts a Go string to a C string. It is one of the CGO special functions to convert between Go and C types. See cgo documentation for more information.\nunsafe.Pointer converts a C pointer to a generic Go pointer. And (*C.UInt8) casts the Go pointer back to a C pointer.\nUnfortunately, CGO cannot cast a C string to a (*C.UInt8) directly. The following will fail to compile:\n(*C.UInt8)(C.CString(secret)) We must go through an intermediate cast to unsafe.Pointer, representing a void C pointer.\nAdditional topics Our custom C and Go code was always in the same file in the above examples. However, the C code can be in a separate file and linked to our Go executable.\nOther getting started guides Recently, we explained how to build a Chrome extension without any additional tools. Also, we wrote a guide to creating a React Hello World app. CGO Hello World fail video ","date":"2024-01-18T00:00:00Z","image":"https://victoronsoftware.com/posts/using-c-and-go-with-cgo-is-tricky/cgo-hello-world-fail_hu_3ce4f49279202857.png","permalink":"https://victoronsoftware.com/posts/using-c-and-go-with-cgo-is-tricky/","title":"Using C and Go with CGO is tricky"},{"content":"What are GitHub Actions? GitHub Actions are a way to automate your software development workflows. They are similar to CI/CD tools like Jenkins, CircleCI, and TravisCI. However, GitHub Actions are built into GitHub.\nGitHub Actions are not entirely free, but they have very high usage limits for open-source projects. For private repositories, you can run up to 2,000 minutes per month for free. After that, you will be charged.\nGitHub Actions for non-CI/CD tasks However, GitHub Actions are not just for CI/CD. You can use them for many general-purpose tasks. For example, you can use them as an extension of your application to perform tasks such as:\ngenerating aggregate reports updating a database sending notifications general data processing and many others A GitHub Action can run arbitrary code, taking inputs from multiple sources such as API calls, databases, and files.\nYou can use a GitHub Action as a worker for your application. For example, you can use it to process data from a database and then send a notification to a user. Or you can use it to generate a report and upload it to a file server.\nAlthough GitHub Actions in open-source repositories are public, they can still use secrets that are not accessible to the public. For example, secrets can be API keys and database access credentials.\nA real-world GitHub Action doing data processing Below is an example GitHub Action that does general data processing. It uses API calls to download data from NVD (National Vulnerability Database), generates files from this data, and then creates a release. Subsequently, the application can download these files and use them directly without making the API calls or processing the data itself.\nGitHub gist: The GitHub Action does a checkout of our application code and runs a script cmd/cve/generate.go to generate the files. Then, it publishes the generated files as a new release. As a final step, it deletes any old releases.\nA note of caution. GitHub monitors for cryptocurrency mining and other abusive behavior. So, keep that in mind and be careful with process-intensive actions.\nUse GitHub Actions for general-purpose tasks video Other articles related to GitHub How to reuse workflows and steps in GitHub Actions What happens in a GitHub pull request after a git merge How to create a custom GitHub Action using TypeScript Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-01-11T00:00:00Z","image":"https://victoronsoftware.com/posts/use-github-actions-for-general-purpose-tasks/GitHub-action_hu_62538675a4025730.png","permalink":"https://victoronsoftware.com/posts/use-github-actions-for-general-purpose-tasks/","title":"Use GitHub Actions for general-purpose tasks"},{"content":"Fuzz testing is a software automated testing technique where random inputs are provided to the software under test. My background is in hardware verification, which uses sophisticated methodologies for pseudorandom testing, so I wanted to see what the Go library had to offer out of the box.\nA Go fuzz test can run as:\na normal unit test a test with fuzzing A fuzz test is written similarly to a normal unit test in a *_test.go file, with the following changes. It must have a Fuzz prefix and use the testing.F struct instead of the usual testing.T struct.\nfunc FuzzSample(f *testing.F) { Here is a workflow for using fuzz testing. First, you create a fuzz test. Then, you run it with fuzzing to automatically find failing corner cases and make any fixes. Thirdly, you include the test and the corner cases in your continuous integration testing suite.\nCreate a fuzz test When creating a fuzz test, you should provide a corpus of initial seed inputs. These are the inputs the test will use before applying randomization. Add the seed corpus with the Add method. For example:\nf.Add(tc.Num, tc.Name) f.Add(uint8(0), \u0026#34;\u0026#34;) The inputs to the Add method indicate which types will be fuzzed, and these types must match the subsequent call to the Fuzz method:\nf.Fuzz(func(t *testing.T, num uint8, name string) { The fuzz test can randomize any number of inputs, as long as they are one of the supported types.\nRun the test with fuzzing To run the test with fuzzing, use the -fuzz switch, like:\ngo test -fuzz FuzzSample The test will continuously run on all your CPUs until it fails, or you kill it:\n=== RUN FuzzSample fuzz: elapsed: 0s, gathering baseline coverage: 0/11 completed fuzz: elapsed: 0s, gathering baseline coverage: 11/11 completed, now fuzzing with 12 workers fuzz: elapsed: 3s, execs: 432199 (144036/sec), new interesting: 0 (total: 11) fuzz: elapsed: 6s, execs: 871147 (146328/sec), new interesting: 0 (total: 11) A sample failure:\nfailure while testing seed corpus entry: FuzzSample/49232526a5eabbdc fuzz: elapsed: 1s, gathering baseline coverage: 10/11 completed --- FAIL: FuzzSample (1.03s) --- FAIL: FuzzSample (0.00s) fuzz_test.go:21: Found 0 The failures are automatically added to the seed corpus. The seed corpus includes the initial inputs that were added with the Add method as well as any new fails. These new seed corpus files are automatically created in the testdata/fuzz/Fuzz* directory. Sample contents of one such file:\ngo test fuzz v1 byte(\u0026#39;\\x01\u0026#39;) string(\u0026#34;0a0000\u0026#34;) Adding the failure to the seed corpus means that the failing case will always run when this test is run again as a unit test or with fuzzing.\nNow, you must fix the failing test and continue the loop of fuzzing and fixing.\nInclude the test in continuous integration When checking in the test to your repository, you must either include the testdata/fuzz/Fuzz* files or convert those files into individual Add method calls in your test. Once the test is checked in, all the inputs in the seed corpus will run as part of the standard Go unit test flow.\nInitial impressions Fuzz testing appears to be a good approach to help the development of small functions with limited scope. The library documentation mentions the following about the function under test:\nThis function should be fast and deterministic, and its behavior should not depend on shared state.\nI plan to give fuzzing a try the next time I develop such a function. I will share the results on this blog.\nConcerns and Issues Native fuzzing support was added to Go in 1.18 and seems like a good initial approach. However, it feels limited in features and usability. The types of functions, fast and deterministic, that fuzzing is intended for are generally not very interesting when testing real applications. They are good examples for students learning how to code. However, more interesting testing scenarios include:\nFunctions accessing remote resources in parallel, such as APIs or databases Functions with asynchronous code Secondly, the fuzzing library does not provide a good way to guide the randomization of inputs and does not give feedback about the input state space already covered. It does provide line coverage information, but that doesn\u0026rsquo;t help for unknown corner cases.\nIf one of my inputs is intended to be a percentage, then I want most of the fuzzing to concentrate on the legal range of 0-100, as opposed to all numbers. This lack of constraints becomes a problem when adding additional inputs to the fuzzing function, as the available state space of inputs expands exponentially. If the state space of inputs is huge, there is no guarantee that fuzzing accomplished its goal of finding all corner cases, leaving the developer with a false sense of confidence in their code.\nLastly, the fuzz test is hard to maintain. The seed corpus is stored in files without any context regarding what corner case each seed is hitting. Software engineers unfamiliar with fuzz testing will find this extremely confusing. If the fuzz test needs to be extended in the future with additional inputs or different types, the old seed corpus will become useless. It will be worse than useless \u0026ndash; the test will not run, and the developer unfamiliar with fuzz testing will not have a clear idea why.\nfuzz_test.go:16: wrong number of values in corpus entry: 2, want 3 That said, understanding the fuzz testing limitation, I’m willing to try fuzz testing for more interesting test cases, such as database accesses. I will report my findings in a future post.\nGitHub gist: Further reading Benchmarking performance with Go Measure Go test execution time Unit testing a Chrome Extension Go fuzz testing video Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2024-01-04T00:00:00Z","image":"https://victoronsoftware.com/posts/fuzz-testing-with-go/fuzz_hu_21d109764cbeab5f.png","permalink":"https://victoronsoftware.com/posts/fuzz-testing-with-go/","title":"Fuzz testing in Go"},{"content":"In the ever-evolving landscape of device management and cybersecurity, understanding the mechanics behind tools like Fleet is not just about technical curiosity; it\u0026rsquo;s about empowering IT professionals to safeguard digital assets more effectively. Fleet gathers telemetry from various devices, from laptops to virtual machines, using osquery. At the heart of this system lies a crucial feature: Fleet policies.\nPolicies in Fleet are more than just rules; they are the gatekeepers of your device\u0026rsquo;s security, ensuring stringent adherence to security standards. By dissecting how Fleet policies operate \u0026ldquo;under the hood,\u0026rdquo; IT administrators and security professionals can gain invaluable insights. These insights allow for setting up efficient security protocols and rapid response to potential vulnerabilities, a necessity in a landscape where cyber threats are constantly evolving. This article delves into the inner workings of Fleet policies, providing you with the knowledge to better configure, manage, and leverage these policies for optimal device security and efficiency.\nPolicy creation Policies can be created from the web UI, the command-line interface called fleetctl with config files, or the REST API. The user creates a policy and selects which devices need to be checked using that policy. Policies can be global or team-specific.\nWhen a policy is created, a record for it is stored in the policies table of the MySQL database. A Fleet deployment consists of several servers behind a load balancer, so storing the record in the DB makes all servers aware of the new policy.\nPolicy execution Policies are executed on the devices, which are called hosts in Fleet, according to the FLEET_OSQUERY_POLICY_UPDATE_INTERVAL, which is set to 1 hour by default. This interval can be adjusted with the environment variable or set from the server’s command line.\nPolicies are simply SQL queries that return a true or false result, so the flow they use on the hosts is the same as other queries. Hosts check in with Fleet servers every 10 seconds (the default) and access the /api/v1/osquery/distributed/read API endpoint. The server checks when the policy was last executed to determine whether it should be executed again. If so, the server adds the policy to its response. For example, this policy in the server response checks if the macOS firewall is enabled:\n{ \u0026#34;queries\u0026#34;: { \u0026#34;fleet_policy_query_9\u0026#34;: \u0026#34;SELECT 1 FROM alf WHERE global_state \u0026gt;= 1;\u0026#34; }, \u0026#34;discovery\u0026#34;: { \u0026#34;fleet_policy_query_9\u0026#34;: \u0026#34;SELECT 1\u0026#34; } } Once the host has executed the policy, it writes the result to the server. The server updates the result in the policy_membership table of the MySQL database. At this point, the Host Details page on the web UI is updated with the policy result.\nForce policy execution on a device The user can force the host to execute all of its policies by clicking the Refetch link:\nPolicy results aggregation However, the main Policies page is not updated. This page shows the counts of all passing and failing hosts for each policy. A worker process on one of the Fleet servers updates it once an hour. The worker calculates the counts and stores them in the policy_stats table in the database. This is done for better performance of the UI. For customers with 100,000s of hosts that asynchronously report their policy results, calculating the passing and failing counts in real time was noticeably slow.\nSummary Understanding the intricacies of Fleet policies is essential for IT professionals managing a fleet of devices. This deep dive into the mechanics of Fleet policies — from creation to execution — provides you with the necessary insights to optimize your cybersecurity strategy effectively. By leveraging these policies, you can ensure stringent security standards across your network, enhancing your organization\u0026rsquo;s digital defense. As the cyber landscape evolves, tools like Fleet remain crucial in maintaining robust and responsive security protocols. We encourage you to apply these insights in your Fleet usage, and as always, we welcome your feedback and experiences in the Fleet community Slack channels.\nUnderstanding the intricacies of Fleet policies video This article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2023-12-30T00:00:00Z","image":"https://victoronsoftware.com/posts/understanding-the-intricacies-of-fleet-policies/understanding-the-intricacies-of-fleet-policies-main-policies-page-1999x978@2x_hu_fe52665b2f097e05.png","permalink":"https://victoronsoftware.com/posts/understanding-the-intricacies-of-fleet-policies/","title":"Understanding the intricacies of Fleet policies"},{"content":" Fleet is an open-source platform for managing and gathering telemetry from devices such as laptops, desktops, VMs, etc. Osquery agents run on these devices and report to the Fleet server. One of Fleet’s features is the ability to query information from the devices in near real-time, called live queries. This article discusses how live queries work “under the hood.”\nWhy a live query? Live queries enable administrators to ask near real-time questions of all online devices, such as checking the encryption status of SSH keys across endpoints, or obtaining the uptime of each server within their purview. This enables them to promptly identify and address any issues, thereby reducing downtime and maintaining operational efficiency. These tasks, which would be time-consuming and complex if done manually, are streamlined through live queries, offering real-time insights into the status and posture of the entire fleet of devices helping IT and security.\nLive queries under the hood Live queries can be run from the web UI, the command-line interface called fleetctl, or the REST API. The user creates a query and selects which devices will run that query. Here is an example using fleetctl to obtain the operating system name and version for all devices:\nfleetctl query --query \u0026#34;select name, version from os_version;\u0026#34; --labels \u0026#34;All Hosts\u0026#34; When a client initiates a live query, the server first creates a Query Campaign record in the MySQL database. A Fleet deployment consists of several servers behind a load balancer, so storing the record in the DB makes all servers aware of the new query campaign.\nQuery campaign As devices called Hosts in Fleet check in with the servers, they receive instructions to run a query. For example:\n{ \u0026#34;queries\u0026#34;: { \u0026#34;fleet_distributed_query_140\u0026#34;: \u0026#34;SELECT name, version FROM os_version;\u0026#34; }, \u0026#34;discovery\u0026#34;: { \u0026#34;fleet_distributed_query_140\u0026#34;: \u0026#34;SELECT 1\u0026#34; } } Then, the osquery agents run the actual query on their host, and write the result back to a Fleet server. As a server receives the result, it publishes it to the common cache using Redis Pub/Sub.\nOnly the one server communicating with the client subscribes to the results. It processes the data from the cache, keeps track of how many hosts reported back, and communicates results back to the client. The web UI and fleetctl interfaces use a WebSockets API, and results are reported as they come in. The REST API, on the other hand, only sends a response after all online hosts have reported their query results.\nDiscover more Fleet’s live query feature represents a powerful tool in the arsenal of IT and security administrators. By harnessing the capabilities of live queries, tasks that once required extensive manual effort can now be executed swiftly and efficiently. This real-time querying ability enhances operational efficiency and significantly bolsters security and compliance measures across a range of devices.\nThe integration of Fleet with Osquery agents, the flexibility offered by interfaces like the web UI, fleetctl, and the REST API, and the efficient data handling through mechanisms like Redis Pub/Sub and WebSockets API all come together to create a robust, real-time telemetry gathering system. This system is designed to keep you informed about the current state of your device fleet, helping you make informed decisions quickly.\nAs you reflect on the capabilities of live queries with Fleet, consider your network environment\u0026rsquo;s unique challenges and needs. What questions could live queries help you answer about your devices? Whether it\u0026rsquo;s security audits, performance monitoring, or compliance checks, live queries offer a dynamic solution to address these concerns.\nWe encourage you to explore the possibilities and share your thoughts or questions. Perhaps you’re facing a specific query challenge or an innovative use case you’ve discovered. Whatever it may be, the world of live queries is vast and ripe for exploration. Join us in Fleet’s Slack forums to engage with a community of like-minded professionals and deepen your understanding of what live queries can achieve in your environment.\nAPI Documentation:\nRun live query with REST API Run live query with WebSockets This article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2023-12-29T00:00:00Z","image":"https://victoronsoftware.com/posts/get-current-telemetry-from-your-devices-with-live-queries/Live%20Query_hu_ff2f4cbef3523a64.png","permalink":"https://victoronsoftware.com/posts/get-current-telemetry-from-your-devices-with-live-queries/","title":"Get current telemetry from your devices with live queries"},{"content":"When starting to code in Go, we encountered the following situation. We needed to create an empty slice, so we did:\nslice := []string{} However, my IDE flagged it as a warning, and pointed me to this Go style guide passage, which recommended using a nil slice instead:\nvar slice []string This recommendation didn\u0026rsquo;t seem right. How can a nil variable be better? Won’t we run into issues like null pointer exceptions and other annoyances? Well, as it turns out, that’s not how slices work in Go. When declaring a nil slice, it is not the dreaded null pointer. It is still a slice. This slice includes a slice header, but its value just happens to be nil.\nThe main difference between a nil slice and an empty slice is the following. A nil slice compared to nil will return true. That’s pretty much it.\nif slice == nil { fmt.Println(\u0026#34;Slice is nil.\u0026#34;) } else { fmt.Println(\u0026#34;Slice is NOT nil.\u0026#34;) } When printing a nil slice, it will print like an empty slice:\nfmt.Printf(\u0026#34;Slice is: %v\\n\u0026#34;, slice) Slice is: [] You can append to a nil slice:\nslice = append(slice, \u0026#34;bozo\u0026#34;) You can loop over a nil slice, and the code will not enter the for loop:\nfor range slice { fmt.Println(\u0026#34;We are in a for loop.\u0026#34;) } The length of a nil slice is 0:\nfmt.Printf(\u0026#34;len: %#v\\n\u0026#34;, len(slice)) len: 0 And, of course, you can pass a nil slice by pointer. That’s right \u0026ndash; pass a nil slice by pointer.\nfunc passByPointer(slice *[]string) { fmt.Printf(\u0026#34;passByPointer len: %#v\\n\u0026#34;, len(*slice)) *slice = append(*slice, \u0026#34;bozo\u0026#34;) } You will get the updated slice if the underlying slice is reassigned.\npassByPointer(\u0026amp;slice) fmt.Printf(\u0026#34;len after passByPointer: %#v\\n\u0026#34;, len(slice)) len after passByPointer: 1 The code above demonstrates that a nil slice is not a nil pointer. On the other hand, you cannot dereference a nil pointer like you can a nil slice. This code causes a crash:\nvar nullSlice *[]string fmt.Printf(\u0026#34;Crash: %#v\\n\u0026#34;, len(*nullSlice)) Here\u0026rsquo;s the full gist:\nFurther reading Recently, we wrote about overriding methods in Go. Watch nil slice vs empty slice video ","date":"2023-12-28T00:00:00Z","image":"https://victoronsoftware.com/posts/nil-slice-versus-empty-slice-in-go/cover_hu_81820af2b5d05211.png","permalink":"https://victoronsoftware.com/posts/nil-slice-versus-empty-slice-in-go/","title":"Nil slice versus empty slice in Go"},{"content":" Matter is a recent open-source standard for connecting devices such as light switches, door locks, motion sensors, and many others. The major goals of the standard are compatibility and interoperability. This means that you will no longer need to be an expert hacker when trying to control devices from multiple manufacturers under a single application. Apple, Amazon, and Google are some of the major members driving the standard. This is great news for the majority of adopters who haven’t yet fully embraced home automation and security.\nThe Matter specification is published by the Connectivity Standards Alliance (CSA) and includes a software development kit. Version 1.0 of the specification was released in October of 2022. In 2023, we saw a slew of new devices and software upgrades compatible with Matter. Version 1.2 of the specification was published in October of 2023. However, this latest specification is still missing support for a few important device categories such as cameras and major appliances. Cameras are a top priority for the CSA, and we may see Matter-compatible cameras in 2024.\nMatter is an important step for the management of IoT devices because it finally brings true interoperability where it has been sorely missing for so many years. No longer will device manufacturers need to decide and budget precious software resources to support Amazon Alexa, Google Home, Apple HomeKit, or another connectivity hub. Customers will no longer be locked into using one of the major home automation providers. And home automation solutions from smaller companies will come onto the market.\nAn important feature of Matter is multi-admin, which means that devices can be read and controlled by multiple clients. In Matter terminology, the device, such as a motion sensor, is called a server or node, and the applications controlling it are called clients. For example, a light switch may be simultaneously controlled by the manufacturer’s app, by Alexa, and by the user\u0026rsquo;s hand-written custom API client.\nMulti-admin support means that a home or business may use one application to control their locks, switches, and security sensors, and another application for reading telemetry from those same devices. Businesses will find it easier to integrate physical security with cyber security. For example, suppose a business’s device management server uses Matter to subscribe to the office door lock. It receives an alert that User A has entered their code. Afterwards, via regular scheduled telemetry, it notices a successful login to Computer B. The business SIEM (security information and event management) system should immediately flag this suspicious sequence of events.\nOf course, the example above can be accomplished today by writing some custom code or using a third party integration. What Matter brings is scalability to such security approaches. The code and integration will no longer need to be redone for each new device and version that comes onto the market.\n","date":"2023-12-20T00:00:00Z","image":"https://victoronsoftware.com/posts/physical-security-meets-cybersecurity-with-matter/cover_hu_a06383bdd079baf5.png","permalink":"https://victoronsoftware.com/posts/physical-security-meets-cybersecurity-with-matter/","title":"Physical security meets cybersecurity with Matter"},{"content":"A prepared statement is a feature of modern databases intended to help execute the same SQL statement multiple times. For example, the following statement is a prepared statement:\nSELECT id, name FROM users WHERE email = ?; The presence of an unspecified parameter, labeled “?”, makes it a prepared statement. When a prepared statement is sent to the database, it is compiled, optimized, and stored in memory on the database server. Subsequently, the client application may execute the same prepared statement multiple times with different parameter values. This results in a speedup.\nPrepared statements are well suited for long and complex queries that require significant compilation and optimization times. They are kept prepared on the DB server, and the application must only pass the parameters to execute them.\nAnother benefit of using prepared statements is the protection they provide against SQL injection. The application does not need to properly escape the parameter values provided to the statement. Because of this protection, many experts recommend always using prepared statements for accessing the database.\nHowever, by always using prepared statements for accessing the database, we force the SQL driver to send the extra prepare command for every ad-hoc statement we execute. The driver sends the following commands:\nPrepare the statement Execute statement with given parameters Close the statement (and deallocate the prepared statement created above) Another issue with prepared statements is the memory requirement. In large application deployments with large numbers of connections, prepared statements can crash your environment. This issue happened to one of our customers.\nA prepared statement is only valid for a single session, which typically maps to a single database connection. If the application runs multiple servers, with many connections, it may end up storing a prepared statement for each one of those sessions.\nFor example, given 100 servers with 100 connections each, we have 10,000 connections to the database. Assuming a memory requirement of 50 KB per prepared statement (derived from the following article), we arrive at the maximum memory requirement of:\n10,000 * 50 KB = 500 MB per single saved prepared statement Some databases also have limits on the number of prepared statements. MySQL’s max_prepared_stmt_count defaults to 16,382 for the entire server. Yes, this is a global limit, and not per session. In the above example, if the application uses prepared statements for every database access, then each database connection will always be using up 1 short-lived prepared statement. A short-lived prepared statement is the prepared statement, as we described above, that will be created for the purposes of executing one statement, and then immediately deallocated afterwards. This means the above application running with a default MySQL config cannot explicitly save any prepared statements \u0026ndash; 10,000 transient prepared statements + 10,000 saved prepared statements is greater than the max_prepared_stmt_count of 16,382.\nThis is extremely inconvenient for application developers, because they must keep track of:\nThe number of saved prepared statements they are using How many application servers are running How many database connections each server has The prepared statement limits of the database This detail can easily be overlooked when scaling applications.\nIn the end, is it really worth using prepared statements, and especially saved prepared statements, in your application? Yes, saved prepared statements can offer performance advantages, especially for complex queries executed frequently. However they must also be kept in check.\nA few ways to mitigate prepared statement issues for large application deployments include:\nLimit the number of database connections per application server Increase the prepared statement limit on the database server(s) Limit the maximum lifespan of connections. When closing a connection, the database will deallocate all prepared statements on that connection. SQL prepared statements are broken when scaling applications video Other articles related to MySQL Optimize MySQL query performance: INSERT with subqueries MySQL deadlock on UPDATE/INSERT upsert pattern Fully supporting Unicode and emojis in your app Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2023-12-14T00:00:00Z","image":"https://victoronsoftware.com/posts/sql-prepared-statements-are-broken-when-scaling-applications/cover_hu_8c3275370153d6d9.png","permalink":"https://victoronsoftware.com/posts/sql-prepared-statements-are-broken-when-scaling-applications/","title":"SQL prepared statements are broken when scaling applications"},{"content":" At Fleet, our developer documentation is spread out throughout the codebase, contained in a multitude of README and Markdown files. Much of the documentation is hosted on our webpage, but not all of it.\nAs developers, we need to be able to quickly search project documentation to find answers to specific questions, such as:\nHow to do a database migration How to run integration tests How to deploy a development version of to a specific OS One solution is to use grep or the IDE environment to search for these answers. Unfortunately, such search methods are not optimized for text search \u0026ndash; they frequently generate no relevant results or too many results that we must manually wade through to find the most appropriate. Specialized documentation search tools, on the other hand, prioritize headings and whole words, search for plural versions of the search terms, and offer other conveniences.\nThe lack of good search capability for engineering docs must be solved in order to scale engineering efforts. It is an issue because of the following side effects:\nEngineers are discouraged from writing documentation Documentation may be duplicated Senior developers are frequently interrupted when people can’t find relevant documentation One solution is to use a documentation service, such as a team wiki, Confluence, or GitBook. GitBook integrates with git repositories, and can push documentation changes. GitBook is free for personal use, which makes it easy to use for open source projects such as fleet and osquery. That said, GitBook is a newcomer to the space, and is still reaching maturity.\nTo set up a personal GitBook, make a fork of the open source projects that contain documentation you’d like to search, and integrate them into GitBook spaces. After indexing is complete, you’ll be able to effectively search the documentation.\nTo keep the forks in sync with the parent repositories, we use Github Actions. Github Actions are free for open source projects. Searching GitHub for sync-fork returned several examples. We ended up using the following:\nname: Sync Fork on: schedule: - cron: \u0026#39;55 * * * *\u0026#39; workflow_dispatch: # on button click jobs: sync: runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v4 with: token: ${{ secrets.WORKFLOW_TOKEN }} fetch-depth: 0 - name: Configure Git run: | git config --global user.name \u0026#34;GitHub Actions Bot\u0026#34; git config --global user.email \u0026#34;actions@github.com\u0026#34; - name: Merge upstream run: | git remote add upstream https://github.com/fleetdm/fleet.git git fetch upstream main git checkout main git merge upstream/main git push origin main The WORKFLOW_TOKEN above is a GitHub personal access token (PAT) that allows reading and writing workflows in this repository. This token is not needed for repositories without workflows.\nIn addition to project documentation, GitBook can be used to synchronize personal documentation that’s being held in a private repository. There are several git-based notebook applications on the market. In addition, Markdown notes from the popular note-taking app Obsidian can be kept in GitHub. This turns GitBook into a true personalized developer documentation database \u0026ndash; one place to search through developer docs as well as your own private notes.\n","date":"2023-11-30T00:00:00Z","image":"https://victoronsoftware.com/posts/you-need-a-personal-dev-docs-db-gitbook/cover_hu_891ec5936e84789e.png","permalink":"https://victoronsoftware.com/posts/you-need-a-personal-dev-docs-db-gitbook/","title":"You need a personal dev docs DB (GitBook)"},{"content":"Traditionally, network routers used dedicated bare metal machines. However, in the last several years, we’ve seen a rise in software-based routers that can be deployed either on bare metal, on a VM, or even on a container. This means these virtual routers can be used to replace existing router software on an older router. They can run in the cloud. Or they can be installed on do-it-yourself (DIY) hardware. A couple popular open source software-based routers are pfSense and OPNsense.\nWhy use a virtual router? For one, these routers offer enterprise-level features such as build-in VPN support, traffic analysis, and extensive diagnostics, among others. Another reason is that having a virtual router gives you the ability to experiment \u0026ndash; you can install multiple routers on top of your hypervisor, and try all of them out. A third reason is that the virtual router may be only one of many VMs that you run on your hardware. You can use the same piece of hardware to run a router, an ad-blocking service, a media server, and other applications.\nAdvanced virtual router installation and set up When setting up our virtual router, we chose to use PCI Passthrough to allow the virtual router direct access to the NIC hardware. Direct access to hardware improves the latency of our internet traffic. In addition, we wanted our hypervisor to sit behind the router, and not be exposed to the public. This reduces the attack surface for potential bad agents. However, routing hypervisor traffic through the router made our setup a bit tricker. It is like the chicken or the egg dilemma \u0026ndash; how do you put your hypervisor behind the router when the hypervisor is responsible for managing the router? Below is the approach we used when installing pfSense on top of Proxmox Virtual Environment (PVE).\nFor the initial installation, we did not use PCI Passthrough and instead used a virtual network bridge (vmbr0). We configured the router VM to start on boot.\nInitial virtual router configuration This allowed us to continue controlling the virtual router through the PVE web GUI. We set up the router and enabled access to it through the serial interface, which we used in the next step. Then, we put the system into its final configuration.\nFinal virtual router configuration In order to finish configuring, we had to plug in a monitor and keyboard into our hardware. We accessed the virtual router via the serial interface from the PVE command line:\nqm terminal 100 We updated the WAN interface to use eth0. At this point, the LAN interface eth1 had access to the internet.\nIn addition, we added a second LAN interface for the network bridge (vmbr0). We made sure firewall configurations for both LAN interfaces were the same.\nNext, from the PVE command line, we updated the PVE IP and gateway to point at the router by modifying the following files.\n/etc/network/interfaces /etc/hosts After rebooting PVE, we had access to the internet and to the PVE Web GUI from our new LAN.\nUpdating router software Using a virtual router with PCI Passthrough creates a unique challenge when doing software updates. What if the new version doesn’t work? What if you lose all internet access.\nWe can mitigate potential issues. First, we recommend always making a backup of the router VM when upgrading. That way we can easily roll back the change. Switching to a backup, however, requires keyboard and monitor access to your hardware, since it must be done via the PVE command line.\nAnother way to safely upgrade is to spin up a second VM running updated router software. The second VM can be either from a backup or brand new. This VM should use virtual network bridges for its connections. Once it is properly configured, we can stop the first router VM and switch the port connections to the second VM. This flow also requires accessing the router via the serial interface to update the WAN/LAN interfaces.\nFurther reading Recently, we have been setting up VLANs on our home network.\nSetting up a virtual router video ","date":"2023-11-22T00:00:00Z","image":"https://victoronsoftware.com/posts/setting-up-a-virtual-router/cover_hu_9b69505134e3fdca.jpeg","permalink":"https://victoronsoftware.com/posts/setting-up-a-virtual-router/","title":"Setting up a virtual router (pfSense on Proxmox)"},{"content":"Keychains are the macOS’s method to track and protect secure information such as passwords, private keys, and certificates. Traditionally, the keychain information was stored in files, such as:\n/Library/Keychains/System.keychain /Library/Keychains/apsd.keychain /System/Library/Keychains/SystemRootCertificates.keychain /Users/\u0026lt;username\u0026gt;/Library/Keychains/login.keychain-db In the last several years, Apple also introduced data protection keychains, such as the iCloud Keychain. Although the file-based keychains above are on the road to deprecation in favor of data protection keychains, current macOS systems still heavily rely on them. It is unclear when, if ever, these keychains will be replaced by data protection keychains.\nInspecting file-based keychains has gotten more difficult as Apple deprecated many of the APIs associated with them, such as SecKeychainOpen. In addition, excessive use of these deprecated APIs may result in corruption of the Login Keychain, as mentioned in this osquery issue. By NOT using the deprecated APIs, the user only has access to the following keychains from the above list:\n/Library/Keychains/System.keychain /Users/\u0026lt;username\u0026gt;/Library/Keychains/login.keychain-db Root certificates are missing. And the APSD (Apple Push Service Daemon) keychain is missing, which is used for device management, among other things.\nSo, how can app developers and IT professionals continue to have access to ALL of these keychain files?\nOne way is to continue using deprecated APIs until they stop working. We recommend making a secure copy of the keychain files before accessing them with the APIs.\nAnother option is to use the macOS security command line tool. For example, to list root certificates, do the following:\nsudo security find-certificate -a /System/Library/Keychains/SystemRootCertificates.keychain A third, and hardest, option is to parse the keychain files yourself. Some details on the keychain format are available. Please leave a comment if you or someone else has created a tool to parse Apple keychains.\nThe fourth option is to use an existing tool, such as osquery. Osquery is an open-source tool built for security and IT professionals. Osquery developers are working on fixing any issues to continue providing access to macOS keychain files via the following tables:\ncertificates keychain_acls keychain_items Watch how to inspect macOS keychain files Note: If you want to comment on this article, please do so on the YouTube video.\n","date":"2023-11-16T00:00:00Z","permalink":"https://victoronsoftware.com/posts/inspecting-keychain-files-on-macos/","title":"Inspecting keychain files on macOS"},{"content":"Authorization is giving permission to a user to do an action on the server. As developers, we must ensure that users are only allowed to do what they are authorized.\nOne way to ensure that authorization has happened is to loudly flag when it hasn\u0026rsquo;t. This is how we do it at Fleet Device Management.\nIn our code base, we use the go-kit library. Most of the general endpoints are created in the handler.go file. For example:\n// user-authenticated endpoints ue := newUserAuthenticatedEndpointer(svc, opts, r, apiVersions...) ue.POST(\u0026#34;/api/_version_/fleet/trigger\u0026#34;, triggerEndpoint, triggerRequest{}) Every endpoint calls kithttp.NewServer and wraps the endpoint with our AuthzCheck. From handler.go:\ne = authzcheck.NewMiddleware().AuthzCheck()(e) return kithttp.NewServer(e, decodeFn, encodeResponse, opts...) This means that after the business logic is processed, the AuthzCheck is called. This check ensures that authorization was checked. Otherwise, an error is returned. From authzcheck.go:\n// If authorization was not checked, return a response that will // marshal to a generic error and log that the check was missed. if !authzctx.Checked() { // Getting to here means there is an authorization-related bug in our code. return nil, authz.CheckMissingWithResponse(response) } This additional check is useful during our development and QA process, to ensure that authorization always happens in our business logic.\nFurther reading Recently, we improved our app\u0026rsquo;s security by reading program arguments from STDIN.\nWatch how we catch missed authorization checks Note: If you want to comment on this article, please do so on the YouTube video.\nThis article originally appeared in Fleet\u0026rsquo;s blog.\n","date":"2023-11-10T00:00:00Z","permalink":"https://victoronsoftware.com/posts/catch-missed-authorization-checks-during-software-development/","title":"Catch missed authorization checks during software development"}]